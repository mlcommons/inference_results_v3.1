benchmarks:
  gptj-99:
    "rouge1": 42.556635
    "rouge2": 19.922265
    "rougeL": 29.688219
    "gen_len": 3615191
  gptj-99.9:
    "rouge1": 42.9435135
    "rouge2": 20.1033765
    "rougeL": 29.9581119
    "gen_len": 3615191
  gptj:
    "rouge1": 42.9865
    "rouge2": 20.1235
    "rougeL": 29.9881
    "gen_len": 3615191
scenarios:
  gptj-99-fp8:
    dataset: cnn_dailymail
    code_dir: gpt-j
    benchmark: gptj-99
    command: PT_USE_FP8_143=1 ENABLE_CALC_DYNAMIC_RANGE=false PROPAGATE_FP8_CASTS=true UPDATE_MME_OUTPUT_PRECISION_FILTER="v_proj,matmul_av" USE_DEFAULT_QUANT_PARAM=true UPDATE_GRAPH_OUTPUT_MME=false ENABLE_EXPERIMENTAL_FLAGS=true python main.py -qf quantization/configuration/examples/quant_on.json --device socket --num_workers 8 --user_conf configs/fp8-99.9.conf --dtype float8
    precision: fp8
    batch_size: 32
  gptj-99.9-fp8:
    dataset: cnn_dailymail
    code_dir: gpt-j
    benchmark: gptj-99.9
    command: PT_USE_FP8_143=1 ENABLE_CALC_DYNAMIC_RANGE=false PROPAGATE_FP8_CASTS=true UPDATE_MME_OUTPUT_PRECISION_FILTER="v_proj,matmul_av" USE_DEFAULT_QUANT_PARAM=true UPDATE_GRAPH_OUTPUT_MME=false ENABLE_EXPERIMENTAL_FLAGS=true python main.py -qf quantization/configuration/examples/quant_on.json --device socket --num_workers 8 --user_conf configs/fp8-99.9.conf --dtype float8
    precision: fp8
    batch_size: 32
