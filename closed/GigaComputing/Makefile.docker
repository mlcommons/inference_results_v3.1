# Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#	 http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This Makefile includes all the variables and targets used to set up NVIDIA's MLPerf Inference docker environment


include $(CURDIR)/Makefile.const


VERSION := $(shell cat $(PROJECT_ROOT)/VERSION)
USE_NIGHTLY ?= 0
USE_NGC ?= 1
PARTNER_DROP ?= 0
PARTNER_RELEASE := 4
IS_L4 ?= 0
UBUNTU_VERSION := 20.04
ifeq ($(SM), 89)
    IS_L4 := 1
endif

# Python ver sometimes used in package like python wheel
# No NGC support for aarch64 yet, disable NGC workflow, TODO: remove after NGC for aarch64 is enabled
PYTHON_VER=cp38
ifeq ($(TARGET_AARCH64)$(IS_SOC), 10)
    PYTHON_VER=cp310
    UBUNTU_VERSION := 22.04
endif

# Docker is supported for all non-xavier x86/aarch64 system.
SUPPORT_DOCKER := 0
ifeq ($(ARCH), $(filter $(ARCH), x86_64 aarch64))
ifneq ($(IS_SOC), 1)
    SUPPORT_DOCKER = 1
else
    ifeq ($(SOC_SM), 87)
        # Docker is supported for Orin.
        SUPPORT_DOCKER = 1
    endif
endif
endif

# Conditional Docker flags
ifndef DOCKER_DETACH
    DOCKER_DETACH := 0
endif
ifndef DOCKER_TAG
    DOCKER_TAG := $(UNAME)-$(ARCH)
endif

RANDOM := $(shell bash -c 'echo $$RANDOM')
DOCKER_NAME := mlperf-inference-$(DOCKER_TAG)-$(RANDOM)-sub

DOCKER_BUILDKIT ?= 1
HOST_VOL ?= ${PWD}
CONTAINER_VOL ?= /work
NO_DOCKER_PULL ?= 0
NO_BUILD ?= 0


# The below paths are for internal use only.
# If any extra mounting path is needed, please use DOCKER_ARGS environment variables
ifneq ($(wildcard /home/scratch.mlperf_inference),)
    DOCKER_MOUNTS += -v /home/scratch.mlperf_inference:/home/scratch.mlperf_inference
endif
ifneq ($(wildcard /home/mlperf_inf_dlrmv2),)
    DOCKER_MOUNTS += -v /home/mlperf_inf_dlrmv2:/home/mlperf_inf_dlrmv2
endif
ifneq ($(wildcard /home/scratch.svc_compute_arch),)
    DOCKER_MOUNTS += -v /home/scratch.svc_compute_arch:/home/scratch.svc_compute_arch
endif
ifneq ($(wildcard /home/scratch.computelab/sudo),)
    DOCKER_MOUNTS += -v /home/scratch.computelab/sudo:/home/scratch.computelab/sudo
endif
ifneq ($(wildcard /home/scratch.dlsim),)
    DOCKER_MOUNTS += -v /home/scratch.dlsim:/home/scratch.dlsim
endif
ifneq ($(wildcard /raid/data),)
    DOCKER_MOUNTS += -v /raid/data:/raid/data
endif
ifneq ($(wildcard $(PROJECT_ROOT)/../../regression),)
    DOCKER_MOUNTS += -v $(PROJECT_ROOT)/../../regression:/regression
endif
ifdef MLPERF_SCRATCH_PATH
    ifneq ($(wildcard $(MLPERF_SCRATCH_PATH)),)
        DOCKER_MOUNTS += -v $(MLPERF_SCRATCH_PATH):$(MLPERF_SCRATCH_PATH)
    else
        $(error Path set in MLPERF_SCRATCH_PATH does not exist!)
    endif
endif

# Handle different nvidia-docker version. Do not use nvidia-docker when running with CPUs
ifeq ($(USE_CPU), 1)
    DOCKER_RUN_CMD := docker run
else ifeq ($(USE_INFERENTIA), 1)
    DOCKER_RUN_CMD := docker run
else ifneq ($(wildcard /usr/bin/nvidia-docker),)
    DOCKER_RUN_CMD := nvidia-docker run
    # Set Environment variables to fix docker client and server version mismatch
    # Related issue: https://github.com/kubernetes-sigs/kubespray/issues/6160
    export DOCKER_API_VERSION=1.40
else
    DOCKER_RUN_CMD := docker run --gpus=all --runtime=nvidia
endif

# If specific DOCKER_COMMAND is not passed, launch interactive docker container session.
ifeq ($(DOCKER_COMMAND),)
    DOCKER_INTERACTIVE_FLAGS = -it
else
    DOCKER_INTERACTIVE_FLAGS =
endif

# Determine docker base image
ifneq ($(IS_SOC), 1)
    DOCKER_IMAGE_NAME := base-cuda$(CUDA_VER)-$(ARCH)-ubuntu$(UBUNTU_VERSION)
    ifeq ($(IS_L4), 1)
        DOCKER_IMAGE_NGC_INTERNAL_NAME := ngc-internal-cuda$(CUDA_VER)-$(ARCH)-ubuntu$(UBUNTU_VERSION)-l4
        DOCKER_IMAGE_NGC_PUBLIC_NAME := mlpinf-$(VERSION)-cuda$(CUDA_VER)-cudnn8.9-$(ARCH)-ubuntu$(UBUNTU_VERSION)-l4-public
        DOCKER_IMAGE_PARTNER_NAME := mlpinf-$(VERSION).$(PARTNER_RELEASE)-cuda$(CUDA_VER)-cudnn8.9-$(ARCH)-ubuntu$(UBUNTU_VERSION)-l4-partner
    else
        DOCKER_IMAGE_NGC_INTERNAL_NAME := ngc-internal-cuda$(CUDA_VER)-$(ARCH)-ubuntu$(UBUNTU_VERSION)
        DOCKER_IMAGE_NGC_PUBLIC_NAME := mlpinf-$(VERSION)-cuda$(CUDA_VER)-cudnn8.9-$(ARCH)-ubuntu$(UBUNTU_VERSION)-public
        DOCKER_IMAGE_PARTNER_NAME := mlpinf-$(VERSION).$(PARTNER_RELEASE)-cuda$(CUDA_VER)-cudnn8.9-$(ARCH)-ubuntu$(UBUNTU_VERSION)-partner
    endif
    ifeq ($(USE_NGC), 1)
        ifeq ($(EXTERNAL_USER), 0)
            BASE_IMAGE ?= gitlab-master.nvidia.com/mlpinf/mlperf-inference:$(DOCKER_IMAGE_NGC_INTERNAL_NAME)
        else
            ifeq ($(PARTNER_DROP), 1)
                BASE_IMAGE ?= nvcr.io/yrleydyexu3y/mlpinf-partner-v31/mlperf-inference-partner-v31:$(DOCKER_IMAGE_PARTNER_NAME)
            else
                BASE_IMAGE ?= nvcr.io/nvidia/mlperf/mlperf-inference:$(DOCKER_IMAGE_NGC_PUBLIC_NAME)
            endif
        endif
    else
        # Check if we are on intranet
        ifeq ($(EXTERNAL_USER), 0)
            BASE_IMAGE ?= gitlab-master.nvidia.com/mlpinf/mlperf-inference:$(DOCKER_IMAGE_NAME)
            ifeq ($(TARGET_AARCH64), 1)
                CUDA_LONG_VER := 12.2.0
                DISTRO := ubuntu2204
                SHA256 := 6950bd04caba9bf0fa75f8242ece9d34550685bfaa4b42d9ac341acbac7d6608
                BASE_IMAGE := nvcr.io/nvidia/cuda:$(CUDA_LONG_VER)-devel-ubuntu$(UBUNTU_VERSION)@sha256:$(SHA256)
            endif
        else
            ifeq ($(CUDA_VER), 12.2)
                ifeq ($(TARGET_X86_64), 1)
                    BASE_IMAGE ?= nvidia/cuda:12.2.0-devel-ubuntu$(UBUNTU_VERSION)@sha256:cd59e2522b130d72772cc7db4d63f3eedbc981138f825694655bb66e4c7dd2e3
                else ifeq ($(TARGET_AARCH64), 1)
                    BASE_IMAGE ?= nvidia/cuda:12.2.0-devel-ubuntu$(UBUNTU_VERSION)@sha256:6950bd04caba9bf0fa75f8242ece9d34550685bfaa4b42d9ac341acbac7d6608
                else
                    $(error MLPerf Inference only supports x86 and aarch64 system now.)
                endif
            else
                $(error MLPerf Inference v3.1 code requires cuda version 12.2 on Non-SOC systems)
            endif
        endif
    endif
else
    CUDA_VER := 11.4
    DOCKER_IMAGE_NAME := base-cuda$(CUDA_VER)-$(ARCH)-orin
    DOCKER_IMAGE_NGC_INTERNAL_NAME := ngc-internal-cuda$(CUDA_VER)-$(ARCH)-orin
    DOCKER_IMAGE_NGC_PUBLIC_NAME := mlpinf-$(VERSION)-cuda$(CUDA_VER)-cudnn8.6-$(ARCH)-orin-public
    DOCKER_IMAGE_PARTNER_NAME := mlpinf-$(VERSION).$(PARTNER_RELEASE)-cuda$(CUDA_VER)-cudnn8.6-$(ARCH)-orin-partner
    ifeq ($(SOC_SM), 87) # orin check
        ifeq ($(USE_NGC), 1)
            ifeq ($(EXTERNAL_USER), 0)
                BASE_IMAGE ?= gitlab-master.nvidia.com/mlpinf/mlperf-inference:$(DOCKER_IMAGE_NGC_INTERNAL_NAME)
            else
                ifeq ($(PARTNER_DROP), 1)
                    BASE_IMAGE ?= nvcr.io/yrleydyexu3y/mlpinf-partner-v31/mlperf-inference-partner-v31:$(DOCKER_IMAGE_PARTNER_NAME)
                else
                    BASE_IMAGE ?= nvcr.io/nvidia/mlperf/mlperf-inference:$(DOCKER_IMAGE_NGC_PUBLIC_NAME)
                endif
            endif
        else
            # Check if we are on intranet
            ifeq ($(EXTERNAL_USER), 0)
                BASE_IMAGE ?= gitlab-master.nvidia.com/mlpinf/mlperf-inference:$(DOCKER_IMAGE_NAME)
            else
                BASE_IMAGE ?= nvcr.io/nvidia/l4t-base:35.3.1
            endif
        endif
    else
        $(error MLPerf Inference SOC only supports Orin system now.)
    endif
endif # soc check

DOCKER_FILENAME := Dockerfile.$(ARCH)
ifeq ($(IS_SOC), 1)
    ifeq ($(SOC_SM), 87)
        DOCKER_FILENAME = Dockerfile.orin
    endif
endif

# Check if the user is currently under NVIDIA intranet. External users should see 1.
.PHONY: check_intranet
check_intranet:
	@echo "EXTERNAL_USER = $(EXTERNAL_USER)"

# Small helper to check if nvidia-docker is installed correctly.
.PHONY: docker_sanity
docker_sanity:
	docker pull nvcr.io/nvidia/cuda:11.0.3-runtime-ubuntu18.04
	$(DOCKER_RUN_CMD) --rm \
		-e NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES} \
		nvcr.io/nvidia/cuda:11.0.3-runtime-ubuntu18.04 nvidia-smi
	@echo "Nvidia-docker is installed correctly!"

# Download pre release wheels before Mitten custom build are publicly available
.PHONY: pull_prerelease_components
pull_prerelease_components:
	@cp $(MLPERF_SCRATCH_PATH)/dependencies/python_wheels/mitten-dev-mlpinf-2095/nvmitten-$(MITTEN_VER)-$(PYTHON_VER)-$(PYTHON_VER)-linux_$(ARCH).whl docker/
	@cp $(MLPERF_SCRATCH_PATH)/dependencies/mitten docker/ -r
ifeq ($(IS_SOC), 0)
# Datacenter fp8
	@cp $(MLPERF_SCRATCH_PATH)/models/bert/faster-transformer-bert-fp8-weights-scales.tar.gz docker/
	@cp /home/mlperf_inference_data/partner_drop/v3.1/TRTLLM.tar.gz docker/
ifeq ($(TARGET_X86_64), 1)
	@cp $(MLPERF_SCRATCH_PATH)/models/GPTJ-6B/fp8-quantized-ammo/*.pth docker/
    # cuBLAS23.07 release
	@cp $(MLPERF_SCRATCH_PATH)/dependencies/cublas/libcublas-12.2.5.1-0.tar.bz2 docker/
endif
endif


# Build the docker image and launch an interactive container.
# For CPU builds, first build the backend libraries and copy them into the working directory
# TODO: Change DALI to public RC version later
.PHONY: prebuild
prebuild:
ifeq ($(USE_NGC), 0)
	@$(MAKE) -f Makefile.docker pull_prerelease_components
endif
	@$(MAKE) -f Makefile.build build_triton_backends
	@$(MAKE) -f Makefile.docker build_docker NO_BUILD?=1
ifneq ($(strip ${DOCKER_DETACH}), 1)
	@$(MAKE) -f Makefile.docker attach_docker || true
endif


# Build the docker image for x86 and aarch64 non-xavier systems.
.PHONY: build_docker
build_docker:
ifeq ($(SUPPORT_DOCKER), 1)
	@echo "Building Docker image"
ifeq ($(NO_DOCKER_PULL), 0)
ifneq ($(USE_CPU), 1)
	docker pull $(BASE_IMAGE)
endif
endif
	DOCKER_BUILDKIT=$(DOCKER_BUILDKIT) docker build -t mlperf-inference:$(DOCKER_TAG)-latest \
		--build-arg BASE_IMAGE=$(BASE_IMAGE) \
		--build-arg CUDA_VER=$(CUDA_VER) \
		--build-arg DRIVER_VER_MAJOR=$(DRIVER_VER_MAJOR) \
		--build-arg USE_CPU=$(USE_CPU) \
		--build-arg USE_NIGHTLY=$(USE_NIGHTLY) \
		--build-arg USE_NGC=$(USE_NGC) \
		--build-arg IS_HOPPER=$(IS_HOPPER) \
		--build-arg EXTERNAL_USER=$(EXTERNAL_USER) \
		--build-arg MITTEN_VER=$(MITTEN_VER) \
		--network host \
		-f docker/$(DOCKER_FILENAME) docker
ifeq ($(NO_BUILD), 0)
	DOCKER_BUILDKIT=$(DOCKER_BUILDKIT) docker build -t mlperf-inference:$(DOCKER_TAG)-latest --no-cache --network host \
		--build-arg BASE_IMAGE=mlperf-inference:$(DOCKER_TAG)-latest \
		-f docker/Dockerfile.build .
endif # NO_BUILD
endif # OS/xavier check


# Add current user into docker image.
.PHONY: docker_add_user
docker_add_user:
ifeq ($(SUPPORT_DOCKER), 1)
	@echo "Adding user account into image"
	DOCKER_BUILDKIT=$(DOCKER_BUILDKIT) docker build -t mlperf-inference:$(DOCKER_TAG) --network host \
		--build-arg BASE_IMAGE=mlperf-inference:$(DOCKER_TAG)-latest \
		--build-arg GID=$(GROUPID) --build-arg UID=$(UID) --build-arg GROUP=$(GROUPNAME) --build-arg USER=$(UNAME) \
		- < docker/Dockerfile.user
endif


# Add user and launch an interactive container session.
.PHONY: attach_docker
attach_docker:
	@$(MAKE) -f Makefile.docker docker_add_user
ifneq ($(USE_INFERENTIA), 1)
	@$(MAKE) -f Makefile.docker launch_docker
endif
ifeq ($(USE_INFERENTIA), 1)
	@$(MAKE) -f Makefile.docker launch_inferentia_docker
endif

# Launch a container session for inferentia.
.PHONY: launch_inferentia_docker
launch_inferentia_docker:
	@echo "Launching inferentia docker"
	docker run --rm $(DOCKER_INTERACTIVE_FLAGS) -w /work \
		-v $(HOST_VOL):$(CONTAINER_VOL) -v ${HOME}:/${HOME}:rw \
		$(shell scripts/get_inferentia_device_list.sh) \
		--cap-add SYS_ADMIN --cap-add SYS_TIME \
		-e "AWS_NEURON_VISIBLE_DEVICES=ALL" \
		-e "USE_INFERENTIA=1" \
		--shm-size=32gb \
		-v /lib/udev:/mylib/udev \
		-v /etc/timezone:/etc/timezone:ro -v /etc/localtime:/etc/localtime:ro \
		--security-opt apparmor=unconfined --security-opt seccomp=unconfined \
		--name $(DOCKER_NAME) -h $(subst _,-,$(shell echo $(DOCKER_NAME) | cut -c -64)) --add-host $(DOCKER_NAME):127.0.0.1 \
		--cpuset-cpus $(shell taskset -c -p $$$$ | awk '{print $$NF}') \
		--user $(UID):$(GROUPID) --net host --device /dev/fuse \
		--ulimit memlock=-1 \
		$(DOCKER_ARGS) \
		-e MLPERF_SCRATCH_PATH=/home/ubuntu/mlperf_scratch \
		-e HOST_HOSTNAME=$(HOSTNAME) \
		mlperf-inference:$(DOCKER_TAG) $(DOCKER_COMMAND)


# Launch a container session.
.PHONY: launch_docker
launch_docker:
ifeq ($(SUPPORT_DOCKER), 1)
	$(DOCKER_RUN_CMD) --rm $(DOCKER_INTERACTIVE_FLAGS) -w /work \
		-v $(HOST_VOL):$(CONTAINER_VOL) -v ${HOME}:/mnt/${HOME} \
		--cap-add SYS_ADMIN --cap-add SYS_TIME \
		-e NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES} \
		-e HISTFILE=/mnt/${HOME}/.mlperf_bash_history \
		--shm-size=32gb \
		--ulimit memlock=-1 \
		-v /etc/timezone:/etc/timezone:ro -v /etc/localtime:/etc/localtime:ro \
		--security-opt apparmor=unconfined --security-opt seccomp=unconfined \
		--name $(DOCKER_NAME) -h $(subst _,-,$(shell echo $(DOCKER_NAME) | cut -c -64)) --add-host $(DOCKER_NAME):127.0.0.1 \
		--cpuset-cpus $(shell taskset -c -p $$$$ | awk '{print $$NF}') \
		--user $(UID) --net host --device /dev/fuse \
		$(DOCKER_MOUNTS) $(DOCKER_ARGS) \
		-e MLPERF_SCRATCH_PATH=$(MLPERF_SCRATCH_PATH) \
		-e HOST_HOSTNAME=$(HOSTNAME) \
		-e SUBMITTER="GigaComputing" \
		-v /data/dlrmv2/files:/home/mlperf_inf_dlrmv2 \
		$(shell if [ $(MIG_CONF) == "ALL" ]; then echo "--gpus all -e NVIDIA_MIG_CONFIG_DEVICES=all"; elif [ $(MIG_CONF) != "OFF" ]; then echo "--gpus '\"device=`bash scripts/mig_get_uuid.sh`\"'"; fi) \
		mlperf-inference:$(DOCKER_TAG) $(DOCKER_COMMAND)
endif
