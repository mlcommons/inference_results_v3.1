diff --git a/src/transformers/models/gptj/modeling_gptj.py b/src/transformers/models/gptj/modeling_gptj.py
index 3a1f99dd7..9d9866353 100644
--- a/src/transformers/models/gptj/modeling_gptj.py
+++ b/src/transformers/models/gptj/modeling_gptj.py
@@ -20,6 +20,7 @@ from typing import Optional, Tuple, Union
 import torch
 import torch.fx
 import torch.utils.checkpoint
+import intel_extension_for_pytorch as ipex
 from torch import nn
 from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
 
@@ -106,10 +107,10 @@ class GPTJAttention(nn.Module):
             )
         self.scale_attn = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32)).to(torch.get_default_dtype())
 
-        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
-        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
-        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
-        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
+        self.k_proj = ipex.nn.modules.INT4Linear(self.embed_dim, self.embed_dim, bias=False)
+        self.v_proj = ipex.nn.modules.INT4Linear(self.embed_dim, self.embed_dim, bias=False)
+        self.q_proj = ipex.nn.modules.INT4Linear(self.embed_dim, self.embed_dim, bias=False)
+        self.out_proj = ipex.nn.modules.INT4Linear(self.embed_dim, self.embed_dim, bias=False)
         self.rotary_dim = config.rotary_dim
         pos_embd_dim = self.rotary_dim or self.embed_dim
         self.embed_positions = create_sinusoidal_positions(max_positions, pos_embd_dim)
@@ -272,8 +273,8 @@ class GPTJMLP(nn.Module):
         super().__init__()
         embed_dim = config.n_embd
 
-        self.fc_in = nn.Linear(embed_dim, intermediate_size)
-        self.fc_out = nn.Linear(intermediate_size, embed_dim)
+        self.fc_in = ipex.nn.modules.INT4Linear(embed_dim, intermediate_size)
+        self.fc_out = ipex.nn.modules.INT4Linear(intermediate_size, embed_dim)
 
         self.act = ACT2FN[config.activation_function]
         self.dropout = nn.Dropout(config.resid_pdrop)
@@ -738,7 +739,7 @@ class GPTJForCausalLM(GPTJPreTrainedModel):
     def __init__(self, config):
         super().__init__(config)
         self.transformer = GPTJModel(config)
-        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)
+        self.lm_head = ipex.nn.modules.INT4Linear(config.n_embd, config.vocab_size)
 
         # Model parallel
         self.model_parallel = False
@@ -868,7 +869,7 @@ class GPTJForCausalLM(GPTJPreTrainedModel):
         # Set device for model parallelism
         if self.model_parallel:
             torch.cuda.set_device(self.transformer.first_device)
-            hidden_states = hidden_states.to(self.lm_head.weight.device)
+            hidden_states = hidden_states.to(self.lm_head.qweight.device)
 
         # make sure sampling in fp16 works correctly and
         # compute loss in fp32 to match with mesh-tf version
