diff --git a/csrc/cpu/aten/FlashAttention.cpp b/csrc/cpu/aten/FlashAttention.cpp
new file mode 100644
index 000000000..284231558
--- /dev/null
+++ b/csrc/cpu/aten/FlashAttention.cpp
@@ -0,0 +1,43 @@
+#include <torch/all.h>
+#include "FlashAttention.h"
+#include <torch/csrc/autograd/function.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+DEFINE_DISPATCH(flash_attention_kernel_stub);
+
+/*
+*Caculate the flash attention SDPA. 
+*@param query
+*@param key
+*@param value
+*@param scale_attn
+*@param attention_mask
+*@return attn_outs
+*/
+at::Tensor flash_attention_forward_cpu(
+    at::Tensor query,
+    at::Tensor key,
+    at::Tensor value,
+    const double scale_attn,
+    at::Tensor attention_mask){
+  return flash_attention_kernel_stub(
+      kCPU, query, key, value, scale_attn, attention_mask);
+}
+
+} // namespace cpu
+} // namespace torch_ipex
+
+namespace {
+
+TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
+  m.def(
+      "flash_attention(Tensor query, Tensor key, Tensor value, \
+       float scale_attn, Tensor attention_mask)-> Tensor");
+  m.impl(
+      "flash_attention",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::flash_attention_forward_cpu);
+}
+}
diff --git a/csrc/cpu/aten/FlashAttention.h b/csrc/cpu/aten/FlashAttention.h
new file mode 100644
index 000000000..8e8e39b90
--- /dev/null
+++ b/csrc/cpu/aten/FlashAttention.h
@@ -0,0 +1,29 @@
+#pragma once
+
+#include <ATen/ATen.h>
+#include <dyndisp/DispatchStub.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+namespace {
+
+at::Tensor flash_attention(
+    at::Tensor query,
+    at::Tensor key,
+    at::Tensor value,
+    const double scale_attn,
+    at::Tensor attention_mask);
+}
+
+using flash_attention_kernel_fn = at::Tensor (*)(
+    at::Tensor query,
+    at::Tensor key,
+    at::Tensor value,
+    const double scale_attn,
+    at::Tensor attention_mask);
+
+DECLARE_DISPATCH(flash_attention_kernel_fn, flash_attention_kernel_stub);
+
+} // namespace cpu
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/TPPGEMM.cpp b/csrc/cpu/aten/TPPGEMM.cpp
index fe8d7c7de..645696758 100644
--- a/csrc/cpu/aten/TPPGEMM.cpp
+++ b/csrc/cpu/aten/TPPGEMM.cpp
@@ -5,38 +5,73 @@
 namespace torch_ipex {
 namespace cpu {
 
-DEFINE_DISPATCH(fc_in_kernel_stub);
-DEFINE_DISPATCH(fc_out_kernel_stub);
-DEFINE_DISPATCH(fc_plain_kernel_stub);
-DEFINE_DISPATCH(qkv_kernel_stub);
+DEFINE_DISPATCH(tpp_linear_nobias_kernel_stub);
+DEFINE_DISPATCH(tpp_linear_bias_kernel_stub);
+DEFINE_DISPATCH(tpp_linear_gelu_kernel_stub);
+DEFINE_DISPATCH(tpp_linear_silu_kernel_stub);
+DEFINE_DISPATCH(tpp_linear_relu_kernel_stub);
+DEFINE_DISPATCH(tpp_linear_add_kernel_stub);
+DEFINE_DISPATCH(tpp_linear_mul_kernel_stub);
+DEFINE_DISPATCH(tpp_linear_add_add_kernel_stub);
 
+at::Tensor tpp_linear_nobias_forward_cpu(at::Tensor& t_in, at::Tensor& t_wt) {
+  return tpp_linear_nobias_kernel_stub(kCPU, t_in, t_wt);
+}
+
+at::Tensor tpp_linear_bias_forward_cpu(
+    at::Tensor& t_in,
+    at::Tensor& t_wt,
+    at::Tensor& t_bias) {
+  return tpp_linear_bias_kernel_stub(kCPU, t_in, t_wt, t_bias);
+}
+
+at::Tensor tpp_linear_gelu_forward_cpu(
+    at::Tensor& t_in,
+    at::Tensor& t_wt,
+    at::Tensor& t_bias) {
+  return tpp_linear_gelu_kernel_stub(kCPU, t_in, t_wt, t_bias);
+}
 
-at::Tensor qkv_gemm_forward_cpu(at::Tensor& t_in, at::Tensor& t_wt) {
-  return qkv_kernel_stub(kCPU, t_in, t_wt);
+at::Tensor tpp_linear_silu_forward_cpu(
+    at::Tensor& t_in,
+    at::Tensor& t_wt,
+    at::Tensor& t_bias) {
+  return tpp_linear_silu_kernel_stub(kCPU, t_in, t_wt, t_bias);
 }
 
-at::Tensor fc_in_gemm_forward_cpu(
+at::Tensor tpp_linear_relu_forward_cpu(
     at::Tensor& t_in,
     at::Tensor& t_wt,
     at::Tensor& t_bias) {
-  return fc_in_kernel_stub(kCPU, t_in, t_wt, t_bias);
+  return tpp_linear_relu_kernel_stub(kCPU, t_in, t_wt, t_bias);
 }
 
-at::Tensor fc_plain_gemm_forward_cpu(
+at::Tensor tpp_linear_add_forward_cpu(
     at::Tensor& t_in,
+    at::Tensor& t_in1,
+    at::Tensor& t_wt,
+    at::Tensor& t_bias,
+    double scale) {
+  return tpp_linear_add_kernel_stub(kCPU, t_in, t_in1, t_wt, t_bias, scale);
+}
+
+at::Tensor tpp_linear_mul_forward_cpu(
+    at::Tensor& t_in,
+    at::Tensor& t_in1,
     at::Tensor& t_wt,
     at::Tensor& t_bias) {
-  return fc_plain_kernel_stub(kCPU, t_in, t_wt, t_bias);
+  return tpp_linear_mul_kernel_stub(kCPU, t_in, t_in1, t_wt, t_bias);
 }
 
-at::Tensor fc_out_gemm_forward_cpu(
+at::Tensor tpp_linear_add_add_forward_cpu(
     at::Tensor& t_in,
     at::Tensor& t_in1,
     at::Tensor& t_in2,
     at::Tensor& t_wt,
     at::Tensor& t_bias,
     double scale) {
-  return fc_out_kernel_stub(kCPU, t_in, t_in1, t_in2, t_wt, t_bias, scale);
+  return tpp_linear_add_add_kernel_stub(
+      kCPU, t_in, t_in1, t_in2, t_wt, t_bias, scale);
 }
 
 } // namespace cpu
@@ -44,31 +79,75 @@ at::Tensor fc_out_gemm_forward_cpu(
 
 namespace {
 
+TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
+  m.def("tpp_linear(Tensor (a!)t_in, Tensor (a!)t_wt)-> Tensor out");
+  m.impl(
+      "tpp_linear",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::tpp_linear_nobias_forward_cpu);
+}
 
 TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
-  m.def("qkv_gemm(Tensor (a!)t_in, Tensor (a!)t_wt)-> Tensor out");
+  m.def(
+      "tpp_linear_bias(Tensor (a!)t_in, Tensor (a!)t_wt, Tensor (a!)t_bias)-> Tensor out");
   m.impl(
-      "qkv_gemm", c10::DispatchKey::CPU, torch_ipex::cpu::qkv_gemm_forward_cpu);
+      "tpp_linear_bias",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::tpp_linear_bias_forward_cpu);
 }
 
 TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
   m.def(
-      "fc_in_gemm(Tensor (a!)t_in, Tensor (a!)t_wt, Tensor (a!)t_bias)-> Tensor out");
-  m.impl("fc_in_gemm", c10::DispatchKey::CPU, torch_ipex::cpu::fc_in_gemm_forward_cpu);
+      "tpp_linear_gelu(Tensor (a!)t_in, Tensor (a!)t_wt, Tensor (a!)t_bias)-> Tensor out");
+  m.impl(
+      "tpp_linear_gelu",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::tpp_linear_gelu_forward_cpu);
 }
 
 TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
   m.def(
-      "fc_plain_gemm(Tensor (a!)t_in, Tensor (a!)t_wt, Tensor (a!)t_bias)-> Tensor out");
-  m.impl("fc_plain_gemm", c10::DispatchKey::CPU, torch_ipex::cpu::fc_plain_gemm_forward_cpu);
+      "tpp_linear_add_add(Tensor (a!)t_in, Tensor (a!)t_in1, Tensor (a!)t_in2, Tensor (a!)t_wt, Tensor (a!)t_bias, float scale )-> Tensor out");
+  m.impl(
+      "tpp_linear_add_add",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::tpp_linear_add_add_forward_cpu);
 }
 
+TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
+  m.def(
+      "tpp_linear_relu(Tensor (a!)t_in, Tensor (a!)t_wt, Tensor (a!)t_bias)-> Tensor out");
+  m.impl(
+      "tpp_linear_relu",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::tpp_linear_relu_forward_cpu);
+}
 
 TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
   m.def(
-      "fc_out_gemm(Tensor (a!)t_in, Tensor (a!)t_in1, Tensor (a!)t_in2, Tensor (a!)t_wt, Tensor (a!)t_bias, float scale )-> Tensor out");
-  m.impl("fc_out_gemm", c10::DispatchKey::CPU,
-  torch_ipex::cpu::fc_out_gemm_forward_cpu);
+      "tpp_linear_silu(Tensor (a!)t_in, Tensor (a!)t_wt, Tensor (a!)t_bias)-> Tensor out");
+  m.impl(
+      "tpp_linear_silu",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::tpp_linear_silu_forward_cpu);
+}
+
+TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
+  m.def(
+      "tpp_linear_add(Tensor (a!)t_in, Tensor (a!)t_in1, Tensor (a!)t_wt, Tensor (a!)t_bias, float scale )-> Tensor out");
+  m.impl(
+      "tpp_linear_add",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::tpp_linear_add_forward_cpu);
+}
+
+TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
+  m.def(
+      "tpp_linear_mul(Tensor (a!)t_in, Tensor (a!)t_in1, Tensor (a!)t_wt, Tensor (a!)t_bias )-> Tensor out");
+  m.impl(
+      "tpp_linear_mul",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::tpp_linear_mul_forward_cpu);
 }
 
 } // namespace
\ No newline at end of file
diff --git a/csrc/cpu/aten/TPPGEMM.h b/csrc/cpu/aten/TPPGEMM.h
index d1749e24f..311549b2e 100644
--- a/csrc/cpu/aten/TPPGEMM.h
+++ b/csrc/cpu/aten/TPPGEMM.h
@@ -7,36 +7,72 @@ namespace torch_ipex {
 namespace cpu {
 
 namespace {
-at::Tensor fc_in_kernel_impl(
+at::Tensor tpp_linear_nobias_forward_cpu(at::Tensor& t_in, at::Tensor& t_wt);
+
+at::Tensor tpp_linear_bias_forward_cpu(
     at::Tensor& t_in,
     at::Tensor& t_wt,
     at::Tensor& t_bias);
 
-at::Tensor fc_plain_kernel_impl(
+at::Tensor tpp_linear_gelu_forward_cpu(
     at::Tensor& t_in,
     at::Tensor& t_wt,
     at::Tensor& t_bias);
 
+at::Tensor tpp_linear_silu_forward_cpu(
+    at::Tensor& t_in,
+    at::Tensor& t_wt,
+    at::Tensor& t_bias);
+
+at::Tensor tpp_linear_relu_forward_cpu(
+    at::Tensor& t_in,
+    at::Tensor& t_wt,
+    at::Tensor& t_bias);
 
-at::Tensor fc_out_kernel_impl(
+at::Tensor tpp_linear_add_forward_cpu(
     at::Tensor& t_in,
     at::Tensor& t_in1,
-    at::Tensor& t_in2,
     at::Tensor& t_wt,
     at::Tensor& t_bias,
     double scale);
 
-at::Tensor qkv_kernel_impl(at::Tensor& t_in, at::Tensor& t_wt);
+at::Tensor tpp_linear_mul_forward_cpu(
+    at::Tensor& t_in,
+    at::Tensor& t_in1,
+    at::Tensor& t_wt,
+    at::Tensor& t_bias);
+
+at::Tensor tpp_linear_add_add_forward_cpu(
+    at::Tensor& t_in,
+    at::Tensor& t_in1,
+    at::Tensor& t_in2,
+    at::Tensor& t_wt,
+    at::Tensor& t_bias,
+    double scale);
 
 } // namespace
 
-using fc_in_kernel_impl_fn =
+using tpp_linear_nobias_impl_fn = at::Tensor (*)(at::Tensor&, at::Tensor&);
+
+using tpp_linear_bias_kernel_impl_fn =
     at::Tensor (*)(at::Tensor&, at::Tensor&, at::Tensor&);
 
-using fc_plain_kernel_impl_fn =
+using tpp_linear_gelu_kernel_impl_fn =
     at::Tensor (*)(at::Tensor&, at::Tensor&, at::Tensor&);
 
-using fc_out_kernel_impl_fn = at::Tensor (*)(
+using tpp_linear_silu_kernel_impl_fn =
+    at::Tensor (*)(at::Tensor&, at::Tensor&, at::Tensor&);
+
+using tpp_linear_relu_kernel_impl_fn =
+    at::Tensor (*)(at::Tensor&, at::Tensor&, at::Tensor&);
+
+using tpp_linear_add_kernel_impl_fn =
+    at::Tensor (*)(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, double);
+
+using tpp_linear_mul_kernel_impl_fn =
+    at::Tensor (*)(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&);
+
+using tpp_linear_add_add_kernel_impl_fn = at::Tensor (*)(
     at::Tensor&,
     at::Tensor&,
     at::Tensor&,
@@ -44,12 +80,16 @@ using fc_out_kernel_impl_fn = at::Tensor (*)(
     at::Tensor&,
     double);
 
-using qkv_kernel_impl_fn = at::Tensor (*)(at::Tensor&, at::Tensor&);
-
-DECLARE_DISPATCH(fc_plain_kernel_impl_fn, fc_plain_kernel_stub);
-DECLARE_DISPATCH(fc_in_kernel_impl_fn, fc_in_kernel_stub);
-DECLARE_DISPATCH(fc_out_kernel_impl_fn, fc_out_kernel_stub);
-DECLARE_DISPATCH(qkv_kernel_impl_fn, qkv_kernel_stub);
+DECLARE_DISPATCH(tpp_linear_nobias_impl_fn, tpp_linear_nobias_kernel_stub);
+DECLARE_DISPATCH(tpp_linear_bias_kernel_impl_fn, tpp_linear_bias_kernel_stub);
+DECLARE_DISPATCH(tpp_linear_gelu_kernel_impl_fn, tpp_linear_gelu_kernel_stub);
+DECLARE_DISPATCH(tpp_linear_silu_kernel_impl_fn, tpp_linear_silu_kernel_stub);
+DECLARE_DISPATCH(tpp_linear_relu_kernel_impl_fn, tpp_linear_relu_kernel_stub);
+DECLARE_DISPATCH(tpp_linear_add_kernel_impl_fn, tpp_linear_add_kernel_stub);
+DECLARE_DISPATCH(tpp_linear_mul_kernel_impl_fn, tpp_linear_mul_kernel_stub);
+DECLARE_DISPATCH(
+    tpp_linear_add_add_kernel_impl_fn,
+    tpp_linear_add_add_kernel_stub);
 
 } // namespace cpu
 } // namespace torch_ipex
\ No newline at end of file
diff --git a/csrc/cpu/aten/kernels/FlashAttentionKrnl.cpp b/csrc/cpu/aten/kernels/FlashAttentionKrnl.cpp
new file mode 100644
index 000000000..3f7363624
--- /dev/null
+++ b/csrc/cpu/aten/kernels/FlashAttentionKrnl.cpp
@@ -0,0 +1,239 @@
+#include <ATen/Tensor.h>
+#include <aten/FlashAttention.h>
+#include <torch/all.h>
+#include <torch/csrc/autograd/function.h>
+#include <limits>
+#include "mkl.h"
+#include "vec/vec.h"
+
+namespace torch_ipex {
+namespace cpu {
+
+namespace {
+
+const int64_t qsplit_size = 384;
+const int64_t kvsplit_size = 512;
+
+#if defined(CPU_CAPABILITY_AVX512)
+using namespace torch_ipex::cpu::kernel;
+
+template <typename scalar_t>
+void _mha_mul_softmax_bf16_kernel(
+    float* a,
+    scalar_t* b,
+    float* dst,
+    float* max,
+    float* sum,
+    const int& qsize,
+    const int& kvsize,
+    const int& headsize,
+    const int& idx) {
+  float tmp_max = 0.f, tmp_sum = 0.f, sum_old = 0.f, exp_tmp = 0.f;
+
+  for (int i = 0; i < qsize; ++i) {
+    sum_old = sum[i];
+
+    _dil_reduce_max_fusion_kernel(
+        a + i * kvsize, kvsize, a + i * kvsize, tmp_max);
+    tmp_max = max[i] > tmp_max ? max[i] : tmp_max;
+
+    tmp_sum = tmp_max;
+    _dil_exp_reduce_sum_fusion_kernel(
+        a + i * kvsize, kvsize, a + i * kvsize, tmp_sum);
+    exp_tmp = exp(max[i] - tmp_max);
+    sum[i] = tmp_sum + exp_tmp * sum[i];
+    max[i] = tmp_max;
+
+    _dil_normalization_kernel<scalar_t>(
+        a + i * kvsize, sum[i], kvsize, b + i * kvsize);
+
+    if (idx) {
+      _mha_update_sum_max_kernel(
+          dst + i * headsize,
+          sum_old,
+          sum[i],
+          exp_tmp,
+          headsize,
+          dst + i * headsize);
+    }
+  }
+}
+
+at::Tensor flash_base_kernel(
+    at::BFloat16* query,
+    at::BFloat16* key,
+    at::BFloat16* value,
+    at::BFloat16* attn_mask,
+    const int64_t& qStride,
+    const int64_t& kStride,
+    const int64_t& vStride,
+    const int64_t& batchSize,
+    const int64_t& qSize,
+    const int64_t& kvSize,
+    const int64_t& num_head,
+    const int64_t& headSize,
+    const int64_t& hiddenSize,
+    const double& scale) {
+  at::Tensor output = at::empty({batchSize, qSize, hiddenSize}, at::kBFloat16);
+
+  int64_t qSplitSize = qSize >= qsplit_size ? qsplit_size : qSize;
+  int64_t kvSplitSize = kvSize >= kvsplit_size ? kvsplit_size : kvSize;
+
+  int64_t qSlice = (qSize - 1) / qSplitSize + 1;
+  int64_t qTail = (qSize - 1) % qSplitSize + 1;
+  int64_t kvSlice = (kvSize - 1) / kvSplitSize + 1;
+  int64_t kvTail = (kvSize - 1) % kvSplitSize + 1;
+
+  int64_t num_thread = omp_get_max_threads();
+
+  at::Tensor qk_fp32 =
+      at::empty({num_thread, qSplitSize, kvSplitSize}, at::kFloat);
+  at::Tensor qk_bf16 =
+      at::empty({num_thread, qSplitSize, kvSplitSize}, at::kBFloat16);
+  at::Tensor qk_max = at::empty({num_thread, qSplitSize}, at::kFloat);
+  at::Tensor qk_sum = at::empty({num_thread, qSplitSize}, at::kFloat);
+  at::Tensor dst_fp32 =
+      at::empty({num_thread, qSplitSize, headSize}, at::kFloat);
+
+#pragma omp parallel for collapse(3)
+  for (int i = 0; i < batchSize; ++i) {
+    for (int j = 0; j < num_head; ++j) {
+      for (int k = 0; k < qSlice; ++k) {
+        int qBlockSize = (k == qSlice - 1) ? qTail : qSplitSize;
+        int ompIdx = omp_get_thread_num();
+        _init_mha_buffer_kernel(
+            qk_max.data_ptr<float>() + ompIdx * qSplitSize,
+            qk_sum.data_ptr<float>() + ompIdx * qSplitSize,
+            qBlockSize);
+
+        for (int l = 0; l < kvSlice; ++l) {
+          int kvBlockSize = (l == kvSlice - 1) ? kvTail : kvSplitSize;
+          cblas_gemm_bf16bf16f32(
+              CblasRowMajor,
+              CblasNoTrans,
+              CblasTrans,
+              qBlockSize,
+              kvBlockSize,
+              headSize,
+              float(1.f / scale),
+              (const MKL_BF16*)(query + i * qSize * qStride + headSize * j + k * qSplitSize * qStride),
+              qStride,
+              (const MKL_BF16*)(key + i * kvSize * kStride + headSize * j + l * kvSplitSize * kStride),
+              kStride,
+              0.f,
+              qk_fp32.data_ptr<float>() + ompIdx * qSplitSize * kvSplitSize,
+              kvBlockSize);
+
+          // update attention weights with attention mask
+          for (int r = 0; r < qBlockSize; r++) {
+            _dil_add_kernel<at::BFloat16>(
+              attn_mask + i * qSize * kvSize + (k * qSplitSize + r) * kvSize + l * kvSplitSize,
+              qk_fp32.data_ptr<float>() + ompIdx * qSplitSize * kvSplitSize + r * kvBlockSize,
+              kvBlockSize);
+          }
+
+          _mha_mul_softmax_bf16_kernel<at::BFloat16>(
+              qk_fp32.data_ptr<float>() + ompIdx * qSplitSize * kvSplitSize,
+              qk_bf16.data_ptr<at::BFloat16>() +
+                  ompIdx * qSplitSize * kvSplitSize,
+              dst_fp32.data_ptr<float>() + ompIdx * qSplitSize * headSize,
+              qk_max.data_ptr<float>() + ompIdx * qSplitSize,
+              qk_sum.data_ptr<float>() + ompIdx * qSplitSize,
+              qBlockSize,
+              kvBlockSize,
+              headSize,
+              l);
+
+          cblas_gemm_bf16bf16f32(
+              CblasRowMajor,
+              CblasNoTrans,
+              CblasNoTrans,
+              qBlockSize,
+              headSize,
+              kvBlockSize,
+              1.f,
+              (const MKL_BF16*)(qk_bf16.data_ptr<at::BFloat16>() + ompIdx * qSplitSize * kvSplitSize),
+              kvBlockSize,
+              (const MKL_BF16*)(value + i * kvSize * vStride + headSize * j + l * kvSplitSize * vStride),
+              vStride,
+              l == 0 ? 0.f : 1.f,
+              dst_fp32.data_ptr<float>() + ompIdx * qSplitSize * headSize,
+              headSize);
+        }
+        _reorder_mha_output_kernel<at::BFloat16>(
+            dst_fp32.data_ptr<float>() + ompIdx * qSplitSize * headSize,
+            output.data_ptr<at::BFloat16>() + i * qSize * hiddenSize +
+                headSize * j + k * qSplitSize * hiddenSize,
+            qBlockSize,
+            headSize,
+            hiddenSize);
+      }
+    }
+  }
+  return output;
+}
+#endif
+
+at::Tensor flash_attention_kernel_impl(
+    at::Tensor query,
+    at::Tensor key,
+    at::Tensor value,
+    const double scale_attn,
+    at::Tensor attention_mask) {
+    if (query.scalar_type() != at::kBFloat16
+             || query.dtype() != key.dtype()
+             || query.dtype() != attention_mask.dtype()) {
+        TORCH_CHECK(false, "Q/K/V/AttnMask must be BF16 to use ipex::flash_attention_kernel_impl");
+    }
+    if(query.dim() != 4 || key.dim() != 4 || value.dim() != 4){
+        TORCH_CHECK(false, "Q/K/V must be 4D for ipex::flash_attention_kernel_impl");
+    }
+    TORCH_CHECK(attention_mask.size(1) == 1, "Attetntion mask size(1) != 1 for ipex::flash_attention_kernel_imp");
+
+#if defined(CPU_CAPABILITY_AVX512)
+    int64_t batchSize = query.size(0);
+    int64_t qSize = query.size(1);
+    int64_t kvSize = value.size(1);
+    int64_t num_head = query.size(2);
+    int64_t headSize = query.size(3);
+    int64_t hiddenSize = num_head * headSize;
+
+    int64_t qStride = query.stride(1);
+    int64_t kStride = key.stride(1);
+    int64_t vStride = value.stride(1);
+    auto attn_outputs = flash_base_kernel(
+      query.data_ptr<at::BFloat16>(),
+      key.data_ptr<at::BFloat16>(),
+      value.data_ptr<at::BFloat16>(),
+      attention_mask.data_ptr<at::BFloat16>(),
+      qStride,
+      kStride,
+      vStride,
+      batchSize,
+      qSize,
+      kvSize,
+      num_head,
+      headSize,
+      hiddenSize,
+      scale_attn);
+    return attn_outputs.resize_(
+        {batchSize, qSize, num_head, headSize}).transpose_(1, 2);
+#else
+    key = key.permute({0, 2, 1, 3});
+    query = query.permute({0, 2, 1, 3});
+    value = value.permute({0, 2, 1, 3});
+    auto attn_weights = query.matmul(key.transpose(-1, -2));
+    attn_weights = attn_weights.div(scale_attn);
+    attn_weights = attn_weights + attention_mask;
+    attn_weights = attn_weights.softmax(-1);
+    attn_weights = attn_weights.to(value.dtype());
+    auto out = attn_weights.matmul(value);
+    return out;
+#endif
+}
+} // anonymous namespace
+
+REGISTER_DISPATCH(flash_attention_kernel_stub, &flash_attention_kernel_impl);
+
+} // namespace cpu
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/kernels/MaskedMultiHeadAttentionKrnl.cpp b/csrc/cpu/aten/kernels/MaskedMultiHeadAttentionKrnl.cpp
index 4e3d162ca..7defda0f6 100644
--- a/csrc/cpu/aten/kernels/MaskedMultiHeadAttentionKrnl.cpp
+++ b/csrc/cpu/aten/kernels/MaskedMultiHeadAttentionKrnl.cpp
@@ -1,7 +1,9 @@
+#include <ATen/Tensor.h>
+#include <aten/FlashAttention.h>
 #include <aten/MaskedMultiHeadAttention.h>
-#include <torch/csrc/autograd/function.h>
 #include <torch/all.h>
-#include <ATen/Tensor.h>
+#include <torch/csrc/autograd/function.h>
+#include <limits>
 #include "vec/vec.h"
 
 namespace torch_ipex {
@@ -99,11 +101,11 @@ void reduce_head(
 /* 
 *reduce the attnetion_weights with the value embeeding by the dimension of head_size  for every head 
 */
-template<typename T>
+template<typename T, typename T1>
 void mul_attenion_weights_and_value_of_head(
     float& attn_w,
     const T* v_ptr_start,
-    T* attn_out_start,
+    T1* attn_out_start,
     int64_t head_size,
     bool store_value,
     T* v_cache_start) {
@@ -198,171 +200,416 @@ void mul_attenion_weights_and_value_of_head(
 
 }
 
+template<>
+void mul_attenion_weights_and_value_of_head(
+    float& attn_w,
+    const at::BFloat16* v_ptr_start,
+    float* attn_out_start,
+    int64_t head_size,
+    bool store_value,
+    at::BFloat16* v_cache_start) {
+    auto hsi = 0;
+    #if defined(CPU_CAPABILITY_AVX512)
+    auto vec_size= 16; // 512/32
+    for(hsi=0; hsi <= head_size - vec_size; hsi+=vec_size){
+        //get 1 bfloat16 values from attn_w_ptr_start and broadcast to 16 float32 values
+        auto attn_w_vec_fp32 = _mm512_set1_ps(attn_w);
+        //load 16 bfloat16 values from v_ptr_start and convert to 16 float32 values
+        auto v_vec_bf16 = _mm256_loadu_si256((__m256i*)(v_ptr_start + hsi));
+        auto v_vec_fp32 = torch_ipex::cpu::kernel::convert_bf16_to_fp32(v_vec_bf16);        
+        //load 16 bfloat16 values from attn_out_start and convert to 16 float32 values
+        //auto attn_out_vec_fp32 = torch_ipex::cpu::kernel::convert_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(attn_out_start + hsi)));
+        auto attn_out_vec_fp32 = _mm512_loadu_ps(attn_out_start + hsi);
+        //calculate the new attn_out_vec_fp32 and convert to bfloat16
+        auto attn_out_vec_new = _mm512_fmadd_ps(attn_w_vec_fp32, v_vec_fp32, attn_out_vec_fp32);
+        //auto attn_out_vec_new_bf16 = cvt_fp32_to_bf16(attn_out_vec_new);//_m256i
+        //store the new attn_out_vec_new_bf16 to attn_outs
+        //_mm256_storeu_si256((__m256i*)(attn_out_start + hsi), attn_out_vec_new_bf16);
+        _mm512_storeu_ps(attn_out_start + hsi, attn_out_vec_new);
+        //store the v_vec_bf16 to v_cache
+        if(store_value){
+            _mm256_storeu_si256((__m256i*)(v_cache_start + hsi), v_vec_bf16);
+        }
+    }
+    for(; hsi < head_size; hsi++){
+        attn_out_start[hsi] += attn_w * v_ptr_start[hsi];
+        if(store_value){
+            v_cache_start[hsi] = v_ptr_start[hsi];
+        }
+    }
+    return;
+    #endif
+    for(hsi=0; hsi < head_size; hsi++){
+        attn_out_start[hsi] += attn_w * v_ptr_start[hsi];
+        if(store_value){
+            v_cache_start[hsi] = v_ptr_start[hsi];
+        }
+    }
+
+}
+
+template <typename T>
+void copy_key_value(
+    at::Tensor key_cache,
+    const at::Tensor key,
+    at::Tensor value_cache,
+    const at::Tensor value,
+    int beam_batch) {
+  RECORD_FUNCTION("ipex::copy_key_value", c10::ArrayRef<c10::IValue>({}));
+  auto bs = key.size(0);
+  auto seq_len = key.size(1); // only process cur_len==1
+  auto head_num = key.size(2);
+  auto head_size = key.size(3);
+  auto hidden_size = head_num * head_size;
+  auto key_cache_ptr = key_cache.data_ptr<T>();
+  auto key_ptr = key.data_ptr<T>();
+  auto value_cache_ptr = value_cache.data_ptr<T>();
+  auto value_ptr = value.data_ptr<T>();
+  auto token_stride = beam_batch * hidden_size;
+  auto beam_size = beam_batch / bs;
+#pragma omp parallel for collapse(2)
+  for (auto si = 0; si < seq_len; si++) {
+    for (auto bi = 0; bi < bs; bi++) {
+      auto cache_stride = si * token_stride + bi * beam_size * hidden_size;
+      auto state_stride = (bi * seq_len + si) * hidden_size;
+      auto key_cache_start = key_cache_ptr + cache_stride;
+      auto key_ptr_start = key_ptr + state_stride;
+      torch_ipex::cpu::kernel::move_ker<T, T>(key_cache_start, key_ptr_start, hidden_size);
+      auto value_cache_ptr_start = value_cache_ptr + cache_stride;
+      auto value_ptr_start = value_ptr + state_stride;
+      torch_ipex::cpu::kernel::move_ker<T, T>(value_cache_ptr_start, value_ptr_start, hidden_size);
+    }
+  }
+}
+
 /* 
 *The scale-dot product for indirect access kv chache and fuse matmul+div+add+softmax to improve data reuse
 *@param  query Query embeeding with the of [beam_size*batch, cur_len, head_num, head_size]
 *@param  key Key embeeding with the of [beam_size*batch, cur_len, head_num, head_size]
-*@param  beam_idx Beam info for every token [beam_size, offset]
-*@param  key_cache Cache past key embeeding with the of [max_len, beam_size*batch, cur_len, head_num, head_size]
+*@param  value Key embeeding with the of [beam_size*batch, cur_len, head_num, head_size]
+*@param  key_cache Cache past key embeeding with the of [max_len, beam_size*batch, head_num, head_size]
+*@param  value_chache Cache past value embeeding with the of [max_len, beam_size*batch, head_num, head_size]
+*@param  beam_idx Beam info for every token [max_len, beam_size*batch]
 *@param  offset  The length of decoded(past) token. 
 *@param  scale_factor the sqrt(head_dim).
+*@param  head_mask Which is not used by our kernel now. 
 *@param  attention_mask Which is combined mask for padding mask and casual mask. 
-*@param  value The vaule for current tokens. 
-*@param  value_chache Cache past value embeeding with the of [max_len, beam_size*batch, cur_len, head_num, head_size]
-*@return attn_outs With shape of [beam*bs, head_num, 1, head_size]
+*@return attn_outs, None, key_cache, value_cache, beam_idx
 */
-template<typename T>
-at::Tensor scale_dot_product_for_indirect_access_kv_cache(at::Tensor query, at::Tensor key, const std::vector<std::vector<long>> beam_idx, at::Tensor 
-&key_cache, int offset, float scale_factor, at::Tensor attention_mask, at::Tensor value, at::Tensor &value_cache){
-    RECORD_FUNCTION("ipex::scale_dot_product_for_indirect_access_kv_cache", c10::ArrayRef<c10::IValue>({}));
-    auto bs = query.size(0);//beam_size * batch_size
-    auto cur_len = query.size(1);// only process cur_len==1
-    auto head_num = query.size(2);
-    auto kv_head = key.size(2);
-    auto group_size = head_num / kv_head;    
-    auto head_size = query.size(3);  
-    auto seq_len = offset + cur_len;
-    auto kc_token_stride = bs * kv_head * head_size;
-    auto attn_weights = at::empty({bs, head_num, cur_len, seq_len}, key.options());   
-    query = query.contiguous();
-    key = key.contiguous();
-    auto q_ptr = query.data_ptr<T>();
-    auto k_ptr = key.data_ptr<T>();
-    auto k_cache_ptr = key_cache.data_ptr<T>();
-    auto attn_w_ptr = attn_weights.data_ptr<T>();    
-    auto mask_ptr = attention_mask.data_ptr<T>();
-    auto mask_head_num = attention_mask.size(1);
-    auto mask_token_stride = mask_head_num * cur_len * seq_len;    
-    //value realted 
-    value = value.contiguous();
-    auto attn_outs = at::empty({bs, head_num, cur_len, head_size}, value.options());
-    auto v_ptr = value.data_ptr<T>();
-    auto v_cache_ptr = value_cache.data_ptr<T>();
-    auto attn_out_ptr = attn_outs.data_ptr<T>();    
+template <typename QT, typename VT>
+std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>  scale_dot_product_for_indirect_access_kv_cache(
+    at::Tensor query,
+    at::Tensor key,
+    at::Tensor value,
+    at::Tensor& key_cache,
+    at::Tensor& value_cache,
+    at::Tensor&  beam_idx,    
+    const int64_t offset,
+    const double scale_factor,
+    at::Tensor& attention_mask) {
+  RECORD_FUNCTION(
+      "ipex::scale_dot_product_for_indirect_access_kv_cache",
+      c10::ArrayRef<c10::IValue>({}));
+  int beam_batch = beam_idx.size(1);
+  auto bs = query.size(0);
+  auto cur_len = query.size(1); // only process cur_len==1
+  auto head_num = query.size(2);
+  auto kv_head = key.size(2);
+  auto group_size = head_num / kv_head;
+  auto head_size = query.size(3);
+  auto seq_len = offset + cur_len;
+  auto kc_token_stride = beam_batch * kv_head * head_size;
+  auto attn_weights =
+      at::empty({bs, head_num, cur_len, seq_len}, at::kFloat);
+  query = query.contiguous();
+  key = key.contiguous();
+  auto q_ptr = query.data_ptr<QT>();
+  auto k_ptr = key.data_ptr<QT>();
+  auto k_cache_ptr = key_cache.data_ptr<QT>();
+  auto mask_ptr = attention_mask.data_ptr<QT>();
+  auto mask_head_num = attention_mask.size(1);
+  auto mask_dim2 = attention_mask.size(2);
+  auto mask_bs_stride = mask_head_num * mask_dim2 * seq_len;
+  // value realted
+  value = value.contiguous();
+  auto attn_outs =
+      at::zeros({bs, head_num, cur_len, head_size}, value.options());
+  auto v_ptr = value.data_ptr<VT>();
+  auto v_cache_ptr = value_cache.data_ptr<VT>();
+  auto attn_out_ptr = attn_outs.data_ptr<VT>();
+  //torch_ipex::cpu::kernel::zero_ker(attn_out_ptr, attn_outs.numel());
+  auto attn_w_ptr = attn_weights.data_ptr<float>();
 
-    //query Query embeeding with the of [beam_size*batch, cur_len, head_num, head_size]
-    //key Key embeeding with the of [beam_size*batch, cur_len, head_num, head_size]
-    //key_cache Cache past key embeeding with the of [past_len, beam_size*batch, cur_len, head_num, head_size]
-    //Try to reshape the query to [beam_size*batch, cur_len, head_size, head_num]    
-    #pragma omp parallel for collapse(2)
-    for(auto bi = 0; bi < bs; bi++){
-        for (auto hi = 0; hi < head_num; hi++){
-            auto kv_hi = hi / group_size;//maping the query head to key/value head to support MGA/MQA
-           //printf("group_size:%d hi:%d kv_hi:%d kv_head:%d", group_size, hi, kv_hi, kv_head);
-           //fflush(stdout); 
-           // e.g.,cur_len = 2, past_len=3
-           // query:            t4 t5 
-           // key:  t0 t1 t2 t3 t4 t5
-           //output shape (2, 5)
-           //[qk_t0 qk_t1 qk_t2 qk_t3 qk_t4 -10000.0]
-           //[qk_t0 qk_t1 qk_t2 qk_t3 qk_t4 qk_t5   ]
-           //fused div+add+softmax
-           float p[cur_len][seq_len];
-           auto mask_ptr_start = mask_ptr + bi * mask_token_stride;
-           for(auto query_ti = 0; query_ti < cur_len; query_ti++){
-                for(auto ti = 0; ti < seq_len; ti++){                           
-                    //auto t_out_stride  =  out_stride + query_ti * seq_len;
-                    //auto attn_w_pos = attn_w_ptr + t_out_stride + ti ;
-                    auto q_ptr_start = q_ptr + (bi * cur_len + query_ti) * head_num * head_size  + hi * head_size;                    
-                    auto k_ptr_start = k_ptr + (bi * cur_len + query_ti) * kv_head * head_size + kv_hi * head_size;   
-                    p[query_ti][ti] = 0.0f;                 
-                    if(ti > query_ti + offset){//only caculate the innerproduct for the past token and current token
-                        p[query_ti][ti] = -100000.0;
-                    }else if(ti == query_ti + offset){//caculate the innerproduct for the current token and store the key
-                        auto kc_token_start = ti * kc_token_stride;
-                        auto kc_t_beam_start = kc_token_start + bi * kv_head * head_size;
-                        auto kc_head_start = k_cache_ptr + kc_t_beam_start + kv_hi * head_size;            
-                        reduce_head<T>(q_ptr_start, k_ptr_start, &p[query_ti][ti], head_size, true, kc_head_start);
-                    }else{//caculate the innerproduct for the past token
-                        auto kc_token_start = ti * kc_token_stride;
-                        auto kc_t_beam_start = kc_token_start + beam_idx[bi][ti] * kv_head * head_size;
-                        auto kc_head_start = k_cache_ptr + kc_t_beam_start + kv_hi * head_size;                        
-                        reduce_head<T>(q_ptr_start, kc_head_start, &p[query_ti][ti], head_size, false, nullptr);                
-                    }                                    
-                }                    
+  // beam_idx: [beam_size, offset] for every decoded token, the beam_idx is the
+  // target beam idx for the past token the target beam_idx for the input tokens
+  // are always 0 compute the offset info for the past token std::cout <<
+  // "beam_idx:" << beam_idx << std::endl;  
+  // the targe beam for the past token
+  auto new_beam_idx = std::vector<std::vector<long>>(
+      beam_batch, std::vector<long>(offset + query.size(1), 0));
+  auto b_ptr = beam_idx.data_ptr<long>();
+  if (offset > 0) {
+    // according to the last decoded token to get the target beam for the past
+    // token
+    for (int i = 0; i < bs; i++) {
+      new_beam_idx[i][offset - 1] = b_ptr[(offset - 1) * bs + i];
+      for (int j = offset - 2; j >= 0;
+           j--) { // for the token of input, the target beam is alwarys 0
+        new_beam_idx[i][j] = b_ptr[j * bs + new_beam_idx[i][j + 1]];
+      }
+    }
+  }
+{
+    RECORD_FUNCTION(
+      "ipex::iakv_sdp::matmul(query, key)",
+      c10::ArrayRef<c10::IValue>({}));
+    #pragma omp parallel for collapse(3)
+    for (auto ti = 0; ti < seq_len; ti++) {  
+    for (auto bi = 0; bi < bs; bi++) {        
+        for (auto hi = 0; hi < head_num; hi++) {
+                  
+            for (auto query_ti = 0; query_ti < cur_len; query_ti++) {        
+                auto kv_hi = hi / group_size; // maping the query head to key/value head
+                                            // to support MGA/MQA
+                //printf("beam_batch: %d bi/bs: %d/%d group_size:%d hi:%d kv_hi:%d kv_head:%d \n", beam_batch, bi, bs, group_size, hi, kv_hi, kv_head); fflush(stdout);
+                
+                auto q_ptr_start = q_ptr +
+                    (bi * cur_len + query_ti) * head_num * head_size + hi * head_size;
+                auto attn_w_stride  =  (bi * head_num + hi) * cur_len * seq_len;
+                auto attn_w_pos = attn_w_ptr + attn_w_stride + query_ti * seq_len + ti ;
+                attn_w_pos[0] = 0.0f;
+                auto kc_token_start = ti * kc_token_stride;
+                auto kc_t_beam_start = kc_token_start;
+                if (ti > query_ti + offset) { // only caculate the innerproduct for
+                                                // the past token and current token
+                    attn_w_pos[0] = -10000.0f;
+                } else if (ti == query_ti + offset) { // caculate the innerproduct for
+                                                        // the current token and store
+                                                        // the key
+                    if (cur_len > 1) { // this may occur for processing the promt
+                        auto beam_size = beam_batch / bs;
+                        // need to store key accross beam
+                        kc_t_beam_start =
+                            kc_t_beam_start + bi * beam_size * kv_head * head_size;
+                    } else {
+                        kc_t_beam_start = kc_t_beam_start + bi * kv_head * head_size;
+                    }
+                    auto kc_head_start =
+                        k_cache_ptr + kc_t_beam_start + kv_hi * head_size;
+                    auto k_ptr_start = k_ptr +
+                        (bi * cur_len + ti - offset) * kv_head * head_size +
+                        kv_hi * head_size;
+                    reduce_head<QT>(
+                        q_ptr_start,
+                        k_ptr_start,
+                        attn_w_pos,
+                        head_size,
+                        true,
+                        kc_head_start);
+                } else { // caculate the innerproduct for the past token
+                    if (ti >= offset) {
+                        auto k_ptr_start = k_ptr +
+                            (bi * cur_len + ti - offset) * kv_head * head_size +
+                            kv_hi * head_size;
+                        reduce_head<QT>(
+                            q_ptr_start,
+                            k_ptr_start,
+                            attn_w_pos,
+                            head_size,
+                            false,
+                            nullptr);
+                    } else {
+                        kc_t_beam_start =
+                            kc_t_beam_start + new_beam_idx[bi][ti] * kv_head * head_size;
+                        if (cur_len > 1) {
+                            auto beam_size = beam_batch / bs;
+                            kc_t_beam_start =
+                                kc_t_beam_start + bi * beam_size * kv_head * head_size;
+                        }
+                        //printf("new_beam_idx[bi][ti]:%d \n", new_beam_idx[bi][ti]);
+                        auto kc_head_start =
+                            k_cache_ptr + kc_t_beam_start + kv_hi * head_size;
+                        reduce_head<QT>(
+                            q_ptr_start,
+                            kc_head_start,
+                            attn_w_pos,
+                            head_size,
+                            false,
+                            nullptr);
+                        }
+                }
+            //std::cout << " " << *attn_w_pos;
             }
             
-            //div+add+softmax            
-            #if defined(CPU_CAPABILITY_AVX512)
-            for(auto qi = 0; qi < cur_len; qi++){
-                auto max_val = -100000.0f;
-                torch_ipex::cpu::kernel::_dil_div_add_reduce_max_fusion_kernel<float, T>(&p[qi][0], mask_ptr_start+qi*seq_len, scale_factor, seq_len, &p[qi][0], max_val);
-                torch_ipex::cpu::kernel::_dil_exp_reduce_sum_fusion_kernel(&p[qi][0], seq_len, &p[qi][0], max_val);
-                torch_ipex::cpu::kernel::_dil_normalization_kernel<float>(&p[qi][0], max_val, seq_len, &p[qi][0]);
+        }
+        //std::cout << std::endl;
+        }
+    }
+}
+{
+    RECORD_FUNCTION(
+      "ipex::iakv_sdp::div_add_softmax",
+      c10::ArrayRef<c10::IValue>({}));
+    #pragma omp parallel for collapse(2)
+    for (auto bi = 0; bi < bs; bi++) {
+        for (auto hi = 0; hi < head_num; hi++) { 
+            for (auto query_ti = 0; query_ti < cur_len; query_ti++) {            
+                auto mask_ptr_start = mask_ptr + bi * mask_bs_stride + (hi%mask_head_num) * mask_dim2 * seq_len;
+                auto attn_w_stride  =  (bi * head_num + hi) * cur_len * seq_len;
+                auto attn_w_query_start = attn_w_ptr + attn_w_stride + query_ti * seq_len;
+                // std::cout << std::endl;
+                // div+add+softmax
+                #if defined(CPU_CAPABILITY_AVX512)
+                        for (auto qi = 0; qi < 1; qi++) {
+                        auto max_val = -100000.0f;
+                        torch_ipex::cpu::kernel::
+                            _dil_div_add_reduce_max_fusion_kernel<float, QT>(
+                                attn_w_query_start,
+                                mask_ptr_start + (query_ti % mask_dim2) * seq_len,
+                                scale_factor,
+                                seq_len,
+                                attn_w_query_start,
+                                max_val);
+                        
+                        torch_ipex::cpu::kernel::_dil_exp_reduce_sum_fusion_kernel(
+                            attn_w_query_start, seq_len, attn_w_query_start, max_val);
+                        torch_ipex::cpu::kernel::_dil_normalization_kernel<float>(
+                            attn_w_query_start, max_val, seq_len, attn_w_query_start);
+                        }
+                #else
+                        assert(
+                            false &&
+                            "AVX512 is required in ipex::scale_dot_product_for_indirect_access_kv_cache");
+                #endif
+                        }
+                    }
+    }
+}
+auto thread_numbers = omp_get_max_threads(); 
+auto private_attn_outs = at::zeros(
+    {thread_numbers, bs, head_num, cur_len, head_size}, at::kFloat);
+auto private_attn_out_flag = at::zeros(
+    {thread_numbers, bs, head_num}, at::kByte);
+auto flag_access = private_attn_out_flag.accessor<uint8_t, 3>();
+auto private_attn_out_ptr = private_attn_outs.data_ptr<float>();
+//torch_ipex::cpu::kernel::zero_ker(private_attn_out_ptr, private_attn_outs.numel());
+auto attn_outs_stride_priv = bs * head_num * cur_len * head_size;
+{
+    RECORD_FUNCTION(
+      "ipex::iakv_sdp::matmul(attn_w, value)",
+      c10::ArrayRef<c10::IValue>({}));
+#pragma omp parallel for collapse(3)
+for (auto vi = 0; vi < seq_len; vi++) {     
+  for (auto bi = 0; bi < bs; bi++) {
+    for (auto hi = 0; hi < head_num; hi++) {
+          
+        for (auto query_ti = 0; query_ti < cur_len; query_ti++) {
+        auto thread_id = omp_get_thread_num();
+        flag_access[thread_id][bi][hi] = 1;
+        auto kv_hi = hi / group_size; // maping the query head to key/value head
+                                        // to support MGA/MQA
+        auto attn_w_stride  =  (bi * head_num + hi) * cur_len * seq_len;
+        auto attn_w_query_start = attn_w_ptr + attn_w_stride + query_ti * seq_len;
+        // calculate weighted value and store the result to attn_outs[bs,
+        // head_num, cur_len, head_size]
+        auto attn_out_head_stride = thread_id * attn_outs_stride_priv + (bi * head_num + hi) * cur_len * head_size;
+        auto attn_out_start =
+            private_attn_out_ptr + attn_out_head_stride + query_ti * head_size;
+        
+        auto vc_token_start = vi * kc_token_stride;
+        if (vi == query_ti + offset) { // caculate the attention values
+                                            // for the current token
+            auto vc_t_beam_start = vc_token_start;
+            if (cur_len > 1) { // this may occur for processing the promt
+                auto beam_size = beam_batch / bs;
+                // removed the redundant computation, need to store key accross
+                // beam
+                vc_t_beam_start =
+                    vc_t_beam_start + bi * beam_size * kv_head * head_size; 
+            } else {
+                vc_t_beam_start = vc_t_beam_start + bi * kv_head * head_size;
             }
-            #else
-            assert(false && "AVX512 is required in ipex::scale_dot_product_for_indirect_access_kv_cache");
-            #endif
-            //calculate weighted value and store the result to attn_outs[bs, head_num, cur_len, head_size]   
-            auto attn_out_head_stride = (bi * head_num + hi) * cur_len * head_size;         
-            for(auto qi = 0; qi < cur_len; qi++){
-                auto attn_out_start = attn_out_ptr + attn_out_head_stride + qi * head_size;
-                for(auto i = 0; i < head_size; i++){
-                    attn_out_start[i] = 0.0f;
-                }
-                for(auto vi = 0; vi < seq_len; vi++){
-                    auto vc_token_start = vi * kc_token_stride;                    
-                    if(vi == qi + offset){//caculate the attention values for the current token
-                        auto vc_t_beam_start = vc_token_start + bi * kv_head * head_size;
-                        auto v_cache_head_start = v_cache_ptr + vc_t_beam_start + kv_hi * head_size;                        
-                        auto v_ptr_start = v_ptr + (bi * cur_len + qi) * kv_head * head_size + kv_hi * head_size;
-                        mul_attenion_weights_and_value_of_head<T>(p[qi][vi], v_ptr_start, attn_out_start, head_size, true, v_cache_head_start);
-                    }else{//caculate attention values for the past token                        
-                        auto vc_t_beam_start = vc_token_start + beam_idx[bi][vi] * kv_head * head_size;
-                        auto v_cache_head_start = v_cache_ptr + vc_t_beam_start + kv_hi * head_size;
-                        mul_attenion_weights_and_value_of_head<T>(p[qi][vi], v_cache_head_start, attn_out_start, head_size, false, nullptr);
-                    }                   
+            auto v_cache_head_start =
+                v_cache_ptr + vc_t_beam_start + kv_hi * head_size;
+            auto v_ptr_start = v_ptr +
+                (bi * cur_len + vi - offset) * kv_head * head_size +
+                kv_hi * head_size;
+            mul_attenion_weights_and_value_of_head<VT,float>(
+                attn_w_query_start[vi],
+                v_ptr_start,
+                attn_out_start,
+                head_size,
+                true,
+                v_cache_head_start);
+        } else if (vi < query_ti + offset) { // caculate attention
+                                                    // values for the past
+                                                    // token
+            if (vi >= offset) {
+                auto v_ptr_start = v_ptr +
+                    (bi * cur_len + vi - offset) * kv_head * head_size +
+                    kv_hi * head_size;
+                mul_attenion_weights_and_value_of_head<VT,float>(
+                    attn_w_query_start[vi],
+                    v_ptr_start,
+                    attn_out_start,
+                    head_size,
+                    false,
+                    nullptr);
+            } else {
+                //printf("new_beam_idx[bi][vi]:%d \n", new_beam_idx[bi][vi]);
+                auto vc_t_beam_start =
+                    vc_token_start + new_beam_idx[bi][vi] * kv_head * head_size;
+                if (cur_len > 1) {
+                    auto beam_size = beam_batch / bs;
+                    // printf("beam_size:%d, kv_head: %d, head_size: %d \n",
+                    // beam_size, kv_head, head_size); fflush(stdout);
+                    vc_t_beam_start =
+                        vc_t_beam_start + bi * beam_size * kv_head * head_size;
                 }
+                auto v_cache_head_start =
+                    v_cache_ptr + vc_t_beam_start + kv_hi * head_size;
+                mul_attenion_weights_and_value_of_head<VT, float>(
+                    attn_w_query_start[vi],
+                    v_cache_head_start,
+                    attn_out_start,
+                    head_size,
+                    false,
+                    nullptr);
             }
-        }        
+        }
+       
+        }
+      }
+      // std::cout << "p:" << p << std::endl;
     }
-    return attn_outs;
+  }
 }
-
-/* 
-*The masked self attention for decoder layer with zero-copy of kv_cache
-*@param  query Query embeeding with the of [beam_size*batch, cur_len, head_num, head_size]
-*@param  key Key embeeding with the of [beam_size*batch, cur_len, head_num, head_size]
-*@param  value Value embeeding with the of [beam_size*batch, cur_len, head_num, head_size] -> Todo may be perf is better with [beam_size*batch, cur_len, head_size, head_num]
-*@param  key_cache Cache past key embeeding with the of [max_seq_len, beam_size*batch, cur_len, head_num, head_size], the past key state is (beam_size, 1, head_num, head_size) for every token
-*@param  value_cache Cache past value embeeding with the of [max_seq_len, beam_size*batch, cur_len, head_num, head_size], the past value state is (beam_size, 1, head_num, head_size) for every token
-*@param  beam_idx Cache past beam_idx with the of [max_positions, bs]
-*@param  offset  The length of decoded(past) token. 
-*@return attn_outs, attn_weights
-*/
-template <typename Q_T, typename V_T> 
-std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>  MaskedMHAKernel(
-    at::Tensor query,
-    at::Tensor key,
-    at::Tensor value,
-    at::Tensor& key_cache,
-    at::Tensor& value_cache,
-    at::Tensor&  beam_idx,
-    const int64_t offset, 
-    const float scale_attn,
-    const int64_t max_positions,
-    const c10::optional<at::Tensor>& head_mask/* optional */,
-    const c10::optional<at::Tensor>& attention_mask/* optional */) {
-    //assert(query.size(1) == 1);
-    //beam_idx: [beam_size, offset] for every decoded token, the beam_idx is the target beam idx for the past token
-    //the target beam_idx for the input tokens are always 0
-    //compute the offset info for the past token 
-    //std::cout << "beam_idx:" << beam_idx << std::endl;
-    auto bs = query.size(0);
-    //the targe beam for the past token 
-    auto new_beam_idx = std::vector<std::vector<long>>(bs, std::vector<long>(offset+query.size(1), 0));
-    auto b_ptr = beam_idx.data_ptr<long>();
-    for(auto i = 0; i < bs; i++){
-        new_beam_idx[i][offset-1] = b_ptr[(offset-1) * bs + i];
-        for(auto j = offset-2; j>=0; j--){//for the token of input, the target beam is alwarys 0 
-            new_beam_idx[i][j] = b_ptr[j*bs+new_beam_idx[i][j+1]]; 
+{
+    RECORD_FUNCTION(
+      "ipex::iakv_sdp::reduction_private_result",
+      c10::ArrayRef<c10::IValue>({}));
+#pragma omp parallel for collapse(3)  
+for (auto bi = 0; bi < bs; bi++) {
+    for (auto hi = 0; hi < head_num; hi++) { 
+        for (auto qi = 0;  qi < cur_len; qi++) {
+            for(auto thread_id = 0; thread_id < thread_numbers; thread_id++){
+                if(flag_access[thread_id][bi][hi] == 0){
+                    continue;
+                }
+                auto attn_out_head_stride = thread_id * attn_outs_stride_priv + (bi * head_num + hi) * cur_len * head_size;
+                auto private_attn_out_start =
+                    private_attn_out_ptr + attn_out_head_stride + qi * head_size;
+                auto attn_outs_start = attn_out_ptr + (bi * head_num + hi) * cur_len * head_size + qi * head_size;
+                torch_ipex::cpu::kernel::add_ker<VT, float>(attn_outs_start, private_attn_out_start, head_size);
+            }
         }
+        
     }
-    //std::cout << "new_beam_idx:" << new_beam_idx << std::endl;
-    auto mask = attention_mask.has_value() ? attention_mask.value():at::zeros({bs, 1, query.size(1), key.size(1)}, query.options());
-    assert(head_mask.has_value() == false && "Head mask is not supported in ipex::scale_dot_product_for_indirect_access_kv_cache");
-    auto attn_outs = scale_dot_product_for_indirect_access_kv_cache<Q_T>(query, key, new_beam_idx, key_cache, offset, scale_attn, mask, value, value_cache);
-    return {attn_outs, attn_outs, key_cache, value_cache, beam_idx};   //ToDO just return attn_weights_origin for debug    
+}
+ 
+}
+
+   return std::make_tuple(attn_outs, at::Tensor(), key_cache, value_cache, beam_idx);
 }
 
 std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>  zero_copy_kv_cache_masked_multihead_self_attention_kernel_impl(
@@ -375,77 +622,75 @@ std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>  zero_cop
     const int64_t offset,
     const double scale_attn,
     const int64_t max_positions,
-    const c10::optional<at::Tensor>& head_mask/* optional */,
-    const c10::optional<at::Tensor>& attention_mask/* optional */) {
-    
-    //std::cout << "new_beam_idx:" << new_beam_idx << std::endl;
+    at::Tensor& attention_mask) {      
     assert(key.scalar_type()==at::kBFloat16 || key.scalar_type()==at::kFloat);
-    if (key.scalar_type() == at::kFloat && value.scalar_type() == at::kFloat) {
-        return MaskedMHAKernel<float, float>(query, key, value, key_cache, value_cache, beam_idx, offset, scale_attn, max_positions, head_mask, attention_mask);
-    }else if(key.scalar_type() == at::kFloat && value.scalar_type() == at::kBFloat16){
-        return MaskedMHAKernel<float, at::BFloat16>(query, key, value, key_cache, value_cache, beam_idx, offset, scale_attn, max_positions, head_mask, attention_mask);
+    if (query.scalar_type() == at::kFloat && value.scalar_type() == at::kFloat) {
+        return scale_dot_product_for_indirect_access_kv_cache<float, float>(query, key, value, key_cache, value_cache, beam_idx, offset, scale_attn, attention_mask);
+    }else if(query.scalar_type() == at::kFloat && value.scalar_type() == at::kBFloat16){
+        return scale_dot_product_for_indirect_access_kv_cache<float, at::BFloat16>(query, key, value, key_cache, value_cache, beam_idx, offset, scale_attn, attention_mask);
     }else if(key.scalar_type() == at::kBFloat16 && value.scalar_type() == at::kFloat){
-        return MaskedMHAKernel<at::BFloat16, float>(query, key, value, key_cache, value_cache, beam_idx, offset, scale_attn, max_positions, head_mask, attention_mask);
+        return scale_dot_product_for_indirect_access_kv_cache<at::BFloat16, float>(query, key, value, key_cache, value_cache, beam_idx, offset, scale_attn, attention_mask);
     }
-    return MaskedMHAKernel<at::BFloat16, at::BFloat16>(query, key, value, key_cache, value_cache, beam_idx, offset, scale_attn, max_positions, head_mask, attention_mask);  
+    return scale_dot_product_for_indirect_access_kv_cache<at::BFloat16, at::BFloat16>(query, key, value, key_cache, value_cache, beam_idx, offset, scale_attn, attention_mask);  
 }
 
 std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor> first_token_masked_mha(
     at::Tensor query,
     at::Tensor key,
     at::Tensor value,
-    const int64_t batch_size,
+    at::Tensor& key_cache,
+    at::Tensor& value_cache,
+    at::Tensor& beam_idx,
+    const int64_t beam_batch,
     const double scale_attn,
     int64_t max_positions,
-    const c10::optional<at::Tensor>& head_mask/* optional */,
-    const c10::optional<at::Tensor>& attention_mask/* optional */
+    at::Tensor attention_mask
 ) {
     
+    auto bs = query.size(0);
     auto query_length = query.size(1);
     auto key_lenght = key.size(1);
     auto kv_head_num = key.size(2);
     auto head_size = key.size(3);
-    auto expand_size = batch_size / query.size(0);
     auto casual_mask = at::full({query_length, key_lenght}, -1e6, query.options());
     casual_mask = at::triu(casual_mask, 1);    
     casual_mask = casual_mask.unsqueeze(0).unsqueeze(0);
+    attention_mask = attention_mask + casual_mask;
     if(max_positions < query_length){
         max_positions = query_length + max_positions;
     }
-    //allocate the kv cache buffer for the first token    
-    auto key_cache = at::zeros({max_positions, batch_size, kv_head_num, head_size}, key.options());
-    auto value_cache = at::zeros({max_positions, batch_size, kv_head_num, head_size}, value.options());    
-    //key [batch_size, seq_len, kv_headm_num, head_size]
-    for (auto i = 0; i < query.size(1); i++) {
-        key_cache.select(0, i).copy_(key.select(1, i).repeat_interleave(expand_size, 0));
-        value_cache.select(0, i).copy_(value.select(1, i).repeat_interleave(expand_size, 0));
-    }
-    //allocate beam_idx buffer for the first token
-    auto beam_idx = at::zeros({max_positions, batch_size}, at::kLong);
-    //ToDo surpport MGQ/MQA
+    if(key.scalar_type() != at::kBFloat16 && key.scalar_type() != at::kFloat){
+        TORCH_CHECK(false, "key and value must be float or bfloat16 to use ipex::masked_multihead_self_attention_kernel_impl");
+    }
+    if (key.scalar_type() == at::kFloat) {
+      copy_key_value<float>(key_cache, key, value_cache, value, beam_batch);
+    } else {
+      copy_key_value<at::BFloat16>(
+          key_cache, key, value_cache, value, beam_batch);
+    }
+    //surpport MGQ/MQA
     //expand the head dimensiopn of key/value to be same to the query
     if(query.size(2) != key.size(2)){
         auto n_req = query.size(2) / key.size(2);
         key = key.repeat_interleave(n_req, 2);
         value = value.repeat_interleave(n_req, 2);
-    }    
-    key = key.permute({0, 2, 1, 3});
-    query = query.permute({0, 2, 1, 3});
-    value = value.permute({0, 2, 1, 3});
-    auto attn_weights = query.matmul(key.transpose(-1, -2));
-    auto attn_weights_origin = attn_weights.clone();
-    attn_weights = attn_weights.div(scale_attn);
-    attn_weights = attn_weights + casual_mask;
-    if (attention_mask.has_value()) {
-        attn_weights = attn_weights + attention_mask.value();
-    }
-    attn_weights = attn_weights.softmax(-1);
-    if (head_mask.has_value()) {
-        attn_weights = attn_weights * head_mask.value();
-    }
-    attn_weights = attn_weights.to(value.dtype());
-    auto attn_outputs = attn_weights.matmul(value);
-    return std::make_tuple(attn_outputs, attn_weights, key_cache, value_cache, beam_idx);
+    }
+    auto attn_weights = at::Tensor();
+    if (key.scalar_type() == at::kBFloat16) {
+        auto attn_outputs = torch_ipex::cpu::flash_attention_kernel_stub(kCPU, query, key, value, scale_attn, attention_mask);
+        return std::make_tuple(attn_outputs, attn_weights, key_cache, value_cache, beam_idx);
+    } else {
+        key = key.permute({0, 2, 1, 3});
+        query = query.permute({0, 2, 1, 3});
+        value = value.permute({0, 2, 1, 3});
+        auto attn_weights = query.matmul(key.transpose(-1, -2));
+        attn_weights = attn_weights.div(scale_attn);
+        attn_weights = attn_weights + attention_mask;
+        attn_weights = attn_weights.softmax(-1);
+        attn_weights = attn_weights.to(value.dtype());
+        auto attn_outputs = attn_weights.matmul(value);
+        return std::make_tuple(attn_outputs, attn_weights, key_cache, value_cache, beam_idx);
+    }
 }
 std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>  masked_multihead_self_attention_kernel_impl(
     at::Tensor query,
@@ -459,31 +704,94 @@ std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>  masked_m
     int64_t max_positions,
     const c10::optional<at::Tensor>& head_mask/* optional */,
     const c10::optional<at::Tensor>& attention_mask/* optional */) {  
-    auto bs = beam_idx.size(1);//need to prepare the fake beam_idx as (max_position, bs) for the first token      
+    if(attention_mask.has_value() == false){
+        TORCH_CHECK(false, "Attention mask is neccessary for ipex::masked_multihead_self_attention_kernel_impl");
+    }
+    if(attention_mask.value().dim() != 4){
+        TORCH_CHECK(false, "Attention mask must be 4D for ipex::masked_multihead_self_attention_kernel_impl");
+    }
+    if(head_mask.has_value() == true){
+        TORCH_CHECK(false, "Head mask is not supported in ipex::masked_multihead_self_attention_kernel_impl");
+    }
+    if (query.dtype() != key.dtype()) {
+        TORCH_CHECK(false, "query and key must have the same data type to use ipex::masked_multihead_self_attention_kernel_impl");
+    }
+    query = query.contiguous();
+    key = key.contiguous();
+    value = value.contiguous();
+    auto attention_mask_v = attention_mask.value().contiguous();
+    attention_mask_v = attention_mask_v.to(query.dtype());
+    auto beam_batch = beam_idx.size(1);//need to prepare the fake beam_idx as (max_position, bs) for the first token      
     auto offset = seq_info.data_ptr<long>()[0];
     auto cache_size = key_cache.size(0);
     auto cur_len = query.size(1);
-    if(offset > 0 && offset + cur_len > cache_size) {
-        auto new_cache_size = cache_size * 2;
-        auto new_key_cache = at::zeros({new_cache_size, bs, key.size(2), key.size(3)}, key.options());
-        auto new_value_cache = at::zeros({new_cache_size, bs, value.size(2), value.size(3)}, value.options());
-        auto new_beam_idx = at::zeros({new_cache_size, bs}, beam_idx.options());
-        new_key_cache.slice(0, 0, cache_size).copy_(key_cache);
-        new_value_cache.slice(0, 0, cache_size).copy_(value_cache);
-        new_beam_idx.slice(0, 0, cache_size).copy_(beam_idx);
-        key_cache = new_key_cache;
-        value_cache = new_value_cache;
-        beam_idx = new_beam_idx;
+    if (offset == 0) {
+      max_positions =
+          max_positions > cur_len ? max_positions : max_positions + cur_len;
+      key_cache = at::empty(
+          {max_positions, beam_batch, key.size(2), key.size(3)}, key.options());
+      value_cache = at::empty(
+          {max_positions, beam_batch, value.size(2), value.size(3)}, value.options());
+      beam_idx = at::empty({max_positions, beam_batch}, beam_idx.options());
+      auto beam_idx_access = beam_idx.accessor<long, 2>();
+      for (auto i = 0; i < max_positions; i++){
+          for (auto j = 0; j < beam_batch; j++){
+              if(key.size(0) == beam_batch){
+                 beam_idx_access[i][j] = j;
+              }else{
+                 auto beam_size = beam_batch / key.size(0);
+                 beam_idx_access[i][j] = j / beam_size * beam_size;
+              }
+            }
+       }
+    } else if (offset > 0 && offset + cur_len > cache_size) {
+      auto new_cache_size = cache_size * 2;
+      auto new_key_cache = at::zeros(
+          {new_cache_size, beam_batch, key.size(2), key.size(3)}, key.options());
+      auto new_value_cache = at::zeros(
+          {new_cache_size, beam_batch, value.size(2), value.size(3)}, value.options());
+      auto new_beam_idx = at::zeros({new_cache_size, beam_batch}, beam_idx.options());
+      new_key_cache.slice(0, 0, cache_size).copy_(key_cache);
+      new_value_cache.slice(0, 0, cache_size).copy_(value_cache);
+      new_beam_idx.slice(0, 0, cache_size).copy_(beam_idx);
+      auto new_beam_idx_access = new_beam_idx.accessor<long, 2>();
+      auto beam_idx_access = beam_idx.accessor<long, 2>();
+      for (auto i = offset; i < new_cache_size; i++){
+          for (auto j = 0; j < beam_batch; j++){
+              new_beam_idx_access[i][j] = beam_idx_access[0][j];
+            }
+      }
+      key_cache = new_key_cache;
+      value_cache = new_value_cache;
+      beam_idx = new_beam_idx;
     }
     if(offset > 0){
-        return zero_copy_kv_cache_masked_multihead_self_attention_kernel_impl(query, key, value, key_cache, value_cache, beam_idx, offset, scale_attn, max_positions, head_mask, attention_mask);
+      return zero_copy_kv_cache_masked_multihead_self_attention_kernel_impl(
+          query,
+          key,
+          value,
+          key_cache,
+          value_cache,
+          beam_idx,
+          offset,
+          scale_attn,
+          max_positions,
+          attention_mask_v);
     }else{
-        return first_token_masked_mha(query, key, value, bs, scale_attn, max_positions, head_mask, attention_mask);
+      return first_token_masked_mha(
+          query,
+          key,
+          value,
+          key_cache,
+          value_cache,
+          beam_idx,
+          beam_batch,
+          scale_attn,
+          max_positions,
+          attention_mask_v);
     }
     
 }
-
-
 } // anonymous namespace
 
 REGISTER_DISPATCH(masked_multihead_self_attention_kernel_stub, &masked_multihead_self_attention_kernel_impl);
diff --git a/csrc/cpu/aten/kernels/TPPGEMMKrnl.cpp b/csrc/cpu/aten/kernels/TPPGEMMKrnl.cpp
index 733dd48ad..d70ac1f77 100644
--- a/csrc/cpu/aten/kernels/TPPGEMMKrnl.cpp
+++ b/csrc/cpu/aten/kernels/TPPGEMMKrnl.cpp
@@ -12,7 +12,7 @@ namespace cpu {
 
 namespace {
 
-at::Tensor fc_plain_kernel_impl(
+at::Tensor tpp_linear_bias_kernel_impl(
     at::Tensor& t_in,
     at::Tensor& t_wt,
     at::Tensor& t_bias) {
@@ -21,41 +21,70 @@ at::Tensor fc_plain_kernel_impl(
   sizes[2] = wt_sizes[0] * wt_sizes[3];
 
   auto t_out = t_in.new_empty(sizes);
-  // std::cout << "YYY " << t_out.dtype() << "  " << t_in.dtype() << std::endl;
   auto dt = t_wt.dtype();
   if (dt == at::kFloat) {
-    torch_ipex::tpp::fc_plain<float>(t_in, t_wt, t_bias, t_out);
+    torch_ipex::tpp::tpp_linear_bias<float>(t_in, t_wt, t_bias, t_out);
   } else if (dt == at::kBFloat16) {
-    torch_ipex::tpp::fc_plain<at::BFloat16>(t_in, t_wt, t_bias, t_out);
+    torch_ipex::tpp::tpp_linear_bias<at::BFloat16>(t_in, t_wt, t_bias, t_out);
   } else {
-    AT_ASSERT(0, "Should not come here %s:%d\n", __FILE__, __LINE__);
+    AT_ASSERT(
+        0,
+        "TPP does not support current weight dtype %s:%d\n",
+        __FILE__,
+        __LINE__);
   }
 
   return t_out;
 }
 
-at::Tensor fc_out_kernel_impl(
+at::Tensor tpp_linear_nobias_kernel_impl(at::Tensor& t_in, at::Tensor& t_wt) {
+  auto sizes = t_in.sizes().vec();
+  auto wt_sizes = t_wt.sizes();
+  sizes[2] = wt_sizes[0] * wt_sizes[3];
+
+  auto t_out = t_in.new_empty(sizes);
+
+  auto dt = t_wt.dtype();
+  if (dt == at::kFloat) {
+    torch_ipex::tpp::tpp_linear_no_bias<float>(t_in, t_wt, t_out);
+  } else if (dt == at::kBFloat16) {
+    torch_ipex::tpp::tpp_linear_no_bias<at::BFloat16>(t_in, t_wt, t_out);
+  } else {
+    AT_ASSERT(
+        0,
+        "TPP does not support current weight dtype %s:%d\n",
+        __FILE__,
+        __LINE__);
+  }
+  return t_out;
+}
+
+at::Tensor tpp_linear_gelu_kernel_impl(
     at::Tensor& t_in,
-    at::Tensor& t_in1,
-    at::Tensor& t_in2,
     at::Tensor& t_wt,
-    at::Tensor& t_bias,
-    double scale) {
-  auto t_out = at::empty_like(t_in1);
+    at::Tensor& t_bias) {
+  auto sizes = t_in.sizes().vec();
+  auto wt_sizes = t_wt.sizes();
+  sizes[2] = wt_sizes[0] * wt_sizes[3];
+
+  auto t_out = t_in.new_empty(sizes);
+
   auto dt = t_wt.dtype();
   if (dt == at::kFloat) {
-    torch_ipex::tpp::fc_out<float>(
-        t_in, t_in1, t_in2, t_wt, t_bias, t_out, scale);
+    torch_ipex::tpp::tpp_linear_gelu<float>(t_in, t_wt, t_bias, t_out);
   } else if (dt == at::kBFloat16) {
-    torch_ipex::tpp::fc_out<at::BFloat16>(
-        t_in, t_in1, t_in2, t_wt, t_bias, t_out, scale);
+    torch_ipex::tpp::tpp_linear_gelu<at::BFloat16>(t_in, t_wt, t_bias, t_out);
   } else {
-    AT_ASSERT(0, "Should not come here %s:%d\n", __FILE__, __LINE__);
+    AT_ASSERT(
+        0,
+        "TPP does not support current weight dtype %s:%d\n",
+        __FILE__,
+        __LINE__);
   }
   return t_out;
 }
 
-at::Tensor fc_in_kernel_impl(
+at::Tensor tpp_linear_silu_kernel_impl(
     at::Tensor& t_in,
     at::Tensor& t_wt,
     at::Tensor& t_bias) {
@@ -67,16 +96,23 @@ at::Tensor fc_in_kernel_impl(
 
   auto dt = t_wt.dtype();
   if (dt == at::kFloat) {
-    torch_ipex::tpp::fc_in<float>(t_in, t_wt, t_bias, t_out);
+    torch_ipex::tpp::tpp_linear_silu<float>(t_in, t_wt, t_bias, t_out);
   } else if (dt == at::kBFloat16) {
-    torch_ipex::tpp::fc_in<at::BFloat16>(t_in, t_wt, t_bias, t_out);
+    torch_ipex::tpp::tpp_linear_silu<at::BFloat16>(t_in, t_wt, t_bias, t_out);
   } else {
-    AT_ASSERT(0, "Should not come here %s:%d\n", __FILE__, __LINE__);
+    AT_ASSERT(
+        0,
+        "TPP does not support current weight dtype %s:%d\n",
+        __FILE__,
+        __LINE__);
   }
   return t_out;
 }
 
-at::Tensor qkv_kernel_impl(at::Tensor& t_in, at::Tensor& t_wt) {
+at::Tensor tpp_linear_relu_kernel_impl(
+    at::Tensor& t_in,
+    at::Tensor& t_wt,
+    at::Tensor& t_bias) {
   auto sizes = t_in.sizes().vec();
   auto wt_sizes = t_wt.sizes();
   sizes[2] = wt_sizes[0] * wt_sizes[3];
@@ -85,20 +121,103 @@ at::Tensor qkv_kernel_impl(at::Tensor& t_in, at::Tensor& t_wt) {
 
   auto dt = t_wt.dtype();
   if (dt == at::kFloat) {
-    torch_ipex::tpp::qkv_gemm<float>(t_in, t_wt, t_out);
+    torch_ipex::tpp::tpp_linear_relu<float>(t_in, t_wt, t_bias, t_out);
+  } else if (dt == at::kBFloat16) {
+    torch_ipex::tpp::tpp_linear_relu<at::BFloat16>(t_in, t_wt, t_bias, t_out);
+  } else {
+    AT_ASSERT(
+        0,
+        "TPP does not support current weight dtype %s:%d\n",
+        __FILE__,
+        __LINE__);
+  }
+  return t_out;
+}
+
+at::Tensor tpp_linear_add_add_kernel_impl(
+    at::Tensor& t_in,
+    at::Tensor& t_in1,
+    at::Tensor& t_in2,
+    at::Tensor& t_wt,
+    at::Tensor& t_bias,
+    double scale) {
+  auto t_out = at::empty_like(t_in1);
+  auto dt = t_wt.dtype();
+  if (dt == at::kFloat) {
+    torch_ipex::tpp::tpp_linear_add_add<float>(
+        t_in, t_in1, t_in2, t_wt, t_bias, t_out, scale);
+  } else if (dt == at::kBFloat16) {
+    torch_ipex::tpp::tpp_linear_add_add<at::BFloat16>(
+        t_in, t_in1, t_in2, t_wt, t_bias, t_out, scale);
+  } else {
+    AT_ASSERT(
+        0,
+        "TPP does not support current weight dtype %s:%d\n",
+        __FILE__,
+        __LINE__);
+  }
+  return t_out;
+}
+
+at::Tensor tpp_linear_add_kernel_impl(
+    at::Tensor& t_in,
+    at::Tensor& t_in1,
+    at::Tensor& t_wt,
+    at::Tensor& t_bias,
+    double scale) {
+  auto t_out = at::empty_like(t_in1);
+  auto dt = t_wt.dtype();
+  if (dt == at::kFloat) {
+    torch_ipex::tpp::tpp_linear_add<float>(
+        t_in, t_in1, t_wt, t_bias, t_out, scale);
+  } else if (dt == at::kBFloat16) {
+    torch_ipex::tpp::tpp_linear_add<at::BFloat16>(
+        t_in, t_in1, t_wt, t_bias, t_out, scale);
+  } else {
+    AT_ASSERT(
+        0,
+        "TPP does not support current weight dtype %s:%d\n",
+        __FILE__,
+        __LINE__);
+  }
+  return t_out;
+}
+
+at::Tensor tpp_linear_mul_kernel_impl(
+    at::Tensor& t_in,
+    at::Tensor& t_in1,
+    at::Tensor& t_wt,
+    at::Tensor& t_bias) {
+  auto t_out = at::empty_like(t_in1);
+  auto dt = t_wt.dtype();
+  if (dt == at::kFloat) {
+    torch_ipex::tpp::tpp_linear_mul<float>(t_in, t_in1, t_wt, t_bias, t_out);
   } else if (dt == at::kBFloat16) {
-    torch_ipex::tpp::qkv_gemm<at::BFloat16>(t_in, t_wt, t_out);
+    torch_ipex::tpp::tpp_linear_mul<at::BFloat16>(
+        t_in, t_in1, t_wt, t_bias, t_out);
   } else {
-    AT_ASSERT(0, "Should not come here %s:%d\n", __FILE__, __LINE__);
+    AT_ASSERT(
+        0,
+        "TPP does not support current weight dtype %s:%d\n",
+        __FILE__,
+        __LINE__);
   }
   return t_out;
 }
 
 } // namespace
 
-REGISTER_DISPATCH(fc_plain_kernel_stub, &fc_plain_kernel_impl);
-REGISTER_DISPATCH(fc_in_kernel_stub, &fc_in_kernel_impl);
-REGISTER_DISPATCH(fc_out_kernel_stub, &fc_out_kernel_impl);
-REGISTER_DISPATCH(qkv_kernel_stub, &qkv_kernel_impl);
+REGISTER_DISPATCH(
+    tpp_linear_nobias_kernel_stub,
+    &tpp_linear_nobias_kernel_impl);
+REGISTER_DISPATCH(tpp_linear_bias_kernel_stub, &tpp_linear_bias_kernel_impl);
+REGISTER_DISPATCH(tpp_linear_gelu_kernel_stub, &tpp_linear_gelu_kernel_impl);
+REGISTER_DISPATCH(tpp_linear_relu_kernel_stub, &tpp_linear_relu_kernel_impl);
+REGISTER_DISPATCH(tpp_linear_silu_kernel_stub, &tpp_linear_silu_kernel_impl);
+REGISTER_DISPATCH(tpp_linear_mul_kernel_stub, &tpp_linear_mul_kernel_impl);
+REGISTER_DISPATCH(tpp_linear_add_kernel_stub, &tpp_linear_add_kernel_impl);
+REGISTER_DISPATCH(
+    tpp_linear_add_add_kernel_stub,
+    &tpp_linear_add_add_kernel_impl);
 } // namespace cpu
 } // namespace torch_ipex
diff --git a/csrc/cpu/jit/fusion_pass.cpp b/csrc/cpu/jit/fusion_pass.cpp
index a9699203e..fee1cf3e0 100644
--- a/csrc/cpu/jit/fusion_pass.cpp
+++ b/csrc/cpu/jit/fusion_pass.cpp
@@ -195,6 +195,8 @@ void IPEXFusionPass(std::shared_ptr<Graph>& graph) {
   graph_rewrite::replaceAtenBatchNormWithIpexBatchNorm(graph);
   // TODO: Some post processing?? ECS/EDC/Peephole???
 
+  graph_rewrite::simplifyAllReduce(graph);
+
   // This path contains two functions:
   // 1. Fuse BF16 Mha for ViT because ViT has a special QKV split algorithm
   // 2. Replace the Matmul OP with MKL or DNNL Matmul kernels to enable
diff --git a/csrc/cpu/jit/passes/graph_rewrite.cpp b/csrc/cpu/jit/passes/graph_rewrite.cpp
index 1730e5833..7619279e5 100644
--- a/csrc/cpu/jit/passes/graph_rewrite.cpp
+++ b/csrc/cpu/jit/passes/graph_rewrite.cpp
@@ -1239,6 +1239,58 @@ void replaceAtenMaxPool2dWithIpexMaxPool2d(std::shared_ptr<Graph>& graph) {
   rewriter_max_pool2d.runOnGraph(graph, filter);
 }
 
+void simplifyAllReduce(std::shared_ptr<Graph>& graph) {
+  std::string all_reduce_v1 = R"(
+    graph(%a, %weight, %reduceop, %tag, %ranks, %group_size, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha, %idx, %no, %dtype, %zero):
+      %r1 = torch_ipex::tpp_linear(%a, %weight)
+      %r2 = deepspeed_comm::all_reduce(%r1, %reduceop, %tag, %ranks, %group_size)
+      %r3 = torch_ipex::tpp_linear_gelu(%b, %fc_in_weight, %fc_in_bias)
+      %r4 = aten::to(%r3, %idx, %no, %no, %dtype)
+      %r5 = aten::contiguous(%r4, %zero)
+      %r6 = torch_ipex::tpp_linear(%r5, %fc_out_weight)
+      %r7 = deepspeed_comm::all_reduce(%r6, %reduceop, %tag, %ranks, %group_size)
+      %r8 = aten::add_(%r7, %fc_out_bias, %alpha)
+      %r = aten::add(%r2, %r8, %alpha)
+      return (%r) )";
+  std::string all_reduce_repl_v1 = R"(
+    graph(%a, %weight, %reduceop, %tag, %ranks, %group_size, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha, %idx, %no, %dtype, %zero):
+      %r1 = torch_ipex::tpp_linear(%a, %weight)
+      %r2 = torch_ipex::tpp_linear_gelu(%b, %fc_in_weight, %fc_in_bias)
+      %r3 = aten::to(%r2, %idx, %no, %no, %dtype)
+      %r4 = aten::contiguous(%r3, %zero)
+      %r5 = torch_ipex::tpp_linear(%r4, %fc_out_weight)
+      %r6 = aten::add(%r1, %r5, %alpha)
+      %r7 = deepspeed_comm::all_reduce(%r6, %reduceop, %tag, %ranks, %group_size)
+      %r = aten::add_(%r7, %fc_out_bias, %alpha)
+      return (%r) )";
+
+  std::string all_reduce_v2 = R"(
+    graph(%a, %weight, %reduceop, %tag, %ranks, %group_size, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha):
+      %r1 = ipex_prepack::linear_run(%a, %weight)
+      %r2 = deepspeed_comm::all_reduce(%r1, %reduceop, %tag, %ranks, %group_size)
+      %r3 = ipex_prepack::linear_gelu_run(%b, %fc_in_weight, %fc_in_bias)
+      %r4 = ipex_prepack::linear_run(%r3, %fc_out_weight)
+      %r5 = deepspeed_comm::all_reduce(%r4, %reduceop, %tag, %ranks, %group_size)
+      %r6 = aten::add_(%r5, %fc_out_bias, %alpha)
+      %r = aten::add(%r2, %r6, %alpha)
+      return (%r) )";
+  std::string all_reduce_repl_v2 = R"(
+    graph(%a, %weight, %reduceop, %tag, %ranks, %group_size, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha):
+      %r1 = ipex_prepack::linear_run(%a, %weight)
+      %r2 = ipex_prepack::linear_gelu_run(%b, %fc_in_weight, %fc_in_bias)
+      %r3 = ipex_prepack::linear_run(%r2, %fc_out_weight)
+      %r4 = aten::add(%r1, %r3, %alpha)
+      %r5 = deepspeed_comm::all_reduce(%r4, %reduceop, %tag, %ranks, %group_size)
+      %r = aten::add_(%r5, %fc_out_bias, %alpha)
+      return (%r) )";
+
+  SubgraphRewriter rewriter_v1, rewriter_v2;
+  rewriter_v1.RegisterRewritePattern(all_reduce_v1, all_reduce_repl_v1);
+  rewriter_v2.RegisterRewritePattern(all_reduce_v2, all_reduce_repl_v2);
+  rewriter_v1.runOnGraph(graph);
+  rewriter_v2.runOnGraph(graph);
+}
+
 } // namespace graph_rewrite
 } // namespace jit
 } // namespace torch_ipex
diff --git a/csrc/cpu/jit/passes/graph_rewrite.h b/csrc/cpu/jit/passes/graph_rewrite.h
index 40b6cf248..1ac984317 100644
--- a/csrc/cpu/jit/passes/graph_rewrite.h
+++ b/csrc/cpu/jit/passes/graph_rewrite.h
@@ -38,7 +38,7 @@ void replaceInteractionWithQInteraction(
 void preprocessSizeForQLstm(std::shared_ptr<torch::jit::Graph>& graph);
 void replaceLstmWithQLstm(std::shared_ptr<torch::jit::Graph>& graph);
 void replaceAddWithQAdd(std::shared_ptr<torch::jit::Graph>& graph);
-
+void simplifyAllReduce(std::shared_ptr<torch::jit::Graph>& graph);
 void replaceFrozenIPEXConvWithAtenConv(
     std::shared_ptr<torch::jit::Graph>& graph);
 void replaceFrozenIPEXLinearWithAtenLinear(
diff --git a/csrc/cpu/tpp/kernels/TPPGEMMKrnl.h b/csrc/cpu/tpp/kernels/TPPGEMMKrnl.h
index 8a7a75e61..43d1bdb88 100644
--- a/csrc/cpu/tpp/kernels/TPPGEMMKrnl.h
+++ b/csrc/cpu/tpp/kernels/TPPGEMMKrnl.h
@@ -23,16 +23,85 @@ static int NCB_BLOCK_SIZE = env2int("NCB_BLOCK_SIZE", 64);
 static const char* GEMM_LOOP_SCHEME =
     getenv("GEMM_LOOP_SCHEME") ? getenv("GEMM_LOOP_SCHEME") : "aCB";
 
-REGISTER_LOCAL_SCOPE(pln_gemm, "pln_gemm"); // linear bias
-REGISTER_LOCAL_SCOPE(qkv_gemm, "qkv_gemm"); //  linear no bias
-
-REGISTER_LOCAL_SCOPE(o_gemm, "o_gemm"); // linear bias + add + add
-REGISTER_LOCAL_SCOPE(i_gemm, "i_gemm"); // linear bias + gelu
+REGISTER_LOCAL_SCOPE(
+    tpp_linear_krnl,
+    "tpp_linear_krnl"); //  linear W/ and W/O bias
+REGISTER_LOCAL_SCOPE(
+    tpp_linear_add_add_krnl,
+    "tpp_linear_add_add_krnl"); // linear bias + add + add
+REGISTER_LOCAL_SCOPE(
+    tpp_linear_gelu_krnl,
+    "tpp_linear_gelu_krnl"); // linear bias + gelu
+
+REGISTER_LOCAL_SCOPE(
+    tpp_linear_mul_krnl,
+    "tpp_linear_mul_krnl"); // linear bias + mul
+REGISTER_LOCAL_SCOPE(
+    tpp_linear_add_krnl,
+    "tpp_linear_add_krnl"); // linear bias + add
+REGISTER_LOCAL_SCOPE(
+    tpp_linear_silu_krnl,
+    "tpp_linear_silu_krnl"); // linear bias + silu
+REGISTER_LOCAL_SCOPE(
+    tpp_linear_relu_krnl,
+    "tpp_linear_relu_krnl"); // linear bias + relu
 
 REGISTER_LOCAL_SCOPE(fftkn, "fftkn");
 
 template <typename T>
-inline void fc_plain(
+inline at::Tensor wt_tensor_for_first_token(at::Tensor& t) {
+  RECORD_SCOPE(fftkn, {t});
+  auto dim = t.dim();
+  if (dim < 5)
+    return t;
+  auto sizes = t.sizes();
+  constexpr long RBS = 2;
+  auto K1 = sizes[0];
+  if (K1 % RBS != 0)
+    return t;
+  auto C1 = sizes[1];
+  auto C2 = sizes[2];
+  auto K2 = sizes[3];
+  auto C3 = sizes[4];
+#if 0
+  auto t_new = t.view({K1/RBS, RBS, C1, C2, K2, C3}).permute({0, 2, 3, 1, 4, 5}).contiguous().view({K1/RBS, C1, C2, RBS*K2, C3});
+#else
+  auto t_new = t.new_empty({K1 / RBS, C1, C2, RBS * K2, C3});
+  auto in = GetVLAPtr<T>(t, {RBS, C1, C2, K2 * C3});
+  auto out = GetVLAPtr<T>(t_new, {C1, C2, RBS, K2 * C3});
+
+#if 1
+  auto cpy_tpp =
+      SCOPEIT(CpyTPP<T>(C2, K2 * C3, K2 * C3, RBS * K2 * C3), EW_COPY);
+
+#pragma omp parallel for collapse(2)
+  for (int i = 0; i < K1 / RBS; i++) {
+    for (int j = 0; j < C1; j++) {
+      for (int k = 0; k < RBS; k++) {
+        cpy_tpp(in[i][k][j][0], out[i][j][0][k]);
+      }
+    }
+  }
+#else
+  auto cpy_tpp =
+      SCOPEIT(CpyTPP<T>(RBS, K2 * C3, C1 * C2 * K2 * C3, K2 * C3), EW_COPY);
+
+#pragma omp parallel for collapse(2)
+  for (int i = 0; i < K1 / RBS; i++) {
+    for (int j = 0; j < C1; j++) {
+      for (int k = 0; k < C2; k++) {
+        cpy_tpp(in[i][0][j][k], out[i][j][k][0]);
+      }
+    }
+  }
+#endif
+
+#endif
+  return t_new;
+}
+
+template <typename T>
+inline void tpp_linear_bias(
     at::Tensor& t_in,
     at::Tensor& t_wt,
     at::Tensor& t_bias,
@@ -78,7 +147,7 @@ inline void fc_plain(
       (BrgemmTPP<T, T>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
 
   {
-    RECORD_SCOPE(pln_gemm, {t_in, t_wt_V});
+    RECORD_SCOPE(tpp_linear_krnl, {t_in, t_wt_V});
     // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
     auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
     auto ogemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
@@ -114,60 +183,164 @@ inline void fc_plain(
   }
 }
 
-template <typename T>
-inline at::Tensor wt_tensor_for_first_token(at::Tensor& t) {
-  RECORD_SCOPE(fftkn, {t});
-  auto dim = t.dim();
-  if (dim < 5)
-    return t;
-  auto sizes = t.sizes();
-  constexpr long RBS = 2;
-  auto K1 = sizes[0];
-  if (K1 % RBS != 0)
-    return t;
-  auto C1 = sizes[1];
-  auto C2 = sizes[2];
-  auto K2 = sizes[3];
-  auto C3 = sizes[4];
-#if 0
-  auto t_new = t.view({K1/RBS, RBS, C1, C2, K2, C3}).permute({0, 2, 3, 1, 4, 5}).contiguous().view({K1/RBS, C1, C2, RBS*K2, C3});
-#else
-  auto t_new = t.new_empty({K1 / RBS, C1, C2, RBS * K2, C3});
-  auto in = GetVLAPtr<T>(t, {RBS, C1, C2, K2 * C3});
-  auto out = GetVLAPtr<T>(t_new, {C1, C2, RBS, K2 * C3});
+template <typename T, typename Tout = T>
+inline void tpp_linear_no_bias(
+    at::Tensor& t_in,
+    at::Tensor& t_wt,
+    at::Tensor& t_out) {
+  auto in_sizes = t_in.sizes();
+  auto BS = in_sizes[0] * in_sizes[1];
+  if (BS > FT_OPT_SIZE) { // first token compute
+    t_wt = wt_tensor_for_first_token<T>(t_wt);
+  }
+  auto wt_sizes = t_wt.sizes();
+  auto C = in_sizes[2];
 
-#if 1
-  auto cpy_tpp =
-      SCOPEIT(CpyTPP<T>(C2, K2 * C3, K2 * C3, RBS * K2 * C3), EW_COPY);
+  auto Nc = wt_sizes[1];
+  auto Hc = C / Nc;
+  auto Nk = wt_sizes[0];
+  auto Hk = wt_sizes[3];
+  auto K = Nk * Hk;
+  auto t_wt_V = torch_ipex::tpp::wt_tensor_for_fwd(Nk, Hk, Nc, Hc, t_wt);
 
-#pragma omp parallel for collapse(2)
-  for (int i = 0; i < K1 / RBS; i++) {
-    for (int j = 0; j < C1; j++) {
-      for (int k = 0; k < RBS; k++) {
-        cpy_tpp(in[i][k][j][0], out[i][j][0][k]);
-      }
-    }
+  auto in = GetVLAPtr<T>(t_in, {Nc, Hc});
+  auto wt_V = GetVLAPtr<T>(t_wt_V, {Nc, Hc * Hk});
+  auto out = GetVLAPtr<Tout>(t_out, {Nk, Hk});
+
+  auto Ncb = Nc;
+  auto BSb = 64L;
+  auto rem = BS % BSb;
+  if (large_cache_opt)
+    Ncb = NCB_BLOCK_SIZE;
+
+  auto zero_tpp = SCOPEIT(SetZeroTPP<Tout>(BSb, Hk, K), EW_ZERO);
+  auto zero_tpp_rem = SCOPEIT(SetZeroTPP<Tout>(rem, Hk, K), EW_ZERO);
+  auto brgemm_tpp = SCOPEITGEMM(
+      (BrgemmTPP<T, Tout>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
+  auto brgemm_tpp_rem = SCOPEITGEMM(
+      (BrgemmTPP<T, Tout>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
+
+  {
+    RECORD_SCOPE(tpp_linear_krnl, {t_in, t_wt_V});
+    // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
+    auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
+    auto gemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
+        {{0, Nc, Ncb, false}, {0, BS, BSb}, {Nk}}, loop_scheme);
+    gemm_loop(
+        [&](int* ind) {
+          int nc = ind[0], s1 = ind[1], nk = ind[2];
+          auto count = nc + Ncb < Nc ? Ncb : Nc - nc;
+          bool is_rem = (s1 + BSb > BS);
+          if (!is_rem) {
+            if (nc == 0) {
+              zero_tpp(out[s1][nk]);
+            }
+            brgemm_tpp(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, true);
+          } else {
+            if (nc == 0) {
+              zero_tpp_rem(out[s1][nk]);
+            }
+            brgemm_tpp_rem(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, false);
+            brgemm_tpp.config();
+          }
+        },
+        [&]() { brgemm_tpp.config(); },
+        [&]() { brgemm_tpp.release(); });
   }
-#else
-  auto cpy_tpp =
-      SCOPEIT(CpyTPP<T>(RBS, K2 * C3, C1 * C2 * K2 * C3, K2 * C3), EW_COPY);
+}
 
-#pragma omp parallel for collapse(2)
-  for (int i = 0; i < K1 / RBS; i++) {
-    for (int j = 0; j < C1; j++) {
-      for (int k = 0; k < C2; k++) {
-        cpy_tpp(in[i][0][j][k], out[i][j][k][0]);
-      }
-    }
+template <typename T>
+inline void tpp_linear_mul(
+    at::Tensor t_in,
+    at::Tensor t_in1,
+    at::Tensor t_wt,
+    at::Tensor t_bias,
+    at::Tensor t_out) {
+  auto in_sizes = t_in.sizes();
+  auto BS = in_sizes[0] * in_sizes[1];
+  if (BS > FT_OPT_SIZE) { // first token compute
+    t_wt = wt_tensor_for_first_token<T>(t_wt);
   }
-#endif
+  auto wt_sizes = t_wt.sizes();
+  auto C = in_sizes[2];
 
-#endif
-  return t_new;
+  auto Nc = wt_sizes[1];
+  auto Hc = C / Nc;
+  auto Nk = wt_sizes[0];
+  auto Hk = wt_sizes[3];
+  auto K = Nk * Hk;
+
+  auto t_wt_V = torch_ipex::tpp::wt_tensor_for_fwd(Nk, Hk, Nc, Hc, t_wt);
+
+  auto in = GetVLAPtr<T>(t_in, {Nc, Hc});
+  auto in1 = GetVLAPtr<T>(t_in1, {Nk, Hk});
+  auto wt_V = GetVLAPtr<T>(t_wt_V, {Nc, Hc * Hk});
+  auto bias = GetVLAPtr<T>(t_bias, {Hk});
+  auto out = GetVLAPtr<T>(t_out, {Nk, Hk});
+
+  auto Ncb = Nc;
+  auto BSb = 64L;
+  auto rem = BS % 64;
+  if (large_cache_opt)
+    Ncb = NCB_BLOCK_SIZE;
+
+  bool with_bias = (t_bias.numel() > 0);
+  auto copy_bias_tpp = SCOPEIT(CpyBiasTPP<T>(BSb, Hk, K), BIAS);
+  auto copy_bias_tpp_rem = SCOPEIT(CpyBiasTPP<T>(rem, Hk, K), BIAS);
+  auto zero_tpp = SCOPEIT(SetZeroTPP<T>(BSb, Hk, K), EW_ZERO);
+  auto zero_tpp_rem = SCOPEIT(SetZeroTPP<T>(rem, Hk, K), EW_ZERO);
+  auto brgemm_tpp = SCOPEITGEMM(
+      (BrgemmTPP<T, T>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
+  auto brgemm_tpp_rem = SCOPEITGEMM(
+      (BrgemmTPP<T, T>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
+  auto mul_tpp = SCOPEIT((MulTPP<T, T>(BSb, Hk, K, K)), EW_MUL);
+  auto mul_tpp_rem = SCOPEIT((MulTPP<T, T>(rem, Hk, K, K)), EW_MUL);
+
+  {
+    RECORD_SCOPE(tpp_linear_mul_krnl, {t_in, t_wt_V});
+    // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
+    auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
+    auto ogemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
+        {{0, Nc, Ncb, false}, {0L, BS, BSb}, {Nk}}, loop_scheme);
+    ogemm_loop(
+        [&](int* ind) {
+          int nc = ind[0], s1 = ind[1], nk = ind[2];
+          auto count = nc + Ncb < Nc ? Ncb : Nc - nc;
+          bool is_rem = (s1 + BSb > BS);
+          if (!is_rem) {
+            if (nc == 0) {
+              if (with_bias) {
+                copy_bias_tpp(bias[nk], out[s1][nk]);
+              } else {
+                zero_tpp(out[s1][nk]);
+              }
+            }
+            brgemm_tpp(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, true);
+            if (!(nc + Ncb < Nc)) { // last nc iter
+              mul_tpp(in1[s1][nk], out[s1][nk], out[s1][nk]);
+            }
+          } else {
+            if (nc == 0) {
+              if (with_bias) {
+                copy_bias_tpp_rem(bias[nk], out[s1][nk]);
+              } else {
+                zero_tpp_rem(out[s1][nk]);
+              }
+            }
+            brgemm_tpp_rem(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, false);
+            brgemm_tpp.config();
+            if (!(nc + Ncb < Nc)) { // last nc iter
+              mul_tpp_rem(in1[s1][nk], out[s1][nk], out[s1][nk]);
+            }
+          }
+        },
+        [&]() { brgemm_tpp.config(); },
+        [&]() { brgemm_tpp.release(); });
+  }
 }
 
 template <typename T>
-inline void fc_out(
+inline void tpp_linear_add_add(
     at::Tensor& t_in,
     at::Tensor& t_in1,
     at::Tensor& t_in2,
@@ -216,7 +389,7 @@ inline void fc_out(
   auto sadd_tpp_rem = SCOPEIT((ScaleAddTPP<T, T>(rem, Hk, K, K)), EW_ADD);
 
   {
-    RECORD_SCOPE(o_gemm, {t_in, t_wt_V});
+    RECORD_SCOPE(tpp_linear_add_add_krnl, {t_in, t_wt_V});
     // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
     auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
     auto ogemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
@@ -253,7 +426,7 @@ inline void fc_out(
 }
 
 template <typename T>
-inline void fc_in(
+inline void tpp_linear_gelu(
     at::Tensor& t_in,
     at::Tensor& t_wt,
     at::Tensor& t_bias,
@@ -295,7 +468,7 @@ inline void fc_in(
   auto gelu_fwd_tpp_rem = SCOPEIT(GeluFwdTPP<T>(rem, Hk, K, K), ACT);
 
   {
-    RECORD_SCOPE(i_gemm, {t_in, t_wt_V});
+    RECORD_SCOPE(tpp_linear_gelu_krnl, {t_in, t_wt_V});
     // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
     auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
     auto igemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
@@ -329,8 +502,14 @@ inline void fc_in(
   }
 }
 
-template <typename T, typename Tout = T>
-inline void qkv_gemm(at::Tensor& t_in, at::Tensor& t_wt, at::Tensor& t_out) {
+template <typename T>
+inline void tpp_linear_add(
+    at::Tensor t_in,
+    at::Tensor t_in1,
+    at::Tensor t_wt,
+    at::Tensor t_bias,
+    at::Tensor t_out,
+    float scale) {
   auto in_sizes = t_in.sizes();
   auto BS = in_sizes[0] * in_sizes[1];
   if (BS > FT_OPT_SIZE) { // first token compute
@@ -344,47 +523,245 @@ inline void qkv_gemm(at::Tensor& t_in, at::Tensor& t_wt, at::Tensor& t_out) {
   auto Nk = wt_sizes[0];
   auto Hk = wt_sizes[3];
   auto K = Nk * Hk;
+
   auto t_wt_V = torch_ipex::tpp::wt_tensor_for_fwd(Nk, Hk, Nc, Hc, t_wt);
 
   auto in = GetVLAPtr<T>(t_in, {Nc, Hc});
+  auto in1 = GetVLAPtr<T>(t_in1, {Nk, Hk});
   auto wt_V = GetVLAPtr<T>(t_wt_V, {Nc, Hc * Hk});
-  auto out = GetVLAPtr<Tout>(t_out, {Nk, Hk});
+  auto bias = GetVLAPtr<T>(t_bias, {Hk});
+  auto out = GetVLAPtr<T>(t_out, {Nk, Hk});
 
   auto Ncb = Nc;
   auto BSb = 64L;
-  auto rem = BS % BSb;
+  auto rem = BS % 64;
   if (large_cache_opt)
     Ncb = NCB_BLOCK_SIZE;
 
-  auto zero_tpp = SCOPEIT(SetZeroTPP<Tout>(BSb, Hk, K), EW_ZERO);
-  auto zero_tpp_rem = SCOPEIT(SetZeroTPP<Tout>(rem, Hk, K), EW_ZERO);
+  bool with_bias = (t_bias.numel() > 0);
+  auto copy_bias_tpp = SCOPEIT(CpyBiasTPP<T>(BSb, Hk, K), BIAS);
+  auto copy_bias_tpp_rem = SCOPEIT(CpyBiasTPP<T>(rem, Hk, K), BIAS);
+  auto zero_tpp = SCOPEIT(SetZeroTPP<T>(BSb, Hk, K), EW_ZERO);
+  auto zero_tpp_rem = SCOPEIT(SetZeroTPP<T>(rem, Hk, K), EW_ZERO);
   auto brgemm_tpp = SCOPEITGEMM(
-      (BrgemmTPP<T, Tout>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
+      (BrgemmTPP<T, T>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
   auto brgemm_tpp_rem = SCOPEITGEMM(
-      (BrgemmTPP<T, Tout>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
+      (BrgemmTPP<T, T>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
+  auto sadd_tpp = SCOPEIT((ScaleAddTPP<T, T>(BSb, Hk, K, K)), EW_ADD);
+  auto sadd_tpp_rem = SCOPEIT((ScaleAddTPP<T, T>(rem, Hk, K, K)), EW_ADD);
 
   {
-    RECORD_SCOPE(qkv_gemm, {t_in, t_wt_V});
+    RECORD_SCOPE(tpp_linear_add_krnl, {t_in, t_wt_V});
     // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
     auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
-    auto gemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
+    auto ogemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
+        {{0, Nc, Ncb, false}, {0L, BS, BSb}, {Nk}}, loop_scheme);
+    ogemm_loop(
+        [&](int* ind) {
+          int nc = ind[0], s1 = ind[1], nk = ind[2];
+          auto count = nc + Ncb < Nc ? Ncb : Nc - nc;
+          bool is_rem = (s1 + BSb > BS);
+          if (!is_rem) {
+            if (nc == 0) {
+              if (with_bias) {
+                copy_bias_tpp(bias[nk], out[s1][nk]);
+              } else {
+                zero_tpp(out[s1][nk]);
+              }
+            }
+            brgemm_tpp(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, true);
+            if (!(nc + Ncb < Nc)) { // last nc iter
+              sadd_tpp(in1[s1][nk], out[s1][nk], scale);
+            }
+          } else {
+            if (nc == 0) {
+              if (with_bias) {
+                copy_bias_tpp_rem(bias[nk], out[s1][nk]);
+              } else {
+                zero_tpp_rem(out[s1][nk]);
+              }
+            }
+            brgemm_tpp_rem(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, false);
+            brgemm_tpp.config();
+            if (!(nc + Ncb < Nc)) { // last nc iter
+              sadd_tpp_rem(in1[s1][nk], out[s1][nk], scale);
+            }
+          }
+        },
+        [&]() { brgemm_tpp.config(); },
+        [&]() { brgemm_tpp.release(); });
+  }
+}
+
+template <typename T>
+inline void tpp_linear_silu(
+    at::Tensor t_in,
+    at::Tensor t_wt,
+    at::Tensor t_bias,
+    at::Tensor t_out) {
+  auto in_sizes = t_in.sizes();
+  auto BS = in_sizes[0] * in_sizes[1];
+  if (BS > FT_OPT_SIZE) { // first token compute
+    t_wt = wt_tensor_for_first_token<T>(t_wt);
+  }
+  auto wt_sizes = t_wt.sizes();
+  auto C = in_sizes[2];
+
+  auto Nc = wt_sizes[1];
+  auto Hc = C / Nc;
+  auto Nk = wt_sizes[0];
+  auto Hk = wt_sizes[3];
+  auto K = Nk * Hk;
+
+  auto t_wt_V = torch_ipex::tpp::wt_tensor_for_fwd(Nk, Hk, Nc, Hc, t_wt);
+
+  auto in = GetVLAPtr<T>(t_in, {Nc, Hc});
+  auto wt_V = GetVLAPtr<T>(t_wt_V, {Nc, Hc * Hk});
+  auto bias = GetVLAPtr<T>(t_bias, {Hk});
+  auto out = GetVLAPtr<T>(t_out, {Nk, Hk});
+
+  auto Ncb = Nc;
+  auto BSb = 64L;
+  auto rem = BS % 64;
+  if (large_cache_opt)
+    Ncb = NCB_BLOCK_SIZE;
+
+  bool with_bias = (t_bias.numel() > 0);
+  auto copy_bias_tpp = SCOPEIT(CpyBiasTPP<T>(BSb, Hk, K), BIAS);
+  auto copy_bias_tpp_rem = SCOPEIT(CpyBiasTPP<T>(rem, Hk, K), BIAS);
+  auto zero_tpp = SCOPEIT(SetZeroTPP<T>(BSb, Hk, K), EW_ZERO);
+  auto zero_tpp_rem = SCOPEIT(SetZeroTPP<T>(rem, Hk, K), EW_ZERO);
+  auto brgemm_tpp = SCOPEITGEMM(
+      (BrgemmTPP<T, T>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
+  auto brgemm_tpp_rem = SCOPEITGEMM(
+      (BrgemmTPP<T, T>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
+  auto silu_fwd_tpp = SCOPEIT(SiLUFwdTPP<T>(BSb, Hk, K, K), ACT);
+  auto silu_fwd_tpp_rem = SCOPEIT(SiLUFwdTPP<T>(rem, Hk, K, K), ACT);
+
+  {
+    RECORD_SCOPE(tpp_linear_silu_krnl, {t_in, t_wt_V});
+    // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
+    auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
+    auto igemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
         {{0, Nc, Ncb, false}, {0, BS, BSb}, {Nk}}, loop_scheme);
-    gemm_loop(
+    igemm_loop(
         [&](int* ind) {
           int nc = ind[0], s1 = ind[1], nk = ind[2];
           auto count = nc + Ncb < Nc ? Ncb : Nc - nc;
           bool is_rem = (s1 + BSb > BS);
           if (!is_rem) {
             if (nc == 0) {
-              zero_tpp(out[s1][nk]);
+              if (with_bias) {
+                copy_bias_tpp(bias[nk], out[s1][nk]);
+              } else {
+                zero_tpp(out[s1][nk]);
+              }
             }
             brgemm_tpp(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, true);
+            if (!(nc + Ncb < Nc)) { // last nc iter
+              silu_fwd_tpp(out[s1][nk], out[s1][nk]);
+            }
           } else {
             if (nc == 0) {
-              zero_tpp_rem(out[s1][nk]);
+              if (with_bias) {
+                copy_bias_tpp_rem(bias[nk], out[s1][nk]);
+              } else {
+                zero_tpp_rem(out[s1][nk]);
+              }
             }
             brgemm_tpp_rem(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, false);
             brgemm_tpp.config();
+            if (!(nc + Ncb < Nc)) { // last nc iter
+              silu_fwd_tpp_rem(out[s1][nk], out[s1][nk]);
+            }
+          }
+        },
+        [&]() { brgemm_tpp.config(); },
+        [&]() { brgemm_tpp.release(); });
+  }
+}
+
+template <typename T>
+inline void tpp_linear_relu(
+    at::Tensor t_in,
+    at::Tensor t_wt,
+    at::Tensor t_bias,
+    at::Tensor t_out) {
+  auto in_sizes = t_in.sizes();
+  auto BS = in_sizes[0] * in_sizes[1];
+  if (BS > FT_OPT_SIZE) { // first token compute
+    t_wt = wt_tensor_for_first_token<T>(t_wt);
+  }
+  auto wt_sizes = t_wt.sizes();
+  auto C = in_sizes[2];
+
+  auto Nc = wt_sizes[1];
+  auto Hc = C / Nc;
+  auto Nk = wt_sizes[0];
+  auto Hk = wt_sizes[3];
+  auto K = Nk * Hk;
+
+  auto t_wt_V = torch_ipex::tpp::wt_tensor_for_fwd(Nk, Hk, Nc, Hc, t_wt);
+
+  auto in = GetVLAPtr<T>(t_in, {Nc, Hc});
+  auto wt_V = GetVLAPtr<T>(t_wt_V, {Nc, Hc * Hk});
+  auto bias = GetVLAPtr<T>(t_bias, {Hk});
+  auto out = GetVLAPtr<T>(t_out, {Nk, Hk});
+
+  auto Ncb = Nc;
+  auto BSb = 64L;
+  auto rem = BS % 64;
+  if (large_cache_opt)
+    Ncb = NCB_BLOCK_SIZE;
+
+  bool with_bias = (t_bias.numel() > 0);
+  auto copy_bias_tpp = SCOPEIT(CpyBiasTPP<T>(BSb, Hk, K), BIAS);
+  auto copy_bias_tpp_rem = SCOPEIT(CpyBiasTPP<T>(rem, Hk, K), BIAS);
+  auto zero_tpp = SCOPEIT(SetZeroTPP<T>(BSb, Hk, K), EW_ZERO);
+  auto zero_tpp_rem = SCOPEIT(SetZeroTPP<T>(rem, Hk, K), EW_ZERO);
+  auto brgemm_tpp = SCOPEITGEMM(
+      (BrgemmTPP<T, T>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
+  auto brgemm_tpp_rem = SCOPEITGEMM(
+      (BrgemmTPP<T, T>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
+  auto relu_fwd_tpp = SCOPEIT(ReLUFwdTPP<T>(BSb, Hk, K, K, false), ACT);
+  auto relu_fwd_tpp_rem = SCOPEIT(ReLUFwdTPP<T>(rem, Hk, K, K, false), ACT);
+
+  {
+    RECORD_SCOPE(tpp_linear_relu_krnl, {t_in, t_wt_V});
+    // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
+    auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
+    auto igemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
+        {{0, Nc, Ncb, false}, {0, BS, BSb}, {Nk}}, loop_scheme);
+    igemm_loop(
+        [&](int* ind) {
+          int nc = ind[0], s1 = ind[1], nk = ind[2];
+          auto count = nc + Ncb < Nc ? Ncb : Nc - nc;
+          bool is_rem = (s1 + BSb > BS);
+          if (!is_rem) {
+            if (nc == 0) {
+              if (with_bias) {
+                copy_bias_tpp(bias[nk], out[s1][nk]);
+              } else {
+                zero_tpp(out[s1][nk]);
+              }
+            }
+            brgemm_tpp(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, true);
+            if (!(nc + Ncb < Nc)) { // last nc iter
+              relu_fwd_tpp(out[s1][nk], out[s1][nk]);
+            }
+          } else {
+            if (nc == 0) {
+              if (with_bias) {
+                copy_bias_tpp_rem(bias[nk], out[s1][nk]);
+              } else {
+                zero_tpp_rem(out[s1][nk]);
+              }
+            }
+            brgemm_tpp_rem(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, false);
+            brgemm_tpp.config();
+            if (!(nc + Ncb < Nc)) { // last nc iter
+              relu_fwd_tpp_rem(out[s1][nk], out[s1][nk]);
+            }
           }
         },
         [&]() { brgemm_tpp.config(); },
diff --git a/csrc/cpu/tpp/timing.h b/csrc/cpu/tpp/timing.h
index e399d038c..f2f7cf684 100644
--- a/csrc/cpu/tpp/timing.h
+++ b/csrc/cpu/tpp/timing.h
@@ -158,6 +158,16 @@ class ScopedTPP {
   template <typename... Types>
   void operator()(Types... vars) {
     ScopedTimer _t(t);
+#ifdef DEBUG_TRACE_TPP
+    if (omp_get_thread_num() == 0) {
+      auto cur_class_name = get_class_name<T>();
+      if (cur_class_name != prev_class_name) {
+        std::cout << "Calling impl " << impl << " for " << cur_class_name
+                  << std::endl;
+        prev_class_name = cur_class_name;
+      }
+    }
+#endif
     if (impl == 0) {
       func(vars...);
     } else if (impl == 1) {
diff --git a/csrc/cpu/tpp/xsmm_functors.h b/csrc/cpu/tpp/xsmm_functors.h
index 9808a0bef..846bfbb7a 100644
--- a/csrc/cpu/tpp/xsmm_functors.h
+++ b/csrc/cpu/tpp/xsmm_functors.h
@@ -934,6 +934,46 @@ class ReduceAddRowTPP {
   AddTPP<Tout, Tout> add;
 };
 
+template <typename Tin, typename Tout = Tin>
+class MulTPP {
+ public:
+  MulTPP() {}
+  MulTPP(int N) : MulTPP(1, N) {}
+  MulTPP(int rows, int cols) : MulTPP(rows, cols, cols, cols) {}
+  MulTPP(int rows, int cols, int ldi, int ldo)
+      : rows(rows),
+        cols(cols),
+        ldi(ldi),
+        ldo(ldo),
+        kernel(
+            rows,
+            cols,
+            ldi,
+            ldo,
+            XsmmDtype<Tin>(),
+            XsmmDtype<Tout>(),
+            LIBXSMM_DATATYPE_F32,
+            LIBXSMM_MELTW_FLAG_BINARY_NONE,
+            LIBXSMM_MELTW_TYPE_BINARY_MUL) {}
+  void operator()(Tin* in0, Tin* in1, Tout* out) {
+    kernel((void*)in0, (void*)in1, (void*)out);
+  }
+  void ref(Tin* in0, Tin* in1, Tout* out) {
+    for (int r = 0; r < rows; r++) {
+      for (int c = 0; c < cols; c++) {
+        out[r * ldo + c] = (float)in0[r * ldi + c] * (float)in1[r * ldi + c];
+      }
+    }
+  }
+
+ private:
+  int rows = 0;
+  int cols = 0;
+  int ldi;
+  int ldo;
+  BinaryTPP kernel;
+};
+
 template <typename T>
 class BCastMulTPP {
  public:
@@ -2211,20 +2251,28 @@ class SiLUFwdTPP {
             cols,
             ldi,
             ldo,
+            ldo,
+            XsmmDtype<T>(),
             XsmmDtype<T>(),
             XsmmDtype<T>(),
             LIBXSMM_DATATYPE_F32,
             LIBXSMM_MELTW_FLAG_BINARY_NONE,
             LIBXSMM_MELTW_TYPE_BINARY_MUL) {}
-  void operator()(T* in, T* out, T* sigout) {
+  void operator()(T* in, T* out, T* sigout = nullptr) {
+    T tmp[rows * ldo];
+    if (sigout == nullptr)
+      sigout = tmp;
     sigmoid((void*)in, (void*)sigout);
     mul((void*)in, (void*)sigout, (void*)out);
   }
-  void ref(T* in, T* out, T* sigout) {
+  void ref(T* in, T* out, T* sigout = nullptr) {
+    T tmp[rows * ldo];
+    if (sigout == nullptr)
+      sigout = tmp;
     for (int i = 0; i < rows; i++) {
       for (int j = 0; j < cols; j++) {
         sigout[i * ldo + j] = 1. / (1. + exp(-in[i * ldi + j]));
-        out[i * ldo + j] = in[i * ldo + j] * sigout[i * ldo + j];
+        out[i * ldo + j] = in[i * ldi + j] * sigout[i * ldo + j];
       }
     }
   }
diff --git a/csrc/cpu/vec/vec512/perf_kernel/add_softmax.h b/csrc/cpu/vec/vec512/perf_kernel/add_softmax.h
index 656015bb8..dcc064f97 100644
--- a/csrc/cpu/vec/vec512/perf_kernel/add_softmax.h
+++ b/csrc/cpu/vec/vec512/perf_kernel/add_softmax.h
@@ -221,6 +221,31 @@ inline void _dil_normalization_kernel(
   }
 }
 
+template <typename scalar_t>
+inline void _dil_add_kernel(
+    const scalar_t* src,
+    float* dst,
+    const int& size) {
+  __m512 vec_a = {};
+  __m512 vec_out = {};
+
+  int j = 0;
+  for (; j <= size - 16; j += 16) {
+    vec_a = _loadu(src + j);
+    vec_out = _loadu(dst + j);
+    vec_out = _mm512_add_ps(vec_a, vec_out);
+    _storeu(dst + j, vec_out);
+  }
+
+  if (j < size) {
+    __mmask16 mask = (1 << (size - j)) - 1;
+    vec_a = _maskz_loadu(src + j, mask);
+    vec_out = _maskz_loadu(dst + j, mask);
+    vec_out = _mm512_add_ps(vec_out, vec_a);
+    _mask_storeu(dst + j, vec_out, mask);
+  }
+}
+
 inline void _dil_add_reduce_max_fusion_kernel(
     float* a,
     const float* b,
@@ -254,6 +279,32 @@ inline void _dil_add_reduce_max_fusion_kernel(
   max = _mm512_reduce_max_ps(vec_ps_min);
 }
 
+inline void _dil_reduce_max_fusion_kernel(
+    const float* a,
+    const int& size,
+    float* out,
+    float& max) {
+  auto vec_ps_min = _mm512_set1_ps(std::numeric_limits<float>::lowest());
+  auto vec_out = vec_ps_min;
+
+  int i = 0;
+  for (; i <= size - 16; i += 16) {
+    vec_out = _loadu(a + i);
+    vec_ps_min = _mm512_max_ps(vec_ps_min, vec_out);
+    _mm512_storeu_ps(out + i, vec_out);
+  }
+
+  if (i < size) {
+    __mmask16 mask = (1 << (size - i)) - 1;
+    vec_out = _maskz_loadu(a + i, mask);
+    vec_ps_min = _mm512_mask_max_ps(vec_ps_min, mask, vec_out, vec_ps_min);
+    _mm512_mask_storeu_ps(out + i, mask, vec_out);
+  }
+
+  // NOTE: _mm512_reduce_max_ps is sequence instruction
+  max = _mm512_reduce_max_ps(vec_ps_min);
+}
+
 inline void _dil_mul_reduce_max_fusion_kernel(
     const float* a,
     const float& scale,
diff --git a/csrc/cpu/vec/vec512/vec512_bfloat16.h b/csrc/cpu/vec/vec512/vec512_bfloat16.h
index eeea99cff..c80923425 100644
--- a/csrc/cpu/vec/vec512/vec512_bfloat16.h
+++ b/csrc/cpu/vec/vec512/vec512_bfloat16.h
@@ -312,6 +312,40 @@ inline __attribute__((always_inline)) void add_ker(
   }
 }
 
+template <>
+inline __attribute__((always_inline)) void add_ker(
+    at::BFloat16* inout,
+    const float* in,
+    int64_t len) {
+  int64_t i = 0;
+#pragma unroll(2)
+  for (i = 0; i < len - 31; i += 32) {
+    auto in1 = _mm512_loadu_ps(in + i);
+    auto in2 = _mm512_loadu_ps(in + i + 16);
+    auto inout1 = cvt_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(inout + i)));
+    auto inout2 = cvt_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(inout + i + 16)));
+    inout1 = _mm512_add_ps(inout1, in1);
+    inout2 = _mm512_add_ps(inout2, in2);
+    _mm256_storeu_si256((__m256i*)(inout + i), cvt_fp32_to_bf16(inout1));
+    _mm256_storeu_si256((__m256i*)(inout + i + 16), cvt_fp32_to_bf16(inout2));
+  }
+
+  if (i < len - 15) {
+    auto in1 = _mm512_loadu_ps(in + i);
+    auto inout1 = cvt_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(inout + i)));
+    inout1 = _mm512_add_ps(inout1, in1);
+    _mm256_storeu_si256((__m256i*)(inout + i), cvt_fp32_to_bf16(inout1));
+    i += 16;
+  }
+
+  if (i < len) {
+    auto mask = (1 << (len - i)) - 1;
+    auto in1 = _mm512_maskz_loadu_ps(mask, in + i);
+    auto inout1 = cvt_bf16_to_fp32(_mm256_maskz_loadu_epi16(mask, inout + i));
+    inout1 = _mm512_add_ps(inout1, in1);
+    _mm256_mask_storeu_epi16(inout + i, mask, cvt_fp32_to_bf16(inout1));
+  }
+}
 template <>
 inline __attribute__((always_inline)) void move_ker(
     at::BFloat16* out,
diff --git a/examples/cpu/inference/python/llm/README.md b/examples/cpu/inference/python/llm/README.md
new file mode 100644
index 000000000..263692401
--- /dev/null
+++ b/examples/cpu/inference/python/llm/README.md
@@ -0,0 +1,147 @@
+# Text Generation
+We provide the inference benchmarking script `run_generation.py` for large language models text generation.<br/>
+Support large language models, such as GPT-J, LLaMA, GPT-Neox.<br/>
+And script `run_generation_with_deepspeed.py` for distributed with DeepSpeed.<br/>
+And script `run_model_int8.py` for int8.<br/>
+
+## Setup
+```bash
+WORK_DIR=$PWD
+# GCC 12.3 is required, please set it firstly
+# Create environment (conda recommended)
+conda create -n llm python=3.9 -y
+# install deps
+conda install gcc=12.3 gxx=12.3 cxx-compiler -c conda-forge -y
+conda install cmake ninja mkl mkl-include -y
+conda install gperftools -c conda-forge -y
+
+# Install PyTorch
+python -m pip install torch==2.1.0.dev20230711+cpu torchvision==0.16.0.dev20230711+cpu torchaudio==2.1.0.dev20230711+cpu --index-url https://download.pytorch.org/whl/nightly/cpu
+
+# Install IPEX with semi-compiler, require gcc 12.3
+rm -rf llvm-project && mkdir llvm-project && cd llvm-project
+wget https://github.com/llvm/llvm-project/releases/download/llvmorg-16.0.6/cmake-16.0.6.src.tar.xz
+wget https://github.com/llvm/llvm-project/releases/download/llvmorg-16.0.6/llvm-16.0.6.src.tar.xz
+tar -xf cmake-16.0.6.src.tar.xz && mv cmake-16.0.6.src cmake
+tar -xf llvm-16.0.6.src.tar.xz && mv llvm-16.0.6.src llvm
+mkdir build && cd build
+cmake ../llvm -DCMAKE_INSTALL_PREFIX=${PWD}/_install/llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_TARGETS_TO_BUILD=X86 -DLLVM_INCLUDE_TESTS=OFF -DLLVM_INCLUDE_EXAMPLES=OFF -DLLVM_ENABLE_TERMINFO=OFF -DLLVM_INCLUDE_BENCHMARKS=OFF -DCMAKE_CXX_FLAGS="-D_GLIBCXX_USE_CXX11_ABI=0"
+make install -j$(nproc)
+ln -s ${PWD}/_install/llvm/bin/llvm-config ${CONDA_PREFIX}/bin/llvm-config-13
+cd ../../
+
+git clone --branch llm_feature_branch https://github.com/intel-innersource/frameworks.ai.pytorch.ipex-cpu
+cd frameworks.ai.pytorch.ipex-cpu
+git submodule sync && git submodule update --init --recursive
+export DNNL_GRAPH_BUILD_COMPILER_BACKEND=1
+export CXXFLAGS="${CXXFLAGS} -D__STDC_FORMAT_MACROS"
+python setup.py install
+cd ../
+
+# Install transformers
+pip install transformers==4.28.1
+# Install others deps
+pip install cpuid accelerate datasets sentencepiece protobuf==3.20.3
+
+# Setup environment variables for performance on Xeon
+export LD_PRELOAD=${CONDA_PREFIX}/lib/libstdc++.so.6
+export KMP_BLOCKTIME=INF
+export KMP_TPAUSE=0
+export KMP_SETTINGS=1
+export KMP_AFFINITY=granularity=fine,compact,1,0
+export KMP_FORJOIN_BARRIER_PATTERN=dist,dist
+export KMP_PLAIN_BARRIER_PATTERN=dist,dist
+export KMP_REDUCTION_BARRIER_PATTERN=dist,dist
+export LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libiomp5.so # Intel OpenMP
+# Tcmalloc is a recommended malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support.
+export LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libtcmalloc.so
+
+# [Optional] install neural-compressor for GPT-J INT8 only
+pip install neural-compressor==2.2
+
+# [Optional] The following is only for DeepSpeed case
+git clone https://github.com/delock/DeepSpeedSYCLSupport
+cd DeepSpeedSYCLSupport
+git checkout gma/run-opt-branch
+python -m pip install -r requirements/requirements.txt
+python setup.py install
+cd ../
+git clone https://github.com/oneapi-src/oneCCL.git
+cd oneCCL
+mkdir build
+cd build
+cmake ..
+make -j install
+source _install/env/setvars.sh
+cd ../..
+
+# Get the sample prompt.json
+# Make sure the downloaded prompt.json file is under the same directory as that of the python scripts mentioned above.
+wget https://intel-extension-for-pytorch.s3.amazonaws.com/miscellaneous/llm/prompt.json
+
+```
+
+
+## Supported Model List
+```
+<MODEL ID> in
+(1) "EleutherAI/gpt-j-6b" (model id from transformers Hub)
+(2) "EleutherAI/gpt-neox-20b" (model id from transformers Hub)
+(3) Llama 2 Model directory path
+Note: Above models are well supported with all optimizations like indirect access KV cache, fused ROPE, and prepacked TPP Linear (fp32/bf16). For other LLM models (like OPT, Bloom...), we could still run with this BKC, and may get parts of optimizations like prepacked TPP Linear (fp32/bf16), and we are working in progress to cover all optimizations to these other LLM models, which will expand the model list above.
+```
+* Llama 2 model conversion steps:
+    1) [transformers conversion tool](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py) (Verified [meta-llama/Llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat) and [meta-llama/Llama-2-13b-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)).
+    2) Follow [instructions](https://github.com/facebookresearch/llama#access-on-hugging-face) to download model files for conversion.
+    3) Decompress the downloaded model file.
+    4) Follow [instructions](https://github.com/facebookresearch/llama-recipes#model-conversion-to-hugging-face) to convert the model.
+    5) Launch example scripts with the place holder <MODEL_ID> substituted by the --output_dir argument value of the conversion script.
+
+
+## Single Instance Performance
+```bash
+# Get prompt file to the path of scripts
+mv PATH/TO/prompt.json WORK_DIR
+
+# bfloat16 benchmark
+OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run_generation.py --benchmark -m <MODEL_ID> --dtype bfloat16 --ipex --jit
+
+# int8 benchmark
+## (1) Do quantization to get the quantized model
+mkdir saved_results
+
+## GPT-J quantization
+python run_gpt-j_int8.py --ipex-smooth-quant --lambada --output-dir "saved_results" --jit --int8-bf16-mixed -m <GPTJ MODEL_ID>
+## Llama 2 quantization
+python run_llama_int8.py --ipex-smooth-quant --lambada --output-dir "saved_results" --jit --int8-bf16-mixed -m <LLAMA MODEL_ID>
+## GPT-NEOX quantization
+python run_gpt-neox_int8.py --ipex-weight-only-quantization --lambada --output-dir "saved_results" --jit --int8 -m <GPT-NEOX MODEL_ID>
+
+## (2) Run int8 performance test (note that GPT-NEOX uses --int8 instead of --int8-bf16-mixed)
+OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <cpu list> python run_<MODEL>_int8.py -m <MODEL_ID> --quantized-model-path "./saved_results/best_model.pt" --benchmark --jit --int8-bf16-mixed
+```
+## Single Instance Accuracy
+```bash
+# bfloat16
+OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run_generation.py --accuracy-only -m <MODEL_ID> --dtype bfloat16 --ipex --jit --lambada
+
+# Quantization as a performance part
+# (1) Do quantization to get the quantized model as mentioned above
+# (2) Run int8 accuracy test (note that GPT-NEOX uses --int8 instead of --int8-bf16-mixed)
+OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <cpu list> python run_<MODEL>_int8.py -m <MODEL ID> --quantized-model-path "./saved_results/best_model.pt" --accuracy-only --jit --int8-bf16-mixed --lambada
+```
+
+## Distributed Performance with DeepSpeed (autoTP)
+```bash
+export DS_SHM_ALLREDUCE=1
+unset KMP_AFFINITY
+
+# Get prompt file to the path of scripts
+mv PATH/TO/prompt.json WORK_DIR
+
+# Run GPTJ/LLAMA with bfloat16  DeepSpeed
+deepspeed --bind_cores_to_rank run_generation_with_deepspeed.py --benchmark -m <MODEL_ID> --dtype bfloat16 --ipex --jit
+
+# Run GPT-NeoX with ipex weight only quantization
+deepspeed --bind_cores_to_rank run_generation_with_deepspeed.py --benchmark -m EleutherAI/gpt-neox-20b --dtype float32 --ipex --jit --ipex-weight-only-quantization
+```
diff --git a/examples/cpu/inference/python/llm/run_generation.py b/examples/cpu/inference/python/llm/run_generation.py
index 32ba07c6e..12c7570f7 100644
--- a/examples/cpu/inference/python/llm/run_generation.py
+++ b/examples/cpu/inference/python/llm/run_generation.py
@@ -21,7 +21,7 @@ from transformers import (
 MODEL_CLASSES = {
     "gpt-j": (AutoModelForCausalLM, AutoTokenizer),
     "gpt-neox": (AutoModelForCausalLM, AutoTokenizer),
-    "llama": (LlamaForCausalLM, LlamaTokenizer),
+    "llama": (AutoModelForCausalLM, LlamaTokenizer),
     "auto": (AutoModelForCausalLM, AutoTokenizer),
 }
 
diff --git a/examples/cpu/inference/python/llm/run_generation_with_deepspeed.py b/examples/cpu/inference/python/llm/run_generation_with_deepspeed.py
index c2565de04..d16286ab2 100644
--- a/examples/cpu/inference/python/llm/run_generation_with_deepspeed.py
+++ b/examples/cpu/inference/python/llm/run_generation_with_deepspeed.py
@@ -27,7 +27,7 @@ from transformers import (
 MODEL_CLASSES = {
     "gpt-j": (AutoModelForCausalLM, AutoTokenizer),
     "gpt-neox": (AutoModelForCausalLM, AutoTokenizer),
-    "llama": (LlamaForCausalLM, LlamaTokenizer),
+    "llama": (AutoModelForCausalLM, LlamaTokenizer),
     "auto": (AutoModelForCausalLM, AutoTokenizer),
 }
 
diff --git a/intel_extension_for_pytorch/__init__.py b/intel_extension_for_pytorch/__init__.py
index b2124295a..51b7331cb 100644
--- a/intel_extension_for_pytorch/__init__.py
+++ b/intel_extension_for_pytorch/__init__.py
@@ -32,7 +32,10 @@ except BaseException:
     )
 
 from .frontend import optimize
-from .cpu.transformers import _optimize_transformers
+from .cpu.transformers import (
+    _optimize_transformers,
+    _set_optimized_model_for_generation,
+)
 from .frontend import enable_auto_channels_last, disable_auto_channels_last
 from .frontend import set_fp32_math_mode, get_fp32_math_mode, FP32MathMode
 from .cpu._auto_kernel_selection import _enable_dnnl, _disable_dnnl, _using_dnnl
diff --git a/intel_extension_for_pytorch/cpu/tpp/fused_llm.py b/intel_extension_for_pytorch/cpu/tpp/fused_llm.py
index 788052dd8..10877702c 100644
--- a/intel_extension_for_pytorch/cpu/tpp/fused_llm.py
+++ b/intel_extension_for_pytorch/cpu/tpp/fused_llm.py
@@ -55,8 +55,24 @@ def GPTNeoXLayer_forward(
     return outputs
 
 
+def LlamaMLP_forward_distributed(self, x):
+    gate = torch.ops.torch_ipex.tpp_linear_silu(
+        x, self.gate_proj.weight, x.new_empty(0)
+    )
+    up = torch.ops.torch_ipex.tpp_linear_mul(
+        x, gate, self.up_proj.weight, x.new_empty(0)
+    )
+    return self.down_proj(up)
+
+
 def LlamaMLP_forward(self, x):
-    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
+    gate = torch.ops.torch_ipex.tpp_linear_silu(
+        x, self.gate_proj.weight, x.new_empty(0)
+    )
+    up = torch.ops.torch_ipex.tpp_linear_mul(
+        x, gate, self.up_proj.weight, x.new_empty(0)
+    )
+    return up
 
 
 def LlamaDecoderLayer_forward(
@@ -86,8 +102,13 @@ def LlamaDecoderLayer_forward(
     # Fully Connected
     residual = hidden_states
     hidden_states = self.post_attention_layernorm(hidden_states)
-    hidden_states = self.mlp(hidden_states)
-    hidden_states = residual + hidden_states
+    hidden_states = torch.ops.torch_ipex.tpp_linear_add(
+        self.mlp(hidden_states),
+        residual,
+        self.mlp.down_proj.weight,
+        hidden_states.new_empty(0),
+        1.0,
+    )
 
     outputs = (hidden_states,)
 
@@ -103,7 +124,7 @@ def LlamaDecoderLayer_forward(
 def GPTJMLP_forward(
     self, hidden_states: Optional[torch.FloatTensor]
 ) -> torch.FloatTensor:
-    hidden_states = torch.ops.torch_ipex.fc_in_gemm(
+    hidden_states = torch.ops.torch_ipex.tpp_linear_gelu(
         hidden_states, self.fc_in.weight, self.fc_in.bias
     )
     return hidden_states
@@ -112,7 +133,7 @@ def GPTJMLP_forward(
 def GPTJMLP_forward_distributed(
     self, hidden_states: Optional[torch.FloatTensor]
 ) -> torch.FloatTensor:
-    hidden_states = torch.ops.torch_ipex.fc_in_gemm(
+    hidden_states = torch.ops.torch_ipex.tpp_linear_gelu(
         hidden_states, self.fc_in.weight, self.fc_in.bias
     )
     hidden_states = self.fc_out(hidden_states)
@@ -147,7 +168,7 @@ def GPTJBlock_forward(
     outputs = attn_outputs[1:]
 
     feed_forward_hidden_states = self.mlp(hidden_states)
-    hidden_states = torch.ops.torch_ipex.fc_out_gemm(
+    hidden_states = torch.ops.torch_ipex.tpp_linear_add_add(
         feed_forward_hidden_states,
         attn_output,
         residual,
diff --git a/intel_extension_for_pytorch/cpu/transformers/__init__.py b/intel_extension_for_pytorch/cpu/transformers/__init__.py
index 4121dec28..43464e704 100644
--- a/intel_extension_for_pytorch/cpu/transformers/__init__.py
+++ b/intel_extension_for_pytorch/cpu/transformers/__init__.py
@@ -1 +1,2 @@
 from .optimize import _optimize_transformers
+from .optimize import _set_optimized_model_for_generation
diff --git a/intel_extension_for_pytorch/cpu/transformers/attentions.py b/intel_extension_for_pytorch/cpu/transformers/attentions.py
index 180ab1dd8..9ae3fb06f 100644
--- a/intel_extension_for_pytorch/cpu/transformers/attentions.py
+++ b/intel_extension_for_pytorch/cpu/transformers/attentions.py
@@ -487,8 +487,8 @@ class _LlamaAttention_GQA(nn.Module):
         self.hidden_size = module.hidden_size
         self.num_heads = module.num_heads
         self.num_kv_heads = (
-            self.config.num_attention_kv_heads
-            if hasattr(self.config, "num_attention_kv_heads")
+            self.config.num_key_value_heads
+            if hasattr(self.config, "num_key_value_heads")
             else module.num_attention_heads
         )
         self.head_dim = self.hidden_size // self.num_heads
@@ -518,16 +518,18 @@ class _LlamaAttention_GQA(nn.Module):
             else 2048
         )
 
-    def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
-        """torch.repeat_interleave(x, dim=2, repeats=n_rep)"""
-        bs, slen, n_kv_heads, head_dim = x.shape
+    def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
+        """
+        This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
+        num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
+        """
+        batch, num_key_value_heads, slen, head_dim = hidden_states.shape
         if n_rep == 1:
-            return x
-        return (
-            x[:, :, :, None, :]
-            .expand(bs, slen, n_kv_heads, n_rep, head_dim)
-            .reshape(bs, slen, n_kv_heads * n_rep, head_dim)
+            return hidden_states
+        hidden_states = hidden_states[:, :, None, :, :].expand(
+            batch, num_key_value_heads, n_rep, slen, head_dim
         )
+        return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
 
     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
         return (
@@ -618,20 +620,15 @@ class _LlamaAttention_GQA(nn.Module):
                 torch.tensor(past_key_value[3] + query.shape[1], dtype=torch.long),
             )
         else:
-            # repeat k/v heads if n_kv_heads < n_heads
-            key = self.repeat_kv(
-                key, self.n_rep
-            )  # (bs, seqlen, n_local_heads, head_dim)
-            value = self.repeat_kv(
-                value, self.n_rep
-            )  # (bs, seqlen, n_local_heads, head_dim)
             value_states = value.transpose(1, 2)
             query_states = query.transpose(1, 2)
             key_states = key.transpose(1, 2)
             kv_seq_len = key_states.shape[-2]
 
             past_key_value = None
-
+            # repeat k/v heads if n_kv_heads < n_heads
+            key_states = self.repeat_kv(key_states, self.n_rep)
+            value_states = self.repeat_kv(value_states, self.n_rep)
             attn_weights = torch.matmul(
                 query_states, key_states.transpose(2, 3)
             ) / math.sqrt(self.head_dim)
@@ -936,6 +933,62 @@ class _GPTNeoXAttention(nn.Module):
         return outputs
 
 
+def OPTAttention_forward(
+    self,
+    hidden_states: torch.Tensor,
+    key_value_states: Optional[torch.Tensor] = None,
+    past_key_value: Optional[Tuple[torch.Tensor]] = None,
+    attention_mask: Optional[torch.Tensor] = None,
+    layer_head_mask: Optional[torch.Tensor] = None,
+    output_attentions: bool = False,
+) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
+    """Input shape: Batch x Time x Channel"""
+
+    # if key_value_states are provided this layer is used as a cross-attention layer
+    # for the decoder
+    is_cross_attention = key_value_states is not None
+
+    bsz, tgt_len, _ = hidden_states.size()
+
+    if is_cross_attention and past_key_value is not None:
+        key = past_key_value[0].view(bsz, tgt_len, self.num_heads, self.head_dim).contiguous()
+        value = past_key_value[1].view(bsz, tgt_len, self.num_heads, self.head_dim).contiguous()
+    elif is_cross_attention:
+        key = self.k_proj(key_value_states).view(bsz, tgt_len, self.num_heads, self.head_dim).contiguous()
+        value = self.v_proj(key_value_states).view(bsz, tgt_len, self.num_heads, self.head_dim).contiguous()
+    else:
+        key = self.k_proj(hidden_states).view(bsz, tgt_len, self.num_heads, self.head_dim).contiguous()
+        value = self.v_proj(hidden_states).view(bsz, tgt_len, self.num_heads, self.head_dim).contiguous()
+    if past_key_value is None:
+        past_key_value = (torch.randn(0), torch.randn(0), torch.zeros(2048, 4, dtype=torch.long), torch.zeros(1, dtype=torch.long))
+    query = self.q_proj(hidden_states).view(bsz, tgt_len, self.num_heads, self.head_dim).contiguous()
+    key_cache = past_key_value[0].contiguous()
+    value_cache = past_key_value[1].contiguous()
+    beam_idx = past_key_value[2].contiguous()
+    decoded_tokens = past_key_value[3].contiguous()[0]
+    attn_output, attn_weights, key_cache, value_cache, beam_idx = torch.ops.torch_ipex.masked_multihead_self_attention(query, key, value, key_cache, value_cache, beam_idx, decoded_tokens, 1/self.scaling, 2048, layer_head_mask, attention_mask)
+
+    if self.is_decoder:
+        past_key_value=(key_cache, value_cache, beam_idx, torch.tensor(past_key_value[3]+query.shape[1], dtype=torch.long))
+
+    if not output_attentions:
+        attn_weights_reshaped = None
+    attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)
+    if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):
+        raise ValueError(
+            f"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is"
+            f" {attn_output.size()}"
+        )
+    attn_output = attn_output.transpose(1, 2)
+
+    # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be
+    # partitioned aross GPUs when using tensor-parallelism.
+    attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)
+
+    attn_output = self.out_proj(attn_output)
+
+    return attn_output, attn_weights_reshaped, past_key_value
+
 def _reorder_cache(
     self, past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor
 ) -> Tuple[Tuple[torch.Tensor]]:
diff --git a/intel_extension_for_pytorch/cpu/transformers/generation.py b/intel_extension_for_pytorch/cpu/transformers/generation.py
index 84e9ffd76..665593adc 100644
--- a/intel_extension_for_pytorch/cpu/transformers/generation.py
+++ b/intel_extension_for_pytorch/cpu/transformers/generation.py
@@ -217,9 +217,11 @@ def _beam_search(
             re.search("GPTJ", self.config.architectures[0])
             or re.search("llama", self.config.architectures[0], re.IGNORECASE)
             or re.search("gptneox", self.config.architectures[0], re.IGNORECASE)
+            or re.search("OPT", self.config.architectures[0], re.IGNORECASE)
         ):
             first_token = False
             input_bs = input_ids.size()[0]
+            has_position_id = True
             if model_inputs["past_key_values"] is None:
                 first_token = True
             if first_token:
@@ -268,20 +270,53 @@ def _beam_search(
                             for i in range(self.config.num_hidden_layers)
                         ]
                     )
+                elif re.search("OPT", self.config.architectures[0], re.IGNORECASE):
+                    beam_idx_tmp = torch.zeros(
+                        (2048, int(batch_size * num_beams)), dtype=torch.long
+                    ).contiguous()
+                    model_inputs["past_key_values"] = tuple(
+                        [
+                            (
+                                torch.zeros([1, 1, 1, 1]).contiguous(),
+                                torch.zeros([1, 1, 1, 1]).contiguous(),
+                                beam_idx_tmp,
+                                torch.zeros(1, dtype=torch.long).contiguous(),
+                            )
+                            for i in range(self.config.num_hidden_layers)
+                        ]
+                    )
+                    has_position_id = False
 
             if hasattr(self, "trace_graph"):
                 if first_token:
-                    model_inputs["attention_mask"] = model_inputs["attention_mask"][
-                        :1, :
-                    ]
-                    model_inputs["input_ids"] = model_inputs["input_ids"][:1, :]
-                    model_inputs["position_ids"] = model_inputs["position_ids"][:1, :]
+                    new_attention_mask = model_inputs["attention_mask"][
+                        :batch_size
+                    ].clone()
+                    new_input_ids = model_inputs["input_ids"][:batch_size].clone()
+                    if has_position_id:
+                        new_position_ids = model_inputs["position_ids"][:batch_size].clone()
+                    for i in range(batch_size):
+                        new_attention_mask[i] = model_inputs["attention_mask"][
+                            i * num_beams
+                        ]
+                        new_input_ids[i] = model_inputs["input_ids"][i * num_beams]
+                        if has_position_id:
+                            new_position_ids[i] = model_inputs["position_ids"][
+                                i * num_beams
+                            ]
+                    model_inputs["attention_mask"] = new_attention_mask
+                    model_inputs["input_ids"] = new_input_ids
+                    if has_position_id:
+                        model_inputs["position_ids"] = new_position_ids
                 model_inputs.pop("use_cache", None)
                 model_inputs.pop("token_type_ids", None)
-                outputs = self.trace_graph(**model_inputs)
+                if first_token and hasattr(self, "trace_graph_first"):
+                    outputs = self.trace_graph_first(**model_inputs)
+                else:
+                    outputs = self.trace_graph(**model_inputs)
                 if first_token and len(model_inputs["past_key_values"][0]) == 4:
                     outputs = list(outputs)
-                    outputs[0] = outputs[0].repeat_interleave(input_bs, dim=0)
+                    outputs[0] = outputs[0].repeat_interleave(num_beams, dim=0)
                     outputs = tuple(outputs)
                 if synced_gpus and this_peer_finished:
                     cur_len = cur_len + 1
@@ -565,6 +600,7 @@ def _greedy_search(
             re.search("GPTJ", self.config.architectures[0])
             or re.search("llama", self.config.architectures[0], re.IGNORECASE)
             or re.search("gptneox", self.config.architectures[0], re.IGNORECASE)
+            or re.search("OPT", self.config.architectures[0], re.IGNORECASE)
         ):
             first_token = False
             input_bs = input_ids.size()[0]
@@ -616,6 +652,21 @@ def _greedy_search(
                             for i in range(self.config.num_hidden_layers)
                         ]
                     )
+                elif re.search("OPT", self.config.architectures[0], re.IGNORECASE):
+                    beam_idx_tmp = torch.zeros(
+                        (2048, int(input_bs)), dtype=torch.long
+                    ).contiguous()
+                    model_inputs["past_key_values"] = tuple(
+                        [
+                            (
+                                torch.zeros([1, 1, 1, 1]).contiguous(),
+                                torch.zeros([1, 1, 1, 1]).contiguous(),
+                                beam_idx_tmp,
+                                torch.zeros(1, dtype=torch.long).contiguous(),
+                            )
+                            for i in range(self.config.num_hidden_layers)
+                        ]
+                    )
 
             if hasattr(self, "trace_graph"):
                 model_inputs.pop("use_cache", None)
diff --git a/intel_extension_for_pytorch/cpu/transformers/models.py b/intel_extension_for_pytorch/cpu/transformers/models.py
index 7f4384544..2fab54d24 100644
--- a/intel_extension_for_pytorch/cpu/transformers/models.py
+++ b/intel_extension_for_pytorch/cpu/transformers/models.py
@@ -7,6 +7,7 @@ from transformers.modeling_outputs import (
     CausalLMOutputWithPast,
 )
 from transformers.utils import logging
+import random
 
 logger = logging.get_logger(__name__)
 
@@ -523,6 +524,149 @@ def GPTNeoXModel_forward(
     )
 
 
+def OPTDecoder_forward(
+    self,
+    input_ids: torch.LongTensor = None,
+    attention_mask: Optional[torch.Tensor] = None,
+    head_mask: Optional[torch.Tensor] = None,
+    past_key_values: Optional[List[torch.FloatTensor]] = None,
+    inputs_embeds: Optional[torch.FloatTensor] = None,
+    use_cache: Optional[bool] = None,
+    output_attentions: Optional[bool] = None,
+    output_hidden_states: Optional[bool] = None,
+    return_dict: Optional[bool] = None,
+) -> Union[Tuple, BaseModelOutputWithPast]:
+    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+    output_hidden_states = (
+        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+    )
+    use_cache = use_cache if use_cache is not None else self.config.use_cache
+
+    return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+    # retrieve input_ids and inputs_embeds
+    if input_ids is not None and inputs_embeds is not None:
+        raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
+    elif input_ids is not None:
+        input_shape = input_ids.size()
+        input_ids = input_ids.view(-1, input_shape[-1])
+    elif inputs_embeds is not None:
+        input_shape = inputs_embeds.size()[:-1]
+    else:
+        raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
+
+    if inputs_embeds is None:
+        inputs_embeds = self.embed_tokens(input_ids)
+
+    batch_size, seq_length = input_shape
+    past_key_values_length = 0
+    if past_key_values is not None and len(past_key_values[0]) !=4: #not discrete kv cache
+        past_key_values_length = past_key_values[0][0].shape[2]
+    elif past_key_values is not None and len(past_key_values[0]) ==4: #discrete kv cache
+        past_key_values_length = past_key_values[0][3]
+    # required mask seq length can be calculated via length of past
+    mask_seq_length = past_key_values_length + seq_length
+
+    # embed positions
+    if attention_mask is None:
+        attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)
+    causal_attention_mask = self._prepare_decoder_attention_mask(
+        attention_mask, input_shape, inputs_embeds, past_key_values_length
+    )
+    pos_embeds = self.embed_positions(attention_mask, past_key_values_length)
+
+    if self.project_in is not None:
+        inputs_embeds = self.project_in(inputs_embeds)
+
+    hidden_states = inputs_embeds + pos_embeds
+
+    if self.gradient_checkpointing and self.training:
+        if use_cache:
+            logger.warning_once(
+                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+            )
+            use_cache = False
+
+    # decoder layers
+    all_hidden_states = () if output_hidden_states else None
+    all_self_attns = () if output_attentions else None
+    next_decoder_cache = () if use_cache else None
+
+    # check if head_mask has a correct number of layers specified if desired
+    for attn_mask, mask_name in zip([head_mask], ["head_mask"]):
+        if attn_mask is not None:
+            if attn_mask.size()[0] != (len(self.layers)):
+                raise ValueError(
+                    f"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for"
+                    f" {head_mask.size()[0]}."
+                )
+
+    for idx, decoder_layer in enumerate(self.layers):
+        # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
+        if output_hidden_states:
+            all_hidden_states += (hidden_states,)
+
+        dropout_probability = random.uniform(0, 1)
+        if self.training and (dropout_probability < self.layerdrop):
+            continue
+
+        past_key_value = past_key_values[idx] if past_key_values is not None else None
+
+        if self.gradient_checkpointing and self.training:
+
+            def create_custom_forward(module):
+                def custom_forward(*inputs):
+                    # None for past_key_value
+                    return module(*inputs, output_attentions, None)
+
+                return custom_forward
+
+            layer_outputs = torch.utils.checkpoint.checkpoint(
+                create_custom_forward(decoder_layer),
+                hidden_states,
+                causal_attention_mask,
+                head_mask[idx] if head_mask is not None else None,
+                None,
+            )
+        else:
+            layer_outputs = decoder_layer(
+                hidden_states,
+                attention_mask=causal_attention_mask,
+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),
+                past_key_value=past_key_value,
+                output_attentions=output_attentions,
+                use_cache=use_cache,
+            )
+
+        hidden_states = layer_outputs[0]
+
+        if use_cache:
+            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
+
+        if output_attentions:
+            all_self_attns += (layer_outputs[1],)
+
+    if self.final_layer_norm is not None:
+        hidden_states = self.final_layer_norm(hidden_states)
+
+    if self.project_out is not None:
+        hidden_states = self.project_out(hidden_states)
+
+    # add hidden states from the last decoder layer
+    if output_hidden_states:
+        all_hidden_states += (hidden_states,)
+
+    next_cache = next_decoder_cache if use_cache else None
+    if not return_dict:
+        return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
+    return BaseModelOutputWithPast(
+        last_hidden_state=hidden_states,
+        past_key_values=next_cache,
+        hidden_states=all_hidden_states,
+        attentions=all_self_attns,
+    )
+
+
 def GPTJForCausalLM_forward(
     self,
     input_ids: Optional[torch.LongTensor] = None,
@@ -718,3 +862,58 @@ def GPTNeoXForCausalLM_forward(
         hidden_states=outputs.hidden_states,
         attentions=outputs.attentions,
     )
+
+def OPTForCausalLM_forward(
+    self,
+    input_ids: torch.LongTensor = None,
+    attention_mask: Optional[torch.Tensor] = None,
+    past_key_values: Optional[List[torch.FloatTensor]] = None,
+    head_mask: Optional[torch.Tensor] = None,
+    inputs_embeds: Optional[torch.FloatTensor] = None,
+    labels: Optional[torch.LongTensor] = None,
+    use_cache: Optional[bool] = None,
+    output_attentions: Optional[bool] = None,
+    output_hidden_states: Optional[bool] = None,
+    return_dict: Optional[bool] = None,
+) -> Union[Tuple, CausalLMOutputWithPast]:
+    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+    output_hidden_states = (
+        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+    )
+    return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
+    outputs = self.model.decoder(
+        input_ids=input_ids,
+        attention_mask=attention_mask,
+        head_mask=head_mask,
+        past_key_values=past_key_values,
+        inputs_embeds=inputs_embeds,
+        use_cache=use_cache,
+        output_attentions=output_attentions,
+        output_hidden_states=output_hidden_states,
+        return_dict=return_dict,
+    )
+
+    logits = self.lm_head(outputs[0]).contiguous()
+
+    loss = None
+    if labels is not None:
+        # Shift so that tokens < n predict n
+        shift_logits = logits[..., :-1, :].contiguous()
+        shift_labels = labels[..., 1:].contiguous()
+        # Flatten the tokens
+        loss_fct = CrossEntropyLoss()
+        loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))
+
+    if not return_dict:
+        output = (logits,) + outputs[1:]
+        return (loss,) + output if loss is not None else output
+
+    return CausalLMOutputWithPast(
+        loss=loss,
+        logits=logits,
+        past_key_values=outputs.past_key_values,
+        hidden_states=outputs.hidden_states,
+        attentions=outputs.attentions,
+    )
\ No newline at end of file
diff --git a/intel_extension_for_pytorch/cpu/transformers/optimize.py b/intel_extension_for_pytorch/cpu/transformers/optimize.py
index c4105a59a..51562a206 100644
--- a/intel_extension_for_pytorch/cpu/transformers/optimize.py
+++ b/intel_extension_for_pytorch/cpu/transformers/optimize.py
@@ -56,6 +56,17 @@ def is_distributed(m):
         is_distributed(sub_m)
 
 
+def _set_optimized_model_for_generation(
+    model,
+    optimized_model,
+    first_token_optimized_model=None,
+):
+    if first_token_optimized_model is not None:
+        setattr(model, "trace_graph_first", first_token_optimized_model)
+
+    setattr(model, "trace_graph", optimized_model)
+
+
 def _optimize_transformers(
     model,
     dtype=torch.float,
@@ -101,11 +112,10 @@ def _optimize_transformers(
             # tpp rope optimization has transformers version requirements
             installed_pkg = {pkg.key for pkg in pkg_resources.working_set}
             min_version = "4.28.0"
-            max_version = "4.30.0"
             if "transformers" not in installed_pkg:
                 raise RuntimeError(
-                    "optimize_transformers optimization requires transformers package and its version between {} and {}, fallback due to not meet".format(
-                        min_version, max_version
+                    "optimize_transformers optimization requires transformers package and its version at least {} , fallback due to not meet".format(
+                        min_version
                     )
                 )
 
@@ -113,12 +123,10 @@ def _optimize_transformers(
             from packaging import version
 
             trans_version = transformers.__version__
-            if version.parse(trans_version) < version.parse(
-                min_version
-            ) or version.parse(trans_version) > version.parse(max_version):
+            if version.parse(trans_version) < version.parse(min_version):
                 raise RuntimeError(
-                    "optimize_transformers optimization requires the transformers with version: between {} and {} while now transformers== {}, fallback due to not meet".format(
-                        min_version, max_version, trans_version
+                    "optimize_transformers optimization requires the transformers with version: at least {} while now transformers== {}, fallback due to not meet".format(
+                        min_version, trans_version
                     )
                 )
 
@@ -133,6 +141,7 @@ def _optimize_transformers(
                 _LlamaAttention_GQA,
                 _GPTJAttention,
                 _GPTNeoXAttention,
+                OPTAttention_forward,
                 _reorder_cache,
             )
             from intel_extension_for_pytorch.cpu.tpp.fused_llm import (
@@ -142,25 +151,29 @@ def _optimize_transformers(
                 GPTNeoXMLP_forward,
                 GPTNeoXLayer_forward,
                 LlamaMLP_forward,
+                LlamaMLP_forward_distributed,
                 LlamaDecoderLayer_forward,
             )
             from .models import (
                 GPTJModel_forward,
                 LlamaModel_forward,
                 GPTNeoXModel_forward,
+                OPTDecoder_forward,
                 GPTJForCausalLM_forward,
                 LlamaForCausalLM_forward,
                 GPTNeoXForCausalLM_forward,
+                OPTForCausalLM_forward,
             )
 
             well_supported_model = (
                 re.search("GPTJ", model.config.architectures[0], re.IGNORECASE)
                 or re.search("llama", model.config.architectures[0], re.IGNORECASE)
                 or re.search("gptneox", model.config.architectures[0], re.IGNORECASE)
+                or re.search("OPT", model.config.architectures[0], re.IGNORECASE)
             )
             if not well_supported_model:
                 warnings.warn(
-                    "optimize_transformers currently well supports Llama, GPT-J, GPT-Neox"
+                    "optimize_transformers currently well supports Llama, GPT-J, GPT-Neox, OPT"
                 )
 
             if not inplace:
@@ -198,6 +211,12 @@ def _optimize_transformers(
                         "forward",
                         GPTNeoXForCausalLM_forward,
                     )
+                elif re.search("OPT", model.config.architectures[0], re.IGNORECASE):
+                    convert_function(
+                        _model,
+                        "forward",
+                        OPTForCausalLM_forward,
+                    )
 
                 convert_forward(
                     _model,
@@ -214,13 +233,23 @@ def _optimize_transformers(
                     transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel,
                     GPTNeoXModel_forward,
                 )
+                convert_forward(
+                    _model,
+                    transformers.models.opt.modeling_opt.OPTDecoder,
+                    OPTDecoder_forward,
+                )
+                convert_forward(
+                    _model,
+                    transformers.models.opt.modeling_opt.OPTAttention,
+                    OPTAttention_forward,
+                )
                 convert_class(
                     _model,
                     transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention,
                     _GPTNeoXAttention,
                     _model.config,
                 )
-                if hasattr(_model.config, "num_attention_kv_heads"):
+                if hasattr(_model.config, "num_key_value_heads"):
                     convert_class(
                         _model,
                         transformers.models.llama.modeling_llama.LlamaAttention,
@@ -249,29 +278,48 @@ def _optimize_transformers(
                         _prepare_decoder_attention_mask,
                     )
                 else:
-                    # linear-wise optimizations
-                    _enable_tpp()
-                    _model = optimize(_model.eval(), dtype=dtype, inplace=True)
-                    # linear-postop-wise optimizations
-
-                    is_distributed(_model)
-                    if not distributed:
-                        convert_forward(
-                            _model,
-                            transformers.models.gptj.modeling_gptj.GPTJBlock,
-                            GPTJBlock_forward,
-                        )
-                        convert_forward(
-                            _model,
-                            transformers.models.gptj.modeling_gptj.GPTJMLP,
-                            GPTJMLP_forward,
-                        )
+                    if not re.search("OPT", model.config.architectures[0], re.IGNORECASE):
+                        # linear-wise optimizations
+                        _enable_tpp()
+                        _model = optimize(_model.eval(), dtype=dtype, inplace=True)
+
+                        # linear-postop-wise optimizations
+                        is_distributed(_model)
+                        if not distributed:
+                            convert_forward(
+                                _model,
+                                transformers.models.gptj.modeling_gptj.GPTJBlock,
+                                GPTJBlock_forward,
+                            )
+                            convert_forward(
+                                _model,
+                                transformers.models.gptj.modeling_gptj.GPTJMLP,
+                                GPTJMLP_forward,
+                            )
+                            convert_forward(
+                                _model,
+                                transformers.models.llama.modeling_llama.LlamaDecoderLayer,
+                                LlamaDecoderLayer_forward,
+                            )
+                            convert_forward(
+                                _model,
+                                transformers.models.llama.modeling_llama.LlamaMLP,
+                                LlamaMLP_forward,
+                            )
+
+                        else:
+                            convert_forward(
+                                _model,
+                                transformers.models.llama.modeling_llama.LlamaMLP,
+                                LlamaMLP_forward_distributed,
+                            )
+                            convert_forward(
+                                _model,
+                                transformers.models.gptj.modeling_gptj.GPTJMLP,
+                                GPTJMLP_forward_distributed,
+                            )
                     else:
-                        convert_forward(
-                            _model,
-                            transformers.models.gptj.modeling_gptj.GPTJMLP,
-                            GPTJMLP_forward_distributed,
-                        )
+                        _model = optimize(_model.eval(), dtype=dtype, inplace=True)
             else:
                 raise RuntimeError(
                     "optimize_transformers optimization currently supports dtype: torch.float, torch.bfloat16, torch.int8, will cover more soon."
diff --git a/intel_extension_for_pytorch/nn/utils/_parameter_wrapper.py b/intel_extension_for_pytorch/nn/utils/_parameter_wrapper.py
index 897ff8f64..8680dcdae 100644
--- a/intel_extension_for_pytorch/nn/utils/_parameter_wrapper.py
+++ b/intel_extension_for_pytorch/nn/utils/_parameter_wrapper.py
@@ -4,7 +4,10 @@ import functools
 import contextlib
 import types
 import warnings
-from intel_extension_for_pytorch.cpu._auto_kernel_selection import _using_dnnl, _using_tpp
+from intel_extension_for_pytorch.cpu._auto_kernel_selection import (
+    _using_dnnl,
+    _using_tpp,
+)
 from intel_extension_for_pytorch import frontend
 from intel_extension_for_pytorch.nn.utils._weight_prepack import (
     _IPEXLinear,
@@ -14,6 +17,7 @@ from intel_extension_for_pytorch.nn.utils._weight_prepack import (
     _IPEXConvTranspose2d,
     _IPEXConvTranspose3d,
     _IPEXLinearAllreduce,
+    _IPEXLmHeadLinearAllreduce,
     may_import_deepspeed_modules,
 )
 
@@ -31,12 +35,17 @@ def IPEX_WEIGHT_PREPACK_MODULE_CPU():
 
     deepspeed_modules = may_import_deepspeed_modules()
     if deepspeed_modules is not None:
-        LinearAllreduce, LinearLayer = deepspeed_modules
-        deepspeed_modules = {
+        LinearAllreduce, LinearLayer = deepspeed_modules[:2]
+        deepspeed_modules_mapping = {
             LinearLayer: _IPEXLinear,
             LinearAllreduce: _IPEXLinearAllreduce,
         }
-        torch_modules.update(deepspeed_modules)
+        if len(deepspeed_modules) > 2:
+            LmHeadLinearAllreduce = deepspeed_modules[2]
+            deepspeed_modules_mapping.update(
+                {LmHeadLinearAllreduce: _IPEXLmHeadLinearAllreduce}
+            )
+        torch_modules.update(deepspeed_modules_mapping)
 
     return torch_modules
 
@@ -433,6 +442,7 @@ class ParameterWrapper(object):
             assert target_module in (
                 _IPEXLinear,
                 _IPEXLinearAllreduce,
+                _IPEXLmHeadLinearAllreduce,
             )
             self.linear_prepack(module, is_training)
 
@@ -546,8 +556,13 @@ class ParameterWrapper(object):
                 )
             self.pack_weight(use_dnnl)
         else:
-            from intel_extension_for_pytorch.nn.utils import Apply_TPPLinear_weight_prepack
+            from intel_extension_for_pytorch.nn.utils import (
+                Apply_TPPLinear_weight_prepack,
+            )
+
             Apply_TPPLinear_weight_prepack(module, dtype=module.weight.dtype)
+            self.parameter.data = module.weight.data
+            self.parameter = module.weight
 
     def load_cast_and_prepack(self, module, param):
         # load from state dict
diff --git a/intel_extension_for_pytorch/nn/utils/_weight_prepack.py b/intel_extension_for_pytorch/nn/utils/_weight_prepack.py
index 0a851efa5..34deceef9 100644
--- a/intel_extension_for_pytorch/nn/utils/_weight_prepack.py
+++ b/intel_extension_for_pytorch/nn/utils/_weight_prepack.py
@@ -10,9 +10,14 @@ from intel_extension_for_pytorch.cpu.tpp.utils.blocked_layout import (
     BlockedParameter,
     get_vnni_blocking,
 )
+
+from intel_extension_for_pytorch.cpu._auto_kernel_selection import _using_tpp
+
 logger = logging.getLogger(__name__)
 
 USE_LOW_PREC_PARAMS = True
+
+
 def TPPLinear_weight_prepack(m, bk=None, bc=None, layer_dtype=torch.float32):
     m.__class__ = _IPEXLinear
     m.weight = BlockedParameter(m.weight.data)
@@ -44,26 +49,44 @@ def TPPLinear_weight_prepack(m, bk=None, bc=None, layer_dtype=torch.float32):
         m.bias.set_blocking_param((None, None, layer_dtype))
     return m
 
-def Apply_TPPLinear_weight_prepack(m, dtype, device='cpu'):
-    if m.bias is not None and m.weight.size()[0] == 50400:
+
+def Apply_TPPLinear_weight_prepack(m, dtype, device="cpu"):
+    if (m.weight.size()[0] == 50400 or m.weight.size()[0] == 32000) and m.weight.size()[
+        1
+    ] % 64 == 0:
         m = TPPLinear_weight_prepack(m, 100, 64, dtype)
-    else:
+    elif m.weight.size()[0] % 16 == 0 and m.weight.size()[1] % 64 == 0:
         m = TPPLinear_weight_prepack(m, 16, 64, dtype)
+    else:
+        setattr(m, "tpp_fallback", True)
+        return
+    setattr(m, "tpp_fallback", False)
 
     block(m)
 
+
 def block(model):
     for m in model.modules():
         if hasattr(m, "maybe_block_params"):
             m.maybe_block_params()
 
+
 def may_import_deepspeed_modules():
     try:
         # import deepspeed in a global space will raise circular import error
         # intel-extension-for-deepspeed imports both IPEX and deepspeed
         from deepspeed.module_inject.layers import LinearAllreduce, LinearLayer
 
-        return LinearAllreduce, LinearLayer
+        ds_layers = [LinearAllreduce, LinearLayer]
+
+        # TODO: remove this logic once deepspeed LmHeadLinearAllreduce change has been upstream-ed.
+        try:
+            from deepspeed.module_inject.layers import LmHeadLinearAllreduce
+
+            ds_layers.append(LmHeadLinearAllreduce)
+            return ds_layers
+        except ImportError:
+            return ds_layers
     except ImportError:
         return None
 
@@ -71,7 +94,8 @@ def may_import_deepspeed_modules():
 installed_pkg = {pkg.key for pkg in pkg_resources.working_set}
 if "deepspeed" in installed_pkg:
     from deepspeed import comm
-    DS_SHM_ALLREDUCE = os.getenv('DS_SHM_ALLREDUCE')
+
+    DS_SHM_ALLREDUCE = os.getenv("DS_SHM_ALLREDUCE")
 
     def _all_reduce(self, reduceOp, tag, ranks, group_size):
         if DS_SHM_ALLREDUCE == "1":
@@ -99,8 +123,8 @@ def _all_reduce_and_bias_add(mp_group, original_bias, output):
         )
 
     if original_bias is not None:
-        output += original_bias    
-    
+        output += original_bias
+
     return output
 
 
@@ -213,15 +237,21 @@ class _IPEXConv3d(_IPEXConvNd):
 class _IPEXLinear(_IPEXPrepackModule):
     def __init__(self):
         super(_IPEXLinear, self).__init__()
+
     def maybe_block_params(self):
         self.weight.block()
         if self.bias is not None:
             self.bias.block()
 
+    def pre_ipex_gemm(self, input):
+        return input
+
     def post_ipex_gemm(self, output):
         return output
 
     def forward(self, x):
+        x = self.pre_ipex_gemm(x)
+
         if self.use_dnnl:
             output = torch.ops.torch_ipex.ipex_linear(
                 x,
@@ -231,10 +261,16 @@ class _IPEXLinear(_IPEXPrepackModule):
                 self.out_features,
             )
         elif self.use_tpp:
-            if self.bias is not None:
-                output = torch.ops.torch_ipex.fc_plain_gemm(x, self.weight, self.bias)
+            if self.tpp_fallback:
+                output = torch.nn.functional.linear(x, self.weight, self.bias)
             else:
-                output = torch.ops.torch_ipex.qkv_gemm(x, self.weight)
+                x = x.to(self.weight.dtype).contiguous()
+                if self.bias is not None:
+                    output = torch.ops.torch_ipex.tpp_linear_bias(
+                        x, self.weight, self.bias
+                    )
+                else:
+                    output = torch.ops.torch_ipex.tpp_linear(x, self.weight)
         else:
             output = torch.ops.torch_ipex.ipex_MKLSGEMM(
                 x,
@@ -302,6 +338,21 @@ class _IPEXLinearAllreduce(_IPEXLinear):
         return _all_reduce_and_bias_add(self.mp_group, self.original_bias, output)
 
 
+class _IPEXLmHeadLinearAllreduce(_IPEXLinear):
+    def __init__(self):
+        super(_IPEXLmHeadLinearAllreduce, self).__init__()
+
+    def pre_ipex_gemm(self, input):
+        assert (
+            input.shape[-1] % self.world_size == 0
+        ), "please ensure input.shape[-1] % self.world_size == 0"
+        input_shard = input.shape[-1] // self.world_size
+        return input[:, :, self.rank * input_shard : (self.rank + 1) * input_shard]
+
+    def post_ipex_gemm(self, output):
+        return _all_reduce_and_bias_add(self.mp_group, self.original_bias, output)
+
+
 class _IPEXConvTransposeNd(_IPEXPrepackModule):
     __constants__ = [
         "stride",
@@ -434,11 +485,21 @@ def weight_prepack_with_ipex(model, optimizer, params_attr, device_type="cpu"):
         if param_wrapper.can_prepack(m, is_training):
             new_m = IPEX_WEIGHT_PREPACK_MODULE_CPU()[m.__class__]()
             all_reduce_bias = m.bias
-            if isinstance(new_m, _IPEXLinearAllreduce):
+            if isinstance(new_m, (_IPEXLinearAllreduce, _IPEXLmHeadLinearAllreduce)):
                 m.bias = None
-            param_wrapper.prepack(m, is_training)
+            if _using_tpp():
+                weight_key = m.weight
+                param_wrapper.prepack(m, is_training)
+                if m.tpp_fallback:
+                    setattr(new_m, "tpp_fallback", True)
+                params_attr[m.weight] = params_attr.pop(weight_key)
+                del weight_key
+
+            else:
+                param_wrapper.prepack(m, is_training)
+
             new_m.__dict__ = m.__dict__
-            if isinstance(new_m, _IPEXLinearAllreduce):
+            if isinstance(new_m, (_IPEXLinearAllreduce, _IPEXLmHeadLinearAllreduce)):
                 new_m.original_bias = all_reduce_bias
             new_m.ctx = param_wrapper.op_ctx
             setattr(new_m, "weight_wrapper", param_wrapper)  # noqa: B010
diff --git a/intel_extension_for_pytorch/quantization/_quantize.py b/intel_extension_for_pytorch/quantization/_quantize.py
index 8416ac27b..46258be55 100644
--- a/intel_extension_for_pytorch/quantization/_quantize.py
+++ b/intel_extension_for_pytorch/quantization/_quantize.py
@@ -115,7 +115,7 @@ def _may_insert_deepspeed_modules(
 ):
     deepspeed_modules = may_import_deepspeed_modules()
     if deepspeed_modules is not None:
-        LinearAllreduce, LinearLayer = deepspeed_modules
+        LinearAllreduce, LinearLayer = deepspeed_modules[:2]
         deepspeed_modules = {
             LinearLayer: q_linear_layer_module,
             LinearAllreduce: q_linear_all_reduce_module,
@@ -207,7 +207,7 @@ class DynamicQuantizedLinearLayer(_IPEXDynamicQuantizedLinear):
         _FLOAT_MODULE = [torch.nn.Linear]
         deepspeed_modules = may_import_deepspeed_modules()
         if deepspeed_modules is not None:
-            _, LinearLayer = deepspeed_modules
+            LinearLayer = deepspeed_modules[1]
             _FLOAT_MODULE.extend([LinearLayer])
         return _FLOAT_MODULE
 
@@ -236,7 +236,7 @@ class DynamicQuantizedLinearAllreduce(_IPEXDynamicQuantizedLinear):
         assert (
             deepspeed_modules is not None
         ), "DynamicQuantizedLinearAllreduce requires deepspeed to be installed"
-        LinearAllreduce, _ = deepspeed_modules
+        LinearAllreduce = deepspeed_modules[0]
         _FLOAT_MODULE = [LinearAllreduce]
         return _FLOAT_MODULE
 
@@ -284,7 +284,7 @@ def may_quantize_deepspeed_modules(
 ):
     deepspeed_modules = may_import_deepspeed_modules()
     if deepspeed_modules is not None:
-        LinearAllreduce, LinearLayer = deepspeed_modules
+        LinearAllreduce, LinearLayer = deepspeed_modules[:2]
         module_mappings.update(IPEX_QUANTIZATION_MODULE)
         deepspeed_qconfig_spec = {
             LinearLayer: q_config,
diff --git a/tests/cpu/iakv_test.py b/tests/cpu/iakv_test.py
new file mode 100644
index 000000000..ce25d421e
--- /dev/null
+++ b/tests/cpu/iakv_test.py
@@ -0,0 +1,88 @@
+import torch
+import torch.nn as nn
+import intel_extension_for_pytorch as ipex
+from common_utils import TestCase
+import unittest
+from typing import Optional, Tuple, Union
+from torch.nn import functional as F
+import time 
+
+class MaskedMHA(torch.nn.Module):
+    def __init__(self, hidden_size=4096, n_head=16, n_head_kv=16, head_dim = 256):
+        super().__init__()
+        self.num_heads = n_head
+        self.num_kv = n_head_kv
+        self.head_dim = head_dim
+        self.query_key_value = nn.Linear(hidden_size, (n_head_kv * 2 + n_head) * head_dim)
+        
+    def _split_heads(self, fused_qkv: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+        """
+        Split the last dimension into (num_heads, head_dim), results share same memory
+        storage as `fused_qkv`
+
+        Args:
+            fused_qkv (`torch.tensor`, *required*): [batch_size, seq_length, (num_heads + kv_num * 2) * head_dim]
+
+        Returns:
+            query: [batch_size, seq_length, num_heads, head_dim]
+            key: [batch_size, seq_length, kv_num, head_dim]
+            value: [batch_size, seq_length, kv_num, head_dim]
+        """
+        bs = fused_qkv.shape[0]
+        query_layer = fused_qkv[:, :, : self.num_heads * self.head_dim]
+        query_layer = query_layer.view(bs, -1, self.num_heads, self.head_dim)
+        key_layer = fused_qkv[:, :, self.num_heads * self.head_dim : (self.num_heads + self.num_kv) * self.head_dim]
+        key_layer = key_layer.view(bs, -1, self.num_kv, self.head_dim)
+        value_layer = fused_qkv[:, :, (self.num_heads + self.num_kv) * self.head_dim :]
+        value_layer = value_layer.view(bs, -1, self.num_kv, self.head_dim)
+        return query_layer, key_layer, value_layer
+    
+    def _repeat_kv(self, x: torch.Tensor, n_rep: int) -> torch.Tensor:
+        "torch.repeat_interleave(x, dim=2, repeats=n_rep)"
+        bs, slen, n_kv_heads, head_dim = x.shape
+        if n_rep == 1:
+            return x
+        return(
+            x[:,:,:,None,:]
+            .expand(bs, slen, n_kv_heads, n_rep, head_dim)
+            .reshape(bs, slen, n_kv_heads * n_rep, head_dim)
+        )    
+        
+    def forward(self, input_t, key_cache, value_cache, max_position, attention_mask, beam_idx, indirect_access_kv_cache=True, offset=0):
+        head_size= self.head_dim
+        #self.query_key_value(input_t)
+        #linear_res=  torch.randn(input_t.shape[0], input_t.shape[1], (self.num_heads + self.num_kv * 2) * head_size, dtype=torch.bfloat16)
+        query = torch.randn(input_t.shape[0], input_t.shape[1], self.num_heads, self.head_dim, dtype=torch.bfloat16)
+        key = query.clone()
+        value = key.clone()
+        return torch.ops.torch_ipex.masked_multihead_self_attention(query, key, value, key_cache, value_cache, beam_idx, offset, head_size**0.5, max_position, None, attention_mask)
+
+mha = MaskedMHA()
+max_seq_len=2048
+head_num=16
+beam_size=4
+head_size=256
+batch_size=1
+input_t = torch.randn(batch_size*beam_size, 1, head_num * head_size, dtype=torch.bfloat16)
+key_cache_iakv = torch.randn(max_seq_len, beam_size*batch_size, head_num, head_size, dtype=torch.bfloat16) 
+value_cache_iakv = torch.randn(max_seq_len, beam_size*batch_size, head_num, head_size, dtype=torch.bfloat16)
+beam_idx = torch.zeros(max_seq_len, beam_size*batch_size, dtype=torch.int64)   
+offset = 2016 
+attention_mask = torch.zeros(batch_size*beam_size, 1, 1, offset+1, dtype=torch.bfloat16)  
+count =10000
+total_time = 0
+for i in range(count):
+    start =time.time()
+    with torch.inference_mode(), torch.no_grad(), torch.autocast(
+                            device_type="cpu",
+                            enabled=True,
+                            dtype=torch.bfloat16,
+                        ):
+        indirect_access_kv_cache_output, _, key_cache_iakv, value_cache_iakv, beam_idx = mha(input_t, key_cache_iakv, value_cache_iakv, max_seq_len, attention_mask, beam_idx, True, torch.tensor(offset))
+    end = time.time()
+    if i>=5:
+        total_time += end-start
+print("iakv time: ", total_time/(count-5))
+        
+        
+    
\ No newline at end of file
diff --git a/tests/cpu/test_deepspeed.py b/tests/cpu/test_deepspeed.py
index 03b7b033c..ef7b1d0eb 100644
--- a/tests/cpu/test_deepspeed.py
+++ b/tests/cpu/test_deepspeed.py
@@ -5,13 +5,14 @@ import unittest
 
 import torch
 import torch.nn as nn
-from torch.testing._internal.common_utils import TestCase
+from torch.testing._internal.jit_utils import JitTestCase
 from torch.testing import FileCheck
 import intel_extension_for_pytorch as ipex
 from intel_extension_for_pytorch.nn.utils._weight_prepack import (
     may_import_deepspeed_modules,
     _IPEXLinear,
     _IPEXLinearAllreduce,
+    _IPEXLmHeadLinearAllreduce,
 )
 from intel_extension_for_pytorch.quantization import prepare, convert
 from intel_extension_for_pytorch.quantization._quantize import (
@@ -45,7 +46,6 @@ class MyBlock(nn.Module):
         return z
 
 
-# For deepspeed support, please do not change the name of the class.
 class MyModel(nn.Module):
     def __init__(self):
         super().__init__()
@@ -58,19 +58,103 @@ class MyModel(nn.Module):
         return x
 
 
+# For deepspeed support, please do not change the name of the class.
+class MyLmHeadModel(nn.Module):
+    def __init__(self):
+        super().__init__()
+        # For deepspeed support, please do not change the ModuleList structure of the class.
+        self.linears = nn.ModuleList([MyBlock()])
+        self.lm_head = nn.Linear(2, 2)
+
+    def forward(self, x):
+        for l in self.linears:
+            x = l(x)
+        x = self.lm_head(x)
+        return x
+
+
 # The class DeepSpeedTestM is written for deepspeed to recognize the modules and to be functional.
 # Please do not change it.
 class DeepSpeedTestM(nn.Module):
+    def __init__(self, module_type):
+        super().__init__()
+        self.linear = module_type()
+
+    def forward(self, x):
+        z = self.linear(x)
+        return z
+
+
+class GPTJAttention(nn.Module):
     def __init__(self):
         super().__init__()
-        self.linear = MyModel()
+        self.q_proj = nn.Linear(4096, 4096, bias=False)
+        self.out_proj = nn.Linear(4096, 4096, bias=False)
+
+    def forward(self, x):
+        x = self.q_proj(x)
+        z = self.out_proj(x)
+        return z
+
+
+class GPTJMLP(nn.Module):
+    def __init__(self, krnl="tpp"):
+        super().__init__()
+        self.krnl = krnl
+        self.fc_in = nn.Linear(4096, 16384, bias=True)
+        self.fc_out = nn.Linear(16384, 4096, bias=True)
+        self.dropout = nn.Dropout()
+
+    def forward(self, x):
+        if self.krnl is "onednn":
+            x = self.fc_in(x)
+            x = nn.functional.gelu(x, approximate="tanh")
+        else:
+            x = torch.ops.torch_ipex.tpp_linear_gelu(
+                x, self.fc_in.weight, self.fc_in.bias
+            )
+        x = self.fc_out(x)
+        x = self.dropout(x)
+        return x
+
+
+class GPTJBlock(nn.Module):
+    def __init__(self, krnl):
+        super().__init__()
+        self.ln = nn.LayerNorm(4096, eps=1e-05)
+        self.attn = GPTJAttention()
+        self.mlp = GPTJMLP(krnl)
+
+    def forward(self, x):
+        x = self.ln(x)
+        y = self.attn(x)
+        z = self.mlp(x)
+        x = y + z + x
+        return x
+
+
+class GPTJModel(nn.Module):
+    def __init__(self, krnl):
+        super().__init__()
+        self.linears = nn.ModuleList([GPTJBlock(krnl)])
+
+    def forward(self, x):
+        for l in self.linears:
+            x = l(x)
+        return x
+
+
+class GPTJTestM(nn.Module):
+    def __init__(self, krnl):
+        super().__init__()
+        self.linear = GPTJModel(krnl)
 
     def forward(self, x):
         z = self.linear(x)
         return z
 
 
-class DeepspeedTester(TestCase):
+class DeepspeedTester(JitTestCase):
     def _get_ds_model(self, m_linear):
         import deepspeed
 
@@ -91,32 +175,49 @@ class DeepspeedTester(TestCase):
     def test_ipex_optimize(self):
         deepspeed_modules = may_import_deepspeed_modules()
         if deepspeed_modules is not None:
-            LinearAllreduce, LinearLayer = deepspeed_modules
-            x = torch.randn(2, 4)
-            m_linear = DeepSpeedTestM().eval()
+            LinearAllreduce, LinearLayer = deepspeed_modules[:2]
+            # TODO: remove check_lm_head logic once deepspeed LmHeadLinearAllreduce change has been upstream-ed.
+            check_lm_head = False
+            if len(deepspeed_modules) == 3:
+                check_lm_head = True
+                LmHeadLinearAllreduce = deepspeed_modules[2]
+
+            x = torch.randn(2, 3, 4)
+            m_linear = DeepSpeedTestM(MyLmHeadModel).eval()
             y = m_linear(x)
 
             ds_model = self._get_ds_model(m_linear)
             self.assertTrue(module_found(ds_model, LinearLayer))
             self.assertTrue(module_found(ds_model, LinearAllreduce))
+            if check_lm_head:
+                self.assertTrue(module_found(ds_model, LmHeadLinearAllreduce))
 
             optimized = ipex.optimize(ds_model.eval(), inplace=True)
-            jit_optimized = torch.jit.trace(optimized, x)
-            jit_optimized = torch.jit.freeze(jit_optimized)
-            self.assertTrue(module_found(optimized, _IPEXLinear))
-            self.assertTrue(module_found(optimized, _IPEXLinearAllreduce))
 
-            optimized = optimized(x)
-            jit_res = jit_optimized(x)
-            self.assertEqual(y, jit_res)
-            self.assertEqual(y, optimized)
+            with torch.no_grad():
+
+                y_optimized = optimized(x)
+                self.assertEqual(y, y_optimized)
+
+                jit_optimized = torch.jit.trace(optimized, x)
+                jit_optimized = torch.jit.freeze(jit_optimized)
+                self.assertTrue(module_found(optimized, _IPEXLinear))
+                self.assertTrue(module_found(optimized, _IPEXLinearAllreduce))
+
+                if check_lm_head:
+                    self.assertTrue(module_found(optimized, _IPEXLmHeadLinearAllreduce))
+
+                jit_optimized(x)
+                graph = jit_optimized.graph_for(x)
+                jit_res = jit_optimized(x)
+                self.assertEqual(y, jit_res)
 
     def _test_quantization(self, dynamic_qconfig, qmodules, graph_strings):
         deepspeed_modules = may_import_deepspeed_modules()
         if deepspeed_modules is not None:
-            LinearAllreduce, LinearLayer = deepspeed_modules
+            LinearAllreduce, LinearLayer = deepspeed_modules[:2]
             x = torch.randn(2, 4)
-            m_linear = DeepSpeedTestM().eval()
+            m_linear = DeepSpeedTestM(MyModel).eval()
             y = m_linear(x)
 
             ds_model = self._get_ds_model(m_linear)
@@ -181,6 +282,37 @@ class DeepspeedTester(TestCase):
             ["torch_ipex::ipex_woq_linear", "deepspeed_comm::all_reduce"],
         )
 
+    def test_simplify_allreduce_for_gptj(self):
+        deepspeed_modules = may_import_deepspeed_modules()
+        if deepspeed_modules is not None:
+            ds_pattern = "deepspeed_comm::all_reduce"
+            x = torch.rand(4, 32, 4096)
+            for krnl in ["onednn", "tpp"]:
+                m = GPTJTestM(krnl).eval()
+                ds_model = self._get_ds_model(m)
+                if krnl is "tpp":
+                    ipex.tpp.Apply_TPP_optimization(
+                        ds_model, dtype=torch.bfloat16, distributed=True
+                    )
+                optimized = ipex.optimize(
+                    ds_model.eval(),
+                    inplace=True,
+                    auto_kernel_selection=True if krnl is "onednn" else False,
+                )
+                with torch.no_grad():
+                    y = optimized(x)
+                    jit_optimized = torch.jit.trace(
+                        optimized, x, strict=False, check_trace=False
+                    )
+                    jit_optimized = torch.jit.freeze(jit_optimized)
+                    graph = jit_optimized.graph_for(x)
+                    self.assertGraphContainsExactly(graph, ds_pattern, 2)
+                    jit_optimized(x)
+                    graph = jit_optimized.graph_for(x)
+                    self.assertGraphContainsExactly(graph, ds_pattern, 1)
+                    jit_res = jit_optimized(x)
+                    self.assertEqual(y, jit_res)
+
 
 if __name__ == "__main__":
     deepspeed_modules = may_import_deepspeed_modules()
diff --git a/tests/cpu/test_masked_mha.py b/tests/cpu/test_masked_mha.py
index 579fa660b..aa3ddc579 100644
--- a/tests/cpu/test_masked_mha.py
+++ b/tests/cpu/test_masked_mha.py
@@ -4,7 +4,6 @@ import intel_extension_for_pytorch as ipex
 from common_utils import TestCase
 import unittest
 from typing import Optional, Tuple, Union
-from torch.nn import functional as F
 
 class MaskedMHA(torch.nn.Module):
     def __init__(self, hidden_size=4096, n_head=16, n_head_kv=16, head_dim = 256):
@@ -85,113 +84,129 @@ class MaskedMHA(torch.nn.Module):
 class MaskedMHATest(TestCase):
     def test_mha(self):
         beam_size_list = [1, 4]
-        batch_size = 1 
+        batch_size_list = [1, 2, 4]
         head_size = 256
         head_num = 16
-        head_num_kv_list = [1, 4, 8, 16]
+        head_num_kv_list = [1, 4, 16]
         max_seq_len = 64
-        first_seq_len = 2             
-        for beam_size in beam_size_list:
-            for head_num_kv in head_num_kv_list:
-                key_cache = None
-                value_cache = None
-                offset = 0  
-                mha = MaskedMHA(n_head=head_num, n_head_kv=head_num_kv, head_dim = head_size)
-                #first token decode
-                input_t = torch.randn(batch_size, first_seq_len, head_num * head_size, dtype=torch.float32)
-                key_cache_iakv = torch.randn(max_seq_len, beam_size*batch_size, head_num, head_size, dtype=torch.float32) 
-                value_cache_iakv = torch.randn(max_seq_len, beam_size*batch_size, head_num, head_size, dtype=torch.float32)
-                beam_idx = torch.zeros(max_seq_len, beam_size*batch_size, dtype=torch.int64)        
-                #create attention mask and causal mask
-                attention_mask = torch.zeros(batch_size, 1, first_seq_len, first_seq_len, dtype=torch.float32)
-                casual_mask = torch.full((first_seq_len, first_seq_len), -1e6, dtype=input_t.dtype)
-                casual_mask = casual_mask.triu(1)    
-                casual_mask = casual_mask.unsqueeze(0).unsqueeze(0)
-                attention_mask = attention_mask + casual_mask #combine the attention mask and causal mask
-                #UT for first token with fp32        
-                naive_output, _, key_cache, value_cache, _ = mha(input_t, None, None, max_seq_len, attention_mask, None, None)
-                indirect_access_kv_cache_output, _, key_cache_iakv, value_cache_iakv, beam_idx = mha(input_t, key_cache_iakv, value_cache_iakv, max_seq_len, attention_mask, beam_idx, True, torch.tensor(offset))      
-                print("head_num:", head_num, "head_num_kv: ", head_num_kv)
-                self.assertEqual(naive_output, indirect_access_kv_cache_output)     
-                key_cache = key_cache.repeat_interleave(beam_size, dim=0)
-                value_cache = value_cache.repeat_interleave(beam_size, dim=0)    
-                self.assertEqual(key_cache.transpose(0,1), key_cache_iakv[0:first_seq_len,:,:,:])
-                self.assertEqual(value_cache.transpose(0,1), value_cache_iakv[0:first_seq_len,:,:,:])         
-                beam_idx_t = torch.zeros(beam_size*batch_size, dtype=torch.int64)
-                beam_idx[offset] = beam_idx_t
-                #reorder cache for naive impelementation
-                key_cache = torch.index_select(key_cache, 0, beam_idx_t)
-                value_cache = torch.index_select(value_cache, 0, beam_idx_t)
-                    
-                # #UT for first token with bf16
-                input_t_bf16 = input_t.bfloat16()
-                key_cache_iakv_bf16 = key_cache_iakv.bfloat16()
-                value_cache_iakv_bf16 = value_cache_iakv.bfloat16()
-                attention_mask_bf16 = attention_mask.bfloat16()
-                with torch.inference_mode(), torch.no_grad(), torch.autocast(
-                    device_type="cpu",
-                    enabled=True,
-                    dtype=torch.bfloat16,
-                ):
-                    naive_output_bf16, _, key_cache_bf16, value_cache_bf16, _ = mha(input_t_bf16, None, None, max_seq_len, attention_mask_bf16, None, None)
-                    indirect_access_kv_cache_output_bf16, _, key_cache_iakv_bf16, value_cache_iakv_bf16, beam_idx = mha(input_t_bf16, key_cache_iakv_bf16, value_cache_iakv_bf16, max_seq_len, attention_mask_bf16, beam_idx, True, torch.tensor(offset))
-                    self.assertEqual(naive_output_bf16, indirect_access_kv_cache_output_bf16)      
-                    key_cache_bf16 = torch.index_select(key_cache_bf16, 0, beam_idx_t)
-                    value_cache_bf16 = torch.index_select(value_cache_bf16, 0, beam_idx_t)
-                offset = offset + first_seq_len
-                #UT for next token with fp32
-                input_t = torch.randn(beam_size * batch_size, 1, head_num * head_size, dtype=torch.float32)
-                attention_mask = torch.zeros(beam_size*batch_size, 1, 1, offset+1, dtype=torch.float32)
-                naive_output, _, key_cache, value_cache, _ = mha(input_t, key_cache, value_cache, max_seq_len, attention_mask, None, None)
-                indirect_access_kv_cache_output, _, key_cache_iakv, value_cache_iakv, beam_idx = mha(input_t, key_cache_iakv, value_cache_iakv, max_seq_len, attention_mask, beam_idx, True, torch.tensor(offset))      
-                self.assertEqual(naive_output, indirect_access_kv_cache_output)
-                self.assertEqual(key_cache.transpose(0,1)[offset], key_cache_iakv[offset,:,:,:])
-                self.assertEqual(value_cache.transpose(0,1)[offset], value_cache_iakv[offset,:,:,:])    
-                # #UT for next token with bf16
-                input_t_bf16 = input_t.bfloat16()
-                attention_mask_bf16 = attention_mask.bfloat16()
-                with torch.inference_mode(), torch.no_grad(), torch.autocast(
-                    device_type="cpu",
-                    enabled=True,
-                    dtype=torch.bfloat16,
-                ):
-                    naive_output_bf16, _, key_cache_bf16, value_cache_bf16, _ = mha(input_t_bf16, key_cache_bf16, value_cache_bf16, max_seq_len, attention_mask_bf16, None, None)
-                    indirect_access_kv_cache_output_bf16, _, key_cache_iakv_bf16, value_cache_iakv_bf16, beam_idx = mha(input_t_bf16, key_cache_iakv_bf16, value_cache_iakv_bf16, max_seq_len, attention_mask_bf16, beam_idx, True, torch.tensor(offset))
-                    self.assertEqual(naive_output_bf16, indirect_access_kv_cache_output_bf16, prec=0.05)
-                    self.assertEqual(key_cache_bf16.transpose(0,1)[offset], key_cache_iakv_bf16[offset,:,:,:])
-                    self.assertEqual(value_cache_bf16.transpose(0,1)[offset], value_cache_iakv_bf16[offset,:,:,:])       
+        first_seq_len = 32
+        for batch_size in batch_size_list:          
+            for beam_size in beam_size_list:
+                for head_num_kv in head_num_kv_list:
+                    key_cache = None
+                    value_cache = None
+                    offset = 0  
+                    mha = MaskedMHA(n_head=head_num, n_head_kv=head_num_kv, head_dim = head_size)
+                    #first token decode
+                    input_t = torch.randn(batch_size, first_seq_len, head_num * head_size, dtype=torch.float32)
+                    key_cache_iakv = torch.randn(max_seq_len, beam_size*batch_size, head_num, head_size, dtype=torch.float32) 
+                    value_cache_iakv = torch.randn(max_seq_len, beam_size*batch_size, head_num, head_size, dtype=torch.float32)
+                    beam_idx = torch.zeros(max_seq_len, beam_size*batch_size, dtype=torch.int64)        
+                    #create attention mask and causal mask
+                    attention_mask = torch.zeros(batch_size, 1, first_seq_len, first_seq_len, dtype=torch.float32)
+                    casual_mask = torch.full((first_seq_len, first_seq_len), -1e6, dtype=input_t.dtype)
+                    casual_mask = casual_mask.triu(1)    
+                    casual_mask = casual_mask.unsqueeze(0).unsqueeze(0)
+                    attention_mask = attention_mask + casual_mask #combine the attention mask and causal mask                    
+                    #UT for first token with fp32        
+                    naive_output, _, key_cache, value_cache, _ = mha(input_t, None, None, max_seq_len, attention_mask, None, None)
+                    indirect_access_kv_cache_output, _, key_cache_iakv, value_cache_iakv, beam_idx = mha(input_t, key_cache_iakv, value_cache_iakv, max_seq_len, attention_mask, beam_idx, True, torch.tensor(offset))      
+                    #import pdb ; pdb.set_trace()
+                    print("batch_size:", batch_size, "head_num:", head_num, "head_num_kv: ", head_num_kv)
+                    #self.assertEqual(naive_output, indirect_access_kv_cache_output)     
+                    key_cache = key_cache.repeat_interleave(beam_size, dim=0)
+                    value_cache = value_cache.repeat_interleave(beam_size, dim=0) 
+                    for i in range(batch_size):
+                        self.assertEqual(key_cache.transpose(0,1)[:,i*beam_size, :, :], key_cache_iakv[0:first_seq_len, i*beam_size,:,:])
+                        self.assertEqual(value_cache.transpose(0,1)[:,i*beam_size, :, :], value_cache_iakv[0:first_seq_len, i*beam_size,:,:])                             
                     if beam_size == 4:    
-                        beam_idx_t = torch.tensor([1,3,0,0]).repeat(batch_size)
+                        beam_idx_t = torch.zeros(beam_size*batch_size, dtype=torch.int64)
+                        for i in range(1, batch_size):
+                            beam_idx_t[i*beam_size:i*beam_size+beam_size] = beam_idx_t[i*beam_size:i*beam_size+beam_size] + i*beam_size                              
                     elif beam_size == 1:
-                        beam_idx_t = torch.tensor([0]).repeat(batch_size)
+                        beam_idx_t = torch.arange(batch_size)
                     beam_idx[offset] = beam_idx_t
-                    offset = offset + 1
                     #reorder cache for naive impelementation
                     key_cache = torch.index_select(key_cache, 0, beam_idx_t)
-                    value_cache = torch.index_select(value_cache, 0, beam_idx_t)     
-                    key_cache_bf16 = torch.index_select(key_cache_bf16, 0, beam_idx_t)
-                    value_cache_bf16 = torch.index_select(value_cache_bf16, 0, beam_idx_t)     
-                #UT for next token with fp32
-                input_t = torch.randn(beam_size * batch_size, 1, head_num * head_size, dtype=torch.float32)
-                attention_mask = torch.zeros(beam_size*batch_size, 1, 1, offset+1, dtype=torch.float32)
-                naive_output, _, key_cache, value_cache, _ = mha(input_t, key_cache, value_cache, max_seq_len, attention_mask, None, None)
-                indirect_access_kv_cache_output, _, key_cache_iakv, value_cache_iakv, beam_idx = mha(input_t, key_cache_iakv, value_cache_iakv, max_seq_len, attention_mask, beam_idx, True, torch.tensor(offset))      
-                self.assertEqual(naive_output, indirect_access_kv_cache_output)
-                self.assertEqual(key_cache.transpose(0,1)[offset], key_cache_iakv[offset,:,:,:])
-                self.assertEqual(value_cache.transpose(0,1)[offset], value_cache_iakv[offset,:,:,:])    
-                # #UT for next token with bf16
-                input_t_bf16 = input_t.bfloat16()
-                attention_mask_bf16 = attention_mask.bfloat16()
-                with torch.inference_mode(), torch.no_grad(), torch.autocast(
-                    device_type="cpu",
-                    enabled=True,
-                    dtype=torch.bfloat16,
-                ):
-                    naive_output_bf16, _, key_cache_bf16, value_cache_bf16, _ = mha(input_t_bf16, key_cache_bf16, value_cache_bf16, max_seq_len, attention_mask_bf16, None, None)
-                    indirect_access_kv_cache_output_bf16, _, key_cache_iakv_bf16, value_cache_iakv_bf16, beam_idx = mha(input_t_bf16, key_cache_iakv_bf16, value_cache_iakv_bf16, max_seq_len, attention_mask_bf16, beam_idx, True, torch.tensor(offset))
-                    self.assertEqual(naive_output_bf16, indirect_access_kv_cache_output_bf16, prec=0.05)
-                    self.assertEqual(key_cache_bf16.transpose(0,1)[offset], key_cache_iakv_bf16[offset,:,:,:])
-                    self.assertEqual(value_cache_bf16.transpose(0,1)[offset], value_cache_iakv_bf16[offset,:,:,:])    
+                    value_cache = torch.index_select(value_cache, 0, beam_idx_t)
+                        
+                    # # #UT for first token with bf16
+                    input_t_bf16 = input_t.bfloat16()
+                    key_cache_iakv_bf16 = key_cache_iakv.bfloat16()
+                    value_cache_iakv_bf16 = value_cache_iakv.bfloat16()
+                    attention_mask_bf16 = attention_mask.bfloat16()
+                    with torch.inference_mode(), torch.no_grad(), torch.autocast(
+                        device_type="cpu",
+                        enabled=True,
+                        dtype=torch.bfloat16,
+                    ):
+                        naive_output_bf16, _, key_cache_bf16, value_cache_bf16, _ = mha(input_t_bf16, None, None, max_seq_len, attention_mask_bf16, None, None)
+                        indirect_access_kv_cache_output_bf16, _, key_cache_iakv_bf16, value_cache_iakv_bf16, beam_idx = mha(input_t_bf16, key_cache_iakv_bf16, value_cache_iakv_bf16, max_seq_len, attention_mask_bf16, beam_idx, True, torch.tensor(offset))
+                        self.assertEqual(naive_output_bf16, indirect_access_kv_cache_output_bf16, prec=2e-2)
+                        key_cache_bf16 = key_cache_bf16.repeat_interleave(beam_size, dim=0)
+                        value_cache_bf16 = value_cache_bf16.repeat_interleave(beam_size, dim=0) 
+                        for i in range(batch_size):
+                            self.assertEqual(key_cache_bf16.transpose(0,1)[:,i*beam_size, :, :], key_cache_iakv_bf16[0:first_seq_len, i*beam_size,:,:])
+                            self.assertEqual(value_cache_bf16.transpose(0,1)[:,i*beam_size, :, :], value_cache_iakv_bf16[0:first_seq_len, i*beam_size,:,:])      
+                        key_cache_bf16 = torch.index_select(key_cache_bf16, 0, beam_idx_t)
+                        value_cache_bf16 = torch.index_select(value_cache_bf16, 0, beam_idx_t)
+                                
+                    offset = offset + first_seq_len
+                    #UT for next token with fp32
+                    input_t = torch.randn(beam_size * batch_size, 1, head_num * head_size, dtype=torch.float32)
+                    attention_mask = torch.zeros(beam_size*batch_size, 1, 1, offset+1, dtype=torch.float32)
+                    naive_output, _, key_cache, value_cache, _ = mha(input_t, key_cache, value_cache, max_seq_len, attention_mask, None, None)
+                    indirect_access_kv_cache_output, _, key_cache_iakv, value_cache_iakv, beam_idx = mha(input_t, key_cache_iakv, value_cache_iakv, max_seq_len, attention_mask, beam_idx, True, torch.tensor(offset))      
+                    self.assertEqual(naive_output, indirect_access_kv_cache_output)
+                    self.assertEqual(key_cache.transpose(0,1)[offset], key_cache_iakv[offset,:,:,:])
+                    self.assertEqual(value_cache.transpose(0,1)[offset], value_cache_iakv[offset,:,:,:])    
+                    # #UT for next token with bf16
+                    input_t_bf16 = input_t.bfloat16()
+                    attention_mask_bf16 = attention_mask.bfloat16()
+                    with torch.inference_mode(), torch.no_grad(), torch.autocast(
+                        device_type="cpu",
+                        enabled=True,
+                        dtype=torch.bfloat16,
+                    ):
+                        naive_output_bf16, _, key_cache_bf16, value_cache_bf16, _ = mha(input_t_bf16, key_cache_bf16, value_cache_bf16, max_seq_len, attention_mask_bf16, None, None)
+                        indirect_access_kv_cache_output_bf16, _, key_cache_iakv_bf16, value_cache_iakv_bf16, beam_idx = mha(input_t_bf16, key_cache_iakv_bf16, value_cache_iakv_bf16, max_seq_len, attention_mask_bf16, beam_idx, True, torch.tensor(offset))
+                        self.assertEqual(naive_output_bf16, indirect_access_kv_cache_output_bf16, prec=0.05)
+                        self.assertEqual(key_cache_bf16.transpose(0,1)[offset], key_cache_iakv_bf16[offset,:,:,:])
+                        self.assertEqual(value_cache_bf16.transpose(0,1)[offset], value_cache_iakv_bf16[offset,:,:,:])       
+                        if beam_size == 4:    
+                            beam_idx_t = torch.tensor([1,3,0,0]).repeat(batch_size)
+                            for i in range(1, batch_size):
+                                beam_idx_t[i*beam_size:i*beam_size+beam_size] = beam_idx_t[i*beam_size:i*beam_size+beam_size] + i*beam_size                           
+                        elif beam_size == 1:
+                            beam_idx_t = torch.arange(batch_size)
+                        beam_idx[offset] = beam_idx_t
+                        offset = offset + 1
+                        #reorder cache for naive impelementation
+                        key_cache = torch.index_select(key_cache, 0, beam_idx_t)
+                        value_cache = torch.index_select(value_cache, 0, beam_idx_t)     
+                        key_cache_bf16 = torch.index_select(key_cache_bf16, 0, beam_idx_t)
+                        value_cache_bf16 = torch.index_select(value_cache_bf16, 0, beam_idx_t)     
+                    #UT for next token with fp32
+                    input_t = torch.randn(beam_size * batch_size, 1, head_num * head_size, dtype=torch.float32)
+                    attention_mask = torch.zeros(beam_size*batch_size, 1, 1, offset+1, dtype=torch.float32)
+                    naive_output, _, key_cache, value_cache, _ = mha(input_t, key_cache, value_cache, max_seq_len, attention_mask, None, None)
+                    indirect_access_kv_cache_output, _, key_cache_iakv, value_cache_iakv, beam_idx = mha(input_t, key_cache_iakv, value_cache_iakv, max_seq_len, attention_mask, beam_idx, True, torch.tensor(offset))      
+                    self.assertEqual(naive_output, indirect_access_kv_cache_output)
+                    self.assertEqual(key_cache.transpose(0,1)[offset], key_cache_iakv[offset,:,:,:])
+                    self.assertEqual(value_cache.transpose(0,1)[offset], value_cache_iakv[offset,:,:,:])    
+                    # #UT for next token with bf16
+                    input_t_bf16 = input_t.bfloat16()
+                    attention_mask_bf16 = attention_mask.bfloat16()
+                    with torch.inference_mode(), torch.no_grad(), torch.autocast(
+                        device_type="cpu",
+                        enabled=True,
+                        dtype=torch.bfloat16,
+                    ):
+                        naive_output_bf16, _, key_cache_bf16, value_cache_bf16, _ = mha(input_t_bf16, key_cache_bf16, value_cache_bf16, max_seq_len, attention_mask_bf16, None, None)
+                        indirect_access_kv_cache_output_bf16, _, key_cache_iakv_bf16, value_cache_iakv_bf16, beam_idx = mha(input_t_bf16, key_cache_iakv_bf16, value_cache_iakv_bf16, max_seq_len, attention_mask_bf16, beam_idx, True, torch.tensor(offset))
+                        self.assertEqual(naive_output_bf16, indirect_access_kv_cache_output_bf16, prec=0.05)
+                        self.assertEqual(key_cache_bf16.transpose(0,1)[offset], key_cache_iakv_bf16[offset,:,:,:])
+                        self.assertEqual(value_cache_bf16.transpose(0,1)[offset], value_cache_iakv_bf16[offset,:,:,:])    
                              
 if __name__ == "__main__":
     test = unittest.main()
