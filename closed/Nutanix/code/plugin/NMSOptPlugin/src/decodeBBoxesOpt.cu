/*
 * Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <cstring>
#include <iostream>
#include <type_traits>
#include <vector>

#include <fstream>

#include <cuda_fp16.h>

#include "NvInferPlugin.h"
#include "ssdOpt.h"
#include "ssdOptMacros.h"
//#include "ssd_internal.h"
#include "fast_divmod.h"
#include "nmsPluginOpt.h"

template <typename T>
void saveDeviceBuffer(const T* buf, size_t count, std::ofstream& output)
{
    T* hBuf = new T[count];
    cudaDeviceSynchronize();
    cudaMemcpy(hBuf, buf, count * sizeof(T), cudaMemcpyDeviceToHost);

    for (size_t i = 0; i < count; i += 4)
    {
        printf("%d ", i / 4);
        for (int j = 0; j < 4; j++)
        {
            printf("%f ", hBuf[i + j]);
        }
        printf("\n");
    }

    output.write((char*) hBuf, count * sizeof(T));
    delete[] hBuf;
}

namespace nvinfer1
{
namespace plugin
{

template <typename T_BBOX, int NUM_LAYERS>
struct DecodeBBoxesOptData
{
    const T_BBOX* loc_data[NUM_LAYERS];
    int feature_size_pow2[NUM_LAYERS];
    int end_layer_prior[NUM_LAYERS];
    int num_anchors[NUM_LAYERS];
    int box_channels[NUM_LAYERS];
    int conf_channels[NUM_LAYERS];
    uint32_t num_anchors_mul[NUM_LAYERS];
    uint32_t num_anchors_shr[NUM_LAYERS];
    bool reshape_before_permute;
    bool concatInputs;
    bool packed32_nchw;
};

/* This function maps the input index to the corresponding loc_data offset.
The input "loc_data" is composed of "num_layers" loc tensors from the CONV
layers in SSD. These tensors are in NCHW layout.
The input index is broken down to 4 components: i, c, d, n
i - box coordinate (max 4)
c - class (max num_loc_classes)
d - prior (max num_priors)
n - batch size

For SSD-MobileNet:
The transformed loc_data is generated by:
1. loc_data[id_layer] -> NCHW
2. permute(1,2,0) -> NHWC
3. reshape -> (N, H*W*C/num_loc_classes/4, num_loc_classes, 4) or (N, num_priors_layer, num_loc_classes, 4)
4. concat(axis=1, num_layers) -> (N, num_priors, num_loc_classes, 4)
5. flatten -> (N, num_priors * num_loc_classes *4, 1, 1)
Therefore, before concat, each loc_data[id_layer] effectively has:
num_anchors_layer = C/num_loc_classes/4
num_priors_layer = H*W*num_anchors_layer = H*W*C/num_loc_classes/4
num_box_coors = 4
After concat, the num_priors_layer (id_layer=0...5) are concatenated together, so
num_priors = sum(num_priors_layer)
Correspondingly, giving an index from the result of step 5 above, the mapping process will
a. Compute its i (box corrdinate), c (class), d (prior), n (batch size) accordingly
b. Using d, find the corresponding id_layer using num_priors_layer and num_priors
c. Compute prior_in_layer which is prior in id_layer
d. Using prior_in_layer to get the original value of hw. hw = priors_in_layer / num_anchors_layer. And correspondingly
anchor = priors_in_layer % num_anchors_layer e. C = num_anchors_layer * num_loc_classes * 4, the original value of ch
can be computed using anchor, c (class), and i (box coordinate). f. With n, hw and ch, the original index in an id_layer
can be computed according to NCHW layerout e.g. for ssd-mobilenet-v1, layer 0: C = 12 HW = 19x19 num_anchors_layer = 3
num_loc_classes = 1 (shared)
num_priors_layer = 1083

For SSD-ResNet34, it is a bit different since reshape happens before permute
1. loc_data[id_layer] -> NCHW
2. reshape -> (N, num_loc_classes, 4,  C/num_loc_classes/4*H*W) or (N, num_loc_classes, 4, num_priors_layer)
2. permute(0,2,1) -> (N, num_loc_classes, num_priors_layer, 4)
4. concat(axis=2, num_layers) -> (N, num_loc_classes, num_priors, 4)
5. flatten -> (N, num_loc_classes*num_priors*4, 1, 1)
num_anchors_layer = C/num_loc_classes/4
num_priors_layer = num_anchors_layer*H*W = C/num_loc_classes/4*H*W
num_box_coors = 4
After concat, the num_priors_layer (id_layer=0...5) are concatenated together, so
num_priors = sum(num_priors_layer)
Correspondingly, giving an index from the result of step 5 above, the mapping process will
a. Compute its i (box corrdinate), d (prior), c (class), n (batch size) accordingly
b. Using d, find the corresponding id_layer using num_priors_layer and num_priors
c. Compute prior_in_layer which is prior in id_layer
d. Using prior_in_layer to get the original value of anchor. anchor = priors_in_layer / HW. And correspondingly hw =
priors_in_layer % HW e. C = num_loc_classes*4*num_anchors_layer, the original value of ch can be computed using c
(class), i (box coordinate) and anchor. f. With n, hw and ch, the original index in an id_layer can be computed
according to NCHW layerout (g). An alternative to step d), e), and f) is that since CHW = num_loc_classes * 4 *
C/num_loc_classes/4*HW = num_loc_classes*4*num_priors_layer, the original index in NCHW layout can be computed using
prior_in_layer, i (box coordinate) and c (class).

Dealing with inputs with NC/32HW32 packed layout:
This means the loc_data[id_layer] would be in NC/32HW32 format instead of NCHW. As a result, after n, ch, hw are
computed, extra steps are needed to map them back to the original index: h. packed_C = (C+31)/32 i. packed_ch = ch/32 j.
packed_ch_offset = ch%32 k. Get the packed index for NC/32HW32 using h),i) and j)


*/

template <typename T_BBOX, int NUM_LAYERS>
__device__ inline void mapLocData(const int c, int d, const int n, const T_BBOX* const* loc_data,
    const int* feature_size_pow2, const int* end_layer_prior, const int* num_anchors_data,
    const uint32_t* num_anchors_mul_data, const uint32_t* num_anchors_shr_data, const int num_loc_classes,
    T_BBOX mapped_loc_data[4], const bool reshape_before_permute, const bool concatInputs, const int* box_channels,
    const int* conf_channels, const bool packed32_nchw)
{

    // find layer_id
    int start_layer_prior = 0;
    int prior_in_layer = 0;
    int num_anchors, num_anchors_mul, num_anchors_shr;
    int num_hw;
    int num_ch;
    const T_BBOX* loc_data_layer;

#pragma unroll
    for (int layer = 0; layer < NUM_LAYERS; layer++)
    {
        if (d < end_layer_prior[layer])
        {
            num_anchors = num_anchors_data[layer];
            num_anchors_mul = num_anchors_mul_data[layer];
            num_anchors_shr = num_anchors_shr_data[layer];
            num_hw = feature_size_pow2[layer];
            num_ch = (concatInputs) ? box_channels[layer] + conf_channels[layer] : box_channels[layer];
            prior_in_layer = d - start_layer_prior;
            loc_data_layer = loc_data[layer];
            // there should be a break here - but this would make all access to the arrays dynamic
            // due to compiler deoptimizations
            d = INT_MAX;
        }

        start_layer_prior = end_layer_prior[layer];
    }

    // Transform id_in_layer from HWC -> CHW

    if (reshape_before_permute)
    {
        int hw = prior_in_layer % num_hw;
        int anchor = prior_in_layer / num_hw;

#pragma unroll
        for (int i = 0; i < 4; ++i)
        {
            int ch = (c * 4 + i) * num_anchors + anchor;
            int mappedIndex;
            if (packed32_nchw)
            {
                int packed_num_ch = (num_ch + 31) / 32;

                int packed_ch = ch >> 5;        // ch/32;
                int packed_ch_offset = ch & 31; // ch%32;

                mappedIndex = ((n * packed_num_ch + packed_ch) * num_hw + hw) * 32 + packed_ch_offset;
            }
            else
            {
                mappedIndex = (n * num_ch + ch) * num_hw + hw;
            }

            mapped_loc_data[i] = loc_data_layer[mappedIndex];
        }
    }
    else
    {
        // anchor = prior_in_layer % num_anchors;
        // hw = prior_in_layer / num_anchors;
        int anchor, hw;
        fast_divmod(hw, anchor, prior_in_layer, num_anchors, num_anchors_mul, num_anchors_shr);

        int ch_base = (anchor * num_loc_classes + c) * 4;

#pragma unroll
        for (int i = 0; i < 4; ++i)
        {

            int ch = ch_base + i;
            int mappedIndex;
            if (packed32_nchw)
            {
                int packed_num_ch = (num_ch + 31) / 32;

                int packed_ch = ch >> 5;        // ch/32;
                int packed_ch_offset = ch & 31; // ch%32;

                mappedIndex = ((n * packed_num_ch + packed_ch) * num_hw + hw) * 32 + packed_ch_offset;
            }
            else
            {
                mappedIndex = (n * num_ch + ch) * num_hw + hw;
            }
            mapped_loc_data[i] = loc_data_layer[mappedIndex];
        }
    }
}

////////////////////////////////////////////////////////////////////////////////////////////////////

inline __device__ float half_to_float(uint16_t h)
{
    float f;
    asm volatile("cvt.f32.f16 %0, %1;\n" : "=f"(f) : "h"(h));
    return f;
}

////////////////////////////////////////////////////////////////////////////////////////////////////

inline __device__ float4 to_float4(uint2 v)
{
    uint16_t lo_x, hi_x, lo_y, hi_y;
    asm volatile("mov.b32 {%0, %1}, %2;\n" : "=h"(lo_x), "=h"(hi_x) : "r"(v.x));
    asm volatile("mov.b32 {%0, %1}, %2;\n" : "=h"(lo_y), "=h"(hi_y) : "r"(v.y));
    return make_float4(half_to_float(lo_x), half_to_float(hi_x), half_to_float(lo_y), half_to_float(hi_y));
}

////////////////////////////////////////////////////////////////////////////////////////////////////

inline __device__ float4 to_float4(float4 v)
{
    return v;
}

////////////////////////////////////////////////////////////////////////////////////////////////////

inline __device__ uint32_t float2_to_half2(float2 f)
{
    union {
        uint32_t u32;
        uint16_t u16[2];
    } tmp;
#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800
    asm volatile("cvt.rn.f16x2.f32 %0, %1, %2;\n" : "=r"(tmp.u32) : "f"(f.y), "f"(f.x));
#else
    asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[0]) : "f"(f.x));
    asm volatile("cvt.rn.f16.f32 %0, %1;\n" : "=h"(tmp.u16[1]) : "f"(f.y));
#endif
    return tmp.u32;
}

////////////////////////////////////////////////////////////////////////////////////////////////////

inline __device__ void from_float4(uint2& dst, float4 src)
{
    dst.x = float2_to_half2(make_float2(src.x, src.y));
    dst.y = float2_to_half2(make_float2(src.z, src.w));
}

////////////////////////////////////////////////////////////////////////////////////////////////////

inline __device__ void from_float4(float4& dst, float4 src)
{
    dst = src;
}

////////////////////////////////////////////////////////////////////////////////////////////////////

template <typename T_BBOX, unsigned nthds_per_cta, int NUM_LAYERS>
__launch_bounds__(nthds_per_cta) __global__
    void decodeBBoxesOpt_kernel(const int nthreads, const CodeTypeSSD code_type, const bool variance_encoded_in_target,
        const int num_priors, const uint32_t num_priors_mul, const uint32_t num_priors_shr, const bool share_location,
        const int num_loc_classes, const uint32_t num_loc_classes_mul, const uint32_t num_loc_classes_shr,
        const int background_label_id, const bool clip_bbox, const T_BBOX* prior_data, T_BBOX* bbox_data,
        const DecodeBBoxesOptData<T_BBOX, NUM_LAYERS> decodeBBoxesOptData)
{
    using VECTOR_TYPE = typename std::conditional<sizeof(T_BBOX) == 4, float4, uint2>::type;

    // if (blockIdx.x == 0 && threadIdx.x == 0) printf("sizeof VECTOR_TYPE = %d\n", sizeof(VECTOR_TYPE));
    // assert(0);

    for (int index = blockIdx.x * nthds_per_cta + threadIdx.x; index < nthreads; index += nthds_per_cta * gridDim.x)
    {
        const T_BBOX* const* loc_data = &decodeBBoxesOptData.loc_data[0];
        const int* feature_size_pow2 = &decodeBBoxesOptData.feature_size_pow2[0];
        const int* end_layer_prior = &decodeBBoxesOptData.end_layer_prior[0];
        const int* num_anchors = &decodeBBoxesOptData.num_anchors[0];
        const uint32_t* num_anchors_mul = &decodeBBoxesOptData.num_anchors_mul[0];
        const uint32_t* num_anchors_shr = &decodeBBoxesOptData.num_anchors_shr[0];
        const bool reshape_before_permute = decodeBBoxesOptData.reshape_before_permute;
        const bool packed32_nchw = decodeBBoxesOptData.packed32_nchw;
        const int* box_channels = &decodeBBoxesOptData.box_channels[0];
        const int* conf_channels = &decodeBBoxesOptData.conf_channels[0];
        const bool concatInputs = decodeBBoxesOptData.concatInputs;

        // index has been already been divided by 4 (num_box_coors) before passed to this function
        // c = (index) % num_loc_classes;
        // d = (index / num_loc_classes) % num_priors;
        // n = (index / num_loc_classes / num_priors);
        int c, c_div, d, n;
        fast_divmod(c_div, c, index, num_loc_classes, num_loc_classes_mul, num_loc_classes_shr);
        fast_divmod(n, d, c_div, num_priors, num_priors_mul, num_priors_shr);

        if (!share_location && c == background_label_id)
        {
            // Ignore background class if not share_location.
            return;
        }
        const int pi = d * 4;
        const int vi = pi + num_priors * 4;

        T_BBOX locData[4];
        mapLocData<T_BBOX, NUM_LAYERS>(c, d, n, loc_data, feature_size_pow2, end_layer_prior, num_anchors,
            num_anchors_mul, num_anchors_shr, num_loc_classes, locData, reshape_before_permute, concatInputs,
            box_channels, conf_channels, packed32_nchw);

        float4 bbox_data_val;

        // float4 prior_data_p = to_float4(
        // *(reinterpret_cast<const VECTOR_TYPE*>
        //     (prior_data + pi)));
        VECTOR_TYPE val = *(reinterpret_cast<const VECTOR_TYPE*>(prior_data + pi));
        float4 prior_data_p = to_float4(val);

        const float4 one_float4 = make_float4(1.0F, 1.0F, 1.0F, 1.0F);
        // variance is encoded in target, we simply need to add the offset
        // predictions.
        // OR
        // variance is encoded in bbox, we need to scale the offset accordingly.
        const float4 prior_data_v = (!variance_encoded_in_target)
            ? to_float4(*(reinterpret_cast<const VECTOR_TYPE*>(prior_data + vi)))
            : one_float4;

        const float p_xmin = prior_data_p.x;
        const float p_ymin = prior_data_p.y;
        const float p_xmax = prior_data_p.z;
        const float p_ymax = prior_data_p.w;

        const float prior_width = p_xmax - p_xmin;
        const float prior_height = p_ymax - p_ymin;

        if (code_type == CodeTypeSSD::CORNER)
        {
            bbox_data_val.x = p_xmin + (float) locData[0] * prior_data_v.x;
            bbox_data_val.y = p_ymin + (float) locData[1] * prior_data_v.y;
            bbox_data_val.z = p_xmax + (float) locData[2] * prior_data_v.z;
            bbox_data_val.w = p_ymax + (float) locData[3] * prior_data_v.w;
        }
        else if (code_type == CodeTypeSSD::CENTER_SIZE)
        {
            const float prior_center_x = (p_xmin + p_xmax) / 2.0F;
            const float prior_center_y = (p_ymin + p_ymax) / 2.0F;

            // mapping index to original input loc_data[NUM_LAYERS]
            const float xmin = locData[0];
            const float ymin = locData[1];
            const float xmax = locData[2];
            const float ymax = locData[3];

            float decode_bbox_center_x, decode_bbox_center_y;
            float decode_bbox_width, decode_bbox_height;

            decode_bbox_center_x = prior_data_v.x * xmin * prior_width + prior_center_x;
            decode_bbox_center_y = prior_data_v.y * ymin * prior_height + prior_center_y;
            decode_bbox_width = __expf(prior_data_v.z * xmax) * prior_width;
            decode_bbox_height = __expf(prior_data_v.w * ymax) * prior_height;

            bbox_data_val = make_float4(decode_bbox_center_x - decode_bbox_width / 2.0F,
                decode_bbox_center_y - decode_bbox_height / 2.0F, decode_bbox_center_x + decode_bbox_width / 2.0F,
                decode_bbox_center_y + decode_bbox_height / 2.0F);
        }
        else if (code_type == CodeTypeSSD::CORNER_SIZE)
        {
            float p_size;

            bbox_data_val.x = p_xmin + (float) locData[0] * prior_width * prior_data_v.x;
            bbox_data_val.y = p_ymin + (float) locData[1] * prior_height * prior_data_v.y;
            bbox_data_val.z = p_xmax + (float) locData[2] * prior_width * prior_data_v.z;
            bbox_data_val.w = p_ymax + (float) locData[3] * prior_height * prior_data_v.w;
        }
        else if (code_type == CodeTypeSSD::TF_CENTER)
        {
            const float priorCenterX = (p_xmin + p_xmax) / 2.0F;
            const float priorCenterY = (p_ymin + p_ymax) / 2.0F;

            const float ymin = locData[0];
            const float xmin = locData[1];
            const float ymax = locData[2];
            const float xmax = locData[3];

            float bboxCenterX, bboxCenterY;
            float bboxWidth, bboxHeight;

            bboxCenterX = prior_data_v.x * xmin * prior_width + priorCenterX;
            bboxCenterY = prior_data_v.y * ymin * prior_height + priorCenterY;
            bboxWidth = __expf(prior_data_v.z * xmax) * prior_width;
            bboxHeight = __expf(prior_data_v.w * ymax) * prior_height;

            bbox_data_val = make_float4(bboxCenterX - bboxWidth / 2.0F, bboxCenterY - bboxHeight / 2.0F,
                bboxCenterX + bboxWidth / 2.0F, bboxCenterY + bboxHeight / 2.0F);
        }
        else
        {
            // Unknown code type.
            assert("Unknown Box decode code type");
        }
        if (clip_bbox)
        {
            bbox_data_val.x = fmaxf(fminf(bbox_data_val.x, 1.0F), 0.0F);
            bbox_data_val.y = fmaxf(fminf(bbox_data_val.y, 1.0F), 0.0F);
            bbox_data_val.z = fmaxf(fminf(bbox_data_val.z, 1.0F), 0.0F);
            bbox_data_val.w = fmaxf(fminf(bbox_data_val.w, 1.0F), 0.0F);
        }
        // store to gmem
        from_float4(*(reinterpret_cast<VECTOR_TYPE*>(bbox_data + 4 * index)), bbox_data_val);
    }
}

template <typename T_BBOX, int NUM_LAYERS>
ssdStatus_t decodeBBoxesOpt_gpu(cudaStream_t stream, const int nthreads, const CodeTypeSSD code_type,
    const bool variance_encoded_in_target, const int num_priors, const bool share_location, const int num_loc_classes,
    const int background_label_id, const bool clip_bbox, const void* const* loc_data, const void* prior_data,
    void* bbox_data, const int num_layers, const int* feature_size, const int* num_anchors, const int* box_channels,
    const int* conf_channels, const bool reshape_before_permute, const bool concatInputs, const bool packed32_nchw)
{
    const int BS = 512;
    const int GS = (nthreads / 4 + BS - 1) / BS;

    DecodeBBoxesOptData<T_BBOX, NUM_LAYERS> decodeBBoxesOptData;
    decodeBBoxesOptData.reshape_before_permute = reshape_before_permute;
    decodeBBoxesOptData.packed32_nchw = packed32_nchw;
    decodeBBoxesOptData.concatInputs = concatInputs;
    int end_layer_prior = 0;
    DEBUG_PRINTF("num_loc_classes = %d\n", num_loc_classes);
    for (int layer = 0; layer < NUM_LAYERS; ++layer)
    {
        DEBUG_PRINTF("feature_size[layer] = %d\n", feature_size[layer]);
        end_layer_prior = end_layer_prior + num_anchors[layer] * feature_size[layer] * feature_size[layer];
        decodeBBoxesOptData.end_layer_prior[layer] = end_layer_prior;
        decodeBBoxesOptData.feature_size_pow2[layer] = feature_size[layer] * feature_size[layer];
        decodeBBoxesOptData.box_channels[layer] = box_channels[layer];
        decodeBBoxesOptData.conf_channels[layer] = conf_channels[layer];

        DEBUG_PRINTF("layer %d, end_layer_prior = %d, box_channels = %d, conf_channels = %d\n", layer, end_layer_prior,
            box_channels[layer], conf_channels[layer]);

        find_divisor(
            decodeBBoxesOptData.num_anchors_mul[layer], decodeBBoxesOptData.num_anchors_shr[layer], num_anchors[layer]);
    }
    std::memcpy(decodeBBoxesOptData.num_anchors, num_anchors, NUM_LAYERS * sizeof(int));
    std::memcpy(decodeBBoxesOptData.loc_data, loc_data, NUM_LAYERS * sizeof(void*));

    // determine constants for efficient integer division
    uint32_t num_loc_classes_mul, num_loc_classes_shr;
    uint32_t num_priors_mul, num_priors_shr;
    find_divisor(num_loc_classes_mul, num_loc_classes_shr, num_loc_classes);
    find_divisor(num_priors_mul, num_priors_shr, num_priors);

    decodeBBoxesOpt_kernel<T_BBOX, BS, NUM_LAYERS><<<GS, BS, 0, stream>>>(nthreads / 4, code_type,
        variance_encoded_in_target, num_priors, num_priors_mul, num_priors_shr, share_location, num_loc_classes,
        num_loc_classes_mul, num_loc_classes_shr, background_label_id, clip_bbox, (const T_BBOX*) prior_data,
        (T_BBOX*) bbox_data, decodeBBoxesOptData);

    static int iter = 0;
    if (iter == -1)
    {
        // debug

        auto output_file = std::ofstream("decoded_bboxes.bin", std::ios::binary);

        // std::vector<int> header = {num_top_k, num_preds_per_class, segments};
        // output_file.write((char *) &header[0], header.size() * sizeof(int));
        saveDeviceBuffer((const float*) bbox_data, nthreads, output_file);
        output_file.close();

        // exit(1);
    }
    else
    {
        iter++;
    };

    CSC(cudaGetLastError(), STATUS_FAILURE);
    return STATUS_SUCCESS;
}

// decodeBBoxesOpt LAUNCH CONFIG{{{
typedef ssdStatus_t (*dbbFunc)(cudaStream_t, const int, const CodeTypeSSD, const bool, const int, const bool, const int,
    const int, const bool, const void* const*, const void*, void*, const int, const int*, const int*, const int*,
    const int*, const bool, const bool, const bool);

struct dbbLaunchConfig
{
    DType_t t_bbox;
    int num_layers;
    dbbFunc function;

    dbbLaunchConfig(DType_t t_bbox, int num_layers)
        : t_bbox(t_bbox)
        , num_layers(num_layers)
    {
    }
    dbbLaunchConfig(DType_t t_bbox, int num_layers, dbbFunc function)
        : t_bbox(t_bbox)
        , num_layers(num_layers)
        , function(function)
    {
    }
    bool operator==(const dbbLaunchConfig& other)
    {
        return (t_bbox == other.t_bbox && num_layers == other.num_layers);
    }
};

static std::vector<dbbLaunchConfig> dbbFuncVec;

bool decodeBBoxesOptInit()
{
    dbbFuncVec.push_back(dbbLaunchConfig(DataType::kFLOAT, 5, decodeBBoxesOpt_gpu<float, 5>));
    dbbFuncVec.push_back(dbbLaunchConfig(DataType::kFLOAT, 6, decodeBBoxesOpt_gpu<float, 6>));
    // dbbFuncVec.push_back(dbbLaunchConfig(DataType::kHALF, 5,
    //                                      decodeBBoxesOpt_gpu<__half, 5>));
    // dbbFuncVec.push_back(dbbLaunchConfig(DataType::kHALF, 6,
    //                                      decodeBBoxesOpt_gpu<__half, 6>));
    return true;
}

static bool initialized = decodeBBoxesOptInit();

//}}}

ssdStatus_t decodeBBoxesOpt(cudaStream_t stream, const int nthreads, const CodeTypeSSD code_type,
    const bool variance_encoded_in_target, const int num_priors, const bool share_location, const int num_loc_classes,
    const int background_label_id, const bool clip_bbox, const DType_t DT_BBOX, const void* const* loc_data,
    const void* prior_data, void* bbox_data, const int num_layers, const int* feature_size, const int* num_anchors,
    const int* box_channels, const int* conf_channels, const bool packed32_nchw, const bool reshape_before_permute,
    const bool concatInputs)
{
    dbbLaunchConfig lc = dbbLaunchConfig(DT_BBOX, num_layers);
    for (unsigned i = 0; i < dbbFuncVec.size(); ++i)
    {
        if (lc == dbbFuncVec[i])
        {
            DEBUG_PRINTF("decodeBBox kernel %d\n", i);
            return dbbFuncVec[i].function(stream, nthreads, code_type, variance_encoded_in_target, num_priors,
                share_location, num_loc_classes, background_label_id, clip_bbox, loc_data, prior_data, bbox_data,
                num_layers, feature_size, num_anchors, box_channels, conf_channels, reshape_before_permute,
                concatInputs, packed32_nchw);
        }
    }
    std::cerr << "Bbox data type or num_layers is not supported" << std::endl;
    return STATUS_BAD_PARAM;
}

} // namespace plugin
} // namespace nvinfer1
