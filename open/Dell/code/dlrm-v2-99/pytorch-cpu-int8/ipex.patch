diff --git a/cmake/Modules/FindAVX.cmake b/cmake/Modules/FindAVX.cmake
index 43eb271b..0390c929 100644
--- a/cmake/Modules/FindAVX.cmake
+++ b/cmake/Modules/FindAVX.cmake
@@ -196,4 +196,4 @@ CHECK_SSE(CXX "AMX" " ;-mavx512f -mavx512dq -mavx512vl -mavx512bw -mavx512bf16 -
 CHECK_SSE(C "AVX512_FP16" " ;-mavx512f -mavx512dq -mavx512vl -mavx512bw -mavx512bf16 -mfma\
  -mamx-tile -mamx-int8 -mamx-bf16 -mavx512fp16;/arch:AVX512")
 CHECK_SSE(CXX "AVX512_FP16" " ;-mavx512f -mavx512dq -mavx512vl -mavx512bw -mavx512bf16 -mfma\
- -mamx-tile -mamx-int8 -mamx-bf16 -mavx512fp16;/arch:AVX512")
+ -mamx-tile -mamx-int8 -mamx-bf16 -mavx512fp16;/arch:AVX512")
\ No newline at end of file
diff --git a/cmake/Modules/FindoneMKL.cmake b/cmake/Modules/FindoneMKL.cmake
index c4c979c8..55a710b5 100644
--- a/cmake/Modules/FindoneMKL.cmake
+++ b/cmake/Modules/FindoneMKL.cmake
@@ -32,16 +32,14 @@ set(ONEMKL_CPU_LIBS)
 
 set(mkl_root_hint)
 if(BUILD_WITH_XPU)
-  if(DEFINED ENV{MKL_DPCPP_ROOT})
-    set(mkl_root_hint $ENV{MKL_DPCPP_ROOT})
-  elseif(DEFINED ENV{MKLROOT})
+  if(DEFINED ENV{MKLROOT})
     set(mkl_root_hint $ENV{MKLROOT})
   elseif(DEFINED ENV{MKL_ROOT})
     set(mkl_root_hint $ENV{MKL_ROOT})
   elseif(MKL_ROOT)
     set(mkl_root_hint ${MKL_ROOT})
   else()
-    message(FATAL_ERROR "Please set oneMKL root path by MKL_DPCPP_ROOT, or MKLROOT, or MKL_ROOT.")
+    message(FATAL_ERROR "Please set oneMKL root path by MKLROOT, or MKL_ROOT.")
   endif()
 else()
   # install mkl-include and mkl-static for CPU build
@@ -60,7 +58,7 @@ else()
   get_filename_component(mkl_root_hint "${mkl_inc}/../" ABSOLUTE)
 endif()
 
-# Try to find Intel MKL DPCPP header
+# Try to find Intel MKL header
 find_file(MKL_HEADER NAMES mkl.h PATHS ${mkl_root_hint}
     PATH_SUFFIXES include NO_DEFAULT_PATH)
 
@@ -71,13 +69,25 @@ get_filename_component(ONEMKL_INCLUDE_DIR "${MKL_HEADER}/.." ABSOLUTE)
 
 if(BUILD_STATIC_ONEMKL)
   set(LIB_PREFIX ${CMAKE_STATIC_LIBRARY_PREFIX})
-  set(LIB_SUFFIX ${CMAKE_STATIC_LIBRARY_SUFFIX})
+  if (NOT WINDOWS)
+    set(LIB_SUFFIX ${CMAKE_STATIC_LIBRARY_SUFFIX})
+  else()
+    set(LIB_SUFFIX ".lib")
+  endif()
 else()
   set(LIB_PREFIX ${CMAKE_SHARED_LIBRARY_PREFIX})
-  set(LIB_SUFFIX ${CMAKE_SHARED_LIBRARY_SUFFIX})
+  if (NOT WINDOWS)
+    set(LIB_SUFFIX ${CMAKE_SHARED_LIBRARY_SUFFIX})
+  else()
+    set(LIB_SUFFIX "_dll.lib")
+  endif()
 endif()
 
-set(MKL_THREAD "${LIB_PREFIX}mkl_gnu_thread${LIB_SUFFIX}")
+if (NOT WINDOWS)
+  set(MKL_THREAD "${LIB_PREFIX}mkl_gnu_thread${LIB_SUFFIX}")
+else()
+  set(MKL_THREAD "${LIB_PREFIX}mkl_intel_thread${LIB_SUFFIX}")
+endif()
 find_library(MKL_LIB_THREAD ${MKL_THREAD} HINTS ${mkl_root_hint}
     PATH_SUFFIXES lib lib/intel64 NO_DEFAULT_PATH)
 if(NOT MKL_THREAD)
diff --git a/cmake/cpu/BuildFlags.cmake b/cmake/cpu/BuildFlags.cmake
index b5073da2..6d7bf9cd 100644
--- a/cmake/cpu/BuildFlags.cmake
+++ b/cmake/cpu/BuildFlags.cmake
@@ -33,6 +33,10 @@ set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-strict-overflow")
 set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-strict-aliasing")
 set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-error=deprecated-declarations")
 set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-ignored-qualifiers")
+set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-attributes")
+set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-parentheses")
+set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-format")
+set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-deprecated-declarations")
 if (CMAKE_COMPILER_IS_GNUCXX AND NOT (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.0.0))
   set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-stringop-overflow")
 endif()
diff --git a/cmake/cpu/IsaCodegen.cmake b/cmake/cpu/IsaCodegen.cmake
index de557161..408be644 100644
--- a/cmake/cpu/IsaCodegen.cmake
+++ b/cmake/cpu/IsaCodegen.cmake
@@ -2,8 +2,6 @@ include(CheckCXXCompilerFlag)
 
 FIND_PACKAGE(AVX)
 
-set(OPT_FLAG "-O3 ")
-
 file(GLOB_RECURSE cpu_kernel_cpp_in "${IPEX_CPU_ROOT_DIR}/aten/kernels/*.cpp")
 list(APPEND IPEX_CPU_CPP_ISA_SRCS_ORIGIN ${cpu_kernel_cpp_in})
 
@@ -20,8 +18,6 @@ if(COMPILER_SUPPORTS_NO_AVX256_SPLIT)
   set(CPU_NO_AVX256_SPLIT_FLAGS "-mno-avx256-split-unaligned-load -mno-avx256-split-unaligned-store")
 endif(COMPILER_SUPPORTS_NO_AVX256_SPLIT)
 
-set(AVX512_OPTIMIZE_FLAGS "-mprefer-vector-width=512")
-
 # Keep Default config to align to pytorch, but use AVX2 parameters as its real implement.
 list(APPEND CPU_CAPABILITY_NAMES "DEFAULT")
 if(MSVC)
@@ -36,9 +32,8 @@ if(CXX_AVX512_FP16_FOUND)
   if(MSVC)
     list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG}/arch:AVX512") # TODO: CHECK HERE
   else(MSVC)
-    list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG} -D__AVX512F__ ${AVX512_OPTIMIZE_FLAGS} -DCPU_CAPABILITY_AVX512 \
-    -DCPU_CAPABILITY_AVX512_VNNI -DCPU_CAPABILITY_AVX512_BF16 -DCPU_CAPABILITY_AVX512_AMX \
-    -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vnni \
+    list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG} -D__AVX512F__ -DCPU_CAPABILITY_AVX512 -DCPU_CAPABILITY_AVX512_VNNI \
+    -DCPU_CAPABILITY_AVX512_BF16 -DCPU_CAPABILITY_AVX512_AMX -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vnni \
     -mavx512bf16 -mfma -mamx-tile -mamx-int8 -mamx-bf16 -mavx512fp16")
   endif(MSVC)
 else(CXX_AVX512_FP16_FOUND)
@@ -53,8 +48,8 @@ if(CXX_AMX_FOUND)
   if(MSVC)
     list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG}/arch:AVX512") # TODO: CHECK HERE
   else(MSVC)
-    list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG} -D__AVX512F__ ${AVX512_OPTIMIZE_FLAGS} -DCPU_CAPABILITY_AVX512 \
-    -DCPU_CAPABILITY_AVX512_VNNI -DCPU_CAPABILITY_AVX512_BF16 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vnni -mavx512bf16 -mfma \
+    list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG} -D__AVX512F__ -DCPU_CAPABILITY_AVX512 -DCPU_CAPABILITY_AVX512_VNNI \
+    -DCPU_CAPABILITY_AVX512_BF16 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vnni -mavx512bf16 -mfma \
     -mamx-tile -mamx-int8 -mamx-bf16")
   endif(MSVC)
 else(CXX_AMX_FOUND)
@@ -69,7 +64,7 @@ if(CXX_AVX512_BF16_FOUND)
   if(MSVC)
     list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG}/arch:AVX512") # TODO: CHECK HERE
   else(MSVC)
-    list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG} -D__AVX512F__ ${AVX512_OPTIMIZE_FLAGS} -DCPU_CAPABILITY_AVX512 \
+    list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG} -D__AVX512F__ -DCPU_CAPABILITY_AVX512 \
     -DCPU_CAPABILITY_AVX512_VNNI -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vnni \
     -mavx512bf16 -mfma")
   endif(MSVC)
@@ -85,7 +80,7 @@ if(CXX_AVX512_VNNI_FOUND)
   if(MSVC)
     list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG}/arch:AVX512") # TODO: CHECK HERE
   else(MSVC)
-    list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG} -D__AVX512F__ ${AVX512_OPTIMIZE_FLAGS} -DCPU_CAPABILITY_AVX512 \
+    list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG} -D__AVX512F__ -DCPU_CAPABILITY_AVX512 \
      -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vnni -mfma")
   endif(MSVC)
 else(CXX_AVX512_VNNI_FOUND)
@@ -100,7 +95,7 @@ if(CXX_AVX512_FOUND)
   if(MSVC)
     list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG}/arch:AVX512") # TODO: CHECK HERE
   else(MSVC)
-    list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG} -D__AVX512F__ ${AVX512_OPTIMIZE_FLAGS} -mavx512f -mavx512bw -mavx512vl -mavx512dq -mfma")
+    list(APPEND CPU_CAPABILITY_FLAGS "${OPT_FLAG} -D__AVX512F__ -mavx512f -mavx512bw -mavx512vl -mavx512dq -mfma")
   endif(MSVC)
 else(CXX_AVX512_FOUND)
   if(CMAKE_COMPILER_IS_GNUCXX)
diff --git a/csrc/cpu/aten/Conv.cpp b/csrc/cpu/aten/Conv.cpp
index 726bbd41..22de36b0 100644
--- a/csrc/cpu/aten/Conv.cpp
+++ b/csrc/cpu/aten/Conv.cpp
@@ -227,10 +227,12 @@ at::Tensor convolution_backward_input(
   if (is_channels_last_contiguous) {
     return grad_input;
   } else {
-    return mkldnn_to_dense(new_with_itensor_mkldnn(
-        std::move(mkldnn_grad_input),
-        optTypeMetaToScalarType(grad_output.options().dtype_opt()),
-        grad_output.options().device_opt()));
+    return mkldnn_to_dense(
+               new_with_itensor_mkldnn(
+                   std::move(mkldnn_grad_input),
+                   optTypeMetaToScalarType(grad_output.options().dtype_opt()),
+                   grad_output.options().device_opt()))
+        .contiguous(memory_format);
   }
 }
 
@@ -370,9 +372,12 @@ std::tuple<at::Tensor, at::Tensor, at::Tensor> convolution_backward_kernel(
 
 std::tuple<at::Tensor, at::Tensor, at::Tensor> convolution_backward(
     const at::Tensor& input,
+    const at::Tensor& weight,
+    const c10::optional<at::Tensor>& bias_opt,
     const at::Tensor& grad_output,
     std::array<bool, 3> output_mask,
-    const at::Tensor& op_context) {
+    const at::Tensor& op_context,
+    c10::optional<bool> weight_channels_last) {
   return reinterpret_cast<IpexConvolutionOpContext*>(
              op_context.data_ptr<int64_t>()[0])
       ->run_backward(input, grad_output, output_mask);
@@ -426,7 +431,9 @@ at::Tensor IPEXConvolutionOp::forward(
   ctx->saved_data["weight_requires_grad"] = weight.requires_grad();
   ctx->saved_data["bias_requires_grad"] =
       bias_opt.has_value() && bias_opt.value().requires_grad() ? true : false;
-  ctx->save_for_backward({input});
+  ctx->saved_data["bias_opt"] = bias_opt;
+  ctx->saved_data["weight_channels_last"] = weight_channels_last;
+  ctx->save_for_backward({input, weight});
 
   return _forward(
       input,
@@ -451,15 +458,25 @@ torch::autograd::variable_list IPEXConvolutionOp::backward(
   output_mask[0] = ctx->saved_data["input_requires_grad"].toBool();
   output_mask[1] = ctx->saved_data["weight_requires_grad"].toBool();
   output_mask[2] = ctx->saved_data["bias_requires_grad"].toBool();
+  auto bias_opt = ctx->saved_data["bias_opt"].toOptional<at::Tensor>();
+  auto weight_channels_last =
+      ctx->saved_data["weight_channels_last"].toOptional<bool>();
   auto saved = ctx->get_saved_variables();
   at::Tensor input = saved[0];
+  at::Tensor weight = saved[1];
   at::Tensor grad_input, grad_weight, grad_bias;
   static auto op =
       torch::Dispatcher::singleton()
           .findSchemaOrThrow("torch_ipex::convolution_backward", "")
           .typed<decltype(convolution_backward)>();
-  std::tie(grad_input, grad_weight, grad_bias) =
-      op.call(input, grad_outputs[0], output_mask, op_context);
+  std::tie(grad_input, grad_weight, grad_bias) = op.call(
+      input,
+      weight,
+      bias_opt,
+      grad_outputs[0],
+      output_mask,
+      op_context,
+      weight_channels_last);
   return {
       grad_input,
       grad_weight,
@@ -564,8 +581,8 @@ TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
       torch_ipex::cpu::convolution_forward_impl);
   // bw
   m.def(
-      "convolution_backward(Tensor input, Tensor grad_output, bool[3] out_mask, "
-      "Tensor W_prepack) -> (Tensor, Tensor, Tensor)");
+      "convolution_backward(Tensor input, Tensor weight, Tensor? bias, Tensor grad_output, bool[3] out_mask, "
+      "Tensor W_prepack, bool? weight_channels_last) -> (Tensor, Tensor, Tensor)");
   m.impl(
       "convolution_backward",
       c10::DispatchKey::CPU,
diff --git a/csrc/cpu/aten/ConvTranspose.cpp b/csrc/cpu/aten/ConvTranspose.cpp
index dd706720..035f60aa 100644
--- a/csrc/cpu/aten/ConvTranspose.cpp
+++ b/csrc/cpu/aten/ConvTranspose.cpp
@@ -288,7 +288,9 @@ at::Tensor IPEXConvTransposeOp::forward(
   ctx->saved_data["weight_requires_grad"] = weight.requires_grad();
   ctx->saved_data["bias_requires_grad"] =
       bias_opt.has_value() && bias_opt.value().requires_grad() ? true : false;
-  ctx->save_for_backward({input});
+  ctx->saved_data["bias_opt"] = bias_opt;
+  ctx->saved_data["weight_channels_last"] = weight_channels_last;
+  ctx->save_for_backward({input, weight});
 
   return _forward(
       input,
@@ -413,9 +415,12 @@ std::tuple<at::Tensor, at::Tensor> conv_transpose_backward_weights(
 
 std::tuple<at::Tensor, at::Tensor, at::Tensor> conv_transpose_backward(
     const at::Tensor& input,
+    const at::Tensor& weight,
+    const c10::optional<at::Tensor>& bias_opt,
     const at::Tensor& grad_output_t,
     std::array<bool, 3> output_mask,
-    const at::Tensor& op_context) {
+    const at::Tensor& op_context,
+    c10::optional<bool> weight_channels_last) {
   return reinterpret_cast<IpexConvTransposeOpContext*>(
              op_context.data_ptr<int64_t>()[0])
       ->run_backward(input, grad_output_t, output_mask);
@@ -440,7 +445,19 @@ conv_transpose_backward_kernel_impl(
   RECORD_FUNCTION(
       "torch_ipex::conv_transpose_backward", c10::ArrayRef<c10::IValue>({}));
 
-  auto memory_format = input.suggest_memory_format();
+  bool use_channels_last =
+      input.suggest_memory_format() == at::MemoryFormat::ChannelsLast ||
+      input.suggest_memory_format() == at::MemoryFormat::ChannelsLast3d ||
+      weight_channels_last;
+
+  auto memory_format = at::MemoryFormat::Contiguous;
+  if (use_channels_last) {
+    if (input.dim() == 4) {
+      memory_format = at::MemoryFormat::ChannelsLast;
+    } else {
+      memory_format = at::MemoryFormat::ChannelsLast3d;
+    }
+  }
   at::Tensor grad_output = grad_output_t.contiguous(memory_format);
 
   at::Tensor grad_input, grad_weight, grad_bias;
@@ -482,11 +499,25 @@ torch::autograd::variable_list IPEXConvTransposeOp::backward(
   output_mask[0] = ctx->saved_data["input_requires_grad"].toBool();
   output_mask[1] = ctx->saved_data["weight_requires_grad"].toBool();
   output_mask[2] = ctx->saved_data["bias_requires_grad"].toBool();
+  auto bias_opt = ctx->saved_data["bias_opt"].toOptional<at::Tensor>();
+  auto weight_channels_last =
+      ctx->saved_data["weight_channels_last"].toOptional<bool>();
   auto saved = ctx->get_saved_variables();
   at::Tensor input = saved[0];
+  at::Tensor weight = saved[1];
   at::Tensor grad_input, grad_weight, grad_bias;
-  std::tie(grad_input, grad_weight, grad_bias) =
-      conv_transpose_backward(input, grad_outputs[0], output_mask, op_context);
+  static auto op =
+      torch::Dispatcher::singleton()
+          .findSchemaOrThrow("torch_ipex::conv_transpose_backward", "")
+          .typed<decltype(conv_transpose_backward)>();
+  std::tie(grad_input, grad_weight, grad_bias) = op.call(
+      input,
+      weight,
+      bias_opt,
+      grad_outputs[0],
+      output_mask,
+      op_context,
+      weight_channels_last);
   return {
       grad_input,
       grad_weight,
@@ -602,8 +633,8 @@ TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
       c10::DispatchKey::AutocastCPU,
       torch_ipex::autocast::conv_transpose);
   m.def(
-      "conv_transpose_backward(Tensor input, Tensor grad_out, bool[3] output_mask, "
-      "Tensor W_prepack) "
+      "conv_transpose_backward(Tensor input, Tensor weight, Tensor? bias_opt, Tensor grad_out, bool[3] output_mask, "
+      "Tensor W_prepack, bool? weight_channels_last) "
       " -> (Tensor, Tensor, Tensor)");
   m.impl(
       "conv_transpose_backward",
diff --git a/csrc/cpu/aten/EmbeddingBag.cpp b/csrc/cpu/aten/EmbeddingBag.cpp
index 3dff9e5f..c554b7e7 100644
--- a/csrc/cpu/aten/EmbeddingBag.cpp
+++ b/csrc/cpu/aten/EmbeddingBag.cpp
@@ -114,10 +114,10 @@ at::Tensor dil_qembeddingbag(
     at::ScalarType o_dtype) {
   /*
   pointer to torch_ipex::cpu::embedding_bag_int8_kernel_impl(
-      weight, indices, offsets, include_last_offset);
+      weight, indices, offsets, o_scale, include_last_offset);
   */
   return torch_ipex::cpu::embedding_bag_int8_kernel_stub(
-      kCPU, weight, indices, offsets, include_last_offset);
+      kCPU, weight, indices, offsets, o_scale, include_last_offset);
 }
 
 } // namespace cpu
diff --git a/csrc/cpu/aten/EmbeddingBag.h b/csrc/cpu/aten/EmbeddingBag.h
index e7d018d2..7f7d2058 100644
--- a/csrc/cpu/aten/EmbeddingBag.h
+++ b/csrc/cpu/aten/EmbeddingBag.h
@@ -35,6 +35,7 @@ at::Tensor embedding_bag_int8_kernel_impl(
     const at::Tensor& qweight,
     const at::Tensor& indices,
     const at::Tensor& offsets,
+    double o_scale,
     bool include_last_offset);
 
 } // namespace
@@ -60,6 +61,7 @@ using embedding_bag_int8_kernel_fn = at::Tensor (*)(
     const at::Tensor&,
     const at::Tensor&,
     const at::Tensor&,
+    double,
     bool);
 DECLARE_DISPATCH(embedding_bag_int8_kernel_fn, embedding_bag_int8_kernel_stub);
 
diff --git a/csrc/cpu/aten/FlashAttention.cpp b/csrc/cpu/aten/FlashAttention.cpp
deleted file mode 100644
index 28423155..00000000
--- a/csrc/cpu/aten/FlashAttention.cpp
+++ /dev/null
@@ -1,43 +0,0 @@
-#include <torch/all.h>
-#include "FlashAttention.h"
-#include <torch/csrc/autograd/function.h>
-
-namespace torch_ipex {
-namespace cpu {
-
-DEFINE_DISPATCH(flash_attention_kernel_stub);
-
-/*
-*Caculate the flash attention SDPA. 
-*@param query
-*@param key
-*@param value
-*@param scale_attn
-*@param attention_mask
-*@return attn_outs
-*/
-at::Tensor flash_attention_forward_cpu(
-    at::Tensor query,
-    at::Tensor key,
-    at::Tensor value,
-    const double scale_attn,
-    at::Tensor attention_mask){
-  return flash_attention_kernel_stub(
-      kCPU, query, key, value, scale_attn, attention_mask);
-}
-
-} // namespace cpu
-} // namespace torch_ipex
-
-namespace {
-
-TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
-  m.def(
-      "flash_attention(Tensor query, Tensor key, Tensor value, \
-       float scale_attn, Tensor attention_mask)-> Tensor");
-  m.impl(
-      "flash_attention",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::flash_attention_forward_cpu);
-}
-}
diff --git a/csrc/cpu/aten/FlashAttention.h b/csrc/cpu/aten/FlashAttention.h
deleted file mode 100644
index 8e8e39b9..00000000
--- a/csrc/cpu/aten/FlashAttention.h
+++ /dev/null
@@ -1,29 +0,0 @@
-#pragma once
-
-#include <ATen/ATen.h>
-#include <dyndisp/DispatchStub.h>
-
-namespace torch_ipex {
-namespace cpu {
-
-namespace {
-
-at::Tensor flash_attention(
-    at::Tensor query,
-    at::Tensor key,
-    at::Tensor value,
-    const double scale_attn,
-    at::Tensor attention_mask);
-}
-
-using flash_attention_kernel_fn = at::Tensor (*)(
-    at::Tensor query,
-    at::Tensor key,
-    at::Tensor value,
-    const double scale_attn,
-    at::Tensor attention_mask);
-
-DECLARE_DISPATCH(flash_attention_kernel_fn, flash_attention_kernel_stub);
-
-} // namespace cpu
-} // namespace torch_ipex
diff --git a/csrc/cpu/aten/Linear.cpp b/csrc/cpu/aten/Linear.cpp
index 1f034afd..848a9237 100644
--- a/csrc/cpu/aten/Linear.cpp
+++ b/csrc/cpu/aten/Linear.cpp
@@ -177,6 +177,8 @@ at::Tensor linear_eltwise_forward(
 
 std::tuple<at::Tensor, at::Tensor, at::Tensor> linear_backward(
     const at::Tensor& input,
+    const at::Tensor& weight,
+    const c10::optional<at::Tensor>& bias,
     const at::Tensor& grad_output,
     std::array<bool, 3> output_mask,
     const at::Tensor& op_context) {
@@ -226,12 +228,13 @@ at::Tensor IPEXLinearOp::forward(
   ctx->saved_data["bias_requires_grad"] =
       bias.has_value() && bias.value().requires_grad() ? true : false;
   ctx->saved_data["eltwise"] = eltwise;
+  ctx->saved_data["bias"] = bias;
   auto output =
       _forward(input, weight, bias, eltwise, op_context, out_features);
   if (eltwise == NotFused)
-    ctx->save_for_backward({input});
+    ctx->save_for_backward({input, weight});
   else
-    ctx->save_for_backward({input, output});
+    ctx->save_for_backward({input, weight, output});
   return output;
 }
 
@@ -242,6 +245,7 @@ torch::autograd::tensor_list IPEXLinearOp::backward(
 
   auto saved = ctx->get_saved_variables();
   at::Tensor input = saved[0];
+  at::Tensor weight = saved[1];
   auto op_context = ctx->saved_data["op_context"].toTensor();
   std::array<bool, 3> output_mask;
   output_mask[0] = ctx->saved_data["input_requires_grad"].toBool();
@@ -249,11 +253,12 @@ torch::autograd::tensor_list IPEXLinearOp::backward(
   output_mask[2] = ctx->saved_data["bias_requires_grad"].toBool();
   int64_t eltwise = ctx->saved_data["eltwise"].toInt();
   auto batch_size = ctx->saved_data["batch_size"].toOptional<int64_t>();
+  auto bias = ctx->saved_data["bias"].toOptional<at::Tensor>();
   at::Tensor grad_output;
   if (eltwise == NotFused) {
     grad_output = grad_outputs[0];
   } else {
-    at::Tensor output = saved[1];
+    at::Tensor output = saved[2];
     grad_output = eltwise == ReLU
         ? relu_use_dst_for_bwd(grad_outputs[0], output)
         : sigmoid_use_dst_for_bwd(grad_outputs[0], output);
@@ -264,7 +269,7 @@ torch::autograd::tensor_list IPEXLinearOp::backward(
                        .findSchemaOrThrow("torch_ipex::linear_backward", "")
                        .typed<decltype(linear_backward)>();
   std::tie(grad_input, grad_weight, grad_bias) =
-      op.call(input, grad_output, output_mask, op_context);
+      op.call(input, weight, bias, grad_output, output_mask, op_context);
   // must have save nums of output with inputs args
   return {
       grad_input,
@@ -300,64 +305,15 @@ at::Tensor ipex_linear_eltwise(
 }
 
 DEFINE_DISPATCH(woq_linear_packB_stub);
-DEFINE_DISPATCH(woq_tpp_gemm_packB_stub);
 at::Tensor woq_linear_pack_weight(
     const at::Tensor& weight,
-    const at::Tensor& scales,
     const at::Tensor& zero_points,
-    int64_t lowp_mode) {
-#ifdef WOQ_TPP_KERNEL
-  // TPP kernel does not support edge cases
-  // It generates packed weight in 4d (Nc, Kc, block_k, block_n)
-  auto N = weight.size(0), K = weight.size(1);
-  // For TPP kernel, we only consider even K
-  if (K % 2 == 0) {
-    bool is_int4 = weight.scalar_type() == c10::kQUInt4x2;
-    // int num_threads = at::get_num_threads();
-    size_t block_n = 32;
-    if (lowp_mode == 0) {
-      block_n = 16;
-    }
-    size_t block_k = 64;
-    while (K % block_k != 0) {
-      block_k /= 2;
-    }
-    assert(block_k > 0);
-    if (is_int4) {
-      // Create a new non-quantized tensor in data type uint8 (Byte)
-      // One uint8 holds two int4 values. Compressed along K.
-      // N is padded to the nearest multiple of block_n.
-      int64_t K_int4_compressed = K / 2;
-      int64_t N_int4 = N % block_n
-          ? N / block_n * block_n + block_n
-          : N;
-      at::Tensor weight_int4 = at::empty(
-          {N_int4, K_int4_compressed},
-          device(c10::kCPU).dtype(c10::kByte)
-      );
-      int64_t weight_size_bytes = weight.numel() / 2;
-      int64_t weight_int4_size_bytes = weight_int4.numel();
-      int64_t pad_size_bytes = weight_int4_size_bytes - weight_size_bytes;
-      std::memcpy(weight_int4.data_ptr(), weight.data_ptr(), weight_size_bytes);
-      std::memset((uint8_t*)weight_int4.data_ptr() + weight_size_bytes, 0, pad_size_bytes);
-      return woq_tpp_gemm_packB_stub(kCPU, weight_int4, is_int4, block_n, block_k, lowp_mode);
-    }
-    if (!(N % block_n) && !(K % block_k)) {
-      return woq_tpp_gemm_packB_stub(kCPU, weight, is_int4, block_n, block_k, lowp_mode);
-    }
-  }
-#endif
-  return woq_linear_packB_stub(kCPU, weight, scales, zero_points);
+    const at::Tensor& scales) {
+  return woq_linear_packB_stub(kCPU, weight, zero_points, scales);
 }
 
 DEFINE_DISPATCH(woq_linear_unpackB_stub);
-DEFINE_DISPATCH(woq_tpp_gemm_unpackB_stub);
-at::Tensor woq_linear_unpack_weight(const at::Tensor& weight, bool is_int4, int64_t lowp_mode) {
-#ifdef WOQ_TPP_KERNEL
-  if (weight.dim() > 2) {
-    return woq_tpp_gemm_unpackB_stub(kCPU, weight, is_int4, lowp_mode);
-  }
-#endif
+at::Tensor woq_linear_unpack_weight(const at::Tensor& weight) {
   return woq_linear_unpackB_stub(kCPU, weight);
 }
 
@@ -365,325 +321,50 @@ DEFINE_DISPATCH(woq_gemm_kernel_stub);
 void woq_linear_kernel_output(
     const at::Tensor& self,
     const at::Tensor& weight,
-    const at::Tensor& scales_float,
     const at::Tensor& zero_points_float,
+    const at::Tensor& scales_float,
     const at::Tensor& bias,
-    int64_t lowp_mode,
     at::Tensor& output) {
   woq_gemm_kernel_stub(
-      kCPU,
-      self,
-      weight,
-      scales_float,
-      zero_points_float,
-      bias,
-      lowp_mode,
-      output);
+      kCPU, self, weight, zero_points_float, scales_float, bias, output);
 }
 
-DEFINE_DISPATCH(woq_tpp_gemm_kernel_stub);
 at::Tensor woq_linear_kernel(
     const at::Tensor& self,
     const at::Tensor& weight,
-    const std::vector<at::Tensor>& scales_list,
-    const std::vector<at::Tensor>& zps_list,
-    const std::vector<at::Tensor>& bias_list,
-    bool is_int4,
-    int64_t lowp_mode,
-    int64_t num_concats) {
-#ifdef WOQ_TPP_KERNEL
-  if (weight.dim() > 2) {
-    return woq_tpp_gemm_kernel_stub(
-      kCPU,
-      self,
-      weight,
-      scales_list,
-      zps_list,
-      bias_list,
-      is_int4,
-      lowp_mode,
-      num_concats,
-      FUSE_NONE, // no post op fusion
-      std::vector<at::Tensor>()
-    );
-  }
-#endif
-  auto input_size = self.sizes();
-  std::vector<int64_t> output_size(input_size.begin(), input_size.end() - 1);
-  output_size.push_back(weight.size(0));
-  auto output = at::empty(output_size, self.options());
-  output.set_requires_grad(self.requires_grad());
-  woq_linear_kernel_output(
-      self,
-      weight,
-      scales_list[0],
-      zps_list[0],
-      bias_list[0],
-      lowp_mode,
-      output);
-  if (num_concats > 1) {
-    // View as [..., num_concats, N/num_concats], transpose then make contiguous
-    // Finally view back as output shape
-    auto out_shape = output.sizes().vec();
-    out_shape.insert(out_shape.end() - 1, num_concats);
-    out_shape.back() /= num_concats;
-    return output.view(out_shape).transpose(0, -2).contiguous().view(output.sizes().vec());
+    const at::Tensor& zero_points_float,
+    const at::Tensor& scales_float,
+    const at::Tensor& bias) {
+  TORCH_CHECK(
+      weight.is_quantized(),
+      "Weight only quantized linear: weight should be quantized!");
+  // TODO Will support optimized impl
+  if (self.scalar_type() != c10::ScalarType::Float ||
+      weight.scalar_type() == c10::ScalarType::QUInt4x2) {
+    auto w = weight.dequantize();
+    auto x = self.to(c10::ScalarType::Float);
+    auto out = at::linear(x, w, bias);
+    return out.to(self.scalar_type());
+  } else {
+    auto input_size = self.sizes();
+    std::vector<int64_t> output_size(input_size.begin(), input_size.end() - 1);
+    output_size.push_back(weight.size(0));
+    auto output = at::empty(output_size, self.options());
+    output.set_requires_grad(self.requires_grad());
+    woq_linear_kernel_output(
+        self, weight, zero_points_float, scales_float, bias, output);
+    return output;
   }
-  return output;
 }
 
 at::Tensor woq_linear_forward(
     const at::Tensor& input,
     const at::Tensor& op_context) {
-  RECORD_FUNCTION(
-      "torch_ipex::ipex_woq_linear", c10::ArrayRef<c10::IValue>({}));
   return reinterpret_cast<IpexWoqLinearOpContext*>(
              op_context.data_ptr<int64_t>()[0])
       ->run(input);
 }
 
-DEFINE_DISPATCH(woq_gemm_eltwise_kernel_stub);
-void woq_linear_eltwise_kernel_output(
-    const at::Tensor& self,
-    const at::Tensor& weight,
-    const at::Tensor& scales_float,
-    const at::Tensor& zero_points_float,
-    const at::Tensor& bias,
-    const c10::string_view& post_op,
-    const torch::List<c10::optional<at::Scalar>>& scalars,
-    const c10::optional<c10::string_view>& algorithm,
-    int64_t lowp_mode,
-    at::Tensor& output) {
-  woq_gemm_eltwise_kernel_stub(
-      kCPU,
-      self,
-      weight,
-      scales_float,
-      zero_points_float,
-      bias,
-      post_op,
-      scalars,
-      algorithm,
-      lowp_mode,
-      output);
-}
-
-at::Tensor woq_linear_eltwise_kernel(
-    const at::Tensor& self,
-    const at::Tensor& weight,
-    const std::vector<at::Tensor>& scales_list,
-    const std::vector<at::Tensor>& zps_list,
-    const std::vector<at::Tensor>& bias_list,
-    const c10::string_view& post_op,
-    const torch::List<c10::optional<at::Scalar>>& scalars,
-    const c10::optional<c10::string_view>& algorithm,
-    bool is_int4,
-    int64_t lowp_mode,
-    int64_t num_concats) {
-#ifdef WOQ_TPP_KERNEL
-  int64_t post_op_fusion_type = post_op == "gelu" ? FUSE_GELU : FUSE_NONE;
-  if (weight.dim() > 2) {
-    return woq_tpp_gemm_kernel_stub(
-      kCPU,
-      self,
-      weight,
-      scales_list,
-      zps_list,
-      bias_list,
-      is_int4,
-      lowp_mode,
-      num_concats,
-      post_op_fusion_type,
-      std::vector<at::Tensor>()
-    );
-  }
-#endif
-  auto input_size = self.sizes();
-  std::vector<int64_t> output_size(input_size.begin(), input_size.end() - 1);
-  output_size.push_back(weight.size(0));
-  auto output = at::empty(output_size, self.options());
-  output.set_requires_grad(self.requires_grad());
-  woq_linear_eltwise_kernel_output(
-      self,
-      weight,
-      scales_list[0],
-      zps_list[0],
-      bias_list[0],
-      post_op,
-      scalars,
-      algorithm,
-      lowp_mode,
-      output);
-  return output;
-}
-
-at::Tensor woq_linear_gelu_forward(
-    const at::Tensor& input,
-    const at::Tensor& op_context) {
-  RECORD_FUNCTION(
-      "torch_ipex::woq_linear_gelu", c10::ArrayRef<c10::IValue>({}));
-  return reinterpret_cast<IpexWoqLinearOpContext*>(
-             op_context.data_ptr<int64_t>()[0])
-      ->run_eltwise(input, "gelu", torch::List<c10::optional<at::Scalar>>(), "none");
-}
-
-at::Tensor woq_linear_add_kernel(
-    const at::Tensor& self,
-    const at::Tensor& weight,
-    const std::vector<at::Tensor>& scales_list,
-    const std::vector<at::Tensor>& zps_list,
-    const std::vector<at::Tensor>& bias_list,
-    bool is_int4,
-    int64_t lowp_mode,
-    int64_t num_concats,
-    at::Tensor& accumu,
-    const c10::optional<at::Scalar>& alpha) {
-  c10::Scalar a = alpha.has_value() ? alpha.value() : 1.0f;
-#ifdef WOQ_TPP_KERNEL
-  if (weight.dim() > 2) {
-    auto output = woq_tpp_gemm_kernel_stub(
-      kCPU,
-      self,
-      weight,
-      scales_list,
-      zps_list,
-      bias_list,
-      is_int4,
-      lowp_mode,
-      num_concats,
-      FUSE_NONE, // no eltwise post op
-      std::vector<at::Tensor>()
-    );
-    at::add_out(accumu, output, accumu, a);
-    return accumu;
-  }
-#endif
-  auto input_size = self.sizes();
-  std::vector<int64_t> output_size(input_size.begin(), input_size.end() - 1);
-  output_size.push_back(weight.size(0));
-  auto output = at::empty(output_size, self.options());
-  output.set_requires_grad(self.requires_grad());
-  woq_linear_kernel_output(
-      self,
-      weight,
-      scales_list[0],
-      zps_list[0],
-      bias_list[0],
-      lowp_mode,
-      output);
-  at::add_out(accumu, output, accumu, a);
-  return accumu;
-}
-
-at::Tensor woq_linear_add_kernel(
-    const at::Tensor& self,
-    const at::Tensor& weight,
-    const std::vector<at::Tensor>& scales_list,
-    const std::vector<at::Tensor>& zps_list,
-    const std::vector<at::Tensor>& bias_list,
-    bool is_int4,
-    int64_t lowp_mode,
-    int64_t num_concats,
-    const std::vector<at::Tensor>& others) {
-#ifdef WOQ_TPP_KERNEL
-  if (weight.dim() > 2) {
-    return woq_tpp_gemm_kernel_stub(
-      kCPU,
-      self,
-      weight,
-      scales_list,
-      zps_list,
-      bias_list,
-      is_int4,
-      lowp_mode,
-      num_concats,
-      FUSE_ADD, // post op add
-      others
-    );
-  }
-#endif
-  auto input_size = self.sizes();
-  std::vector<int64_t> output_size(input_size.begin(), input_size.end() - 1);
-  output_size.push_back(weight.size(0));
-  auto output = at::empty(output_size, self.options());
-  output.set_requires_grad(self.requires_grad());
-  woq_linear_kernel_output(
-      self,
-      weight,
-      scales_list[0],
-      zps_list[0],
-      bias_list[0],
-      lowp_mode,
-      output);
-  return at::add(output, others[0]);
-}
-
-at::Tensor woq_linear_add_add_kernel(
-    const at::Tensor& self,
-    const at::Tensor& weight,
-    const std::vector<at::Tensor>& scales_list,
-    const std::vector<at::Tensor>& zps_list,
-    const std::vector<at::Tensor>& bias_list,
-    bool is_int4,
-    int64_t lowp_mode,
-    int64_t num_concats,
-    const std::vector<at::Tensor>& others) {
-#ifdef WOQ_TPP_KERNEL
-  if (weight.dim() > 2) {
-    return woq_tpp_gemm_kernel_stub(
-      kCPU,
-      self,
-      weight,
-      scales_list,
-      zps_list,
-      bias_list,
-      is_int4,
-      lowp_mode,
-      num_concats,
-      FUSE_ADD_ADD, // post op add-add
-      others
-    );
-  }
-#endif
-  auto input_size = self.sizes();
-  std::vector<int64_t> output_size(input_size.begin(), input_size.end() - 1);
-  output_size.push_back(weight.size(0));
-  auto output = at::empty(output_size, self.options());
-  output.set_requires_grad(self.requires_grad());
-  woq_linear_kernel_output(
-      self,
-      weight,
-      scales_list[0],
-      zps_list[0],
-      bias_list[0],
-      lowp_mode,
-      output);
-  auto y = at::add(output, others[0]);
-  return at::add(y, others[1]);
-}
-
-at::Tensor woq_linear_add_forward(
-    const at::Tensor& input,
-    const at::Tensor& op_context,
-    const std::vector<at::Tensor>& others) {
-  RECORD_FUNCTION(
-      "torch_ipex::woq_linear_add", c10::ArrayRef<c10::IValue>({}));
-  return reinterpret_cast<IpexWoqLinearOpContext*>(
-             op_context.data_ptr<int64_t>()[0])
-      ->run_add(input, others);
-}
-
-at::Tensor woq_linear_add_add_forward(
-    const at::Tensor& input,
-    const at::Tensor& op_context,
-    const std::vector<at::Tensor>& others) {
-  RECORD_FUNCTION(
-      "torch_ipex::woq_linear_add_add", c10::ArrayRef<c10::IValue>({}));
-  return reinterpret_cast<IpexWoqLinearOpContext*>(
-             op_context.data_ptr<int64_t>()[0])
-      ->run_add_add(input, others);
-}
-
 } // namespace cpu
 } // namespace torch_ipex
 
@@ -754,41 +435,6 @@ at::Tensor woq_linear_forward(
   return op.call(cpu_cached_cast(target_type, input), op_context);
 }
 
-at::Tensor woq_linear_gelu_forward(
-    const at::Tensor& input,
-    const at::Tensor& op_context) {
-  c10::impl::ExcludeDispatchKeyGuard no_autocastCPU(DispatchKey::AutocastCPU);
-  static auto op = torch::Dispatcher::singleton()
-                       .findSchemaOrThrow("torch_ipex::woq_linear_gelu", "")
-                       .typed<decltype(woq_linear_gelu_forward)>();
-  auto target_type = get_autocast_dtype();
-  return op.call(cpu_cached_cast(target_type, input), op_context);
-}
-
-at::Tensor woq_linear_add_forward(
-    const at::Tensor& input,
-    const at::Tensor& op_context,
-    const std::vector<at::Tensor>& others) {
-  c10::impl::ExcludeDispatchKeyGuard no_autocastCPU(DispatchKey::AutocastCPU);
-  static auto op = torch::Dispatcher::singleton()
-                       .findSchemaOrThrow("torch_ipex::woq_linear_add", "")
-                       .typed<decltype(woq_linear_add_forward)>();
-  auto target_type = get_autocast_dtype();
-  return op.call(cpu_cached_cast(target_type, input), op_context, cpu_cached_cast(target_type, others));
-}
-
-at::Tensor woq_linear_add_add_forward(
-    const at::Tensor& input,
-    const at::Tensor& op_context,
-    const std::vector<at::Tensor>& others) {
-  c10::impl::ExcludeDispatchKeyGuard no_autocastCPU(DispatchKey::AutocastCPU);
-  static auto op = torch::Dispatcher::singleton()
-                       .findSchemaOrThrow("torch_ipex::woq_linear_add_add", "")
-                       .typed<decltype(woq_linear_add_add_forward)>();
-  auto target_type = get_autocast_dtype();
-  return op.call(cpu_cached_cast(target_type, input), op_context, cpu_cached_cast(target_type, others));
-}
-
 } // namespace autocast
 } // namespace torch_ipex
 
@@ -814,33 +460,6 @@ TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
       "ipex_woq_linear",
       c10::DispatchKey::AutocastCPU,
       torch_ipex::autocast::woq_linear_forward);
-  m.def("woq_linear_gelu(Tensor input, Tensor W_prepack) -> Tensor");
-  m.impl(
-      "woq_linear_gelu",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::woq_linear_gelu_forward);
-  m.impl(
-      "woq_linear_gelu",
-      c10::DispatchKey::AutocastCPU,
-      torch_ipex::autocast::woq_linear_gelu_forward);
-  m.def("woq_linear_add(Tensor input, Tensor W_prepack, Tensor[] others) -> Tensor");
-  m.impl(
-      "woq_linear_add",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::woq_linear_add_forward);
-  m.impl(
-      "woq_linear_add",
-      c10::DispatchKey::AutocastCPU,
-      torch_ipex::autocast::woq_linear_add_forward);
-  m.def("woq_linear_add_add(Tensor input, Tensor W_prepack, Tensor[] others) -> Tensor");
-  m.impl(
-      "woq_linear_add_add",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::woq_linear_add_add_forward);
-  m.impl(
-      "woq_linear_add_add",
-      c10::DispatchKey::AutocastCPU,
-      torch_ipex::autocast::woq_linear_add_add_forward);
   // fuse eltwise
   m.def(
       "ipex_linear_eltwise(Tensor input, Tensor weight, Tensor? bias, int eltwise, "
@@ -859,7 +478,7 @@ TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
       torch_ipex::cpu::linear_eltwise_forward);
   // bw
   m.def(
-      "linear_backward(Tensor input, Tensor grad_output, bool[3] out_mask, "
+      "linear_backward(Tensor input, Tensor weight, Tensor? bias, Tensor grad_output, bool[3] out_mask, "
       "Tensor W_prepack) -> (Tensor, Tensor, Tensor)");
   m.impl(
       "linear_backward",
diff --git a/csrc/cpu/aten/Linear.h b/csrc/cpu/aten/Linear.h
index ba987c70..c0394a4c 100644
--- a/csrc/cpu/aten/Linear.h
+++ b/csrc/cpu/aten/Linear.h
@@ -76,119 +76,41 @@ at::Tensor ipex_linear_eltwise(
     const at::Tensor& op_context,
     const c10::optional<int64_t> out_features);
 
-// WOQ linear ops
 at::Tensor woq_linear_pack_weight(
     const at::Tensor& weight,
-    const at::Tensor& scale,
     const at::Tensor& zero_points,
-    int64_t lowp_mode);
+    const at::Tensor& scale);
 
-at::Tensor woq_linear_unpack_weight(const at::Tensor& weight, bool is_int4, int64_t lowp_mode);
+at::Tensor woq_linear_unpack_weight(const at::Tensor& weight);
 
 void woq_linear_kernel_output(
     const at::Tensor& self,
     const at::Tensor& weight,
-    const at::Tensor& scales_float,
     const at::Tensor& zero_points_float,
+    const at::Tensor& scales_float,
     const at::Tensor& bias,
-    int64_t lowp_mode,
     at::Tensor& output);
 
 at::Tensor woq_linear_kernel(
     const at::Tensor& self,
     const at::Tensor& weight,
-    const std::vector<at::Tensor>& scales_list,
-    const std::vector<at::Tensor>& zps_list,
-    const std::vector<at::Tensor>& bias_list,
-    bool is_int4,
-    int64_t lowp_mode,
-    int64_t num_concats);
-
-void woq_linear_eltwise_kernel_output(
-    const at::Tensor& self,
-    const at::Tensor& weight,
-    const at::Tensor& scales_float,
     const at::Tensor& zero_points_float,
-    const at::Tensor& bias,
-    const c10::string_view& post_op,
-    const torch::List<c10::optional<at::Scalar>>& scalars,
-    const c10::optional<c10::string_view>& algorithm,
-    int64_t lowp_mode,
-    at::Tensor& output);
-
-at::Tensor woq_linear_eltwise_kernel(
-    const at::Tensor& self,
-    const at::Tensor& weight,
-    const std::vector<at::Tensor>& scales_list,
-    const std::vector<at::Tensor>& zps_list,
-    const std::vector<at::Tensor>& bias_list,
-    const c10::string_view& post_op,
-    const torch::List<c10::optional<at::Scalar>>& scalars,
-    const c10::optional<c10::string_view>& algorithm,
-    bool is_int4,
-    int64_t lowp_mode,
-    int64_t num_concats);
-
-at::Tensor woq_linear_add_kernel(
-    const at::Tensor& self,
-    const at::Tensor& weight,
-    const std::vector<at::Tensor>& scales_list,
-    const std::vector<at::Tensor>& zps_list,
-    const std::vector<at::Tensor>& bias_list,
-    bool is_int4,
-    int64_t lowp_mode,
-    int64_t num_concats,
-    at::Tensor& accumu,
-    const c10::optional<at::Scalar>& alpha);
-
-at::Tensor woq_linear_add_kernel(
-    const at::Tensor& self,
-    const at::Tensor& weight,
-    const std::vector<at::Tensor>& scales_list,
-    const std::vector<at::Tensor>& zps_list,
-    const std::vector<at::Tensor>& bias_list,
-    bool is_int4,
-    int64_t lowp_mode,
-    int64_t num_concats,
-    const std::vector<at::Tensor>& others);
-
-at::Tensor woq_linear_add_add_kernel(
-    const at::Tensor& self,
-    const at::Tensor& weight,
-    const std::vector<at::Tensor>& scales_list,
-    const std::vector<at::Tensor>& zps_list,
-    const std::vector<at::Tensor>& bias_list,
-    bool is_int4,
-    int64_t lowp_mode,
-    int64_t num_concats,
-    const std::vector<at::Tensor>& others);
+    const at::Tensor& scales_float,
+    const at::Tensor& bias);
 
 namespace {
 void woq_gemm_kernel_impl(
     const at::Tensor& self,
     const at::Tensor& weight,
-    const at::Tensor& scales_float,
     const at::Tensor& zero_points_float,
-    const at::Tensor& bias,
-    int64_t lowp_mode,
-    at::Tensor& output);
-
-void woq_gemm_eltwise_kernel_impl(
-    const at::Tensor& self,
-    const at::Tensor& weight,
     const at::Tensor& scales_float,
-    const at::Tensor& zero_points_float,
     const at::Tensor& bias,
-    const c10::string_view& post_op,
-    const torch::List<c10::optional<at::Scalar>>& scalars,
-    const c10::optional<c10::string_view>& algorithm,
-    int64_t lowp_mode,
     at::Tensor& output);
 
 at::Tensor woq_linear_packB_impl(
     const at::Tensor& weight,
-    const at::Tensor& scales,
-    const at::Tensor& zero_points);
+    const at::Tensor& zero_points,
+    const at::Tensor& scales);
 
 at::Tensor woq_linear_unpackB_impl(const at::Tensor& weight);
 
@@ -200,19 +122,6 @@ using woq_gemm_kernel_fn = void (*)(
     const at::Tensor&,
     const at::Tensor&,
     const at::Tensor&,
-    int64_t,
-    at::Tensor&);
-
-using woq_gemm_eltwise_kernel_fn = void (*)(
-    const at::Tensor&,
-    const at::Tensor&,
-    const at::Tensor&,
-    const at::Tensor&,
-    const at::Tensor&,
-    const c10::string_view&,
-    const torch::List<c10::optional<at::Scalar>>&,
-    const c10::optional<c10::string_view>&,
-    int64_t,
     at::Tensor&);
 
 using woq_linear_packB_fn =
@@ -221,41 +130,7 @@ using woq_linear_packB_fn =
 using woq_linear_unpackB_fn = at::Tensor (*)(const at::Tensor&);
 
 DECLARE_DISPATCH(woq_gemm_kernel_fn, woq_gemm_kernel_stub);
-DECLARE_DISPATCH(woq_gemm_eltwise_kernel_fn, woq_gemm_eltwise_kernel_stub);
 DECLARE_DISPATCH(woq_linear_packB_fn, woq_linear_packB_stub);
 DECLARE_DISPATCH(woq_linear_unpackB_fn, woq_linear_unpackB_stub);
-
-using woq_tpp_gemm_kernel_fn = at::Tensor (*)(
-    const at::Tensor&,
-    const at::Tensor&,
-    const std::vector<at::Tensor>&,
-    const std::vector<at::Tensor>&,
-    const std::vector<at::Tensor>&,
-    bool,
-    int64_t,
-    int64_t,
-    int64_t,
-    const std::vector<at::Tensor>&);
-
-using woq_tpp_gemm_packB_fn =
-    at::Tensor (*)(const at::Tensor&, bool, size_t, size_t, int64_t);
-
-using woq_tpp_gemm_unpackB_fn = at::Tensor (*)(const at::Tensor&, bool, int64_t);
-
-DECLARE_DISPATCH(woq_tpp_gemm_kernel_fn, woq_tpp_gemm_kernel_stub);
-DECLARE_DISPATCH(woq_tpp_gemm_packB_fn, woq_tpp_gemm_packB_stub);
-DECLARE_DISPATCH(woq_tpp_gemm_unpackB_fn, woq_tpp_gemm_unpackB_stub);
-
-#ifdef __GNUC__
-#  include <features.h>
-#  if __GNUC_PREREQ(12,3)
-#  define WOQ_TPP_KERNEL
-#  define FUSE_NONE 0
-#  define FUSE_GELU 1
-#  define FUSE_ADD 2
-#  define FUSE_ADD_ADD 3
-#  endif
-#endif
-
 } // namespace cpu
 } // namespace torch_ipex
diff --git a/csrc/cpu/aten/MLPerfConcat.cpp b/csrc/cpu/aten/MLPerfConcat.cpp
new file mode 100644
index 00000000..3e1f0265
--- /dev/null
+++ b/csrc/cpu/aten/MLPerfConcat.cpp
@@ -0,0 +1,33 @@
+#include "MLPerfConcat.h"
+#include <ATen/AccumulateType.h>
+#include <ATen/Tensor.h>
+#include <torch/all.h>
+#include "autocast/autocast_mode.h"
+
+namespace torch_ipex {
+namespace cpu {
+
+using namespace at;
+DEFINE_DISPATCH(mlperf_concat_kernel_stub);
+
+Tensor mlperf_concat(
+    const at::Tensor& a,
+    const at::Tensor& b) {
+  return mlperf_concat_kernel_stub(kCPU, a, b);
+}
+
+} // namespace cpu
+} // namespace torch_ipex
+
+namespace {
+
+TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
+  m.def(
+      "mlperf_concat(Tensor a, Tensor b) -> Tensor");
+  m.impl(
+      "mlperf_concat",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::mlperf_concat);
+}
+
+} // namespace
diff --git a/csrc/cpu/aten/MLPerfConcat.h b/csrc/cpu/aten/MLPerfConcat.h
new file mode 100644
index 00000000..c7c576c6
--- /dev/null
+++ b/csrc/cpu/aten/MLPerfConcat.h
@@ -0,0 +1,25 @@
+#include <ATen/AccumulateType.h>
+#include <ATen/Tensor.h>
+#include <dyndisp/DispatchStub.h>
+#include <torch/all.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+namespace {
+
+at::Tensor mlperf_concat_kernel_impl(
+    const at::Tensor& a,
+    const at::Tensor& b);
+
+} // namespace
+
+using mlperf_concat_kernel_fn = at::Tensor (*)(
+    const at::Tensor&,
+    const at::Tensor&);
+DECLARE_DISPATCH(
+    mlperf_concat_kernel_fn,
+    mlperf_concat_kernel_stub);
+
+} // namespace cpu
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/MaskedMultiHeadAttention.cpp b/csrc/cpu/aten/MaskedMultiHeadAttention.cpp
deleted file mode 100644
index b982be99..00000000
--- a/csrc/cpu/aten/MaskedMultiHeadAttention.cpp
+++ /dev/null
@@ -1,56 +0,0 @@
-#include <torch/all.h>
-#include "MaskedMultiHeadAttention.h"
-#include <torch/csrc/autograd/function.h>
-
-namespace torch_ipex {
-namespace cpu {
-
-DEFINE_DISPATCH(masked_multihead_self_attention_kernel_stub);
-
-/*
-*Caculate the masked multihead attention for decoder layer in decoder only model. 
-*@param query
-*@param key
-*@param value
-*@param key_cache
-*@param value_cache
-*@param beam_idx
-*@param past_kv_steps
-*@param scale_attn
-*@param max_positions
-*@param head_mask
-*@param attention_mask
-*@return {attn_weights, attn_outs}
-*/
-std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor> masked_multihead_self_attention_forward_cpu(
-    at::Tensor query,
-    at::Tensor key,
-    const at::Tensor value,
-    at::Tensor& key_cache,
-    at::Tensor& value_cache,
-    at::Tensor& beam_idx,
-    at::Tensor seq_info,
-    const double scale_attn,
-    int64_t  max_positions,
-    const c10::optional<at::Tensor>& head_mask/* optional */,
-    const c10::optional<at::Tensor>& attention_mask/* optional */){
-  return masked_multihead_self_attention_kernel_stub(
-      kCPU, query, key, value, key_cache, value_cache, beam_idx, seq_info, scale_attn, max_positions, head_mask, attention_mask);
-}
-
-} // namespace cpu
-} // namespace torch_ipex
-
-namespace {
-
-TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
-  m.def(
-      "masked_multihead_self_attention(Tensor query, Tensor key, Tensor value, Tensor (a!)key_cache, \
-       Tensor (a!)value_cache, Tensor(a!) beam_idx, Tensor seq_info, float scale_attn, int max_positions, \
-       Tensor? head_mask, Tensor? attention_mask)-> (Tensor, Tensor, Tensor, Tensor, Tensor)");
-  m.impl(
-      "masked_multihead_self_attention",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::masked_multihead_self_attention_forward_cpu);
-}
-}
\ No newline at end of file
diff --git a/csrc/cpu/aten/MaskedMultiHeadAttention.h b/csrc/cpu/aten/MaskedMultiHeadAttention.h
deleted file mode 100644
index 8407308e..00000000
--- a/csrc/cpu/aten/MaskedMultiHeadAttention.h
+++ /dev/null
@@ -1,41 +0,0 @@
-#pragma once
-
-#include <ATen/ATen.h>
-#include <dyndisp/DispatchStub.h>
-
-namespace torch_ipex {
-namespace cpu {
-
-namespace {
-
-std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>  masked_multihead_self_attention(
-    at::Tensor query,
-    at::Tensor key,
-    const at::Tensor value,
-    at::Tensor& key_cache,
-    at::Tensor& value_cache,
-    at::Tensor& beam_idx,
-    at::Tensor seq_info,
-    const double scale_attn,
-    int64_t  max_positions,
-    const c10::optional<at::Tensor>& head_mask/* optional */,
-    const c10::optional<at::Tensor>& attention_mask/* optional */);
-}
-
-using masked_multihead_self_attention_kernel_fn = std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>  (*)(
-    at::Tensor query,
-    at::Tensor key,
-    const at::Tensor value,
-    at::Tensor& key_cache,
-    at::Tensor& value_cache,
-    at::Tensor& beam_idx,
-    at::Tensor seq_info,
-    const double scale_attn,
-    int64_t  max_positions,
-    const c10::optional<at::Tensor>& head_mask/* optional */,
-    const c10::optional<at::Tensor>& attention_mask/* optional */);
-
-DECLARE_DISPATCH(masked_multihead_self_attention_kernel_fn, masked_multihead_self_attention_kernel_stub);
-
-} // namespace cpu
-} // namespace torch_ipex
\ No newline at end of file
diff --git a/csrc/cpu/aten/MergedEmbWithCat.cpp b/csrc/cpu/aten/MergedEmbWithCat.cpp
new file mode 100644
index 00000000..a1664d38
--- /dev/null
+++ b/csrc/cpu/aten/MergedEmbWithCat.cpp
@@ -0,0 +1,55 @@
+#include "MergedEmbWithCat.h"
+#include <ATen/AccumulateType.h>
+#include <ATen/Tensor.h>
+#include <torch/all.h>
+#include "autocast/autocast_mode.h"
+#include "cpu/kernels/Embeddingbag.h"
+
+namespace torch_ipex {
+namespace cpu {
+
+using namespace at;
+DEFINE_DISPATCH(merged_emb_with_cat_kernel_stub);
+DEFINE_DISPATCH(qmerged_emb_with_cat_kernel_stub);
+
+Tensor merged_emb_with_cat(
+    const TensorList& weights,
+    const TensorList& index,
+    const Tensor& dense,
+    const IntArrayRef multihot) {
+  return merged_emb_with_cat_kernel_stub(kCPU, weights, index, dense, multihot);
+}
+
+Tensor dil_qmerged_emb_with_cat(
+    const at::TensorList& weights,
+    const at::TensorList& index,
+    const at::Tensor& dense,
+    const at::IntArrayRef multihot,
+    double o_scale,
+    int64_t o_zp,
+    at::ScalarType odtype) {
+  std::vector<double> w_scale;
+  w_scale.reserve(weights.size());
+  for (int i = 0; i < weights.size(); i++){
+    w_scale[i] =at::native::q_scale_quant(weights[i]);
+  }
+  double dx_scale = at::native::q_scale_quant(dense);
+  return qmerged_emb_with_cat_kernel_stub(
+      kCPU, weights, index, dense, multihot, w_scale, dx_scale, o_scale);
+}
+
+} // namespace cpu
+} // namespace torch_ipex
+
+namespace {
+
+TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
+  m.def(
+      "merged_emb_with_cat(Tensor[] weights, Tensor[] index, Tensor dense, int[] multihot) -> Tensor");
+  m.impl(
+      "merged_emb_with_cat",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::merged_emb_with_cat);
+}
+
+} // namespace
diff --git a/csrc/cpu/aten/MergedEmbWithCat.h b/csrc/cpu/aten/MergedEmbWithCat.h
new file mode 100644
index 00000000..eaf9256c
--- /dev/null
+++ b/csrc/cpu/aten/MergedEmbWithCat.h
@@ -0,0 +1,58 @@
+#include <ATen/AccumulateType.h>
+#include <ATen/Tensor.h>
+#include <dyndisp/DispatchStub.h>
+#include <torch/all.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+at::Tensor dil_qmerged_emb_with_cat(
+    const at::TensorList& weights,
+    const at::TensorList& index,
+    const at::Tensor& dense,
+    const at::IntArrayRef multihot,
+    double o_scale,
+    int64_t o_zp,
+    at::ScalarType odtype);
+namespace {
+
+at::Tensor merged_emb_with_cat_kernel_impl(
+    const at::TensorList& weights,
+    const at::TensorList& index,
+    const at::Tensor& dense,
+    const at::IntArrayRef multihot);
+
+at::Tensor qmerged_emb_with_cat_kernel_impl(
+    const at::TensorList& weights,
+    const at::TensorList& index,
+    const at::Tensor& dense,
+    const at::IntArrayRef multihot,
+    const c10::ArrayRef<double> w_scale,
+    double dx_scale,
+    double o_scale);
+
+} // namespace
+
+using merged_emb_with_cat_kernel_fn = at::Tensor (*)(
+    const at::TensorList&,
+    const at::TensorList&,
+    const at::Tensor&,
+    const at::IntArrayRef);
+DECLARE_DISPATCH(
+    merged_emb_with_cat_kernel_fn,
+    merged_emb_with_cat_kernel_stub);
+
+using qmerged_emb_with_cat_kernel_fn = at::Tensor (*)(
+    const at::TensorList&,
+    const at::TensorList&,
+    const at::Tensor&,
+    const at::IntArrayRef,
+    const c10::ArrayRef<double>,
+    double,
+    double);
+DECLARE_DISPATCH(
+    qmerged_emb_with_cat_kernel_fn,
+    qmerged_emb_with_cat_kernel_stub);
+
+} // namespace cpu
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/ROIAlign.cpp b/csrc/cpu/aten/ROIAlign.cpp
index ee47826b..04119bda 100644
--- a/csrc/cpu/aten/ROIAlign.cpp
+++ b/csrc/cpu/aten/ROIAlign.cpp
@@ -38,7 +38,7 @@ at::Tensor ROIAlign_forward_impl(
       aligned);
 }
 
-at::Tensor ROIAlign_backward(
+at::Tensor ROIAlign_backward_impl(
     const at::Tensor& grad,
     const at::Tensor& rois,
     double spatial_scale,
@@ -73,6 +73,37 @@ at::Tensor ROIAlign_backward(
       is_channels_last);
 }
 
+at::Tensor ROIAlign_backward(
+    const at::Tensor& grad,
+    const at::Tensor& rois,
+    double spatial_scale,
+    c10::SymInt pooled_height,
+    c10::SymInt pooled_width,
+    c10::SymInt batch_size,
+    c10::SymInt channels,
+    c10::SymInt height,
+    c10::SymInt width,
+    int64_t sampling_ratio,
+    bool aligned,
+    bool is_channels_last) {
+  static auto op = c10::Dispatcher::singleton()
+                       .findSchemaOrThrow("torch_ipex::ROIAlign_backward", "")
+                       .typed<decltype(ROIAlign_backward)>();
+  return op.call(
+      grad,
+      rois,
+      spatial_scale,
+      pooled_height,
+      pooled_width,
+      batch_size,
+      channels,
+      height,
+      width,
+      sampling_ratio,
+      aligned,
+      is_channels_last);
+}
+
 at::Tensor IPEXROIAlignOp::_forward(
     const at::Tensor& input,
     const at::Tensor& rois,
@@ -134,21 +165,17 @@ torch::autograd::variable_list IPEXROIAlignOp::backward(
     torch::autograd::variable_list grad_outputs) {
   RECORD_FUNCTION("IPEXROIAlignOp::backward", c10::ArrayRef<c10::IValue>({}));
 
-  auto input_shape = ctx->saved_data["input_shape"].toIntVector();
+  auto input_shape = ctx->saved_data["input_shape"].toSymIntVector();
   auto spatial_scale = ctx->saved_data["spatial_scale"].toDouble();
-  auto pooled_height = ctx->saved_data["pooled_height"].toInt();
-  auto pooled_width = ctx->saved_data["pooled_width"].toInt();
+  auto pooled_height = ctx->saved_data["pooled_height"].toSymInt();
+  auto pooled_width = ctx->saved_data["pooled_width"].toSymInt();
   auto sampling_ratio = ctx->saved_data["sampling_ratio"].toInt();
   auto aligned = ctx->saved_data["aligned"].toBool();
   auto is_channels_last = ctx->saved_data["is_channels_last"].toBool();
   auto saved = ctx->get_saved_variables();
   at::Tensor rois = saved[0];
 
-  static auto op = torch::Dispatcher::singleton()
-                       .findSchemaOrThrow("torch_ipex::ROIAlign_backward", "")
-                       .typed<decltype(ROIAlign_backward)>();
-
-  auto grad_input = op.call(
+  auto grad_input = ROIAlign_backward(
       grad_outputs[0],
       rois,
       spatial_scale,
@@ -261,11 +288,11 @@ IPEX_TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
       torch_ipex::cpu::ROIAlign_forward_impl);
   // bw
   m.def(
-      "ROIAlign_backward(Tensor grad, Tensor rois, float spatial_scale, int pooled_height, int pooled_width, int batch_size, int channels, int height, int width, int sampling_ratio, bool aligned, bool is_channels_last) -> Tensor");
+      "ROIAlign_backward(Tensor grad, Tensor rois, float spatial_scale, SymInt pooled_height, SymInt pooled_width, SymInt batch_size, SymInt channels, SymInt height, SymInt width, int sampling_ratio, bool aligned, bool is_channels_last) -> Tensor");
   m.impl(
       "ROIAlign_backward",
       c10::DispatchKey::CPU,
-      torch_ipex::cpu::ROIAlign_backward);
+      torch_ipex::cpu::ROIAlign_backward_impl);
 }
 
 IPEX_TORCH_LIBRARY_FRAGMENT(torchvision, m) {
diff --git a/csrc/cpu/aten/ROIAlign.h b/csrc/cpu/aten/ROIAlign.h
index 778cc482..ca6fdace 100644
--- a/csrc/cpu/aten/ROIAlign.h
+++ b/csrc/cpu/aten/ROIAlign.h
@@ -16,7 +16,7 @@ at::Tensor ROIAlign_forward_impl(
     int64_t sampling_ratio,
     bool aligned);
 
-at::Tensor ROIAlign_backward(
+at::Tensor ROIAlign_backward_impl(
     const at::Tensor& grad,
     const at::Tensor& rois,
     double spatial_scale,
diff --git a/csrc/cpu/aten/RotaryPositionEmbedding.cpp b/csrc/cpu/aten/RotaryPositionEmbedding.cpp
deleted file mode 100644
index 2710826f..00000000
--- a/csrc/cpu/aten/RotaryPositionEmbedding.cpp
+++ /dev/null
@@ -1,40 +0,0 @@
-
-//The orginal python code can be found in https://github.com/huggingface/transformers/blob/main/src/transformers/models/gptj/modeling_gptj.py
-//apply_rotary_pos_emb
-#include <torch/all.h>
-#include "RotaryPositionEmbedding.h"
-#include <torch/csrc/autograd/function.h>
-
-namespace torch_ipex {
-namespace cpu {
-
-DEFINE_DISPATCH(rotary_position_embedding_kernel_stub);
-
-void rotary_position_embedding_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_emb_pos,
-    at::Tensor& t_pos,
-    int64_t N,//N: number of head, H: head size
-    int64_t H,
-    int64_t offset,
-    int64_t rotary_ndims){
-  RECORD_FUNCTION("ipex::rotary_position_embedding", c10::ArrayRef<c10::IValue>({}));
-  return rotary_position_embedding_kernel_stub(
-      kCPU, t_in, t_emb_pos, t_pos, N, H, offset, rotary_ndims);
-}
-
-} // namespace cpu
-} // namespace torch_ipex
-
-namespace {
-
-TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
-  m.def(
-      "rotary_position_embedding(Tensor (a!)t_in, Tensor (a!)t_emb_pos, Tensor (a!)t_pos, int N, int H, int offset, int rotary_ndims)-> ()");
-  m.impl(
-      "rotary_position_embedding",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::rotary_position_embedding_forward_cpu);
-}
-}
-
diff --git a/csrc/cpu/aten/RotaryPositionEmbedding.h b/csrc/cpu/aten/RotaryPositionEmbedding.h
deleted file mode 100644
index 3591a0f9..00000000
--- a/csrc/cpu/aten/RotaryPositionEmbedding.h
+++ /dev/null
@@ -1,33 +0,0 @@
-#pragma once
-
-#include <ATen/ATen.h>
-#include <dyndisp/DispatchStub.h>
-
-namespace torch_ipex {
-namespace cpu {
-
-namespace {
-
-void rotary_position_embedding_kernel_impl(
-    at::Tensor& t_in,
-    at::Tensor& t_emb_pos,
-    at::Tensor& t_pos,
-    int64_t N,//N: number of head, H: head size
-    int64_t H,
-    int64_t offset,
-    int64_t rotary_ndims);
-}
-
-using rotary_position_embedding_kernel_fn = void (*)(
-    at::Tensor& t_in,
-    at::Tensor& t_emb_pos,
-    at::Tensor& t_pos,
-    int64_t N,//N: number of head, H: head size
-    int64_t H,
-    int64_t offset,
-    int64_t rotary_ndims);
-
-DECLARE_DISPATCH(rotary_position_embedding_kernel_fn, rotary_position_embedding_kernel_stub);
-
-} // namespace cpu
-} // namespace torch_ipex
diff --git a/csrc/cpu/aten/TPPGEMM.cpp b/csrc/cpu/aten/TPPGEMM.cpp
deleted file mode 100644
index 64569675..00000000
--- a/csrc/cpu/aten/TPPGEMM.cpp
+++ /dev/null
@@ -1,153 +0,0 @@
-#include "TPPGEMM.h"
-#include <torch/all.h>
-#include <torch/csrc/autograd/function.h>
-#include "tpp/xsmm_functors.h"
-namespace torch_ipex {
-namespace cpu {
-
-DEFINE_DISPATCH(tpp_linear_nobias_kernel_stub);
-DEFINE_DISPATCH(tpp_linear_bias_kernel_stub);
-DEFINE_DISPATCH(tpp_linear_gelu_kernel_stub);
-DEFINE_DISPATCH(tpp_linear_silu_kernel_stub);
-DEFINE_DISPATCH(tpp_linear_relu_kernel_stub);
-DEFINE_DISPATCH(tpp_linear_add_kernel_stub);
-DEFINE_DISPATCH(tpp_linear_mul_kernel_stub);
-DEFINE_DISPATCH(tpp_linear_add_add_kernel_stub);
-
-at::Tensor tpp_linear_nobias_forward_cpu(at::Tensor& t_in, at::Tensor& t_wt) {
-  return tpp_linear_nobias_kernel_stub(kCPU, t_in, t_wt);
-}
-
-at::Tensor tpp_linear_bias_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias) {
-  return tpp_linear_bias_kernel_stub(kCPU, t_in, t_wt, t_bias);
-}
-
-at::Tensor tpp_linear_gelu_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias) {
-  return tpp_linear_gelu_kernel_stub(kCPU, t_in, t_wt, t_bias);
-}
-
-at::Tensor tpp_linear_silu_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias) {
-  return tpp_linear_silu_kernel_stub(kCPU, t_in, t_wt, t_bias);
-}
-
-at::Tensor tpp_linear_relu_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias) {
-  return tpp_linear_relu_kernel_stub(kCPU, t_in, t_wt, t_bias);
-}
-
-at::Tensor tpp_linear_add_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_in1,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias,
-    double scale) {
-  return tpp_linear_add_kernel_stub(kCPU, t_in, t_in1, t_wt, t_bias, scale);
-}
-
-at::Tensor tpp_linear_mul_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_in1,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias) {
-  return tpp_linear_mul_kernel_stub(kCPU, t_in, t_in1, t_wt, t_bias);
-}
-
-at::Tensor tpp_linear_add_add_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_in1,
-    at::Tensor& t_in2,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias,
-    double scale) {
-  return tpp_linear_add_add_kernel_stub(
-      kCPU, t_in, t_in1, t_in2, t_wt, t_bias, scale);
-}
-
-} // namespace cpu
-} // namespace torch_ipex
-
-namespace {
-
-TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
-  m.def("tpp_linear(Tensor (a!)t_in, Tensor (a!)t_wt)-> Tensor out");
-  m.impl(
-      "tpp_linear",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::tpp_linear_nobias_forward_cpu);
-}
-
-TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
-  m.def(
-      "tpp_linear_bias(Tensor (a!)t_in, Tensor (a!)t_wt, Tensor (a!)t_bias)-> Tensor out");
-  m.impl(
-      "tpp_linear_bias",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::tpp_linear_bias_forward_cpu);
-}
-
-TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
-  m.def(
-      "tpp_linear_gelu(Tensor (a!)t_in, Tensor (a!)t_wt, Tensor (a!)t_bias)-> Tensor out");
-  m.impl(
-      "tpp_linear_gelu",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::tpp_linear_gelu_forward_cpu);
-}
-
-TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
-  m.def(
-      "tpp_linear_add_add(Tensor (a!)t_in, Tensor (a!)t_in1, Tensor (a!)t_in2, Tensor (a!)t_wt, Tensor (a!)t_bias, float scale )-> Tensor out");
-  m.impl(
-      "tpp_linear_add_add",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::tpp_linear_add_add_forward_cpu);
-}
-
-TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
-  m.def(
-      "tpp_linear_relu(Tensor (a!)t_in, Tensor (a!)t_wt, Tensor (a!)t_bias)-> Tensor out");
-  m.impl(
-      "tpp_linear_relu",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::tpp_linear_relu_forward_cpu);
-}
-
-TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
-  m.def(
-      "tpp_linear_silu(Tensor (a!)t_in, Tensor (a!)t_wt, Tensor (a!)t_bias)-> Tensor out");
-  m.impl(
-      "tpp_linear_silu",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::tpp_linear_silu_forward_cpu);
-}
-
-TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
-  m.def(
-      "tpp_linear_add(Tensor (a!)t_in, Tensor (a!)t_in1, Tensor (a!)t_wt, Tensor (a!)t_bias, float scale )-> Tensor out");
-  m.impl(
-      "tpp_linear_add",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::tpp_linear_add_forward_cpu);
-}
-
-TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
-  m.def(
-      "tpp_linear_mul(Tensor (a!)t_in, Tensor (a!)t_in1, Tensor (a!)t_wt, Tensor (a!)t_bias )-> Tensor out");
-  m.impl(
-      "tpp_linear_mul",
-      c10::DispatchKey::CPU,
-      torch_ipex::cpu::tpp_linear_mul_forward_cpu);
-}
-
-} // namespace
\ No newline at end of file
diff --git a/csrc/cpu/aten/TPPGEMM.h b/csrc/cpu/aten/TPPGEMM.h
deleted file mode 100644
index 311549b2..00000000
--- a/csrc/cpu/aten/TPPGEMM.h
+++ /dev/null
@@ -1,95 +0,0 @@
-#pragma once
-
-#include <ATen/ATen.h>
-#include <dyndisp/DispatchStub.h>
-
-namespace torch_ipex {
-namespace cpu {
-
-namespace {
-at::Tensor tpp_linear_nobias_forward_cpu(at::Tensor& t_in, at::Tensor& t_wt);
-
-at::Tensor tpp_linear_bias_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias);
-
-at::Tensor tpp_linear_gelu_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias);
-
-at::Tensor tpp_linear_silu_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias);
-
-at::Tensor tpp_linear_relu_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias);
-
-at::Tensor tpp_linear_add_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_in1,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias,
-    double scale);
-
-at::Tensor tpp_linear_mul_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_in1,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias);
-
-at::Tensor tpp_linear_add_add_forward_cpu(
-    at::Tensor& t_in,
-    at::Tensor& t_in1,
-    at::Tensor& t_in2,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias,
-    double scale);
-
-} // namespace
-
-using tpp_linear_nobias_impl_fn = at::Tensor (*)(at::Tensor&, at::Tensor&);
-
-using tpp_linear_bias_kernel_impl_fn =
-    at::Tensor (*)(at::Tensor&, at::Tensor&, at::Tensor&);
-
-using tpp_linear_gelu_kernel_impl_fn =
-    at::Tensor (*)(at::Tensor&, at::Tensor&, at::Tensor&);
-
-using tpp_linear_silu_kernel_impl_fn =
-    at::Tensor (*)(at::Tensor&, at::Tensor&, at::Tensor&);
-
-using tpp_linear_relu_kernel_impl_fn =
-    at::Tensor (*)(at::Tensor&, at::Tensor&, at::Tensor&);
-
-using tpp_linear_add_kernel_impl_fn =
-    at::Tensor (*)(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, double);
-
-using tpp_linear_mul_kernel_impl_fn =
-    at::Tensor (*)(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&);
-
-using tpp_linear_add_add_kernel_impl_fn = at::Tensor (*)(
-    at::Tensor&,
-    at::Tensor&,
-    at::Tensor&,
-    at::Tensor&,
-    at::Tensor&,
-    double);
-
-DECLARE_DISPATCH(tpp_linear_nobias_impl_fn, tpp_linear_nobias_kernel_stub);
-DECLARE_DISPATCH(tpp_linear_bias_kernel_impl_fn, tpp_linear_bias_kernel_stub);
-DECLARE_DISPATCH(tpp_linear_gelu_kernel_impl_fn, tpp_linear_gelu_kernel_stub);
-DECLARE_DISPATCH(tpp_linear_silu_kernel_impl_fn, tpp_linear_silu_kernel_stub);
-DECLARE_DISPATCH(tpp_linear_relu_kernel_impl_fn, tpp_linear_relu_kernel_stub);
-DECLARE_DISPATCH(tpp_linear_add_kernel_impl_fn, tpp_linear_add_kernel_stub);
-DECLARE_DISPATCH(tpp_linear_mul_kernel_impl_fn, tpp_linear_mul_kernel_stub);
-DECLARE_DISPATCH(
-    tpp_linear_add_add_kernel_impl_fn,
-    tpp_linear_add_add_kernel_stub);
-
-} // namespace cpu
-} // namespace torch_ipex
\ No newline at end of file
diff --git a/csrc/cpu/aten/TensorShape.cpp b/csrc/cpu/aten/TensorShape.cpp
index 0cef2d8a..970bbad7 100644
--- a/csrc/cpu/aten/TensorShape.cpp
+++ b/csrc/cpu/aten/TensorShape.cpp
@@ -224,7 +224,7 @@ at::Tensor& cat_out_cpu(
   ScalarType dtype = materialized[valid].get().scalar_type();
   bool serial_dtype =
       (dtype == ScalarType::Double || dtype == ScalarType::Float ||
-       dtype == ScalarType::BFloat16);
+       dtype == ScalarType::BFloat16 || dtype == ScalarType::Half);
   if (all_contiguous && all_same_dtype && serial_dtype) {
     cat_contig_stub(kCPU, result, materialized, dim, all_same_sizes_and_stride);
     return result;
diff --git a/csrc/cpu/aten/kernels/AveragePoolKrnl.cpp b/csrc/cpu/aten/kernels/AveragePoolKrnl.cpp
index 5a5795a2..192b9dbd 100644
--- a/csrc/cpu/aten/kernels/AveragePoolKrnl.cpp
+++ b/csrc/cpu/aten/kernels/AveragePoolKrnl.cpp
@@ -2,7 +2,9 @@
 
 #include <ATen/Dispatch.h>
 #include <ATen/NativeFunctions.h>
+#include <ATen/OpMathType.h>
 #include <ATen/Parallel.h>
+#include <ATen/cpu/vec/functional.h>
 #include <ATen/cpu/vec/vec.h>
 #include <ATen/native/Pool.h>
 #include <ATen/native/cpu/utils.h>
@@ -128,7 +130,10 @@ void cpu_avg_pool(
 }
 
 template <typename scalar_t, bool is_3d>
-void cpu_avg_pool_channels_last(
+typename std::enable_if<
+    std::is_same<scalar_t, at::opmath_type<scalar_t>>::value,
+    void>::type
+cpu_avg_pool_channels_last(
     const at::Tensor& output_,
     const at::Tensor& input_,
     int64_t kW,
@@ -290,8 +295,11 @@ void cpu_avg_pool_channels_last(
   }
 }
 
-template <>
-void cpu_avg_pool_channels_last<at::BFloat16, false>(
+template <typename scalar_t, bool is_3d>
+typename std::enable_if<
+    !std::is_same<scalar_t, at::opmath_type<scalar_t>>::value,
+    void>::type
+cpu_avg_pool_channels_last(
     const at::Tensor& output_,
     const at::Tensor& input_,
     int64_t kW,
@@ -312,8 +320,8 @@ void cpu_avg_pool_channels_last<at::BFloat16, false>(
   auto input = input_.contiguous(memory_format);
   auto output = output_.contiguous(memory_format);
 
-  auto input_data = input.data_ptr<at::BFloat16>();
-  auto output_data = output.data_ptr<at::BFloat16>();
+  auto input_data = input.data_ptr<scalar_t>();
+  auto output_data = output.data_ptr<scalar_t>();
 
   int64_t nbatch = input.size(0);
   int64_t channels = input.size(1);
@@ -322,7 +330,7 @@ void cpu_avg_pool_channels_last<at::BFloat16, false>(
   int64_t output_height = output.size(2);
   int64_t output_width = output.size(3);
 
-  using bVec = at::vec::Vectorized<at::BFloat16>;
+  using bVec = at::vec::Vectorized<scalar_t>;
   using fVec = at::vec::Vectorized<float>;
   // parallel on dim N, H, W
   at::parallel_for(
@@ -337,7 +345,7 @@ void cpu_avg_pool_channels_last<at::BFloat16, false>(
             begin, n, nbatch, oh, output_height, ow, output_width);
 
         // temp buffer for sum, use float as accumulation type
-        // can't reuse output buffer to store sum since it is BFloat16
+        // can't reuse output buffer to store sum since it is BFloat16/Half
         std::unique_ptr<float[]> sum_arr(new float[channels]);
         float* sum = sum_arr.get();
 
@@ -365,7 +373,7 @@ void cpu_avg_pool_channels_last<at::BFloat16, false>(
             }
           }
 
-          at::BFloat16* out = output_data + i * channels;
+          scalar_t* out = output_data + i * channels;
 
           // Pass I: zero the out lane
           int64_t d1 = 0;
@@ -393,7 +401,7 @@ void cpu_avg_pool_channels_last<at::BFloat16, false>(
           // Pass II: compute local sum
           for (const auto ih : c10::irange(ih0, ih1)) {
             for (const auto iw : c10::irange(iw0, iw1)) {
-              at::BFloat16* in = input_data +
+              scalar_t* in = input_data +
                   n * input_height * input_width * channels +
                   ih * input_width * channels + iw * channels;
 
@@ -402,7 +410,7 @@ void cpu_avg_pool_channels_last<at::BFloat16, false>(
                 bVec data_bvec = bVec::loadu(in + d2);
                 fVec data_fvec0, data_fvec1;
                 std::tie(data_fvec0, data_fvec1) =
-                    convert_bfloat16_float(data_bvec);
+                    at::vec::convert_to_float<scalar_t>(data_bvec);
 
                 fVec sum_fvec0 = fVec::loadu(sum + d2) + data_fvec0;
                 fVec sum_fvec1 =
@@ -423,11 +431,12 @@ void cpu_avg_pool_channels_last<at::BFloat16, false>(
             fVec out_fvec1 = fVec::loadu(sum + d3 + fVec::size()) /
                 fVec(float(divide_factor));
 
-            bVec out_bvec = convert_float_bfloat16(out_fvec0, out_fvec1);
+            bVec out_bvec =
+                at::vec::convert_from_float<scalar_t>(out_fvec0, out_fvec1);
             out_bvec.store(out + d3);
           }
           for (; d3 < size; d3++) {
-            out[d3] = at::BFloat16(sum[d3] / divide_factor);
+            out[d3] = scalar_t(sum[d3] / divide_factor);
           }
 
           // move on to next output index
@@ -666,15 +675,16 @@ void avg_pool2d_kernel_impl(
     c10::optional<int64_t> divisor_override) {
   switch (input.suggest_memory_format()) {
     case at::MemoryFormat::Contiguous: {
-      AT_DISPATCH_FLOATING_TYPES_AND2(
+      AT_DISPATCH_FLOATING_TYPES_AND3(
           at::ScalarType::Long,
           at::ScalarType::BFloat16,
+          at::ScalarType::Half,
           input.scalar_type(),
           "avg_pool2d",
           [&] {
-            if (input.scalar_type() == at::ScalarType::BFloat16) {
+            if (at::isReducedFloatingType(input.scalar_type())) {
               cpu_avg_pool<
-                  at::BFloat16,
+                  scalar_t,
                   /*accscalar_t*/ float,
                   /* is_3d */ false>(
                   output,
@@ -746,9 +756,10 @@ void avg_pool2d_kernel_impl(
       break;
     }
     case at::MemoryFormat::ChannelsLast: {
-      AT_DISPATCH_FLOATING_TYPES_AND2(
+      AT_DISPATCH_FLOATING_TYPES_AND3(
           at::ScalarType::Long,
           at::ScalarType::BFloat16,
+          at::ScalarType::Half,
           input.scalar_type(),
           "avg_pool2d_channels_last",
           [&] {
@@ -789,8 +800,9 @@ void avg_pool2d_backward_kernel_impl(
     c10::optional<int64_t> divisor_override) {
   switch (grad_output.suggest_memory_format()) {
     case at::MemoryFormat::Contiguous: {
-      AT_DISPATCH_FLOATING_TYPES_AND(
+      AT_DISPATCH_FLOATING_TYPES_AND2(
           at::ScalarType::BFloat16,
+          at::ScalarType::Half,
           grad_output.scalar_type(),
           "avg_pool2d_backward",
           [&] {
@@ -812,8 +824,9 @@ void avg_pool2d_backward_kernel_impl(
       break;
     }
     case at::MemoryFormat::ChannelsLast: {
-      AT_DISPATCH_FLOATING_TYPES_AND(
+      AT_DISPATCH_FLOATING_TYPES_AND2(
           at::ScalarType::BFloat16,
+          at::ScalarType::Half,
           grad_output.scalar_type(),
           "avg_pool2d_backward_channels_last",
           [&] {
diff --git a/csrc/cpu/aten/kernels/CatKrnl.cpp b/csrc/cpu/aten/kernels/CatKrnl.cpp
index ab33caa8..3dca663e 100644
--- a/csrc/cpu/aten/kernels/CatKrnl.cpp
+++ b/csrc/cpu/aten/kernels/CatKrnl.cpp
@@ -393,8 +393,12 @@ void cat_contig_kernel(
     const at::MaterializedITensorListRef& tensors,
     int64_t dim,
     bool all_same_sizes_and_stride) {
-  AT_DISPATCH_FLOATING_TYPES_AND(
-      ScalarType::BFloat16, result.scalar_type(), "cat_contig_kernel", [&]() {
+  AT_DISPATCH_FLOATING_TYPES_AND2(
+      ScalarType::BFloat16,
+      ScalarType::Half,
+      result.scalar_type(),
+      "cat_contig_kernel",
+      [&]() {
         cpu_cat_contig_dispatch<scalar_t>(
             result, tensors, dim, all_same_sizes_and_stride);
       });
diff --git a/csrc/cpu/aten/kernels/EmbeddingBagKrnl.cpp b/csrc/cpu/aten/kernels/EmbeddingBagKrnl.cpp
index b7652288..3644c7f7 100644
--- a/csrc/cpu/aten/kernels/EmbeddingBagKrnl.cpp
+++ b/csrc/cpu/aten/kernels/EmbeddingBagKrnl.cpp
@@ -114,26 +114,6 @@ static inline at::Tensor expand_values_if_needed(const at::Tensor& values) {
   return values;
 }
 
-static inline at::Tensor _sparse_coo_tensor_unsafe(
-    const at::Tensor& indices,
-    const at::Tensor& values_,
-    c10::ArrayRef<SymInt> size,
-    const at::TensorOptions& options) {
-  at::Tensor values = expand_values_if_needed(values_);
-  assert(options.has_layout() && options.layout() == c10::kSparse);
-  int64_t sparse_dim = indices.size(0);
-  int64_t dense_dim = values.dim() - 1;
-  return at::native::new_with_dims_and_tensor_sparse_symint(
-      sparse_dim,
-      dense_dim,
-      size,
-      indices,
-      values,
-      values.scalar_type(),
-      c10::kSparse,
-      values.device());
-}
-
 template <typename T>
 static inline at::Tensor embedding_bag_sparse_backward_sum_fast(
     const at::Tensor grad,
@@ -169,17 +149,19 @@ static inline at::Tensor embedding_bag_sparse_backward_sum_fast(
   auto dense_options = index_grad.options();
 
   if (index_grad.numel() == 0) {
-    return _sparse_coo_tensor_unsafe(
+    return at::_sparse_coo_tensor_unsafe_symint(
         at::empty({1, 0}, indices.options()),
-        at::empty({0, num_features}, dense_options),
-        weight_size,
-        {});
+        at::empty_symint(
+            {c10::SymInt(0), std::move(num_features)}, dense_options),
+        weight_size);
   }
 
   auto index = indices.reshape({1, -1});
-  auto values = index_grad.reshape({-1, num_features});
+  auto values =
+      index_grad.reshape_symint({c10::SymInt(-1), std::move(num_features)});
 
-  return _sparse_coo_tensor_unsafe(index, values, weight_size, {});
+  return at::_sparse_coo_tensor_unsafe_symint(
+      index, values, weight_size, values.scalar_type());
 }
 
 static inline int64_t count_and_map_uniq(
@@ -311,9 +293,11 @@ at::Tensor embedding_bag_int8_kernel_impl(
     const at::Tensor& qweight,
     const at::Tensor& indices,
     const at::Tensor& offsets,
+    double output_scale,
     bool include_last_offset) {
   int64_t ddim = qweight.size(1);
-  double scale = at::native::q_scale_quant(qweight);
+  double weight_scale = at::native::q_scale_quant(qweight);
+  double inv_o_scale = 1.0 / output_scale;
   int8_t* qweight_data =
       reinterpret_cast<int8_t*>(qweight.data_ptr<at::qint8>());
   int64_t output_size = offsets.numel();
@@ -327,28 +311,49 @@ at::Tensor embedding_bag_int8_kernel_impl(
 
   // init output tensor
   at::QuantizerPtr output_quantizer =
-      at::make_per_tensor_affine_quantizer(scale, /*zp=*/0, at::kQInt8);
+      at::make_per_tensor_affine_quantizer(output_scale, /*zp=*/0, at::kQInt8);
   at::Tensor output = at::new_qtensor(
       /*sizes=*/{output_size, qweight.size(1)},
       qweight.options(),
       output_quantizer);
   int8_t* output_data = reinterpret_cast<int8_t*>(output.data_ptr<at::qint8>());
-
+  bool need_requantize = (output_scale - weight_scale) > 0.0001;
   at::parallel_for(0, output_size, 16, [&](int64_t start, int64_t end) {
+    float fp32_buffer[ddim] __attribute__((aligned(64)));
     for (int64_t i = start; i < end; i++) {
       int8_t* out_data_ptr = &output_data[i * ddim];
       auto inputs_start = offsets_data[i];
       auto inputs_end = i == last_offset ? last_index : offsets_data[i + 1];
-      if (inputs_start >= inputs_end) {
-        zero_ker(out_data_ptr, ddim);
-      } else {
+      if (inputs_end - inputs_start <= 1 and !need_requantize) {
+        // Do not re-quantize when bag-size == 1 for performance consideraion
+        // It is proved to be have enough accuracy on DLRM-V1
+        // We can revise this if other models with embeddingbag are not accurate
+        // enough
         int8_t* select_data_ptr =
             &qweight_data[indices_accessor[inputs_start] * ddim];
         move_ker(out_data_ptr, select_data_ptr, ddim);
-      }
-      for (int64_t s = (inputs_start + 1); s < inputs_end; s++) {
-        int8_t* select_data_ptr = &qweight_data[indices_accessor[s] * ddim];
-        add_ker(out_data_ptr, select_data_ptr, ddim);
+      } else {
+        zero_ker(&fp32_buffer[0], ddim);
+        for (int64_t s = inputs_start; s < inputs_end; s++) {
+          int8_t* select_data_ptr = &qweight_data[indices_accessor[s] * ddim];
+          scale_fp32_and_fma(
+              &fp32_buffer[0], select_data_ptr, weight_scale, ddim);
+        }
+#ifdef CPU_CAPABILITY_AVX2
+        at::vec::QuantizeAvx2<c10::qint8::underlying>(
+            &fp32_buffer[0],
+            out_data_ptr,
+            ddim,
+            inv_o_scale,
+            /*zp=*/0);
+#else
+        at::vec::QuantizeAvx512<c10::qint8::underlying>(
+            &fp32_buffer[0],
+            out_data_ptr,
+            ddim,
+            inv_o_scale,
+            /*zp=*/0);
+#endif
       }
     }
   });
diff --git a/csrc/cpu/aten/kernels/FlashAttentionKrnl.cpp b/csrc/cpu/aten/kernels/FlashAttentionKrnl.cpp
deleted file mode 100644
index 3f736362..00000000
--- a/csrc/cpu/aten/kernels/FlashAttentionKrnl.cpp
+++ /dev/null
@@ -1,239 +0,0 @@
-#include <ATen/Tensor.h>
-#include <aten/FlashAttention.h>
-#include <torch/all.h>
-#include <torch/csrc/autograd/function.h>
-#include <limits>
-#include "mkl.h"
-#include "vec/vec.h"
-
-namespace torch_ipex {
-namespace cpu {
-
-namespace {
-
-const int64_t qsplit_size = 384;
-const int64_t kvsplit_size = 512;
-
-#if defined(CPU_CAPABILITY_AVX512)
-using namespace torch_ipex::cpu::kernel;
-
-template <typename scalar_t>
-void _mha_mul_softmax_bf16_kernel(
-    float* a,
-    scalar_t* b,
-    float* dst,
-    float* max,
-    float* sum,
-    const int& qsize,
-    const int& kvsize,
-    const int& headsize,
-    const int& idx) {
-  float tmp_max = 0.f, tmp_sum = 0.f, sum_old = 0.f, exp_tmp = 0.f;
-
-  for (int i = 0; i < qsize; ++i) {
-    sum_old = sum[i];
-
-    _dil_reduce_max_fusion_kernel(
-        a + i * kvsize, kvsize, a + i * kvsize, tmp_max);
-    tmp_max = max[i] > tmp_max ? max[i] : tmp_max;
-
-    tmp_sum = tmp_max;
-    _dil_exp_reduce_sum_fusion_kernel(
-        a + i * kvsize, kvsize, a + i * kvsize, tmp_sum);
-    exp_tmp = exp(max[i] - tmp_max);
-    sum[i] = tmp_sum + exp_tmp * sum[i];
-    max[i] = tmp_max;
-
-    _dil_normalization_kernel<scalar_t>(
-        a + i * kvsize, sum[i], kvsize, b + i * kvsize);
-
-    if (idx) {
-      _mha_update_sum_max_kernel(
-          dst + i * headsize,
-          sum_old,
-          sum[i],
-          exp_tmp,
-          headsize,
-          dst + i * headsize);
-    }
-  }
-}
-
-at::Tensor flash_base_kernel(
-    at::BFloat16* query,
-    at::BFloat16* key,
-    at::BFloat16* value,
-    at::BFloat16* attn_mask,
-    const int64_t& qStride,
-    const int64_t& kStride,
-    const int64_t& vStride,
-    const int64_t& batchSize,
-    const int64_t& qSize,
-    const int64_t& kvSize,
-    const int64_t& num_head,
-    const int64_t& headSize,
-    const int64_t& hiddenSize,
-    const double& scale) {
-  at::Tensor output = at::empty({batchSize, qSize, hiddenSize}, at::kBFloat16);
-
-  int64_t qSplitSize = qSize >= qsplit_size ? qsplit_size : qSize;
-  int64_t kvSplitSize = kvSize >= kvsplit_size ? kvsplit_size : kvSize;
-
-  int64_t qSlice = (qSize - 1) / qSplitSize + 1;
-  int64_t qTail = (qSize - 1) % qSplitSize + 1;
-  int64_t kvSlice = (kvSize - 1) / kvSplitSize + 1;
-  int64_t kvTail = (kvSize - 1) % kvSplitSize + 1;
-
-  int64_t num_thread = omp_get_max_threads();
-
-  at::Tensor qk_fp32 =
-      at::empty({num_thread, qSplitSize, kvSplitSize}, at::kFloat);
-  at::Tensor qk_bf16 =
-      at::empty({num_thread, qSplitSize, kvSplitSize}, at::kBFloat16);
-  at::Tensor qk_max = at::empty({num_thread, qSplitSize}, at::kFloat);
-  at::Tensor qk_sum = at::empty({num_thread, qSplitSize}, at::kFloat);
-  at::Tensor dst_fp32 =
-      at::empty({num_thread, qSplitSize, headSize}, at::kFloat);
-
-#pragma omp parallel for collapse(3)
-  for (int i = 0; i < batchSize; ++i) {
-    for (int j = 0; j < num_head; ++j) {
-      for (int k = 0; k < qSlice; ++k) {
-        int qBlockSize = (k == qSlice - 1) ? qTail : qSplitSize;
-        int ompIdx = omp_get_thread_num();
-        _init_mha_buffer_kernel(
-            qk_max.data_ptr<float>() + ompIdx * qSplitSize,
-            qk_sum.data_ptr<float>() + ompIdx * qSplitSize,
-            qBlockSize);
-
-        for (int l = 0; l < kvSlice; ++l) {
-          int kvBlockSize = (l == kvSlice - 1) ? kvTail : kvSplitSize;
-          cblas_gemm_bf16bf16f32(
-              CblasRowMajor,
-              CblasNoTrans,
-              CblasTrans,
-              qBlockSize,
-              kvBlockSize,
-              headSize,
-              float(1.f / scale),
-              (const MKL_BF16*)(query + i * qSize * qStride + headSize * j + k * qSplitSize * qStride),
-              qStride,
-              (const MKL_BF16*)(key + i * kvSize * kStride + headSize * j + l * kvSplitSize * kStride),
-              kStride,
-              0.f,
-              qk_fp32.data_ptr<float>() + ompIdx * qSplitSize * kvSplitSize,
-              kvBlockSize);
-
-          // update attention weights with attention mask
-          for (int r = 0; r < qBlockSize; r++) {
-            _dil_add_kernel<at::BFloat16>(
-              attn_mask + i * qSize * kvSize + (k * qSplitSize + r) * kvSize + l * kvSplitSize,
-              qk_fp32.data_ptr<float>() + ompIdx * qSplitSize * kvSplitSize + r * kvBlockSize,
-              kvBlockSize);
-          }
-
-          _mha_mul_softmax_bf16_kernel<at::BFloat16>(
-              qk_fp32.data_ptr<float>() + ompIdx * qSplitSize * kvSplitSize,
-              qk_bf16.data_ptr<at::BFloat16>() +
-                  ompIdx * qSplitSize * kvSplitSize,
-              dst_fp32.data_ptr<float>() + ompIdx * qSplitSize * headSize,
-              qk_max.data_ptr<float>() + ompIdx * qSplitSize,
-              qk_sum.data_ptr<float>() + ompIdx * qSplitSize,
-              qBlockSize,
-              kvBlockSize,
-              headSize,
-              l);
-
-          cblas_gemm_bf16bf16f32(
-              CblasRowMajor,
-              CblasNoTrans,
-              CblasNoTrans,
-              qBlockSize,
-              headSize,
-              kvBlockSize,
-              1.f,
-              (const MKL_BF16*)(qk_bf16.data_ptr<at::BFloat16>() + ompIdx * qSplitSize * kvSplitSize),
-              kvBlockSize,
-              (const MKL_BF16*)(value + i * kvSize * vStride + headSize * j + l * kvSplitSize * vStride),
-              vStride,
-              l == 0 ? 0.f : 1.f,
-              dst_fp32.data_ptr<float>() + ompIdx * qSplitSize * headSize,
-              headSize);
-        }
-        _reorder_mha_output_kernel<at::BFloat16>(
-            dst_fp32.data_ptr<float>() + ompIdx * qSplitSize * headSize,
-            output.data_ptr<at::BFloat16>() + i * qSize * hiddenSize +
-                headSize * j + k * qSplitSize * hiddenSize,
-            qBlockSize,
-            headSize,
-            hiddenSize);
-      }
-    }
-  }
-  return output;
-}
-#endif
-
-at::Tensor flash_attention_kernel_impl(
-    at::Tensor query,
-    at::Tensor key,
-    at::Tensor value,
-    const double scale_attn,
-    at::Tensor attention_mask) {
-    if (query.scalar_type() != at::kBFloat16
-             || query.dtype() != key.dtype()
-             || query.dtype() != attention_mask.dtype()) {
-        TORCH_CHECK(false, "Q/K/V/AttnMask must be BF16 to use ipex::flash_attention_kernel_impl");
-    }
-    if(query.dim() != 4 || key.dim() != 4 || value.dim() != 4){
-        TORCH_CHECK(false, "Q/K/V must be 4D for ipex::flash_attention_kernel_impl");
-    }
-    TORCH_CHECK(attention_mask.size(1) == 1, "Attetntion mask size(1) != 1 for ipex::flash_attention_kernel_imp");
-
-#if defined(CPU_CAPABILITY_AVX512)
-    int64_t batchSize = query.size(0);
-    int64_t qSize = query.size(1);
-    int64_t kvSize = value.size(1);
-    int64_t num_head = query.size(2);
-    int64_t headSize = query.size(3);
-    int64_t hiddenSize = num_head * headSize;
-
-    int64_t qStride = query.stride(1);
-    int64_t kStride = key.stride(1);
-    int64_t vStride = value.stride(1);
-    auto attn_outputs = flash_base_kernel(
-      query.data_ptr<at::BFloat16>(),
-      key.data_ptr<at::BFloat16>(),
-      value.data_ptr<at::BFloat16>(),
-      attention_mask.data_ptr<at::BFloat16>(),
-      qStride,
-      kStride,
-      vStride,
-      batchSize,
-      qSize,
-      kvSize,
-      num_head,
-      headSize,
-      hiddenSize,
-      scale_attn);
-    return attn_outputs.resize_(
-        {batchSize, qSize, num_head, headSize}).transpose_(1, 2);
-#else
-    key = key.permute({0, 2, 1, 3});
-    query = query.permute({0, 2, 1, 3});
-    value = value.permute({0, 2, 1, 3});
-    auto attn_weights = query.matmul(key.transpose(-1, -2));
-    attn_weights = attn_weights.div(scale_attn);
-    attn_weights = attn_weights + attention_mask;
-    attn_weights = attn_weights.softmax(-1);
-    attn_weights = attn_weights.to(value.dtype());
-    auto out = attn_weights.matmul(value);
-    return out;
-#endif
-}
-} // anonymous namespace
-
-REGISTER_DISPATCH(flash_attention_kernel_stub, &flash_attention_kernel_impl);
-
-} // namespace cpu
-} // namespace torch_ipex
diff --git a/csrc/cpu/aten/kernels/MLPerfConcatKrnl.cpp b/csrc/cpu/aten/kernels/MLPerfConcatKrnl.cpp
new file mode 100644
index 00000000..7c061bc1
--- /dev/null
+++ b/csrc/cpu/aten/kernels/MLPerfConcatKrnl.cpp
@@ -0,0 +1,109 @@
+#include <aten/MLPerfConcat.h>
+#include <torch/all.h>
+#include <torch/csrc/autograd/function.h>
+#include "vec/vec.h"
+
+namespace torch_ipex {
+namespace cpu {
+
+namespace {
+
+void concat_ker_64(const float *a, const float *b,
+                   float *c) {
+#if defined(CPU_CAPABILITY_AVX512)
+    __m512 a0, a1, a2, a3;
+    __m512 b0, b1, b2, b3;
+    __m512 c0, c1, c2, c3, c4, c5, c6, c7;
+    alignas(64) static const uint32_t INDEX0[16] = {
+        0, 16,  1, 17,  2, 18,  3, 19,
+        4, 20,  5, 21,  6, 22,  7, 23,
+    };
+    alignas(64) static const uint32_t INDEX1[16] = {
+        8, 24,  9, 25, 10, 26, 11, 27,
+        12, 28, 13, 29, 14, 30, 15, 31
+    };
+    __m512i i0 = _mm512_load_si512((__m512i *)INDEX0);
+    __m512i i1 = _mm512_load_si512((__m512i *)INDEX1);
+
+    a0 = _mm512_load_ps(a);
+    a1 = _mm512_load_ps(a + 16);
+    a2 = _mm512_load_ps(a + 32);
+    a3 = _mm512_load_ps(a + 48);
+
+    b0 = _mm512_load_ps(b);
+    b1 = _mm512_load_ps(b + 16);
+    b2 = _mm512_load_ps(b + 32);
+    b3 = _mm512_load_ps(b + 48);
+
+    c0 = _mm512_permutex2var_ps(a0, i0, b0);
+    c1 = _mm512_permutex2var_ps(a0, i1, b0);
+
+    c2 = _mm512_permutex2var_ps(a1, i0, b1);
+    c3 = _mm512_permutex2var_ps(a1, i1, b1);
+
+    c4 = _mm512_permutex2var_ps(a2, i0, b2);
+    c5 = _mm512_permutex2var_ps(a2, i1, b2);
+
+    c6 = _mm512_permutex2var_ps(a3, i0, b3);
+    c7 = _mm512_permutex2var_ps(a3, i1, b3);
+
+    _mm512_stream_ps(c, c0);
+    _mm512_stream_ps(c + 16, c1);
+    _mm512_stream_ps(c + 32, c2);
+    _mm512_stream_ps(c + 48, c3);
+    _mm512_stream_ps(c + 64, c4);
+    _mm512_stream_ps(c + 80, c5);
+    _mm512_stream_ps(c + 96, c6);
+    _mm512_stream_ps(c + 112, c7);
+#endif
+}
+
+void concat_ker(const int64_t N, const float *a, const float *b, float *c) {
+    int64_t j = 0;
+    for (int64_t i = 0; i < N; ++i) {
+        c[j] = a[i];
+        j++;
+        c[j] = b[i];
+        j++;
+    }
+}
+
+void concat_result(const int64_t N, const float *a, const float *b,
+                   float *c) {
+    constexpr int64_t mb = 64;
+    const int64_t n_b = (N - 1)/ 64 + 1;
+    #pragma omp parallel for
+    for (int64_t n = 0; n < n_b; ++n) {
+        int64_t s = n * mb;
+        int64_t cur_batch = std::min(s + mb, N) - s;
+        if (cur_batch == mb)
+            concat_ker_64(&a[s], &b[s], &c[2*s]);
+        else
+            concat_ker(cur_batch, &a[s], &b[s], &c[2*s]);
+    }
+}
+
+at::Tensor mlperf_concat_kernel_impl(
+    const at::Tensor& a,
+    const at::Tensor& b) {
+
+    int64_t L = a.size(0);
+    at::Tensor output = at::empty({L, 2}, a.options());
+
+    float* a_ptr = a.data_ptr<float>();
+    float* b_ptr = b.data_ptr<float>();
+    float* out_ptr = output.data_ptr<float>();
+
+    concat_result(L, a_ptr, b_ptr, out_ptr);
+
+    return output;
+}
+
+} // anonymous namespace
+
+REGISTER_DISPATCH(
+    mlperf_concat_kernel_stub,
+    &mlperf_concat_kernel_impl);
+
+} // namespace cpu
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/kernels/MaskedMultiHeadAttentionKrnl.cpp b/csrc/cpu/aten/kernels/MaskedMultiHeadAttentionKrnl.cpp
deleted file mode 100644
index 7defda0f..00000000
--- a/csrc/cpu/aten/kernels/MaskedMultiHeadAttentionKrnl.cpp
+++ /dev/null
@@ -1,800 +0,0 @@
-#include <ATen/Tensor.h>
-#include <aten/FlashAttention.h>
-#include <aten/MaskedMultiHeadAttention.h>
-#include <torch/all.h>
-#include <torch/csrc/autograd/function.h>
-#include <limits>
-#include "vec/vec.h"
-
-namespace torch_ipex {
-namespace cpu {
-
-namespace {
-
-template<typename T>
-void reduce_head(
-    const T* q_ptr_start,
-    const T* k_ptr_start,
-    float* attn_w_pos,
-    int64_t head_size,
-    bool store_key,
-    T* k_cache_start) {
-    for(auto hsi = 0; hsi < head_size; hsi++){
-        if(store_key){
-            k_cache_start[hsi]=k_ptr_start[hsi];//cat the key into the key_cache.
-        } 
-        attn_w_pos[0] += q_ptr_start[hsi]*k_ptr_start[hsi];
-    }  
-}
-
-template<>
-void reduce_head(
-    const float* q_ptr_start,
-    const float* k_ptr_start,
-    float* attn_w_pos,
-    int64_t head_size,
-    bool store_key,
-    float* k_cache_start) {
-    auto hsi = 0;   
-    #if defined(CPU_CAPABILITY_AVX512)
-    auto vec_size= 16; // 512/32
-    auto qk_sum_vec = _mm512_setzero_ps();
-    for(hsi=0; hsi <= head_size - vec_size; hsi+=vec_size){
-        auto q_vec = _mm512_loadu_ps(q_ptr_start + hsi);
-        auto k_vec = _mm512_loadu_ps(k_ptr_start + hsi);
-        if(store_key) {
-            _mm512_storeu_ps(k_cache_start + hsi, k_vec);
-        }
-        qk_sum_vec = _mm512_fmadd_ps(q_vec, k_vec, qk_sum_vec);        
-    }
-    attn_w_pos[0] += _mm512_reduce_add_ps(qk_sum_vec);
-    for(; hsi < head_size; hsi++){
-        k_cache_start[hsi]=k_ptr_start[hsi];//cat the key into the key_cache.
-        attn_w_pos[0] += q_ptr_start[hsi]*k_ptr_start[hsi]; 
-    }  
-    return;    
-    #endif
-    for(hsi=0; hsi < head_size; hsi++){
-        k_cache_start[hsi]=k_ptr_start[hsi];//cat the key into the key_cache.
-        attn_w_pos[0] += q_ptr_start[hsi]*k_ptr_start[hsi]; 
-    }
-}
-
-template<>
-void reduce_head(
-    const at::BFloat16* q_ptr_start,
-    const at::BFloat16* k_ptr_start,
-    float* attn_w_pos,
-    int64_t head_size,
-    bool store_key,
-    at::BFloat16* k_cache_start) {
-    auto hsi = 0;   
-    #if defined(CPU_CAPABILITY_AVX512)
-    auto vec_size= 16; // 512/32
-    auto qk_sum_vec = _mm512_setzero_ps();
-    for(hsi=0; hsi <= head_size - vec_size; hsi+=vec_size){
-        //load 16 bfloat16 query from q_ptr_start and convert to 16 float32 values
-        auto q_vec_bf16 = _mm256_loadu_si256((__m256i*)(q_ptr_start + hsi));
-        auto q_vec_fp32 = torch_ipex::cpu::kernel::convert_bf16_to_fp32(q_vec_bf16);        
-        //load 16 bfloat16 key from k_ptr_start and convert to 16 float32 values
-        auto k_vec_bf16 = _mm256_loadu_si256((__m256i*)(k_ptr_start + hsi));
-        auto k_vec_fp32 = torch_ipex::cpu::kernel::convert_bf16_to_fp32(k_vec_bf16);
-        if(store_key) {
-            _mm256_storeu_si256((__m256i*)(k_cache_start + hsi), k_vec_bf16);
-        }
-        qk_sum_vec = _mm512_fmadd_ps(q_vec_fp32, k_vec_fp32, qk_sum_vec);
-    }
-    attn_w_pos[0] += (at::BFloat16)_mm512_reduce_add_ps(qk_sum_vec);
-    for(; hsi < head_size; hsi++){
-        k_cache_start[hsi]=k_ptr_start[hsi];//cat the key into the key_cache.
-        attn_w_pos[0] += q_ptr_start[hsi]*k_ptr_start[hsi]; 
-    }
-    return;
-    #endif
-    for(hsi=0; hsi < head_size; hsi++){
-        k_cache_start[hsi]=k_ptr_start[hsi];//cat the key into the key_cache.
-        attn_w_pos[0] += q_ptr_start[hsi]*k_ptr_start[hsi]; 
-    }
-}
-
-
-/* 
-*reduce the attnetion_weights with the value embeeding by the dimension of head_size  for every head 
-*/
-template<typename T, typename T1>
-void mul_attenion_weights_and_value_of_head(
-    float& attn_w,
-    const T* v_ptr_start,
-    T1* attn_out_start,
-    int64_t head_size,
-    bool store_value,
-    T* v_cache_start) {
-    for(auto hsi = 0; hsi < head_size; hsi++){
-        attn_out_start[hsi] += attn_w * v_ptr_start[hsi];
-        if(store_value){
-            v_cache_start[hsi] = v_ptr_start[hsi];
-        }
-    }  
-}
-
-template<>
-void mul_attenion_weights_and_value_of_head(
-    float& attn_w,
-    const float* v_ptr_start,
-    float* attn_out_start,
-    int64_t head_size,
-    bool store_value,
-    float* v_cache_start) {
-    auto hsi = 0;
-    #if defined(CPU_CAPABILITY_AVX512)
-    auto vec_size= 16; // 512/32
-    for(hsi=0; hsi <= head_size - vec_size; hsi+=vec_size){
-        auto attn_w_vec = _mm512_set1_ps(attn_w);
-        auto v_vec = _mm512_loadu_ps(v_ptr_start + hsi);
-        auto attn_out_vec = _mm512_loadu_ps(attn_out_start + hsi);
-        auto attn_out_vec_new = _mm512_fmadd_ps(attn_w_vec, v_vec, attn_out_vec);
-        _mm512_storeu_ps(attn_out_start + hsi, attn_out_vec_new);
-        if(store_value){
-            _mm512_storeu_ps(v_cache_start + hsi, v_vec);
-        }
-    }
-    for(; hsi < head_size; hsi++){
-        attn_out_start[hsi] += attn_w * v_ptr_start[hsi];
-        if(store_value){
-            v_cache_start[hsi] = v_ptr_start[hsi];
-        }
-    }
-    return;
-    #endif
-    for(hsi=0; hsi < head_size; hsi++){
-        attn_out_start[hsi] += attn_w * v_ptr_start[hsi];
-        if(store_value){
-            v_cache_start[hsi] = v_ptr_start[hsi];
-        }
-    }
-}
-
-template<>
-void mul_attenion_weights_and_value_of_head(
-    float& attn_w,
-    const at::BFloat16* v_ptr_start,
-    at::BFloat16* attn_out_start,
-    int64_t head_size,
-    bool store_value,
-    at::BFloat16* v_cache_start) {
-    auto hsi = 0;
-    #if defined(CPU_CAPABILITY_AVX512)
-    auto vec_size= 16; // 512/32
-    for(hsi=0; hsi <= head_size - vec_size; hsi+=vec_size){
-        //get 1 bfloat16 values from attn_w_ptr_start and broadcast to 16 float32 values
-        auto attn_w_vec_fp32 = _mm512_set1_ps(attn_w);
-        //load 16 bfloat16 values from v_ptr_start and convert to 16 float32 values
-        auto v_vec_bf16 = _mm256_loadu_si256((__m256i*)(v_ptr_start + hsi));
-        auto v_vec_fp32 = torch_ipex::cpu::kernel::convert_bf16_to_fp32(v_vec_bf16);        
-        //load 16 bfloat16 values from attn_out_start and convert to 16 float32 values
-        auto attn_out_vec_fp32 = torch_ipex::cpu::kernel::convert_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(attn_out_start + hsi)));
-        //calculate the new attn_out_vec_fp32 and convert to bfloat16
-        auto attn_out_vec_new = _mm512_fmadd_ps(attn_w_vec_fp32, v_vec_fp32, attn_out_vec_fp32);
-        auto attn_out_vec_new_bf16 = cvt_fp32_to_bf16(attn_out_vec_new);//_m256i
-        //store the new attn_out_vec_new_bf16 to attn_outs
-        _mm256_storeu_si256((__m256i*)(attn_out_start + hsi), attn_out_vec_new_bf16);
-        //store the v_vec_bf16 to v_cache
-        if(store_value){
-            _mm256_storeu_si256((__m256i*)(v_cache_start + hsi), v_vec_bf16);
-        }
-    }
-    for(; hsi < head_size; hsi++){
-        attn_out_start[hsi] += attn_w * v_ptr_start[hsi];
-        if(store_value){
-            v_cache_start[hsi] = v_ptr_start[hsi];
-        }
-    }
-    return;
-    #endif
-    for(hsi=0; hsi < head_size; hsi++){
-        attn_out_start[hsi] += attn_w * v_ptr_start[hsi];
-        if(store_value){
-            v_cache_start[hsi] = v_ptr_start[hsi];
-        }
-    }
-
-}
-
-template<>
-void mul_attenion_weights_and_value_of_head(
-    float& attn_w,
-    const at::BFloat16* v_ptr_start,
-    float* attn_out_start,
-    int64_t head_size,
-    bool store_value,
-    at::BFloat16* v_cache_start) {
-    auto hsi = 0;
-    #if defined(CPU_CAPABILITY_AVX512)
-    auto vec_size= 16; // 512/32
-    for(hsi=0; hsi <= head_size - vec_size; hsi+=vec_size){
-        //get 1 bfloat16 values from attn_w_ptr_start and broadcast to 16 float32 values
-        auto attn_w_vec_fp32 = _mm512_set1_ps(attn_w);
-        //load 16 bfloat16 values from v_ptr_start and convert to 16 float32 values
-        auto v_vec_bf16 = _mm256_loadu_si256((__m256i*)(v_ptr_start + hsi));
-        auto v_vec_fp32 = torch_ipex::cpu::kernel::convert_bf16_to_fp32(v_vec_bf16);        
-        //load 16 bfloat16 values from attn_out_start and convert to 16 float32 values
-        //auto attn_out_vec_fp32 = torch_ipex::cpu::kernel::convert_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(attn_out_start + hsi)));
-        auto attn_out_vec_fp32 = _mm512_loadu_ps(attn_out_start + hsi);
-        //calculate the new attn_out_vec_fp32 and convert to bfloat16
-        auto attn_out_vec_new = _mm512_fmadd_ps(attn_w_vec_fp32, v_vec_fp32, attn_out_vec_fp32);
-        //auto attn_out_vec_new_bf16 = cvt_fp32_to_bf16(attn_out_vec_new);//_m256i
-        //store the new attn_out_vec_new_bf16 to attn_outs
-        //_mm256_storeu_si256((__m256i*)(attn_out_start + hsi), attn_out_vec_new_bf16);
-        _mm512_storeu_ps(attn_out_start + hsi, attn_out_vec_new);
-        //store the v_vec_bf16 to v_cache
-        if(store_value){
-            _mm256_storeu_si256((__m256i*)(v_cache_start + hsi), v_vec_bf16);
-        }
-    }
-    for(; hsi < head_size; hsi++){
-        attn_out_start[hsi] += attn_w * v_ptr_start[hsi];
-        if(store_value){
-            v_cache_start[hsi] = v_ptr_start[hsi];
-        }
-    }
-    return;
-    #endif
-    for(hsi=0; hsi < head_size; hsi++){
-        attn_out_start[hsi] += attn_w * v_ptr_start[hsi];
-        if(store_value){
-            v_cache_start[hsi] = v_ptr_start[hsi];
-        }
-    }
-
-}
-
-template <typename T>
-void copy_key_value(
-    at::Tensor key_cache,
-    const at::Tensor key,
-    at::Tensor value_cache,
-    const at::Tensor value,
-    int beam_batch) {
-  RECORD_FUNCTION("ipex::copy_key_value", c10::ArrayRef<c10::IValue>({}));
-  auto bs = key.size(0);
-  auto seq_len = key.size(1); // only process cur_len==1
-  auto head_num = key.size(2);
-  auto head_size = key.size(3);
-  auto hidden_size = head_num * head_size;
-  auto key_cache_ptr = key_cache.data_ptr<T>();
-  auto key_ptr = key.data_ptr<T>();
-  auto value_cache_ptr = value_cache.data_ptr<T>();
-  auto value_ptr = value.data_ptr<T>();
-  auto token_stride = beam_batch * hidden_size;
-  auto beam_size = beam_batch / bs;
-#pragma omp parallel for collapse(2)
-  for (auto si = 0; si < seq_len; si++) {
-    for (auto bi = 0; bi < bs; bi++) {
-      auto cache_stride = si * token_stride + bi * beam_size * hidden_size;
-      auto state_stride = (bi * seq_len + si) * hidden_size;
-      auto key_cache_start = key_cache_ptr + cache_stride;
-      auto key_ptr_start = key_ptr + state_stride;
-      torch_ipex::cpu::kernel::move_ker<T, T>(key_cache_start, key_ptr_start, hidden_size);
-      auto value_cache_ptr_start = value_cache_ptr + cache_stride;
-      auto value_ptr_start = value_ptr + state_stride;
-      torch_ipex::cpu::kernel::move_ker<T, T>(value_cache_ptr_start, value_ptr_start, hidden_size);
-    }
-  }
-}
-
-/* 
-*The scale-dot product for indirect access kv chache and fuse matmul+div+add+softmax to improve data reuse
-*@param  query Query embeeding with the of [beam_size*batch, cur_len, head_num, head_size]
-*@param  key Key embeeding with the of [beam_size*batch, cur_len, head_num, head_size]
-*@param  value Key embeeding with the of [beam_size*batch, cur_len, head_num, head_size]
-*@param  key_cache Cache past key embeeding with the of [max_len, beam_size*batch, head_num, head_size]
-*@param  value_chache Cache past value embeeding with the of [max_len, beam_size*batch, head_num, head_size]
-*@param  beam_idx Beam info for every token [max_len, beam_size*batch]
-*@param  offset  The length of decoded(past) token. 
-*@param  scale_factor the sqrt(head_dim).
-*@param  head_mask Which is not used by our kernel now. 
-*@param  attention_mask Which is combined mask for padding mask and casual mask. 
-*@return attn_outs, None, key_cache, value_cache, beam_idx
-*/
-template <typename QT, typename VT>
-std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>  scale_dot_product_for_indirect_access_kv_cache(
-    at::Tensor query,
-    at::Tensor key,
-    at::Tensor value,
-    at::Tensor& key_cache,
-    at::Tensor& value_cache,
-    at::Tensor&  beam_idx,    
-    const int64_t offset,
-    const double scale_factor,
-    at::Tensor& attention_mask) {
-  RECORD_FUNCTION(
-      "ipex::scale_dot_product_for_indirect_access_kv_cache",
-      c10::ArrayRef<c10::IValue>({}));
-  int beam_batch = beam_idx.size(1);
-  auto bs = query.size(0);
-  auto cur_len = query.size(1); // only process cur_len==1
-  auto head_num = query.size(2);
-  auto kv_head = key.size(2);
-  auto group_size = head_num / kv_head;
-  auto head_size = query.size(3);
-  auto seq_len = offset + cur_len;
-  auto kc_token_stride = beam_batch * kv_head * head_size;
-  auto attn_weights =
-      at::empty({bs, head_num, cur_len, seq_len}, at::kFloat);
-  query = query.contiguous();
-  key = key.contiguous();
-  auto q_ptr = query.data_ptr<QT>();
-  auto k_ptr = key.data_ptr<QT>();
-  auto k_cache_ptr = key_cache.data_ptr<QT>();
-  auto mask_ptr = attention_mask.data_ptr<QT>();
-  auto mask_head_num = attention_mask.size(1);
-  auto mask_dim2 = attention_mask.size(2);
-  auto mask_bs_stride = mask_head_num * mask_dim2 * seq_len;
-  // value realted
-  value = value.contiguous();
-  auto attn_outs =
-      at::zeros({bs, head_num, cur_len, head_size}, value.options());
-  auto v_ptr = value.data_ptr<VT>();
-  auto v_cache_ptr = value_cache.data_ptr<VT>();
-  auto attn_out_ptr = attn_outs.data_ptr<VT>();
-  //torch_ipex::cpu::kernel::zero_ker(attn_out_ptr, attn_outs.numel());
-  auto attn_w_ptr = attn_weights.data_ptr<float>();
-
-  // beam_idx: [beam_size, offset] for every decoded token, the beam_idx is the
-  // target beam idx for the past token the target beam_idx for the input tokens
-  // are always 0 compute the offset info for the past token std::cout <<
-  // "beam_idx:" << beam_idx << std::endl;  
-  // the targe beam for the past token
-  auto new_beam_idx = std::vector<std::vector<long>>(
-      beam_batch, std::vector<long>(offset + query.size(1), 0));
-  auto b_ptr = beam_idx.data_ptr<long>();
-  if (offset > 0) {
-    // according to the last decoded token to get the target beam for the past
-    // token
-    for (int i = 0; i < bs; i++) {
-      new_beam_idx[i][offset - 1] = b_ptr[(offset - 1) * bs + i];
-      for (int j = offset - 2; j >= 0;
-           j--) { // for the token of input, the target beam is alwarys 0
-        new_beam_idx[i][j] = b_ptr[j * bs + new_beam_idx[i][j + 1]];
-      }
-    }
-  }
-{
-    RECORD_FUNCTION(
-      "ipex::iakv_sdp::matmul(query, key)",
-      c10::ArrayRef<c10::IValue>({}));
-    #pragma omp parallel for collapse(3)
-    for (auto ti = 0; ti < seq_len; ti++) {  
-    for (auto bi = 0; bi < bs; bi++) {        
-        for (auto hi = 0; hi < head_num; hi++) {
-                  
-            for (auto query_ti = 0; query_ti < cur_len; query_ti++) {        
-                auto kv_hi = hi / group_size; // maping the query head to key/value head
-                                            // to support MGA/MQA
-                //printf("beam_batch: %d bi/bs: %d/%d group_size:%d hi:%d kv_hi:%d kv_head:%d \n", beam_batch, bi, bs, group_size, hi, kv_hi, kv_head); fflush(stdout);
-                
-                auto q_ptr_start = q_ptr +
-                    (bi * cur_len + query_ti) * head_num * head_size + hi * head_size;
-                auto attn_w_stride  =  (bi * head_num + hi) * cur_len * seq_len;
-                auto attn_w_pos = attn_w_ptr + attn_w_stride + query_ti * seq_len + ti ;
-                attn_w_pos[0] = 0.0f;
-                auto kc_token_start = ti * kc_token_stride;
-                auto kc_t_beam_start = kc_token_start;
-                if (ti > query_ti + offset) { // only caculate the innerproduct for
-                                                // the past token and current token
-                    attn_w_pos[0] = -10000.0f;
-                } else if (ti == query_ti + offset) { // caculate the innerproduct for
-                                                        // the current token and store
-                                                        // the key
-                    if (cur_len > 1) { // this may occur for processing the promt
-                        auto beam_size = beam_batch / bs;
-                        // need to store key accross beam
-                        kc_t_beam_start =
-                            kc_t_beam_start + bi * beam_size * kv_head * head_size;
-                    } else {
-                        kc_t_beam_start = kc_t_beam_start + bi * kv_head * head_size;
-                    }
-                    auto kc_head_start =
-                        k_cache_ptr + kc_t_beam_start + kv_hi * head_size;
-                    auto k_ptr_start = k_ptr +
-                        (bi * cur_len + ti - offset) * kv_head * head_size +
-                        kv_hi * head_size;
-                    reduce_head<QT>(
-                        q_ptr_start,
-                        k_ptr_start,
-                        attn_w_pos,
-                        head_size,
-                        true,
-                        kc_head_start);
-                } else { // caculate the innerproduct for the past token
-                    if (ti >= offset) {
-                        auto k_ptr_start = k_ptr +
-                            (bi * cur_len + ti - offset) * kv_head * head_size +
-                            kv_hi * head_size;
-                        reduce_head<QT>(
-                            q_ptr_start,
-                            k_ptr_start,
-                            attn_w_pos,
-                            head_size,
-                            false,
-                            nullptr);
-                    } else {
-                        kc_t_beam_start =
-                            kc_t_beam_start + new_beam_idx[bi][ti] * kv_head * head_size;
-                        if (cur_len > 1) {
-                            auto beam_size = beam_batch / bs;
-                            kc_t_beam_start =
-                                kc_t_beam_start + bi * beam_size * kv_head * head_size;
-                        }
-                        //printf("new_beam_idx[bi][ti]:%d \n", new_beam_idx[bi][ti]);
-                        auto kc_head_start =
-                            k_cache_ptr + kc_t_beam_start + kv_hi * head_size;
-                        reduce_head<QT>(
-                            q_ptr_start,
-                            kc_head_start,
-                            attn_w_pos,
-                            head_size,
-                            false,
-                            nullptr);
-                        }
-                }
-            //std::cout << " " << *attn_w_pos;
-            }
-            
-        }
-        //std::cout << std::endl;
-        }
-    }
-}
-{
-    RECORD_FUNCTION(
-      "ipex::iakv_sdp::div_add_softmax",
-      c10::ArrayRef<c10::IValue>({}));
-    #pragma omp parallel for collapse(2)
-    for (auto bi = 0; bi < bs; bi++) {
-        for (auto hi = 0; hi < head_num; hi++) { 
-            for (auto query_ti = 0; query_ti < cur_len; query_ti++) {            
-                auto mask_ptr_start = mask_ptr + bi * mask_bs_stride + (hi%mask_head_num) * mask_dim2 * seq_len;
-                auto attn_w_stride  =  (bi * head_num + hi) * cur_len * seq_len;
-                auto attn_w_query_start = attn_w_ptr + attn_w_stride + query_ti * seq_len;
-                // std::cout << std::endl;
-                // div+add+softmax
-                #if defined(CPU_CAPABILITY_AVX512)
-                        for (auto qi = 0; qi < 1; qi++) {
-                        auto max_val = -100000.0f;
-                        torch_ipex::cpu::kernel::
-                            _dil_div_add_reduce_max_fusion_kernel<float, QT>(
-                                attn_w_query_start,
-                                mask_ptr_start + (query_ti % mask_dim2) * seq_len,
-                                scale_factor,
-                                seq_len,
-                                attn_w_query_start,
-                                max_val);
-                        
-                        torch_ipex::cpu::kernel::_dil_exp_reduce_sum_fusion_kernel(
-                            attn_w_query_start, seq_len, attn_w_query_start, max_val);
-                        torch_ipex::cpu::kernel::_dil_normalization_kernel<float>(
-                            attn_w_query_start, max_val, seq_len, attn_w_query_start);
-                        }
-                #else
-                        assert(
-                            false &&
-                            "AVX512 is required in ipex::scale_dot_product_for_indirect_access_kv_cache");
-                #endif
-                        }
-                    }
-    }
-}
-auto thread_numbers = omp_get_max_threads(); 
-auto private_attn_outs = at::zeros(
-    {thread_numbers, bs, head_num, cur_len, head_size}, at::kFloat);
-auto private_attn_out_flag = at::zeros(
-    {thread_numbers, bs, head_num}, at::kByte);
-auto flag_access = private_attn_out_flag.accessor<uint8_t, 3>();
-auto private_attn_out_ptr = private_attn_outs.data_ptr<float>();
-//torch_ipex::cpu::kernel::zero_ker(private_attn_out_ptr, private_attn_outs.numel());
-auto attn_outs_stride_priv = bs * head_num * cur_len * head_size;
-{
-    RECORD_FUNCTION(
-      "ipex::iakv_sdp::matmul(attn_w, value)",
-      c10::ArrayRef<c10::IValue>({}));
-#pragma omp parallel for collapse(3)
-for (auto vi = 0; vi < seq_len; vi++) {     
-  for (auto bi = 0; bi < bs; bi++) {
-    for (auto hi = 0; hi < head_num; hi++) {
-          
-        for (auto query_ti = 0; query_ti < cur_len; query_ti++) {
-        auto thread_id = omp_get_thread_num();
-        flag_access[thread_id][bi][hi] = 1;
-        auto kv_hi = hi / group_size; // maping the query head to key/value head
-                                        // to support MGA/MQA
-        auto attn_w_stride  =  (bi * head_num + hi) * cur_len * seq_len;
-        auto attn_w_query_start = attn_w_ptr + attn_w_stride + query_ti * seq_len;
-        // calculate weighted value and store the result to attn_outs[bs,
-        // head_num, cur_len, head_size]
-        auto attn_out_head_stride = thread_id * attn_outs_stride_priv + (bi * head_num + hi) * cur_len * head_size;
-        auto attn_out_start =
-            private_attn_out_ptr + attn_out_head_stride + query_ti * head_size;
-        
-        auto vc_token_start = vi * kc_token_stride;
-        if (vi == query_ti + offset) { // caculate the attention values
-                                            // for the current token
-            auto vc_t_beam_start = vc_token_start;
-            if (cur_len > 1) { // this may occur for processing the promt
-                auto beam_size = beam_batch / bs;
-                // removed the redundant computation, need to store key accross
-                // beam
-                vc_t_beam_start =
-                    vc_t_beam_start + bi * beam_size * kv_head * head_size; 
-            } else {
-                vc_t_beam_start = vc_t_beam_start + bi * kv_head * head_size;
-            }
-            auto v_cache_head_start =
-                v_cache_ptr + vc_t_beam_start + kv_hi * head_size;
-            auto v_ptr_start = v_ptr +
-                (bi * cur_len + vi - offset) * kv_head * head_size +
-                kv_hi * head_size;
-            mul_attenion_weights_and_value_of_head<VT,float>(
-                attn_w_query_start[vi],
-                v_ptr_start,
-                attn_out_start,
-                head_size,
-                true,
-                v_cache_head_start);
-        } else if (vi < query_ti + offset) { // caculate attention
-                                                    // values for the past
-                                                    // token
-            if (vi >= offset) {
-                auto v_ptr_start = v_ptr +
-                    (bi * cur_len + vi - offset) * kv_head * head_size +
-                    kv_hi * head_size;
-                mul_attenion_weights_and_value_of_head<VT,float>(
-                    attn_w_query_start[vi],
-                    v_ptr_start,
-                    attn_out_start,
-                    head_size,
-                    false,
-                    nullptr);
-            } else {
-                //printf("new_beam_idx[bi][vi]:%d \n", new_beam_idx[bi][vi]);
-                auto vc_t_beam_start =
-                    vc_token_start + new_beam_idx[bi][vi] * kv_head * head_size;
-                if (cur_len > 1) {
-                    auto beam_size = beam_batch / bs;
-                    // printf("beam_size:%d, kv_head: %d, head_size: %d \n",
-                    // beam_size, kv_head, head_size); fflush(stdout);
-                    vc_t_beam_start =
-                        vc_t_beam_start + bi * beam_size * kv_head * head_size;
-                }
-                auto v_cache_head_start =
-                    v_cache_ptr + vc_t_beam_start + kv_hi * head_size;
-                mul_attenion_weights_and_value_of_head<VT, float>(
-                    attn_w_query_start[vi],
-                    v_cache_head_start,
-                    attn_out_start,
-                    head_size,
-                    false,
-                    nullptr);
-            }
-        }
-       
-        }
-      }
-      // std::cout << "p:" << p << std::endl;
-    }
-  }
-}
-{
-    RECORD_FUNCTION(
-      "ipex::iakv_sdp::reduction_private_result",
-      c10::ArrayRef<c10::IValue>({}));
-#pragma omp parallel for collapse(3)  
-for (auto bi = 0; bi < bs; bi++) {
-    for (auto hi = 0; hi < head_num; hi++) { 
-        for (auto qi = 0;  qi < cur_len; qi++) {
-            for(auto thread_id = 0; thread_id < thread_numbers; thread_id++){
-                if(flag_access[thread_id][bi][hi] == 0){
-                    continue;
-                }
-                auto attn_out_head_stride = thread_id * attn_outs_stride_priv + (bi * head_num + hi) * cur_len * head_size;
-                auto private_attn_out_start =
-                    private_attn_out_ptr + attn_out_head_stride + qi * head_size;
-                auto attn_outs_start = attn_out_ptr + (bi * head_num + hi) * cur_len * head_size + qi * head_size;
-                torch_ipex::cpu::kernel::add_ker<VT, float>(attn_outs_start, private_attn_out_start, head_size);
-            }
-        }
-        
-    }
-}
- 
-}
-
-   return std::make_tuple(attn_outs, at::Tensor(), key_cache, value_cache, beam_idx);
-}
-
-std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>  zero_copy_kv_cache_masked_multihead_self_attention_kernel_impl(
-    at::Tensor query,
-    at::Tensor key,
-    at::Tensor value,
-    at::Tensor& key_cache,
-    at::Tensor& value_cache,
-    at::Tensor&  beam_idx,    
-    const int64_t offset,
-    const double scale_attn,
-    const int64_t max_positions,
-    at::Tensor& attention_mask) {      
-    assert(key.scalar_type()==at::kBFloat16 || key.scalar_type()==at::kFloat);
-    if (query.scalar_type() == at::kFloat && value.scalar_type() == at::kFloat) {
-        return scale_dot_product_for_indirect_access_kv_cache<float, float>(query, key, value, key_cache, value_cache, beam_idx, offset, scale_attn, attention_mask);
-    }else if(query.scalar_type() == at::kFloat && value.scalar_type() == at::kBFloat16){
-        return scale_dot_product_for_indirect_access_kv_cache<float, at::BFloat16>(query, key, value, key_cache, value_cache, beam_idx, offset, scale_attn, attention_mask);
-    }else if(key.scalar_type() == at::kBFloat16 && value.scalar_type() == at::kFloat){
-        return scale_dot_product_for_indirect_access_kv_cache<at::BFloat16, float>(query, key, value, key_cache, value_cache, beam_idx, offset, scale_attn, attention_mask);
-    }
-    return scale_dot_product_for_indirect_access_kv_cache<at::BFloat16, at::BFloat16>(query, key, value, key_cache, value_cache, beam_idx, offset, scale_attn, attention_mask);  
-}
-
-std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor> first_token_masked_mha(
-    at::Tensor query,
-    at::Tensor key,
-    at::Tensor value,
-    at::Tensor& key_cache,
-    at::Tensor& value_cache,
-    at::Tensor& beam_idx,
-    const int64_t beam_batch,
-    const double scale_attn,
-    int64_t max_positions,
-    at::Tensor attention_mask
-) {
-    
-    auto bs = query.size(0);
-    auto query_length = query.size(1);
-    auto key_lenght = key.size(1);
-    auto kv_head_num = key.size(2);
-    auto head_size = key.size(3);
-    auto casual_mask = at::full({query_length, key_lenght}, -1e6, query.options());
-    casual_mask = at::triu(casual_mask, 1);    
-    casual_mask = casual_mask.unsqueeze(0).unsqueeze(0);
-    attention_mask = attention_mask + casual_mask;
-    if(max_positions < query_length){
-        max_positions = query_length + max_positions;
-    }
-    if(key.scalar_type() != at::kBFloat16 && key.scalar_type() != at::kFloat){
-        TORCH_CHECK(false, "key and value must be float or bfloat16 to use ipex::masked_multihead_self_attention_kernel_impl");
-    }
-    if (key.scalar_type() == at::kFloat) {
-      copy_key_value<float>(key_cache, key, value_cache, value, beam_batch);
-    } else {
-      copy_key_value<at::BFloat16>(
-          key_cache, key, value_cache, value, beam_batch);
-    }
-    //surpport MGQ/MQA
-    //expand the head dimensiopn of key/value to be same to the query
-    if(query.size(2) != key.size(2)){
-        auto n_req = query.size(2) / key.size(2);
-        key = key.repeat_interleave(n_req, 2);
-        value = value.repeat_interleave(n_req, 2);
-    }
-    auto attn_weights = at::Tensor();
-    if (key.scalar_type() == at::kBFloat16) {
-        auto attn_outputs = torch_ipex::cpu::flash_attention_kernel_stub(kCPU, query, key, value, scale_attn, attention_mask);
-        return std::make_tuple(attn_outputs, attn_weights, key_cache, value_cache, beam_idx);
-    } else {
-        key = key.permute({0, 2, 1, 3});
-        query = query.permute({0, 2, 1, 3});
-        value = value.permute({0, 2, 1, 3});
-        auto attn_weights = query.matmul(key.transpose(-1, -2));
-        attn_weights = attn_weights.div(scale_attn);
-        attn_weights = attn_weights + attention_mask;
-        attn_weights = attn_weights.softmax(-1);
-        attn_weights = attn_weights.to(value.dtype());
-        auto attn_outputs = attn_weights.matmul(value);
-        return std::make_tuple(attn_outputs, attn_weights, key_cache, value_cache, beam_idx);
-    }
-}
-std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor>  masked_multihead_self_attention_kernel_impl(
-    at::Tensor query,
-    at::Tensor key,
-    at::Tensor value,
-    at::Tensor& key_cache,
-    at::Tensor& value_cache,
-    at::Tensor&  beam_idx,    
-    at::Tensor seq_info,
-    const double scale_attn,
-    int64_t max_positions,
-    const c10::optional<at::Tensor>& head_mask/* optional */,
-    const c10::optional<at::Tensor>& attention_mask/* optional */) {  
-    if(attention_mask.has_value() == false){
-        TORCH_CHECK(false, "Attention mask is neccessary for ipex::masked_multihead_self_attention_kernel_impl");
-    }
-    if(attention_mask.value().dim() != 4){
-        TORCH_CHECK(false, "Attention mask must be 4D for ipex::masked_multihead_self_attention_kernel_impl");
-    }
-    if(head_mask.has_value() == true){
-        TORCH_CHECK(false, "Head mask is not supported in ipex::masked_multihead_self_attention_kernel_impl");
-    }
-    if (query.dtype() != key.dtype()) {
-        TORCH_CHECK(false, "query and key must have the same data type to use ipex::masked_multihead_self_attention_kernel_impl");
-    }
-    query = query.contiguous();
-    key = key.contiguous();
-    value = value.contiguous();
-    auto attention_mask_v = attention_mask.value().contiguous();
-    attention_mask_v = attention_mask_v.to(query.dtype());
-    auto beam_batch = beam_idx.size(1);//need to prepare the fake beam_idx as (max_position, bs) for the first token      
-    auto offset = seq_info.data_ptr<long>()[0];
-    auto cache_size = key_cache.size(0);
-    auto cur_len = query.size(1);
-    if (offset == 0) {
-      max_positions =
-          max_positions > cur_len ? max_positions : max_positions + cur_len;
-      key_cache = at::empty(
-          {max_positions, beam_batch, key.size(2), key.size(3)}, key.options());
-      value_cache = at::empty(
-          {max_positions, beam_batch, value.size(2), value.size(3)}, value.options());
-      beam_idx = at::empty({max_positions, beam_batch}, beam_idx.options());
-      auto beam_idx_access = beam_idx.accessor<long, 2>();
-      for (auto i = 0; i < max_positions; i++){
-          for (auto j = 0; j < beam_batch; j++){
-              if(key.size(0) == beam_batch){
-                 beam_idx_access[i][j] = j;
-              }else{
-                 auto beam_size = beam_batch / key.size(0);
-                 beam_idx_access[i][j] = j / beam_size * beam_size;
-              }
-            }
-       }
-    } else if (offset > 0 && offset + cur_len > cache_size) {
-      auto new_cache_size = cache_size * 2;
-      auto new_key_cache = at::zeros(
-          {new_cache_size, beam_batch, key.size(2), key.size(3)}, key.options());
-      auto new_value_cache = at::zeros(
-          {new_cache_size, beam_batch, value.size(2), value.size(3)}, value.options());
-      auto new_beam_idx = at::zeros({new_cache_size, beam_batch}, beam_idx.options());
-      new_key_cache.slice(0, 0, cache_size).copy_(key_cache);
-      new_value_cache.slice(0, 0, cache_size).copy_(value_cache);
-      new_beam_idx.slice(0, 0, cache_size).copy_(beam_idx);
-      auto new_beam_idx_access = new_beam_idx.accessor<long, 2>();
-      auto beam_idx_access = beam_idx.accessor<long, 2>();
-      for (auto i = offset; i < new_cache_size; i++){
-          for (auto j = 0; j < beam_batch; j++){
-              new_beam_idx_access[i][j] = beam_idx_access[0][j];
-            }
-      }
-      key_cache = new_key_cache;
-      value_cache = new_value_cache;
-      beam_idx = new_beam_idx;
-    }
-    if(offset > 0){
-      return zero_copy_kv_cache_masked_multihead_self_attention_kernel_impl(
-          query,
-          key,
-          value,
-          key_cache,
-          value_cache,
-          beam_idx,
-          offset,
-          scale_attn,
-          max_positions,
-          attention_mask_v);
-    }else{
-      return first_token_masked_mha(
-          query,
-          key,
-          value,
-          key_cache,
-          value_cache,
-          beam_idx,
-          beam_batch,
-          scale_attn,
-          max_positions,
-          attention_mask_v);
-    }
-    
-}
-} // anonymous namespace
-
-REGISTER_DISPATCH(masked_multihead_self_attention_kernel_stub, &masked_multihead_self_attention_kernel_impl);
-
-} // namespace cpu
-} // namespace torch_ipex
\ No newline at end of file
diff --git a/csrc/cpu/aten/kernels/MergedEmbWithCatKrnl.cpp b/csrc/cpu/aten/kernels/MergedEmbWithCatKrnl.cpp
new file mode 100644
index 00000000..48427a2b
--- /dev/null
+++ b/csrc/cpu/aten/kernels/MergedEmbWithCatKrnl.cpp
@@ -0,0 +1,570 @@
+#include <aten/MergedEmbWithCat.h>
+#include <torch/all.h>
+#include <torch/csrc/autograd/function.h>
+#include "vec/vec.h"
+#include <ATen/quantized/Quantizer.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+namespace {
+
+inline void embeddingbag_kern_ps(
+    const int64_t batch,
+    const int64_t num_emb,
+    const int64_t emb_dim,
+    const int64_t num_hot,
+    const int64_t* index,
+    const float* weight,
+    float* result) {
+  // batch int64_t
+  // num_hot int64_t
+  // index, index of 1 embedding table [batch*num_hot], no offset required for
+  // mlperf case weight, embeddingtable weight, [len_emb, emb_dim] float output
+  // result float [batch, num_emb, emb_dim] scale float
+#if defined(CPU_CAPABILITY_AVX512)
+  for (int64_t b = 0; b < batch; ++b) {
+    __m512 x0, x1, x2, x3, x4, x5, x6, x7;
+    int64_t ofst_idx0 = b * num_hot;
+    int64_t idx = index[ofst_idx0] * emb_dim;
+    x0 = _mm512_load_ps(&weight[idx]);
+    x1 = _mm512_load_ps(&weight[idx + 16]);
+    x2 = _mm512_load_ps(&weight[idx + 32]);
+    x3 = _mm512_load_ps(&weight[idx + 48]);
+    x4 = _mm512_load_ps(&weight[idx + 64]);
+    x5 = _mm512_load_ps(&weight[idx + 80]);
+    x6 = _mm512_load_ps(&weight[idx + 96]);
+    x7 = _mm512_load_ps(&weight[idx + 112]);
+    for (int64_t j = 1; j < num_hot; ++j) {
+      idx = index[ofst_idx0 + j] * emb_dim;
+      x0 = _mm512_add_ps(x0, _mm512_load_ps(&weight[idx]));
+      x1 = _mm512_add_ps(x1, _mm512_load_ps(&weight[idx + 16]));
+      x2 = _mm512_add_ps(x2, _mm512_load_ps(&weight[idx + 32]));
+      x3 = _mm512_add_ps(x3, _mm512_load_ps(&weight[idx + 48]));
+      x4 = _mm512_add_ps(x4, _mm512_load_ps(&weight[idx + 64]));
+      x5 = _mm512_add_ps(x5, _mm512_load_ps(&weight[idx + 80]));
+      x6 = _mm512_add_ps(x6, _mm512_load_ps(&weight[idx + 96]));
+      x7 = _mm512_add_ps(x7, _mm512_load_ps(&weight[idx + 112]));
+    }
+    _mm512_store_ps(result, x0);
+    _mm512_store_ps(result + 16, x1);
+    _mm512_store_ps(result + 32, x2);
+    _mm512_store_ps(result + 48, x3);
+    _mm512_store_ps(result + 64, x4);
+    _mm512_store_ps(result + 80, x5);
+    _mm512_store_ps(result + 96, x6);
+    _mm512_store_ps(result + 112, x7);
+    result += (num_emb + 1) * emb_dim;
+  }
+#endif
+}
+
+inline void
+copy_kern_ps(const int64_t batch,
+             const int64_t num_emb,
+             int64_t emb_dim,
+             const float *dense,
+             float *result) {
+#if defined(CPU_CAPABILITY_AVX512)
+    for (int64_t b = 0; b < batch; ++b) {
+        __m512 x0 = _mm512_load_ps(dense);
+        __m512 x1 = _mm512_load_ps(dense + 16);
+        __m512 x2 = _mm512_load_ps(dense + 32);
+        __m512 x3 = _mm512_load_ps(dense + 48);
+        __m512 x4 = _mm512_load_ps(dense + 64);
+        __m512 x5 = _mm512_load_ps(dense + 80);
+        __m512 x6 = _mm512_load_ps(dense + 96);
+        __m512 x7 = _mm512_load_ps(dense + 112);
+        _mm512_store_ps(result, x0);
+        _mm512_store_ps(result + 16, x1);
+        _mm512_store_ps(result + 32, x2);
+        _mm512_store_ps(result + 48, x3);
+        _mm512_store_ps(result + 64, x4);
+        _mm512_store_ps(result + 80, x5);
+        _mm512_store_ps(result + 96, x6);
+        _mm512_store_ps(result + 112, x7);
+        result += (num_emb + 1) * emb_dim;
+        dense += emb_dim;
+    }
+#endif
+}
+
+void embeddingbagcat_f32(
+    int64_t batch,
+    int64_t num_emb,
+    int64_t emb_dim,
+    const int64_t* num_hot,
+    int64_t** index,
+    int64_t** offset,
+    float** weight,
+    float* dense,
+    float* result) {
+  constexpr int64_t mb = 128;
+  const int64_t bb = (batch - 1) / mb + 1;
+#pragma omp parallel for collapse(2)
+  for (int64_t b = 0; b < bb; ++b) {
+    for (int64_t n = 0; n < (num_emb + 1); ++n) {
+      const int64_t cur_batch = std::min(mb * (b + 1), batch) - b * mb;
+      float* r = &result[b *mb *27 *128 + n *128];
+      if (n == 0) {
+        copy_kern_ps(cur_batch,
+                     num_emb,
+                     emb_dim,
+                     &dense[b *mb *128],
+                     r);
+      } else {
+        const int64_t m = n - 1;
+        const int64_t idx_ofs = b * mb * num_hot[m];
+        embeddingbag_kern_ps(cur_batch,
+                            num_emb,
+                            emb_dim,
+                            num_hot[m],
+                            &index[m][idx_ofs],
+                            weight[m],
+                            r);
+      }
+    }
+  }
+}
+
+at::Tensor merged_emb_with_cat_kernel_impl(
+    const at::TensorList& weights,
+    const at::TensorList& index,
+    const at::Tensor& dense,
+    const at::IntArrayRef multihot) {
+  constexpr int64_t num_emb = 26;
+  constexpr int64_t emb_dim = 128;
+//   assert(weights.size() = num_emb);
+//   assert(index.size() = num_emb);
+//   assert(multihot.size() = num_emb);
+
+  int64_t batch_size = index[0].size(0) / multihot[0];
+  float* w_ptr[num_emb];
+  int64_t* index_ptr[num_emb];
+  for (int i = 0; i < num_emb; i++) {
+    w_ptr[i] = weights[i].data_ptr<float>();
+    index_ptr[i] = index[i].data_ptr<int64_t>();
+  }
+  const int64_t* multihot_ptr = multihot.data();
+  at::Tensor output = at::empty({batch_size, (num_emb + 1) * emb_dim}, weights[0].options());
+  float* o_ptr = output.data_ptr<float>();
+  float* d_ptr = dense.data_ptr<float>();
+  embeddingbagcat_f32(
+      batch_size,
+      num_emb,
+      emb_dim,
+      multihot_ptr,
+      index_ptr,
+      index_ptr,
+      w_ptr,
+      d_ptr,
+      o_ptr);
+  return output;
+}
+
+inline void qembeddingbag_kern_ps(
+    const int64_t batch,
+    const int64_t num_emb,
+    const int64_t num_hot,
+    const int64_t emb_dim,
+    const int64_t* index,
+    const int8_t* weight,
+    int8_t* result,
+    const float scale) {
+  // batch int64_t
+  // num_hot int64_t
+  // index, index of 1 embedding table [batch*num_hot], no offset required for
+  // mlperf case weight, embeddingtable weight, [len_emb, emb_dim] int8 output
+  // result int8 [batch, num_emb, emb_dim] scale float
+#if defined(CPU_CAPABILITY_AVX512)
+  __m512 scale_v = _mm512_set1_ps(scale);
+  for (int64_t b = 0; b < batch; ++b) {
+    __m512i x00, x64;
+    __m512i y0, y1, y2, y3, y4, y5, y6, y7;
+    __m512 f0, f1, f2, f3, f4, f5, f6, f7;
+    int64_t ofst_idx0 = b * num_hot;
+    int64_t idx = index[ofst_idx0] * emb_dim;
+    x00 = _mm512_load_si512(&weight[idx]);
+    x64 = _mm512_load_si512(&weight[idx + 64]);
+    y0 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 0));
+    y1 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 1));
+    y2 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 2));
+    y3 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 3));
+    y4 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 0));
+    y5 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 1));
+    y6 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 2));
+    y7 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 3));
+    for (int64_t j = 1; j < num_hot; ++j) {
+      idx = index[ofst_idx0 + j] * emb_dim;
+      x00 = _mm512_load_si512(&weight[idx]);
+      x64 = _mm512_load_si512(&weight[idx + 64]);
+      y0 = _mm512_add_epi32(
+          y0, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 0)));
+      y1 = _mm512_add_epi32(
+          y1, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 1)));
+      y2 = _mm512_add_epi32(
+          y2, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 2)));
+      y3 = _mm512_add_epi32(
+          y3, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 3)));
+      y4 = _mm512_add_epi32(
+          y4, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 0)));
+      y5 = _mm512_add_epi32(
+          y5, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 1)));
+      y6 = _mm512_add_epi32(
+          y6, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 2)));
+      y7 = _mm512_add_epi32(
+          y7, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 3)));
+    }
+    f0 = _mm512_cvt_roundepi32_ps(
+        y0, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f1 = _mm512_cvt_roundepi32_ps(
+        y1, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f2 = _mm512_cvt_roundepi32_ps(
+        y2, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f3 = _mm512_cvt_roundepi32_ps(
+        y3, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f4 = _mm512_cvt_roundepi32_ps(
+        y4, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f5 = _mm512_cvt_roundepi32_ps(
+        y5, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f6 = _mm512_cvt_roundepi32_ps(
+        y6, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f7 = _mm512_cvt_roundepi32_ps(
+        y7, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f0 = _mm512_mul_ps(f0, scale_v);
+    f1 = _mm512_mul_ps(f1, scale_v);
+    f2 = _mm512_mul_ps(f2, scale_v);
+    f3 = _mm512_mul_ps(f3, scale_v);
+    f4 = _mm512_mul_ps(f4, scale_v);
+    f5 = _mm512_mul_ps(f5, scale_v);
+    f6 = _mm512_mul_ps(f6, scale_v);
+    f7 = _mm512_mul_ps(f7, scale_v);
+    y0 = _mm512_cvt_roundps_epi32(
+        f0, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y1 = _mm512_cvt_roundps_epi32(
+        f1, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y2 = _mm512_cvt_roundps_epi32(
+        f2, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y3 = _mm512_cvt_roundps_epi32(
+        f3, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y4 = _mm512_cvt_roundps_epi32(
+        f4, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y5 = _mm512_cvt_roundps_epi32(
+        f5, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y6 = _mm512_cvt_roundps_epi32(
+        f6, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y7 = _mm512_cvt_roundps_epi32(
+        f7, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y0), 0);
+    x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y1), 1);
+    x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y2), 2);
+    x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y3), 3);
+    x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y4), 0);
+    x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y5), 1);
+    x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y6), 2);
+    x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y7), 3);
+    _mm512_store_si512(result, x00);
+    _mm512_store_si512(result + 64, x64);
+    result += (num_emb + 1) * emb_dim;
+  }
+#endif
+}
+
+inline void qembeddingbag_kern_ph(const int64_t batch,
+                                  const int64_t num_emb,
+                                  const int64_t num_hot,
+                                  const int64_t emb_dim,
+                                  const int64_t * index,
+                                  const int8_t *weight,
+                                  int8_t *result,
+                                  const float scale) {
+#if defined(CPU_CAPABILITY_AVX512_FP16)
+    // batch int64_t
+    // num_hot int64_t
+    // index, index of 1 embedding table [batch*num_hot], no offset required for mlperf case
+    // weight, embeddingtable weight, [len_emb, emb_dim] int8
+    // output result int8 [batch, num_emb, emb_dim]
+    // scale float
+    __m512h scale_v = (__m512h)_mm512_broadcast_f32x8(
+        (__m256)_mm512_cvtps_ph(_mm512_set1_ps(scale),
+                                _MM_FROUND_TO_NEAREST_INT));
+    for (int64_t b = 0; b < batch; ++b) {
+        __m512i x00, x64;
+        __m512i y00, y32, y64, y96;
+        __m512h h00, h32, h64, h96;
+        int64_t ofst_idx0 = b * num_hot;
+        int64_t idx = index[ofst_idx0] * emb_dim;
+        x00 = _mm512_load_si512(&weight[idx]);
+        x64 = _mm512_load_si512(&weight[idx + 64]);
+        y00 = _mm512_cvtepi8_epi16(
+            _mm512_extracti32x8_epi32(x00, 0));
+        y32 = _mm512_cvtepi8_epi16(
+            _mm512_extracti32x8_epi32(x00, 1));
+        y64 = _mm512_cvtepi8_epi16(
+            _mm512_extracti32x8_epi32(x64, 0));
+        y96 = _mm512_cvtepi8_epi16(
+            _mm512_extracti32x8_epi32(x64, 1));
+        for (int64_t j = 1; j < num_hot; ++j) {
+            idx = index[ofst_idx0 + j] * emb_dim;
+            x00 = _mm512_load_si512(&weight[idx]);
+            x64 = _mm512_load_si512(&weight[idx + 64]);
+            y00 = _mm512_adds_epi16(y00,
+                                    _mm512_cvtepi8_epi16(
+                                        _mm512_extracti32x8_epi32(x00, 0)));
+            y32 = _mm512_adds_epi16(y32,
+                                    _mm512_cvtepi8_epi16(
+                                        _mm512_extracti32x8_epi32(x00, 1)));
+            y64 = _mm512_adds_epi16(y64,
+                                    _mm512_cvtepi8_epi16(
+                                        _mm512_extracti32x8_epi32(x64, 0)));
+            y96 = _mm512_adds_epi16(y96,
+                                    _mm512_cvtepi8_epi16(
+                                        _mm512_extracti32x8_epi32(x64, 1)));
+        }
+        h00 = _mm512_cvt_roundepi16_ph(y00,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        h32 = _mm512_cvt_roundepi16_ph(y32,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        h64 = _mm512_cvt_roundepi16_ph(y64,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        h96 = _mm512_cvt_roundepi16_ph(y96,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        h00 = _mm512_mul_ph(h00, scale_v);
+        h32 = _mm512_mul_ph(h32, scale_v);
+        h64 = _mm512_mul_ph(h64, scale_v);
+        h96 = _mm512_mul_ph(h96, scale_v);
+        y00 = _mm512_cvt_roundph_epi16(h00,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        y32 = _mm512_cvt_roundph_epi16(h32,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        y64 = _mm512_cvt_roundph_epi16(h64,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        y96 = _mm512_cvt_roundph_epi16(h96,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        x00 = _mm512_inserti64x4(x00, _mm512_cvtsepi16_epi8(y00), 0);
+        x00 = _mm512_inserti64x4(x00, _mm512_cvtsepi16_epi8(y32), 1);
+        x64 = _mm512_inserti64x4(x64, _mm512_cvtsepi16_epi8(y64), 0);
+        x64 = _mm512_inserti64x4(x64, _mm512_cvtsepi16_epi8(y96), 1);
+        _mm512_store_si512(result, x00);
+        _mm512_store_si512(result + 64, x64);
+        result += (num_emb + 1) * emb_dim;
+    }
+#endif
+}
+
+inline void
+scalecopy_kern_ps(const int64_t batch,
+                  const int64_t num_emb,
+                  int64_t emb_dim,
+                  const int8_t *dense,
+                  int8_t *result,
+                  const float scale) {
+#if defined(CPU_CAPABILITY_AVX512)
+    __m512 scale_v = _mm512_set1_ps(scale);
+    for (int64_t b = 0; b < batch; ++b) {
+        __m512i x00, x64;
+        __m512i y0, y1, y2, y3, y4, y5, y6, y7;
+        __m512 f0, f1, f2, f3, f4, f5, f6, f7;
+        x00 = _mm512_load_si512(dense);
+        x64 = _mm512_load_si512(dense + 64);
+        y0 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x00, 0));
+        y1 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x00, 1));
+        y2 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x00, 2));
+        y3 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x00, 3));
+        y4 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x64, 0));
+        y5 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x64, 1));
+        y6 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x64, 2));
+        y7 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x64, 3));
+        f0 = _mm512_cvt_roundepi32_ps(y0,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f1 = _mm512_cvt_roundepi32_ps(y1,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f2 = _mm512_cvt_roundepi32_ps(y2,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f3 = _mm512_cvt_roundepi32_ps(y3,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f4 = _mm512_cvt_roundepi32_ps(y4,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f5 = _mm512_cvt_roundepi32_ps(y5,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f6 = _mm512_cvt_roundepi32_ps(y6,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f7 = _mm512_cvt_roundepi32_ps(y7,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f0 = _mm512_mul_ps(f0, scale_v);
+        f1 = _mm512_mul_ps(f1, scale_v);
+        f2 = _mm512_mul_ps(f2, scale_v);
+        f3 = _mm512_mul_ps(f3, scale_v);
+        f4 = _mm512_mul_ps(f4, scale_v);
+        f5 = _mm512_mul_ps(f5, scale_v);
+        f6 = _mm512_mul_ps(f6, scale_v);
+        f7 = _mm512_mul_ps(f7, scale_v);
+        y0 = _mm512_cvt_roundps_epi32(f0,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y1 = _mm512_cvt_roundps_epi32(f1,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y2 = _mm512_cvt_roundps_epi32(f2,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y3 = _mm512_cvt_roundps_epi32(f3,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y4 = _mm512_cvt_roundps_epi32(f4,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y5 = _mm512_cvt_roundps_epi32(f5,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y6 = _mm512_cvt_roundps_epi32(f6,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y7 = _mm512_cvt_roundps_epi32(f7,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y0), 0);
+        x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y1), 1);
+        x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y2), 2);
+        x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y3), 3);
+        x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y4), 0);
+        x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y5), 1);
+        x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y6), 2);
+        x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y7), 3);
+        _mm512_store_si512(result, x00);
+        _mm512_store_si512(result + 64, x64);
+        result += (num_emb + 1) * emb_dim;
+        dense += emb_dim;
+    }
+#endif
+}
+
+void qembeddingbagcat(
+    int64_t batch,
+    int64_t num_emb,
+    int64_t emb_dim,
+    const int64_t* num_hot,
+    int64_t** index,
+    int64_t** offset,
+    int8_t** weight,
+    int8_t* dense,
+    int8_t* result,
+    const double* in_scale,
+    double dx_scale,
+    double ot_scale) {
+  constexpr int64_t mb = 512;
+  const int64_t bb = (batch - 1) / mb + 1;
+#pragma omp parallel for collapse(2) schedule(auto)
+  for (int64_t b = 0; b < bb; ++b) {
+    for (int64_t n = 0; n < (num_emb + 1); ++n) {
+      const int64_t cur_batch = std::min(mb * (b + 1), batch) - b * mb;
+      // shiyang: emb_dim need to be 128
+      int8_t *r = &result[b *mb *27 *128 + n *128];
+      if (n == 0) {
+        const float scale = dx_scale / ot_scale;
+        scalecopy_kern_ps(cur_batch,
+                          num_emb,
+                          emb_dim,
+                          &dense[b *mb *128],
+                          r,
+                          scale);
+      } else {
+        const int64_t m = n - 1;
+        const int64_t idx_ofs = b *mb *num_hot[m];
+        const float scale = in_scale[m] / ot_scale;
+        qembeddingbag_kern_ps(cur_batch,
+                              num_emb,
+                              num_hot[m],
+                              emb_dim,
+                              &index[m][idx_ofs],
+                              weight[m],
+                              r,
+                              scale);
+      }
+    }
+  }
+}
+
+at::Tensor qmerged_emb_with_cat_kernel_impl(
+    const at::TensorList& weights,
+    const at::TensorList& index,
+    const at::Tensor& dense,
+    const at::IntArrayRef multihot,
+    const c10::ArrayRef<double> w_scale,
+    double dx_scale,
+    double o_scale) {
+  constexpr int64_t num_emb = 26;
+  constexpr int64_t emb_dim = 128;
+
+//   assert(weights.size() = num_emb);
+//   assert(index.size() = num_emb);
+//   assert(multihot.size() = num_emb);
+
+  int64_t batch_size = index[0].size(0) / multihot[0];
+  int8_t* w_ptr[num_emb];
+  int64_t* index_ptr[num_emb];
+  for (int i = 0; i < num_emb; i++) {
+    w_ptr[i] = weights[i].data_ptr<int8_t>();
+    index_ptr[i] = index[i].data_ptr<int64_t>();
+  }
+  const int64_t* multihot_ptr = multihot.data();
+  const double* w_scale_ptr = w_scale.data();
+  at::QuantizerPtr output_quantizer =
+      at::make_per_tensor_affine_quantizer(o_scale, /*zp=*/0, at::kQInt8);
+  at::Tensor output = at::new_qtensor(
+      /*sizes=*/{batch_size, (num_emb + 1) * emb_dim},
+      weights[0].options(),
+      output_quantizer);
+  int8_t* o_ptr = output.data_ptr<int8_t>();
+  int8_t* d_ptr = dense.data_ptr<int8_t>();
+  qembeddingbagcat(
+      batch_size,
+      num_emb,
+      emb_dim,
+      multihot_ptr,
+      index_ptr,
+      index_ptr,
+      w_ptr,
+      d_ptr,
+      o_ptr,
+      w_scale_ptr,
+      dx_scale,
+      o_scale);
+  return output;
+}
+
+} // anonymous namespace
+
+REGISTER_DISPATCH(
+    merged_emb_with_cat_kernel_stub,
+    &merged_emb_with_cat_kernel_impl);
+REGISTER_DISPATCH(
+    qmerged_emb_with_cat_kernel_stub,
+    &qmerged_emb_with_cat_kernel_impl);
+
+} // namespace cpu
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/kernels/RotaryPositionEmbeddingKnl.cpp b/csrc/cpu/aten/kernels/RotaryPositionEmbeddingKnl.cpp
deleted file mode 100644
index 64768220..00000000
--- a/csrc/cpu/aten/kernels/RotaryPositionEmbeddingKnl.cpp
+++ /dev/null
@@ -1,244 +0,0 @@
-#include <aten/RotaryPositionEmbedding.h>
-#include <torch/csrc/autograd/function.h>
-#include <torch/all.h>
-#include <ATen/Tensor.h>
-#include "vec/vec.h"
-
-namespace torch_ipex {
-namespace cpu {
-
-namespace {
-
-template <typename T_in, typename T_emb>
-void apply_rope_along_head(
-    T_in* in_ptr_start,
-    T_emb* cos_start,
-    T_emb* sin_start,
-    int64_t rotary_ndims,   
-    int64_t offset) {
-  for (int h = 0; h < rotary_ndims/2; h++) {//used by lamma, ToDo:vectorization                
-        float in0 = in_ptr_start[h];
-        float in1 = in_ptr_start[h+offset];
-        float sin = sin_start[h];
-        float cos = cos_start[h];
-        float out0 = in0 * cos - in1 * sin;
-        float out1 = in1 * cos + in0 * sin;
-        in_ptr_start[h] = out0;
-        in_ptr_start[h+offset] = out1;
-  }   
-}
-
-template <>
-void apply_rope_along_head(
-    at::BFloat16* in_ptr_start,
-    float* cos_start,
-    float* sin_start,
-    int64_t rotary_ndims,   
-    int64_t offset) {
-    auto vec_size = 16;
-    auto h = 0;
-    #if defined(CPU_CAPABILITY_AVX512)
-    for(h = 0; h <= rotary_ndims/2 - vec_size; h += vec_size) {
-        auto in0 = torch_ipex::cpu::kernel::convert_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(in_ptr_start + h)));
-        auto in1 = torch_ipex::cpu::kernel::convert_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(in_ptr_start + h + offset)));
-        auto sin = _mm512_loadu_ps(sin_start + h);
-        auto cos = _mm512_loadu_ps(cos_start + h);
-        auto out0 = _mm512_sub_ps(_mm512_mul_ps(in0, cos), _mm512_mul_ps(in1, sin));
-        auto out1 = _mm512_add_ps(_mm512_mul_ps(in1, cos), _mm512_mul_ps(in0, sin));
-        _mm256_storeu_si256((__m256i*)(in_ptr_start + h), cvt_fp32_to_bf16(out0));
-        _mm256_storeu_si256((__m256i*)(in_ptr_start + h + offset), cvt_fp32_to_bf16(out1));
-    }
-    for(; h < rotary_ndims/2; h++) {        
-        float in0 = in_ptr_start[h];
-        float in1 = in_ptr_start[h+offset];
-        float sin = sin_start[h];
-        float cos = cos_start[h];
-        float out0 = in0 * cos - in1 * sin;
-        float out1 = in1 * cos + in0 * sin;
-        in_ptr_start[h] = out0;
-        in_ptr_start[h+offset] = out1;
-    }
-    #else
-    for(h=0; h < rotary_ndims/2; h++) {        
-        float in0 = in_ptr_start[h];
-        float in1 = in_ptr_start[h+offset];
-        float sin = sin_start[h];
-        float cos = cos_start[h];
-        float out0 = in0 * cos - in1 * sin;
-        float out1 = in1 * cos + in0 * sin;
-        in_ptr_start[h] = out0;
-        in_ptr_start[h+offset] = out1;
-    }
-    #endif
-}
-
-template <>
-void apply_rope_along_head(
-    at::BFloat16* in_ptr_start,
-    at::BFloat16* cos_start,
-    at::BFloat16* sin_start,
-    int64_t rotary_ndims,   
-    int64_t offset) {
-    auto vec_size = 16;
-    auto h = 0;
-    #if defined(CPU_CAPABILITY_AVX512)
-    for(h = 0; h <= rotary_ndims/2 - vec_size; h += vec_size) {
-        auto in0 = torch_ipex::cpu::kernel::convert_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(in_ptr_start + h)));
-        auto in1 = torch_ipex::cpu::kernel::convert_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(in_ptr_start + h + offset)));
-        auto sin = torch_ipex::cpu::kernel::convert_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(sin_start + h)));
-        auto cos = torch_ipex::cpu::kernel::convert_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(cos_start + h)));
-        auto out0 = _mm512_sub_ps(_mm512_mul_ps(in0, cos), _mm512_mul_ps(in1, sin));
-        auto out1 = _mm512_add_ps(_mm512_mul_ps(in1, cos), _mm512_mul_ps(in0, sin));
-        _mm256_storeu_si256((__m256i*)(in_ptr_start + h), cvt_fp32_to_bf16(out0));
-        _mm256_storeu_si256((__m256i*)(in_ptr_start + h + offset), cvt_fp32_to_bf16(out1));
-    }
-    for(; h < rotary_ndims/2; h++) {        
-        float in0 = in_ptr_start[h];
-        float in1 = in_ptr_start[h+offset];
-        float sin = sin_start[h];
-        float cos = cos_start[h];
-        float out0 = in0 * cos - in1 * sin;
-        float out1 = in1 * cos + in0 * sin;
-        in_ptr_start[h] = out0;
-        in_ptr_start[h+offset] = out1;
-    }
-    #else
-    for(h=0; h < rotary_ndims/2; h++) {        
-        float in0 = in_ptr_start[h];
-        float in1 = in_ptr_start[h+offset];
-        float sin = sin_start[h];
-        float cos = cos_start[h];
-        float out0 = in0 * cos - in1 * sin;
-        float out1 = in1 * cos + in0 * sin;
-        in_ptr_start[h] = out0;
-        in_ptr_start[h+offset] = out1;
-    }
-    #endif
-}
-
-
-template <>
-void apply_rope_along_head(
-    float* in_ptr_start,
-    float* cos_start,
-    float* sin_start,
-    int64_t rotary_ndims,   
-    int64_t offset) {
-    auto vec_size = 16;
-    auto h = 0;
-    #if defined(CPU_CAPABILITY_AVX512)
-    for(h = 0; h <= rotary_ndims/2 - vec_size; h += vec_size) {
-        auto in0 = _mm512_loadu_ps(in_ptr_start + h);
-        auto in1 = _mm512_loadu_ps(in_ptr_start + h + offset);
-        auto sin = _mm512_loadu_ps(sin_start + h);
-        auto cos = _mm512_loadu_ps(cos_start + h);
-        auto out0 = _mm512_sub_ps(_mm512_mul_ps(in0, cos), _mm512_mul_ps(in1, sin));
-        auto out1 = _mm512_add_ps(_mm512_mul_ps(in1, cos), _mm512_mul_ps(in0, sin));
-        _mm512_storeu_ps(in_ptr_start + h, out0);
-        _mm512_storeu_ps(in_ptr_start + h + offset, out1);
-    }
-    for(; h < rotary_ndims/2; h++) {        
-        float in0 = in_ptr_start[h];
-        float in1 = in_ptr_start[h+offset];
-        float sin = sin_start[h];
-        float cos = cos_start[h];
-        float out0 = in0 * cos - in1 * sin;
-        float out1 = in1 * cos + in0 * sin;
-        in_ptr_start[h] = out0;
-        in_ptr_start[h+offset] = out1;
-    }
-    #else
-    for(h=0; h < rotary_ndims/2; h++) {        
-        float in0 = in_ptr_start[h];
-        float in1 = in_ptr_start[h+offset];
-        float sin = sin_start[h];
-        float cos = cos_start[h];
-        float out0 = in0 * cos - in1 * sin;
-        float out1 = in1 * cos + in0 * sin;
-        in_ptr_start[h] = out0;
-        in_ptr_start[h+offset] = out1;
-    }
-    #endif
-}
-template <typename T>
-void ApplyROPEKernel(
-    at::Tensor& t_in,
-    at::Tensor& t_emb_pos,
-    at::Tensor& t_pos,
-    int64_t N,//N: number of head, H: head size
-    int64_t H,
-    int64_t offset,
-    int64_t rotary_ndims) {
-  auto in_sizes = t_in.sizes(); // in[B][S][F]
-  auto MP = t_emb_pos.size(0); // Max Pos
-  auto HR = t_emb_pos.size(1); // rotary_dim
-  auto B = in_sizes[0];
-  auto S = in_sizes[1];
-  auto COFF = HR / 2;
-  t_in = t_in.contiguous();
-  t_emb_pos = t_emb_pos.contiguous();
-  t_pos = t_pos.contiguous();
-  auto in_ptr = t_in.data_ptr<T>(); // [B][S][N][H]
-  auto in_stride_b = S*N*H;
-  auto in_stride_s = N*H;
-  auto emb_pos_ptr = t_emb_pos.data_ptr<float>(); // [MP][HR]
-  auto pos_ptr = t_pos.data_ptr<long>(); // [MB][S]
-  {
-
-#pragma omp parallel for collapse(3)
-    for (int b = 0; b < B; b++) {
-      for (int s = 0; s < S; s++) {
-        for (int n = 0; n < N; n++) {
-          if(1 == offset) {//used by GPT-J 6B
-            for (int h = 0, h2 = 0; h < HR; h += 2, h2++) {
-                auto in_offset =b*in_stride_b+s*in_stride_s+n*H + h;
-                float in0 = in_ptr[in_offset];
-                float in1 = in_ptr[in_offset+1];
-                long  p = pos_ptr[b*S + s];
-                float sin = emb_pos_ptr[p*HR +h2];
-                float cos = emb_pos_ptr[p*HR + COFF + h2];
-                float out0 = in0 * cos - in1 * sin;
-                float out1 = in1 * cos + in0 * sin;
-                in_ptr[in_offset] = out0;
-                in_ptr[in_offset+1] = out1;
-            }
-          }else{              
-              auto in_ptr_start =in_ptr + b*in_stride_b+s*in_stride_s+n*H;
-              long  p = pos_ptr[b*S + s];
-              auto sin_start = emb_pos_ptr + p*HR;
-              auto cos_start = emb_pos_ptr + p*HR + COFF;
-              apply_rope_along_head<T, float>(in_ptr_start, cos_start, sin_start, rotary_ndims, offset);             
-          }       
-        }
-      }
-    }
-  }
-}
-
-void rotary_position_embedding_kernel_impl(
-    at::Tensor& t_in,
-    at::Tensor& t_emb_pos,
-    at::Tensor& t_pos,
-    int64_t N,//N: number of head, H: head size
-    int64_t H,
-    int64_t offset,
-    int64_t rotary_ndims) {
-    if (t_in.scalar_type() == at::kFloat) {
-        ApplyROPEKernel<float>(t_in, t_emb_pos, t_pos, N, H, offset, rotary_ndims);
-    }else if (t_in.scalar_type() == at::kBFloat16) {
-        ApplyROPEKernel<at::BFloat16>(t_in, t_emb_pos, t_pos, N, H, offset, rotary_ndims);  
-    }else if (t_in.scalar_type() == at::kHalf){
-	      ApplyROPEKernel<at::Half>(t_in, t_emb_pos, t_pos, N, H, offset, rotary_ndims);
-    }else{
-       assert(0);
-    }
-
-
-}
-
-} // anonymous namespace
-
-REGISTER_DISPATCH(rotary_position_embedding_kernel_stub, &rotary_position_embedding_kernel_impl);
-
-} // namespace cpu
-} // namespace torch_ipex
diff --git a/csrc/cpu/aten/kernels/TPPGEMMKrnl.cpp b/csrc/cpu/aten/kernels/TPPGEMMKrnl.cpp
deleted file mode 100644
index d70ac1f7..00000000
--- a/csrc/cpu/aten/kernels/TPPGEMMKrnl.cpp
+++ /dev/null
@@ -1,223 +0,0 @@
-
-#include "tpp/kernels/TPPGEMMKrnl.h"
-#include <ATen/record_function.h>
-#include <aten/TPPGEMM.h>
-#include <torch/all.h>
-#include <cstdint>
-#include <iostream>
-#include <vector>
-
-namespace torch_ipex {
-namespace cpu {
-
-namespace {
-
-at::Tensor tpp_linear_bias_kernel_impl(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias) {
-  auto sizes = t_in.sizes().vec();
-  auto wt_sizes = t_wt.sizes();
-  sizes[2] = wt_sizes[0] * wt_sizes[3];
-
-  auto t_out = t_in.new_empty(sizes);
-  auto dt = t_wt.dtype();
-  if (dt == at::kFloat) {
-    torch_ipex::tpp::tpp_linear_bias<float>(t_in, t_wt, t_bias, t_out);
-  } else if (dt == at::kBFloat16) {
-    torch_ipex::tpp::tpp_linear_bias<at::BFloat16>(t_in, t_wt, t_bias, t_out);
-  } else {
-    AT_ASSERT(
-        0,
-        "TPP does not support current weight dtype %s:%d\n",
-        __FILE__,
-        __LINE__);
-  }
-
-  return t_out;
-}
-
-at::Tensor tpp_linear_nobias_kernel_impl(at::Tensor& t_in, at::Tensor& t_wt) {
-  auto sizes = t_in.sizes().vec();
-  auto wt_sizes = t_wt.sizes();
-  sizes[2] = wt_sizes[0] * wt_sizes[3];
-
-  auto t_out = t_in.new_empty(sizes);
-
-  auto dt = t_wt.dtype();
-  if (dt == at::kFloat) {
-    torch_ipex::tpp::tpp_linear_no_bias<float>(t_in, t_wt, t_out);
-  } else if (dt == at::kBFloat16) {
-    torch_ipex::tpp::tpp_linear_no_bias<at::BFloat16>(t_in, t_wt, t_out);
-  } else {
-    AT_ASSERT(
-        0,
-        "TPP does not support current weight dtype %s:%d\n",
-        __FILE__,
-        __LINE__);
-  }
-  return t_out;
-}
-
-at::Tensor tpp_linear_gelu_kernel_impl(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias) {
-  auto sizes = t_in.sizes().vec();
-  auto wt_sizes = t_wt.sizes();
-  sizes[2] = wt_sizes[0] * wt_sizes[3];
-
-  auto t_out = t_in.new_empty(sizes);
-
-  auto dt = t_wt.dtype();
-  if (dt == at::kFloat) {
-    torch_ipex::tpp::tpp_linear_gelu<float>(t_in, t_wt, t_bias, t_out);
-  } else if (dt == at::kBFloat16) {
-    torch_ipex::tpp::tpp_linear_gelu<at::BFloat16>(t_in, t_wt, t_bias, t_out);
-  } else {
-    AT_ASSERT(
-        0,
-        "TPP does not support current weight dtype %s:%d\n",
-        __FILE__,
-        __LINE__);
-  }
-  return t_out;
-}
-
-at::Tensor tpp_linear_silu_kernel_impl(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias) {
-  auto sizes = t_in.sizes().vec();
-  auto wt_sizes = t_wt.sizes();
-  sizes[2] = wt_sizes[0] * wt_sizes[3];
-
-  auto t_out = t_in.new_empty(sizes);
-
-  auto dt = t_wt.dtype();
-  if (dt == at::kFloat) {
-    torch_ipex::tpp::tpp_linear_silu<float>(t_in, t_wt, t_bias, t_out);
-  } else if (dt == at::kBFloat16) {
-    torch_ipex::tpp::tpp_linear_silu<at::BFloat16>(t_in, t_wt, t_bias, t_out);
-  } else {
-    AT_ASSERT(
-        0,
-        "TPP does not support current weight dtype %s:%d\n",
-        __FILE__,
-        __LINE__);
-  }
-  return t_out;
-}
-
-at::Tensor tpp_linear_relu_kernel_impl(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias) {
-  auto sizes = t_in.sizes().vec();
-  auto wt_sizes = t_wt.sizes();
-  sizes[2] = wt_sizes[0] * wt_sizes[3];
-
-  auto t_out = t_in.new_empty(sizes);
-
-  auto dt = t_wt.dtype();
-  if (dt == at::kFloat) {
-    torch_ipex::tpp::tpp_linear_relu<float>(t_in, t_wt, t_bias, t_out);
-  } else if (dt == at::kBFloat16) {
-    torch_ipex::tpp::tpp_linear_relu<at::BFloat16>(t_in, t_wt, t_bias, t_out);
-  } else {
-    AT_ASSERT(
-        0,
-        "TPP does not support current weight dtype %s:%d\n",
-        __FILE__,
-        __LINE__);
-  }
-  return t_out;
-}
-
-at::Tensor tpp_linear_add_add_kernel_impl(
-    at::Tensor& t_in,
-    at::Tensor& t_in1,
-    at::Tensor& t_in2,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias,
-    double scale) {
-  auto t_out = at::empty_like(t_in1);
-  auto dt = t_wt.dtype();
-  if (dt == at::kFloat) {
-    torch_ipex::tpp::tpp_linear_add_add<float>(
-        t_in, t_in1, t_in2, t_wt, t_bias, t_out, scale);
-  } else if (dt == at::kBFloat16) {
-    torch_ipex::tpp::tpp_linear_add_add<at::BFloat16>(
-        t_in, t_in1, t_in2, t_wt, t_bias, t_out, scale);
-  } else {
-    AT_ASSERT(
-        0,
-        "TPP does not support current weight dtype %s:%d\n",
-        __FILE__,
-        __LINE__);
-  }
-  return t_out;
-}
-
-at::Tensor tpp_linear_add_kernel_impl(
-    at::Tensor& t_in,
-    at::Tensor& t_in1,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias,
-    double scale) {
-  auto t_out = at::empty_like(t_in1);
-  auto dt = t_wt.dtype();
-  if (dt == at::kFloat) {
-    torch_ipex::tpp::tpp_linear_add<float>(
-        t_in, t_in1, t_wt, t_bias, t_out, scale);
-  } else if (dt == at::kBFloat16) {
-    torch_ipex::tpp::tpp_linear_add<at::BFloat16>(
-        t_in, t_in1, t_wt, t_bias, t_out, scale);
-  } else {
-    AT_ASSERT(
-        0,
-        "TPP does not support current weight dtype %s:%d\n",
-        __FILE__,
-        __LINE__);
-  }
-  return t_out;
-}
-
-at::Tensor tpp_linear_mul_kernel_impl(
-    at::Tensor& t_in,
-    at::Tensor& t_in1,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias) {
-  auto t_out = at::empty_like(t_in1);
-  auto dt = t_wt.dtype();
-  if (dt == at::kFloat) {
-    torch_ipex::tpp::tpp_linear_mul<float>(t_in, t_in1, t_wt, t_bias, t_out);
-  } else if (dt == at::kBFloat16) {
-    torch_ipex::tpp::tpp_linear_mul<at::BFloat16>(
-        t_in, t_in1, t_wt, t_bias, t_out);
-  } else {
-    AT_ASSERT(
-        0,
-        "TPP does not support current weight dtype %s:%d\n",
-        __FILE__,
-        __LINE__);
-  }
-  return t_out;
-}
-
-} // namespace
-
-REGISTER_DISPATCH(
-    tpp_linear_nobias_kernel_stub,
-    &tpp_linear_nobias_kernel_impl);
-REGISTER_DISPATCH(tpp_linear_bias_kernel_stub, &tpp_linear_bias_kernel_impl);
-REGISTER_DISPATCH(tpp_linear_gelu_kernel_stub, &tpp_linear_gelu_kernel_impl);
-REGISTER_DISPATCH(tpp_linear_relu_kernel_stub, &tpp_linear_relu_kernel_impl);
-REGISTER_DISPATCH(tpp_linear_silu_kernel_stub, &tpp_linear_silu_kernel_impl);
-REGISTER_DISPATCH(tpp_linear_mul_kernel_stub, &tpp_linear_mul_kernel_impl);
-REGISTER_DISPATCH(tpp_linear_add_kernel_stub, &tpp_linear_add_kernel_impl);
-REGISTER_DISPATCH(
-    tpp_linear_add_add_kernel_stub,
-    &tpp_linear_add_add_kernel_impl);
-} // namespace cpu
-} // namespace torch_ipex
diff --git a/csrc/cpu/aten/kernels/WoqLinearKrnl.cpp b/csrc/cpu/aten/kernels/WoqLinearKrnl.cpp
index 4940bffb..d96022c5 100644
--- a/csrc/cpu/aten/kernels/WoqLinearKrnl.cpp
+++ b/csrc/cpu/aten/kernels/WoqLinearKrnl.cpp
@@ -1,7 +1,6 @@
 #include <ATen/ATen.h>
 #include <ATen/Tensor.h>
 #include <aten/Linear.h>
-#include <aten/utils/woq.h>
 #include <emmintrin.h>
 #include <libxsmm.h>
 #include <cstdio>
@@ -9,185 +8,79 @@
 #include <functional>
 #include <iostream>
 #include <memory>
-#include <type_traits>
 #include <unordered_map>
 #include <vector>
-#include "mkl.h"
+#include "assert.h"
 
 namespace torch_ipex {
 namespace cpu {
 namespace {
 
-void print_matrix(float* A, int m, int n, int ld) {
-  for (int i = 0; i < m; i++) {
-    float* A_ = A + i * ld;
-    for (int j = 0; j < n; j++) {
-      std::cout << std::setprecision(23) << A_[j] << " ";
-    }
-    std::cout << std::endl;
-  }
-}
-
-void print_matrix(const uint8_t* A, int m, int n, int ld) {
-  for (int j = 0; j < n; j++) {
-    for (int i = 0; i < m; i++) {
-      const uint8_t* A_ = A + i * ld;
-      uint8_t tmp = (A_[j] << 4) | ((A_[j] & 0xF0) >> 4);
-      printf("%02X ", tmp);
-    }
-    printf("\n");
-  }
-}
-
-// TODO implement optimized kernels for fused op
-// then the following part will be discarded
-using PostopFunc = std::function<at::Tensor&(at::Tensor&)>;
-using PostopFuncGetter = std::function<PostopFunc(
-    const torch::List<c10::optional<at::Scalar>>&,
-    const c10::optional<c10::string_view>&)>;
-
-static PostopFuncGetter postop_func_none =
-    [](const torch::List<c10::optional<at::Scalar>>&,
-       const c10::optional<c10::string_view>&) {
-      return [](at::Tensor& t) -> at::Tensor& { return t; };
-    };
-
-static PostopFuncGetter postop_func_relu =
-    [](const torch::List<c10::optional<at::Scalar>>&,
-       const c10::optional<c10::string_view>&) { return at::relu_; };
-
-static PostopFuncGetter postop_func_gelu =
-    [](const torch::List<c10::optional<at::Scalar>>&,
-       const c10::optional<c10::string_view>& algorithm) {
-      assert(
-          algorithm.has_value() &&
-          (algorithm == "none" || algorithm == "tanh"));
-      return [=](at::Tensor& t) -> at::Tensor& {
-        return at::gelu_(t, algorithm.value());
-      };
-    };
-
-static std::map<c10::string_view, PostopFuncGetter> postop_func_map = {
-    {"none", postop_func_none},
-    {"relu", postop_func_relu},
-    {"gelu", postop_func_gelu}};
-
 #if defined(CPU_CAPABILITY_AVX512)
 #include <immintrin.h>
-
-inline __m256i cvt_fp32_to_bf16(const __m512 src) {
-#if (defined CPU_CAPABILITY_AVX512_BF16)
-  return reinterpret_cast<__m256i>(_mm512_cvtneps_pbh(src));
-#else
-  __m512i value = _mm512_castps_si512(src);
-  __m512i nan = _mm512_set1_epi32(0xffff);
-  auto mask_value = _mm512_cmp_ps_mask(src, src, _CMP_ORD_Q);
-  __m512i ones = _mm512_set1_epi32(0x1);
-  __m512i vec_bias = _mm512_set1_epi32(0x7fff);
-  // uint32_t lsb = (input >> 16) & 1;
-  auto t_value = _mm512_and_si512(_mm512_srli_epi32(value, 16), ones);
-  // uint32_t rounding_bias = 0x7fff + lsb;
-  t_value = _mm512_add_epi32(t_value, vec_bias);
-  // input += rounding_bias;
-  t_value = _mm512_add_epi32(t_value, value);
-  // input = input >> 16;
-  t_value = _mm512_srli_epi32(t_value, 16);
-  // Check NaN before converting back to bf16
-  t_value = _mm512_mask_blend_epi32(mask_value, nan, t_value);
-  return _mm512_cvtusepi32_epi16(t_value);
-#endif
-}
-
-inline void cvt_fp32_to_bf16(
-    BFloat16* dst,
-    const float* src,
-    int m,
-    int n,
-    int ld_dst,
-    int ld_src) {
-  for (int i = 0; i < m; i++) {
-    const float* src0 = src + i * ld_src;
-    BFloat16* dst0 = dst + i * ld_dst;
-    int j;
-    for (j = 0; j < n - 15; j += 16) {
-      auto f32 = _mm512_loadu_ps(src0 + j);
-      _mm256_storeu_si256((__m256i*)(dst0 + j), cvt_fp32_to_bf16(f32));
-    }
-    if (j < n) {
-      auto mask = (1 << (n - j)) - 1;
-      auto f32 = _mm512_maskz_loadu_ps(mask, src0 + j);
-      _mm256_mask_storeu_epi16(dst0 + j, mask, cvt_fp32_to_bf16(f32));
-    }
-  }
-}
-
-inline __m512 cvt_bf16_to_fp32(const __m256i src) {
-  auto y = _mm512_cvtepu16_epi32(src);
-  return _mm512_castsi512_ps(_mm512_bslli_epi128(y, 2));
-}
-
-inline void cvt_bf16_to_fp32(
-    float* dst,
-    const at::BFloat16* src,
-    int m,
-    int n,
-    int ld_dst,
-    int ld_src) {
-  for (int i = 0; i < m; i++) {
-    float* dst0 = dst + i * ld_dst;
-    const BFloat16* src0 = src + i * ld_src;
-    int j = 0;
-    for (; j < n - 15; j += 16) {
-      auto f32 = cvt_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(src0 + j)));
-      _mm512_storeu_ps(dst0 + j, f32);
-    }
-    if (j < n) {
-      auto mask = (1 << (n - j)) - 1;
-      auto f32 = cvt_bf16_to_fp32(_mm256_maskz_loadu_epi16(mask, src0 + j));
-      _mm512_mask_storeu_ps(dst0 + j, mask, f32);
-    }
+#define INDEX(x, y, ld) ((x) * (ld) + (y))
+#define ADDRESS(p, x, y, ld) ((p) + (x) * (ld) + (y))
+// A class for forced loop unrolling at compile time
+// These macro utils and the small gemm intrinsics kernels are implemented
+// based on the initial code by pujiang.he@intel.com.
+template <int i>
+struct compile_time_for {
+  template <typename Lambda, typename... Args>
+  inline static void op(const Lambda& function, Args... args) {
+    compile_time_for<i - 1>::op(function, args...);
+    function(std::integral_constant<int, i - 1>{}, args...);
   }
-}
-
-void print_m512(__m512 reg) {
-  float temp[16];
-  _mm512_store_ps(temp, reg);
-  for (int i = 0; i < 16; ++i) {
-    printf("%.2f ", temp[i]);
+};
+template <>
+struct compile_time_for<1> {
+  template <typename Lambda, typename... Args>
+  inline static void op(const Lambda& function, Args... args) {
+    function(std::integral_constant<int, 0>{}, args...);
   }
-  printf("\n");
-}
-
-void print_m512i(__m512i reg) {
-  int temp[16];
-  _mm512_store_epi32(temp, reg);
-  for (int i = 0; i < 16; ++i) {
-    printf("%08X ", temp[i]);
+};
+template <>
+struct compile_time_for<0> {
+  // 0 loops, do nothing
+  template <typename Lambda, typename... Args>
+  inline static void op(const Lambda& function, Args... args) {}
+};
+// Get mask for last column
+template <int EXPANDED_N, int col>
+constexpr inline unsigned short get_mask(unsigned short mask) {
+  // Not last column, return 0xffffff indicating load/store all 16 floats
+  if constexpr (col < EXPANDED_N / 16 - 1)
+    return (unsigned short)0xffff;
+  else
+    return mask;
+}
+
+template <int EXPANDED_N>
+constexpr inline unsigned short get_mask(int col, unsigned short mask) {
+  // Not last column, return 0xffffff indicating load/store all 16 floats
+  if (col < EXPANDED_N / 16 - 1)
+    return (unsigned short)0xffff;
+  else
+    return mask;
+}
+class IdentityOP {
+ public:
+  __m512 operator()(__m512& v, __mmask16 mask, int row, int col) const {
+    return v;
   }
-  printf("\n");
-}
+};
 
-void print_m128i(__m128i* reg) {
-  uint8_t temp[32];
-  _mm_store_si128((__m128i*)temp, *reg);
-  for (int i = 0; i < 32; ++i) {
-    std::bitset<8> b(temp[i]);
-    std::cout << b << " ";
-  }
-  std::cout << std::endl;
-}
 // This function is for the case of very small M.
 // LINES should be  smaller than 4.
 // N must be smaller than 64 and must be multiple of 16.
 // PREFETCH_K_DIST means prefetch distance in K.
 // ACC means accumulate to C or not.
-// actualN , rowOff are not used in current code version.
+// actualN , rowOff and postop are not used in current code version.
 template <
     int LINES,
     int N,
     int PREFETCH_K_DIST,
     bool ACC,
-    bool bias_add = false>
+    typename Lambda = IdentityOP>
 void small_gemm_smallm(
     const float* A,
     const int8_t* B,
@@ -197,10 +90,10 @@ void small_gemm_smallm(
     int ldc,
     int actualN,
     int K,
-    float* scale,
     float* zero_point,
-    float* bias = NULL,
-    int rowOff = 0) {
+    float* scale,
+    int rowOff = 0,
+    const Lambda& postop = IdentityOP()) {
   constexpr const int COLS = N / 16;
 
   __m512 va;
@@ -265,11 +158,11 @@ void small_gemm_smallm(
   auto store = [&](auto i) {
     constexpr const int line = i / COLS;
     constexpr const int col = i % COLS;
-    if constexpr (bias_add) {
-      __m512 bias_ = _mm512_loadu_ps(bias + col * 16);
-      vc[i] = _mm512_add_ps(vc[i], bias_);
+    if constexpr (std::is_same<Lambda, IdentityOP>::value) {
       _mm512_storeu_ps(ADDRESS(C, line, col * 16, ldc), vc[i]);
     } else {
+      // Apply post op
+      vc[i] = postop(vc[i], 0xffff, rowOff + line, col * 16);
       _mm512_storeu_ps(ADDRESS(C, line, col * 16, ldc), vc[i]);
     }
   };
@@ -277,15 +170,226 @@ void small_gemm_smallm(
   compile_time_for<LINES * COLS>::op(store);
 }
 
-// bf16 * int8 -> fp32
+inline void dequant_(
+    int8_t* B,
+    float* b,
+    __m512 float_zero_point,
+    __m512 float_scale) {
+  const __m128i b_ = _mm_loadu_si128((const __m128i*)B);
+  __m512 vb;
+  vb = _mm512_cvtepi32_ps(_mm512_cvtepi8_epi32(b_));
+  vb = _mm512_sub_ps(vb, float_zero_point);
+  vb = _mm512_mul_ps(vb, float_scale);
+  _mm512_storeu_ps(b, vb);
+}
+
+inline void dequant_(
+    int8_t* B,
+    float* b,
+    __m512 float_zero_point,
+    __m512 float_scale,
+    unsigned short mask) {
+  const __m128i b_ = _mm_maskz_loadu_epi8(mask, (const __m128i*)B);
+  __m512 vb;
+  vb = _mm512_cvtepi32_ps(_mm512_cvtepi8_epi32(b_));
+  vb = _mm512_maskz_sub_ps(mask, vb, float_zero_point);
+  vb = _mm512_maskz_mul_ps(mask, vb, float_scale);
+  _mm512_mask_storeu_ps(b, mask, vb);
+}
+
+// per channel
+// B is packed and not transposed, shape:[BLOCK_K x BLOCK_N]
+template <int BLOCK_K, int BLOCK_N>
+void dequant(int8_t* B, float* b, float* zero_point, float* scale) {
+  const int COLS = BLOCK_N / 16;
+  __m512 float_scale = _mm512_loadu_ps(scale);
+  __m512 float_zero_point = _mm512_loadu_ps(zero_point);
+  for (int k = 0; k < BLOCK_K; k++) {
+    int8_t* src = B;
+    float* dst = b;
+    int j, idx;
+    for (idx = 0, j = 0; j < COLS * 16; j += 16) {
+      __m512 float_scale = _mm512_loadu_ps(scale + j);
+      __m512 float_zero_point = _mm512_loadu_ps(zero_point + j);
+      dequant_(src, dst, float_zero_point, float_scale);
+      src += 16;
+      dst += 16;
+    }
+    if (j < BLOCK_N) {
+      const int res = BLOCK_N - j;
+      unsigned short mask = 0xffff;
+      mask = (1 << res) - 1;
+      __m512 float_scale = _mm512_maskz_loadu_ps(mask, scale + j);
+      __m512 float_zero_point = _mm512_maskz_loadu_ps(mask, zero_point + j);
+      dequant_(src, dst, float_zero_point, float_scale, mask);
+    }
+    B += BLOCK_N;
+    b += BLOCK_N;
+  }
+}
+
+// per channel
+// handle edge cases
+void dequant(
+    int8_t* B,
+    float* b,
+    int K,
+    int N,
+    float* zero_point,
+    float* scale) {
+  const int COLS = N / 16;
+  for (int k = 0; k < K; k++) {
+    int8_t* src = B;
+    float* dst = b;
+    int j;
+    for (j = 0; j < COLS * 16; j += 16) {
+      __m512 float_scale = _mm512_loadu_ps(scale + j);
+      __m512 float_zero_point = _mm512_loadu_ps(zero_point + j);
+      dequant_(src, dst, float_zero_point, float_scale);
+      src += 16;
+      dst += 16;
+    }
+    if (j < N) {
+      const int res = N - j;
+      unsigned short mask = 0xffff;
+      mask = (1 << res) - 1;
+      __m512 float_scale = _mm512_maskz_loadu_ps(mask, scale + j);
+      __m512 float_zero_point = _mm512_maskz_loadu_ps(mask, zero_point + j);
+      dequant_(src, dst, float_zero_point, float_scale, mask);
+    }
+    B += N;
+    b += N;
+  }
+}
+
+// per tensor
+// B is packed and not transposed, shape:[BLOCK_K x BLOCK_N]
+template <int BLOCK_K, int BLOCK_N>
+void dequant(int8_t* B, float* b, float zero_point, float scale) {
+  __m512 float_scale = _mm512_set1_ps(scale);
+  __m512 float_zero_point = _mm512_set1_ps(zero_point);
+  int COLS = BLOCK_N / 16;
+  for (int k = 0; k < BLOCK_K; k++) {
+    int8_t* src = B;
+    float* dst = b;
+    int j;
+    for (j = 0; j < COLS * 16; j += 16) {
+      dequant_(src, dst, float_zero_point, float_scale);
+      src += 16;
+      dst += 16;
+    }
+    if (j < BLOCK_N) { // elements < 16
+      const int res = BLOCK_N - j;
+      unsigned short mask = 0xffff;
+      mask = (1 << res) - 1;
+      dequant_(src, dst, float_zero_point, float_scale, mask);
+    }
+    B += BLOCK_N;
+    b += BLOCK_N;
+  }
+}
+
+// per tensor
+// handle edge cases
+void dequant(int8_t* B, float* b, int K, int N, float zero_point, float scale) {
+  __m512 float_scale = _mm512_set1_ps(scale);
+  __m512 float_zero_point = _mm512_set1_ps(zero_point);
+  int COLS = N / 16;
+  for (int k = 0; k < K; k++) {
+    int8_t* src = B;
+    float* dst = b;
+    int j;
+    for (j = 0; j < COLS * 16; j += 16) {
+      dequant_(src, dst, float_zero_point, float_scale);
+      src += 16;
+      dst += 16;
+    }
+    if (j < N) { // elements < 16
+      const int res = N - j;
+      unsigned short mask = 0xffff;
+      mask = (1 << res) - 1;
+      dequant_(src, dst, float_zero_point, float_scale, mask);
+    }
+    B += N;
+    b += N;
+  }
+}
+
+void add_bias(float* C, float* bias, int M, int N, int ldc) {
+  int COLS = N / 16;
+  int j;
+  for (j = 0; j < COLS * 16; j += 16) {
+    __m512 float_bias = _mm512_loadu_ps(bias + j);
+    float* c = C + j;
+    for (int m = 0; m < M; m++) {
+      __m512 vc = _mm512_loadu_ps(c);
+      vc = _mm512_add_ps(vc, float_bias);
+      _mm512_storeu_ps(c, vc);
+      c += ldc;
+    }
+  }
+  if (j < N) {
+    const int res = N - j;
+    unsigned short mask = 0xffff;
+    mask = (1 << res) - 1;
+    __m512 float_bias = _mm512_maskz_loadu_ps(mask, bias + j);
+    float* c = C + j;
+    for (int m = 0; m < M; m++) {
+      __m512 vc = _mm512_maskz_loadu_ps(mask, c);
+      vc = _mm512_mask_add_ps(vc, mask, vc, float_bias);
+      _mm512_mask_storeu_ps(c, mask, vc);
+      c += ldc;
+    }
+  }
+}
+
+#else // not support AVX512
+
+// per channel
+template <int BLOCK_N, int BLOCK_K>
+void dequant(int8_t* B, float* b, float* zero_point, float* scale) {
+  AT_ASSERTM(false, "Unable to support AVX512!");
+}
+
+void dequant(
+    int8_t* B,
+    float* b,
+    int K,
+    int N,
+    float* zero_point,
+    float* scale) {
+  AT_ASSERTM(false, "Unable to support AVX512!");
+}
+
+// per tensor
+template <int BLOCK_N, int BLOCK_K>
+void dequant(int8_t* B, float* b, float zero_point, float scale) {
+  AT_ASSERTM(false, "Unable to support AVX512!");
+}
+
+void dequant(int8_t* B, float* b, int K, int N, float zero_point, float scale) {
+  AT_ASSERTM(false, "Unable to support AVX512!");
+}
+
+void add_bias(float* C, float* bias, int M, int N, int ldc) {
+  AT_ASSERTM(false, "Unable to support AVX512!");
+}
+
+class IdentityOP {
+ public:
+  int operator()(int col) const {
+    AT_ASSERTM(false, "Unable to support AVX512!");
+  }
+};
+
 template <
     int LINES,
     int N,
     int PREFETCH_K_DIST,
     bool ACC,
-    bool bias_add = false>
+    typename Lambda = IdentityOP>
 void small_gemm_smallm(
-    const BFloat16* A,
+    const float* A,
     const int8_t* B,
     float* C,
     int lda,
@@ -293,1765 +397,217 @@ void small_gemm_smallm(
     int ldc,
     int actualN,
     int K,
-    float* scale,
     float* zero_point,
-    float* bias = NULL,
-    int rowOff = 0) {
-  constexpr const int COLS = N / 16;
+    float* scale,
+    int rowOff = 0,
+    const Lambda& postop = IdentityOP()) {
+  AT_ASSERTM(false, "Unable to support AVX512!");
+}
+#endif
 
-  __m512 va;
-  __m512 vb[COLS];
-  __m512 vc[LINES * COLS];
-  __m512 float_scale[COLS];
-  __m512 float_zero_point[COLS];
+const int BLOCK_N = 64, BLOCK_K = 64, PREFETCH_K = 64;
 
-  // Load scale
-  auto load_scale = [&](auto i) {
-    float_scale[i] = _mm512_loadu_ps(scale + 16 * i);
-  };
-  compile_time_for<COLS>::op(load_scale);
+struct DotMicroKernelKey {
+  bool trans_a;
+  bool trans_b;
+  int lda;
+  int ldb;
+  int ldc;
 
-  // Load zero point
-  auto load_zp = [&](auto i) {
-    float_zero_point[i] = _mm512_loadu_ps(zero_point + 16 * i);
-  };
-  compile_time_for<COLS>::op(load_zp);
+  DotMicroKernelKey(bool trans_a, bool trans_b, int lda, int ldb, int ldc)
+      : trans_a(trans_a), trans_b(trans_b), lda(lda), ldb(ldb), ldc(ldc) {}
 
-  // Load from C or set to 0
-  if constexpr (ACC) {
-    auto loadc = [&](auto i) {
-      constexpr const int row = i / COLS;
-      constexpr const int col = i % COLS;
-      vc[i] = _mm512_loadu_ps(ADDRESS(C, row, col * 16, ldc));
-    };
-    compile_time_for<LINES * COLS>::op(loadc);
-  } else {
-    auto set0 = [&](auto i) { vc[i] = _mm512_setzero_ps(); };
-    compile_time_for<LINES * COLS>::op(set0);
-  }
-
-  auto compute = [&](auto i, int k) {
-    constexpr const int row = i / COLS;
-    constexpr const int col = i % COLS;
-
-    if constexpr (col == 0) {
-      float aa = *ADDRESS(A, row, k, lda); // convert from bf16 to fp32
-      va = _mm512_set1_ps(aa);
-    }
-
-    if constexpr (row == 0) {
-      const __m128i b_ =
-          _mm_loadu_si128((const __m128i*)ADDRESS(B, k, col * 16, ldb));
-      _mm_prefetch(ADDRESS(B, k + PREFETCH_K_DIST, col * 16, ldb), _MM_HINT_T0);
-      vb[col] = _mm512_cvtepi32_ps(_mm512_cvtepi8_epi32(b_));
-      vb[col] = _mm512_sub_ps(vb[col], float_zero_point[col]);
-      vb[col] = _mm512_mul_ps(vb[col], float_scale[col]);
-    }
-
-    constexpr const int idx = INDEX(row, col, COLS);
-    vc[idx] = _mm512_fmadd_ps(va, vb[col], vc[idx]);
-  };
-
-// Accumulate along k
-#pragma unroll(4)
-  for (int k = 0; k < K; ++k) {
-    compile_time_for<LINES * COLS>::op(compute, k);
-  }
-
-  // Store to C
-  auto store = [&](auto i) {
-    constexpr const int line = i / COLS;
-    constexpr const int col = i % COLS;
-    if constexpr (bias_add) {
-      __m512 bias_ = _mm512_loadu_ps(bias + col * 16);
-      vc[i] = _mm512_add_ps(vc[i], bias_);
-      _mm512_storeu_ps(ADDRESS(C, line, col * 16, ldc), vc[i]);
-    } else {
-      _mm512_storeu_ps(ADDRESS(C, line, col * 16, ldc), vc[i]);
-    }
-  };
-
-  compile_time_for<LINES * COLS>::op(store);
-}
-
-static inline __m128i bytesFromNibbles(const uint8_t* rsi) {
-  __m128i tmp = _mm_loadu_si64((const __m128i*)rsi);
-  __m128i bytes = _mm_cvtepu8_epi16(tmp);
-  const __m128i lowMask = _mm_set1_epi8(0xF);
-  __m128i high = _mm_andnot_si128(lowMask, bytes);
-  __m128i low = _mm_and_si128(lowMask, bytes);
-  high = _mm_slli_epi16(high, 4);
-  bytes = _mm_or_si128(low, high);
-  return bytes;
-}
-
-// fp32 * int4 -> fp32
-template <
-    int LINES,
-    int N,
-    int PREFETCH_K_DIST,
-    bool ACC,
-    bool bias_add = false>
-void small_gemm_smallm(
-    const float* A,
-    const uint8_t* B,
-    float* C,
-    int lda,
-    int ldb,
-    int ldc,
-    int actualN,
-    int K,
-    float* scale,
-    float* zero_point,
-    float* bias = NULL,
-    int rowOff = 0) {
-  constexpr int COLS = N / 16;
-  __m512 va;
-  __m512 vb[COLS];
-  __m512 vc[LINES * COLS];
-  __m512 float_scale[COLS];
-  __m512 float_zero_point[COLS];
-  // lookup table converting uint8 to float, 15.0f - 0.0f
-  __m512 lut = _mm512_set_ps(
-      15.0f,
-      14.0f,
-      13.0f,
-      12.0f,
-      11.0f,
-      10.0f,
-      9.0f,
-      8.0f,
-      7.0f,
-      6.0f,
-      5.0f,
-      4.0f,
-      3.0f,
-      2.0f,
-      1.0f,
-      0.0f);
-
-  // Load scale
-  auto load_scale = [&](auto i) {
-    float_scale[i] = _mm512_loadu_ps(scale + 16 * i);
-  };
-  compile_time_for<COLS>::op(load_scale);
-
-  // Load zero point
-  auto load_zp = [&](auto i) {
-    float_zero_point[i] = _mm512_loadu_ps(zero_point + 16 * i);
-  };
-  compile_time_for<COLS>::op(load_zp);
-
-  // Load from C or set to 0
-  if constexpr (ACC) {
-    auto loadc = [&](auto i) {
-      constexpr const int row = i / COLS;
-      constexpr const int col = i % COLS;
-      vc[i] = _mm512_loadu_ps(ADDRESS(C, row, col * 16, ldc));
-    };
-    compile_time_for<LINES * COLS>::op(loadc);
-  } else {
-    auto set0 = [&](auto i) { vc[i] = _mm512_setzero_ps(); };
-    compile_time_for<LINES * COLS>::op(set0);
-  }
-
-  auto compute = [&](auto i, int k) {
-    constexpr const int row = i / COLS;
-    constexpr const int col = i % COLS;
-
-    if constexpr (col == 0) {
-      va = _mm512_set1_ps(*ADDRESS(A, row, k, lda));
-    }
-
-#define ALGORITHM 1
-
-#if ALGORITHM == 0
-    if constexpr (row == 0) {
-      __m128i b_ =
-          bytesFromNibbles(ADDRESS(B, k, col * 8, ldb / 2)); // int4 -> int8
-      _mm_prefetch(
-          ADDRESS(B, k + PREFETCH_K_DIST, col * 8, ldb / 2), _MM_HINT_T0);
-      vb[col] = _mm512_cvtepi32_ps(_mm512_cvtepu8_epi32(b_));
-      vb[col] = _mm512_sub_ps(vb[col], float_zero_point[col]);
-      vb[col] = _mm512_mul_ps(vb[col], float_scale[col]);
-    }
-#else
-    // GCC < 11.3 internal compiler error with constexpr
-    if (col == 0 && row == 0) {
-      static_assert(COLS == 4, "expect register block size 4 for weights");
-      _mm_prefetch(ADDRESS(B, k + PREFETCH_K_DIST, 0, ldb / 2), _MM_HINT_T0);
-      // load 64 elements from ADDRESS(B, k, 0 ldb / 2) with 4-bit each
-      // and then, unpack them and convert them into 64 fp32 numbers held in
-      // four avx512 registers: vb[0] - vb[3]
-      // Load the buffer into a 256-bit register
-      __m256i packed = _mm256_load_si256((__m256i*)ADDRESS(B, k, 0, ldb / 2));
-      __m512i int32[4];
-      {
-        auto low_4bit = _mm512_cvtepu8_epi32(_mm256_castsi256_si128(packed));
-        auto high_4bit = _mm512_srli_epi32(low_4bit, 4);
-        int32[0] = low_4bit;
-        int32[2] = high_4bit;
-      }
-      {
-        auto low_4bit =
-            _mm512_cvtepu8_epi32(_mm256_extracti128_si256(packed, 1));
-        auto high_4bit = _mm512_srli_epi32(low_4bit, 4);
-        int32[1] = low_4bit;
-        int32[3] = high_4bit;
-      }
-
-      auto dequant_int32 = [&](auto idx) {
-        vb[idx] = _mm512_permutexvar_ps(int32[idx], lut);
-        vb[idx] = _mm512_sub_ps(vb[idx], float_zero_point[idx]);
-        vb[idx] = _mm512_mul_ps(vb[idx], float_scale[idx]);
-      };
-      compile_time_for<COLS>::op(dequant_int32);
-    }
-#endif
-
-    constexpr const int idx = INDEX(row, col, COLS);
-    vc[idx] = _mm512_fmadd_ps(va, vb[col], vc[idx]);
-  };
-
-// Accumulate along k
-#pragma unroll(4)
-  for (int k = 0; k < K; ++k) {
-    compile_time_for<LINES * COLS>::op(compute, k);
-  }
-
-  // Store to C
-  auto store = [&](auto i) {
-    constexpr const int line = i / COLS;
-    constexpr const int col = i % COLS;
-    if constexpr (bias_add) {
-      __m512 bias_ = _mm512_loadu_ps(bias + col * 16);
-      vc[i] = _mm512_add_ps(vc[i], bias_);
-      _mm512_storeu_ps(ADDRESS(C, line, col * 16, ldc), vc[i]);
-    } else {
-      _mm512_storeu_ps(ADDRESS(C, line, col * 16, ldc), vc[i]);
-    }
-  };
-
-  compile_time_for<LINES * COLS>::op(store);
-}
-
-// bf16 * int4 -> fp32
-template <
-    int LINES,
-    int N,
-    int PREFETCH_K_DIST,
-    bool ACC,
-    bool bias_add = false>
-void small_gemm_smallm(
-    const BFloat16* A,
-    const uint8_t* B,
-    float* C,
-    int lda,
-    int ldb,
-    int ldc,
-    int actualN,
-    int K,
-    float* scale,
-    float* zero_point,
-    float* bias = NULL,
-    int rowOff = 0) {
-  constexpr int COLS = N / 16;
-  __m512 va;
-  __m512 vb[COLS];
-  __m512 vc[LINES * COLS];
-  __m512 float_scale[COLS];
-  __m512 float_zero_point[COLS];
-  // lookup table converting uint8 to float, 15.0f - 0.0f
-  __m512 lut = _mm512_set_ps(
-      15.0f,
-      14.0f,
-      13.0f,
-      12.0f,
-      11.0f,
-      10.0f,
-      9.0f,
-      8.0f,
-      7.0f,
-      6.0f,
-      5.0f,
-      4.0f,
-      3.0f,
-      2.0f,
-      1.0f,
-      0.0f);
-
-  // Load scale
-  auto load_scale = [&](auto i) {
-    float_scale[i] = _mm512_loadu_ps(scale + 16 * i);
-  };
-  compile_time_for<COLS>::op(load_scale);
-
-  // Load zero point
-  auto load_zp = [&](auto i) {
-    float_zero_point[i] = _mm512_loadu_ps(zero_point + 16 * i);
-  };
-  compile_time_for<COLS>::op(load_zp);
-
-  // Load from C or set to 0
-  if constexpr (ACC) {
-    auto loadc = [&](auto i) {
-      constexpr const int row = i / COLS;
-      constexpr const int col = i % COLS;
-      vc[i] = _mm512_loadu_ps(ADDRESS(C, row, col * 16, ldc));
-    };
-    compile_time_for<LINES * COLS>::op(loadc);
-  } else {
-    auto set0 = [&](auto i) { vc[i] = _mm512_setzero_ps(); };
-    compile_time_for<LINES * COLS>::op(set0);
-  }
-
-  auto compute = [&](auto i, int k) {
-    constexpr const int row = i / COLS;
-    constexpr const int col = i % COLS;
-
-    if constexpr (col == 0) {
-      float aa = *ADDRESS(A, row, k, lda); // convert from bf16 to fp32
-      va = _mm512_set1_ps(aa);
-    }
-
-#define ALGORITHM 1
-
-#if ALGORITHM == 0
-    if constexpr (row == 0) {
-      __m128i b_ =
-          bytesFromNibbles(ADDRESS(B, k, col * 8, ldb / 2)); // int4 -> int8
-      _mm_prefetch(
-          ADDRESS(B, k + PREFETCH_K_DIST, col * 8, ldb / 2), _MM_HINT_T0);
-      vb[col] = _mm512_cvtepi32_ps(_mm512_cvtepu8_epi32(b_));
-      vb[col] = _mm512_sub_ps(vb[col], float_zero_point[col]);
-      vb[col] = _mm512_mul_ps(vb[col], float_scale[col]);
-    }
-#else
-    // GCC < 11.3 internal compiler error with constexpr
-    if (col == 0 && row == 0) {
-      static_assert(COLS == 4, "expect register block size 4 for weights");
-      _mm_prefetch(ADDRESS(B, k + PREFETCH_K_DIST, 0, ldb / 2), _MM_HINT_T0);
-      // load 64 elements from ADDRESS(B, k, 0 ldb / 2) with 4-bit each
-      // and then, unpack them and convert them into 64 fp32 numbers held in
-      // four avx512 registers: vb[0] - vb[3]
-      // Load the buffer into a 256-bit register
-      __m256i packed = _mm256_load_si256((__m256i*)ADDRESS(B, k, 0, ldb / 2));
-      __m512i int32[4];
-      {
-        auto low_4bit = _mm512_cvtepu8_epi32(_mm256_castsi256_si128(packed));
-        auto high_4bit = _mm512_srli_epi32(low_4bit, 4);
-        int32[0] = low_4bit;
-        int32[2] = high_4bit;
-      }
-      {
-        auto low_4bit =
-            _mm512_cvtepu8_epi32(_mm256_extracti128_si256(packed, 1));
-        auto high_4bit = _mm512_srli_epi32(low_4bit, 4);
-        int32[1] = low_4bit;
-        int32[3] = high_4bit;
-      }
-
-      auto dequant_int32 = [&](auto idx) {
-        vb[idx] = _mm512_permutexvar_ps(int32[idx], lut);
-        vb[idx] = _mm512_sub_ps(vb[idx], float_zero_point[idx]);
-        vb[idx] = _mm512_mul_ps(vb[idx], float_scale[idx]);
-      };
-      compile_time_for<COLS>::op(dequant_int32);
-    }
-#endif
-
-    constexpr const int idx = INDEX(row, col, COLS);
-    vc[idx] = _mm512_fmadd_ps(va, vb[col], vc[idx]);
-  };
-
-// Accumulate along k
-#pragma unroll(4)
-  for (int k = 0; k < K; ++k) {
-    compile_time_for<LINES * COLS>::op(compute, k);
-  }
-
-  // Store to C
-  auto store = [&](auto i) {
-    constexpr const int line = i / COLS;
-    constexpr const int col = i % COLS;
-    if constexpr (bias_add) {
-      __m512 bias_ = _mm512_loadu_ps(bias + col * 16);
-      vc[i] = _mm512_add_ps(vc[i], bias_);
-      _mm512_storeu_ps(ADDRESS(C, line, col * 16, ldc), vc[i]);
-    } else {
-      _mm512_storeu_ps(ADDRESS(C, line, col * 16, ldc), vc[i]);
-    }
-  };
-
-  compile_time_for<LINES * COLS>::op(store);
-}
-
-inline void dequant_(
-    int8_t* B,
-    float* b,
-    __m512 float_scale,
-    __m512 float_zero_point) {
-  const __m128i b_ = _mm_loadu_si128((const __m128i*)B);
-  __m512 vb;
-  vb = _mm512_cvtepi32_ps(_mm512_cvtepi8_epi32(b_));
-  vb = _mm512_sub_ps(vb, float_zero_point);
-  vb = _mm512_mul_ps(vb, float_scale);
-  _mm512_storeu_ps(b, vb);
-}
-
-inline void dequant_(
-    int8_t* B,
-    float* b,
-    __m512 float_scale,
-    __m512 float_zero_point,
-    unsigned short mask) {
-  const __m128i b_ = _mm_maskz_loadu_epi8(mask, (const __m128i*)B);
-  __m512 vb;
-  vb = _mm512_cvtepi32_ps(_mm512_cvtepi8_epi32(b_));
-  vb = _mm512_maskz_sub_ps(mask, vb, float_zero_point);
-  vb = _mm512_maskz_mul_ps(mask, vb, float_scale);
-  _mm512_mask_storeu_ps(b, mask, vb);
-}
-
-inline void dequant_(
-    uint8_t* B,
-    float* b,
-    __m512 float_scale,
-    __m512 float_zero_point) {
-  __m128i b_ = bytesFromNibbles(B); // 32 int4 -> 32 int8
-  __m512 vb;
-  vb = _mm512_mul_ps(
-      _mm512_sub_ps(
-          _mm512_cvtepi32_ps(_mm512_cvtepu8_epi32(b_)), float_zero_point),
-      float_scale);
-  _mm512_storeu_ps(b, vb);
-}
-
-inline void dequant_to_bf16_(
-    uint8_t* B,
-    BFloat16* b,
-    __m512 float_scale,
-    __m512 float_zero_point) {
-  __m128i b_ = bytesFromNibbles(B); // 32 int4 -> 32 int8
-  __m512 vb;
-  vb = _mm512_mul_ps(
-      _mm512_sub_ps(
-          _mm512_cvtepi32_ps(_mm512_cvtepu8_epi32(b_)), float_zero_point),
-      float_scale);
-  _mm256_storeu_si256((__m256i*)b, cvt_fp32_to_bf16(vb));
-}
-
-inline void dequant_to_bf16_(
-    int8_t* B,
-    BFloat16* b,
-    __m512 float_scale,
-    __m512 float_zero_point) {
-  const __m128i b_ = _mm_loadu_si128((const __m128i*)B);
-  __m512 vb;
-  vb = _mm512_cvtepi32_ps(_mm512_cvtepi8_epi32(b_));
-  vb = _mm512_sub_ps(vb, float_zero_point);
-  vb = _mm512_mul_ps(vb, float_scale);
-  _mm256_storeu_si256((__m256i*)b, cvt_fp32_to_bf16(vb));
-}
-
-inline void dequant_to_bf16_(
-    int8_t* B,
-    BFloat16* b,
-    __m512 float_scale,
-    __m512 float_zero_point,
-    unsigned short mask) {
-  const __m128i b_ = _mm_maskz_loadu_epi8(mask, (const __m128i*)B);
-  __m512 vb;
-  vb = _mm512_cvtepi32_ps(_mm512_cvtepi8_epi32(b_));
-  vb = _mm512_maskz_sub_ps(mask, vb, float_zero_point);
-  vb = _mm512_maskz_mul_ps(mask, vb, float_scale);
-  _mm256_mask_storeu_epi16(b, mask, cvt_fp32_to_bf16(vb));
-}
-
-// per channel
-// B is packed and not transposed, shape:[BLOCK_K x BLOCK_N]
-template <int BLOCK_K, int BLOCK_N>
-void dequant(int8_t* B, float* b, float* scale, float* zero_point) {
-  const int COLS = BLOCK_N / 16;
-  for (int k = 0; k < BLOCK_K; k++) {
-    int8_t* src = B;
-    float* dst = b;
-    int j, idx;
-    for (idx = 0, j = 0; j < COLS * 16; j += 16) {
-      __m512 float_scale = _mm512_loadu_ps(scale + j);
-      __m512 float_zero_point = _mm512_loadu_ps(zero_point + j);
-      dequant_(src, dst, float_scale, float_zero_point);
-      src += 16;
-      dst += 16;
-    }
-    if (j < BLOCK_N) {
-      const int res = BLOCK_N - j;
-      unsigned short mask = 0xffff;
-      mask = (1 << res) - 1;
-      __m512 float_scale = _mm512_maskz_loadu_ps(mask, scale + j);
-      __m512 float_zero_point = _mm512_maskz_loadu_ps(mask, zero_point + j);
-      dequant_(src, dst, float_scale, float_zero_point, mask);
-    }
-    B += BLOCK_N;
-    b += BLOCK_N;
-  }
-}
-
-// per channel dequant to fp32
-// handle edge cases
-void dequant(
-    int8_t* B,
-    float* b,
-    int K,
-    int N,
-    float* scale,
-    float* zero_point) {
-  const int COLS = N / 16;
-  for (int k = 0; k < K; k++) {
-    int8_t* src = B;
-    float* dst = b;
-    int j;
-    for (j = 0; j < COLS * 16; j += 16) {
-      __m512 float_scale = _mm512_loadu_ps(scale + j);
-      __m512 float_zero_point = _mm512_loadu_ps(zero_point + j);
-      dequant_(src, dst, float_scale, float_zero_point);
-      src += 16;
-      dst += 16;
-    }
-    if (j < N) {
-      const int res = N - j;
-      unsigned short mask = 0xffff;
-      mask = (1 << res) - 1;
-      __m512 float_scale = _mm512_maskz_loadu_ps(mask, scale + j);
-      __m512 float_zero_point = _mm512_maskz_loadu_ps(mask, zero_point + j);
-      dequant_(src, dst, float_scale, float_zero_point, mask);
-    }
-    B += N;
-    b += N;
-  }
-}
-
-// per channel dequant to bf16
-// handle edge cases
-void dequant(
-    int8_t* B,
-    BFloat16* b,
-    int K,
-    int N,
-    float* scale,
-    float* zero_point) {
-  const int COLS = N / 16;
-  for (int k = 0; k < K; k++) {
-    int8_t* src = B;
-    BFloat16* dst = b;
-    int j = 0;
-    for (; j < COLS * 16; j += 16) {
-      __m512 float_scale = _mm512_loadu_ps(scale + j);
-      __m512 float_zero_point = _mm512_loadu_ps(zero_point + j);
-      dequant_to_bf16_(src, dst, float_scale, float_zero_point);
-      src += 16;
-      dst += 16;
-    }
-    if (j < N) {
-      const int res = N - j;
-      unsigned short mask = 0xffff;
-      mask = (1 << res) - 1;
-      __m512 float_scale = _mm512_maskz_loadu_ps(mask, scale + j);
-      __m512 float_zero_point = _mm512_maskz_loadu_ps(mask, zero_point + j);
-      dequant_to_bf16_(src, dst, float_scale, float_zero_point, mask);
-    }
-    B += N;
-    b += N;
-  }
-}
-
-// per tensor
-// B is packed and not transposed, shape:[BLOCK_K x BLOCK_N]
-template <int BLOCK_K, int BLOCK_N>
-void dequant(int8_t* B, float* b, float scale, float zero_point) {
-  __m512 float_scale = _mm512_set1_ps(scale);
-  __m512 float_zero_point = _mm512_set1_ps(zero_point);
-  int COLS = BLOCK_N / 16;
-  for (int k = 0; k < BLOCK_K; k++) {
-    int8_t* src = B;
-    float* dst = b;
-    int j;
-    for (j = 0; j < COLS * 16; j += 16) {
-      dequant_(src, dst, float_scale, float_zero_point);
-      src += 16;
-      dst += 16;
-    }
-    if (j < BLOCK_N) { // elements < 16
-      const int res = BLOCK_N - j;
-      unsigned short mask = 0xffff;
-      mask = (1 << res) - 1;
-      dequant_(src, dst, float_scale, float_zero_point, mask);
-    }
-    B += BLOCK_N;
-    b += BLOCK_N;
-  }
-}
-
-// per tensor
-// handle edge cases
-void dequant(int8_t* B, float* b, int K, int N, float scale, float zero_point) {
-  __m512 float_scale = _mm512_set1_ps(scale);
-  __m512 float_zero_point = _mm512_set1_ps(zero_point);
-  int COLS = N / 16;
-  for (int k = 0; k < K; k++) {
-    int8_t* src = B;
-    float* dst = b;
-    int j;
-    for (j = 0; j < COLS * 16; j += 16) {
-      dequant_(src, dst, float_scale, float_zero_point);
-      src += 16;
-      dst += 16;
-    }
-    if (j < N) { // elements < 16
-      const int res = N - j;
-      unsigned short mask = 0xffff;
-      mask = (1 << res) - 1;
-      dequant_(src, dst, float_scale, float_zero_point, mask);
-    }
-    B += N;
-    b += N;
-  }
-}
-
-// per channel dequantize for int4
-// B is packed and not transposed, shape:[BLOCK_K x BLOCK_N]
-template <int BLOCK_K, int BLOCK_N>
-void dequant(uint8_t* B, float* b, float* scale, float* zero_point) {
-  if constexpr (BLOCK_N == 64) {
-    const int COLS = 4;
-    // lookup table converting uint8 to float, 15.0f - 0.0f
-    __m512 lut = _mm512_set_ps(
-        15.0f,
-        14.0f,
-        13.0f,
-        12.0f,
-        11.0f,
-        10.0f,
-        9.0f,
-        8.0f,
-        7.0f,
-        6.0f,
-        5.0f,
-        4.0f,
-        3.0f,
-        2.0f,
-        1.0f,
-        0.0f);
-    __m512 float_scale[4] = {
-        _mm512_loadu_ps(scale),
-        _mm512_loadu_ps(scale + 16),
-        _mm512_loadu_ps(scale + 32),
-        _mm512_loadu_ps(scale + 48)};
-    __m512 float_zero_point[4] = {
-        _mm512_loadu_ps(zero_point),
-        _mm512_loadu_ps(zero_point + 16),
-        _mm512_loadu_ps(zero_point + 32),
-        _mm512_loadu_ps(zero_point + 48)};
-    for (int k = 0; k < BLOCK_K; k++) {
-      uint8_t* src = B;
-      float* dst = b;
-      __m256i packed = _mm256_load_si256((__m256i*)src);
-      __m512i int32[4];
-      {
-        auto low_4bit = _mm512_cvtepu8_epi32(_mm256_castsi256_si128(packed));
-        auto high_4bit = _mm512_srli_epi32(low_4bit, 4);
-        int32[0] = low_4bit;
-        int32[2] = high_4bit;
-      }
-      {
-        auto low_4bit =
-            _mm512_cvtepu8_epi32(_mm256_extracti128_si256(packed, 1));
-        auto high_4bit = _mm512_srli_epi32(low_4bit, 4);
-        int32[1] = low_4bit;
-        int32[3] = high_4bit;
-      }
-      for (int idx = 0; idx < 4; idx++) {
-        __m512 vb = _mm512_permutexvar_ps(int32[idx], lut);
-        vb = _mm512_sub_ps(vb, float_zero_point[idx]);
-        vb = _mm512_mul_ps(vb, float_scale[idx]);
-        _mm512_storeu_ps(b + idx * 16, vb);
-      }
-      B += BLOCK_N / 2;
-      b += BLOCK_N;
-    }
-  } else {
-    const int COLS = BLOCK_N / 16;
-    for (int k = 0; k < BLOCK_K; k++) {
-      uint8_t* src = B;
-      float* dst = b;
-      int j, idx;
-      for (idx = 0, j = 0; j < COLS * 16; j += 16) {
-        __m512 float_scale = _mm512_loadu_ps(scale + j);
-        __m512 float_zero_point = _mm512_loadu_ps(zero_point + j);
-        dequant_(src, dst, float_scale, float_zero_point);
-        src += 8;
-        dst += 16;
-      }
-      if (j < BLOCK_N) {
-        const int res = BLOCK_N - j;
-        for (int l = 0; l < res; l += 2) {
-          const uint8_t vi = src[l / 2];
-          const int8_t vi0 = vi & 0xf;
-          const int8_t vi1 = vi >> 4;
-          const float v0 = (vi0 - zero_point[j + l]) * scale[j + l];
-          const float v1 = (vi1 - zero_point[j + l + 1]) * scale[j + l + 1];
-          dst[l + 0] = v0;
-          dst[l + 1] = v1;
-        }
-      }
-      B += BLOCK_N / 2;
-      b += BLOCK_N;
-    }
-  }
-}
-
-// per channel dequantize for int4
-// handle edge cases
-void dequant(
-    uint8_t* B,
-    float* b,
-    int K,
-    int N,
-    float* scale,
-    float* zero_point) {
-  if (N % 2 == 0) {
-    if (N == 64) {
-      const int COLS = 4;
-      // lookup table converting uint8 to float, 15.0f - 0.0f
-      __m512 lut = _mm512_set_ps(
-          15.0f,
-          14.0f,
-          13.0f,
-          12.0f,
-          11.0f,
-          10.0f,
-          9.0f,
-          8.0f,
-          7.0f,
-          6.0f,
-          5.0f,
-          4.0f,
-          3.0f,
-          2.0f,
-          1.0f,
-          0.0f);
-      __m512 float_scale[4] = {
-          _mm512_loadu_ps(scale),
-          _mm512_loadu_ps(scale + 16),
-          _mm512_loadu_ps(scale + 32),
-          _mm512_loadu_ps(scale + 48)};
-      __m512 float_zero_point[4] = {
-          _mm512_loadu_ps(zero_point),
-          _mm512_loadu_ps(zero_point + 16),
-          _mm512_loadu_ps(zero_point + 32),
-          _mm512_loadu_ps(zero_point + 48)};
-      for (int k = 0; k < K; k++) {
-        uint8_t* src = B;
-        float* dst = b;
-        __m256i packed = _mm256_load_si256((__m256i*)src);
-        __m512i int32[4];
-        {
-          auto low_4bit = _mm512_cvtepu8_epi32(_mm256_castsi256_si128(packed));
-          auto high_4bit = _mm512_srli_epi32(low_4bit, 4);
-          int32[0] = low_4bit;
-          int32[2] = high_4bit;
-        }
-        {
-          auto low_4bit =
-              _mm512_cvtepu8_epi32(_mm256_extracti128_si256(packed, 1));
-          auto high_4bit = _mm512_srli_epi32(low_4bit, 4);
-          int32[1] = low_4bit;
-          int32[3] = high_4bit;
-        }
-        for (int idx = 0; idx < 4; idx++) {
-          __m512 vb = _mm512_permutexvar_ps(int32[idx], lut);
-          vb = _mm512_sub_ps(vb, float_zero_point[idx]);
-          vb = _mm512_mul_ps(vb, float_scale[idx]);
-          _mm512_storeu_ps(b + idx * 16, vb);
-        }
-        B += N / 2;
-        b += N;
-      }
-    } else {
-      const int COLS = N / 16;
-      for (int k = 0; k < K; k++) {
-        uint8_t* src = B;
-        float* dst = b;
-        int j, idx;
-        for (idx = 0, j = 0; j < COLS * 16; j += 16) {
-          __m512 float_scale = _mm512_loadu_ps(scale + j);
-          __m512 float_zero_point = _mm512_loadu_ps(zero_point + j);
-          dequant_(src, dst, float_scale, float_zero_point);
-          src += 8;
-          dst += 16;
-        }
-        if (j < N) {
-          const int res = N - j;
-          int rr = res / 2;
-          int l = 0;
-          for (; l < rr * 2; l += 2) {
-            const uint8_t vi = src[l / 2];
-            const int8_t vi0 = vi & 0xf;
-            const int8_t vi1 = vi >> 4;
-            const float v0 = (vi0 - zero_point[j + l]) * scale[j + l];
-            const float v1 = (vi1 - zero_point[j + l + 1]) * scale[j + l + 1];
-            dst[l + 0] = v0;
-            dst[l + 1] = v1;
-          }
-          if (l < res) {
-            const uint8_t vi = src[l / 2];
-            const int8_t vi0 = vi & 0xf;
-            const float v0 = (vi0 - zero_point[j + l]) * scale[j + l];
-            dst[l + 0] = v0;
-          }
-        }
-        B += N / 2;
-        b += N;
-      }
-    }
-  } else {
-    int i = 0;
-    for (; i < K * N / 2 * 2; i += 2) {
-      const uint8_t vi = B[i / 2];
-      const int8_t vi0 = vi & 0xf;
-      const int8_t vi1 = vi >> 4;
-      const float v0 = (vi0 - zero_point[i % N]) * scale[i % N];
-      const float v1 = (vi1 - zero_point[(i + 1) % N]) * scale[(i + 1) % N];
-      b[i + 0] = v0;
-      b[i + 1] = v1;
-    }
-    if (i < K * N) {
-      const uint8_t vi = B[i / 2];
-      const int8_t vi0 = vi & 0xf;
-      const float v0 = (vi0 - zero_point[i % N]) * scale[i % N];
-      b[i + 0] = v0;
-    }
-  }
-}
-
-// dequant uint4 weight to bf16
-void dequant(
-    uint8_t* B,
-    BFloat16* b,
-    int K,
-    int N,
-    float* scale,
-    float* zero_point) {
-  if (N % 2 == 0) {
-    if (N == 64) {
-      const int COLS = 4;
-      // lookup table converting uint8 to float, 15.0f - 0.0f
-      __m512 lut = _mm512_set_ps(
-          15.0f,
-          14.0f,
-          13.0f,
-          12.0f,
-          11.0f,
-          10.0f,
-          9.0f,
-          8.0f,
-          7.0f,
-          6.0f,
-          5.0f,
-          4.0f,
-          3.0f,
-          2.0f,
-          1.0f,
-          0.0f);
-      __m512 float_scale[4] = {
-          _mm512_loadu_ps(scale),
-          _mm512_loadu_ps(scale + 16),
-          _mm512_loadu_ps(scale + 32),
-          _mm512_loadu_ps(scale + 48)};
-      __m512 float_zero_point[4] = {
-          _mm512_loadu_ps(zero_point),
-          _mm512_loadu_ps(zero_point + 16),
-          _mm512_loadu_ps(zero_point + 32),
-          _mm512_loadu_ps(zero_point + 48)};
-      for (int k = 0; k < K; k++) {
-        uint8_t* src = B;
-        BFloat16* dst = b;
-        __m256i packed = _mm256_load_si256((__m256i*)src);
-        __m512i int32[4];
-        {
-          auto low_4bit = _mm512_cvtepu8_epi32(_mm256_castsi256_si128(packed));
-          auto high_4bit = _mm512_srli_epi32(low_4bit, 4);
-          int32[0] = low_4bit;
-          int32[2] = high_4bit;
-        }
-        {
-          auto low_4bit =
-              _mm512_cvtepu8_epi32(_mm256_extracti128_si256(packed, 1));
-          auto high_4bit = _mm512_srli_epi32(low_4bit, 4);
-          int32[1] = low_4bit;
-          int32[3] = high_4bit;
-        }
-        for (int idx = 0; idx < 4; idx++) {
-          __m512 vb = _mm512_permutexvar_ps(int32[idx], lut);
-          vb = _mm512_sub_ps(vb, float_zero_point[idx]);
-          vb = _mm512_mul_ps(vb, float_scale[idx]);
-          _mm256_storeu_si256((__m256i*)(b + idx * 16), cvt_fp32_to_bf16(vb));
-        }
-        B += N / 2;
-        b += N;
-      }
-    } else {
-      const int COLS = N / 16;
-      for (int k = 0; k < K; k++) {
-        uint8_t* src = B;
-        BFloat16* dst = b;
-        int j, idx;
-        for (idx = 0, j = 0; j < COLS * 16; j += 16) {
-          __m512 float_scale = _mm512_loadu_ps(scale + j);
-          __m512 float_zero_point = _mm512_loadu_ps(zero_point + j);
-          dequant_to_bf16_(src, dst, float_scale, float_zero_point);
-          src += 8;
-          dst += 16;
-        }
-        if (j < N) {
-          const int res = N - j;
-          int rr = res / 2;
-          int l = 0;
-          for (; l < rr * 2; l += 2) {
-            const uint8_t vi = src[l / 2];
-            const int8_t vi0 = vi & 0xf;
-            const int8_t vi1 = vi >> 4;
-            const float v0 = (vi0 - zero_point[j + l]) * scale[j + l];
-            const float v1 = (vi1 - zero_point[j + l + 1]) * scale[j + l + 1];
-            dst[l + 0] = v0;
-            dst[l + 1] = v1;
-          }
-          if (l < res) {
-            const uint8_t vi = src[l / 2];
-            const int8_t vi0 = vi & 0xf;
-            const float v0 = (vi0 - zero_point[j + l]) * scale[j + l];
-            dst[l + 0] = v0;
-          }
-        }
-        B += N / 2;
-        b += N;
-      }
-    }
-  } else {
-    int i = 0;
-    for (; i < K * N / 2 * 2; i += 2) {
-      const uint8_t vi = B[i / 2];
-      const int8_t vi0 = vi & 0xf;
-      const int8_t vi1 = vi >> 4;
-      const float v0 = (vi0 - zero_point[i % N]) * scale[i % N];
-      const float v1 = (vi1 - zero_point[(i + 1) % N]) * scale[(i + 1) % N];
-      b[i + 0] = v0;
-      b[i + 1] = v1;
-    }
-    if (i < K * N) {
-      const uint8_t vi = B[i / 2];
-      const int8_t vi0 = vi & 0xf;
-      const float v0 = (vi0 - zero_point[i % N]) * scale[i % N];
-      b[i + 0] = v0;
-    }
-  }
-}
-
-void add_bias(float* C, float* bias, int M, int N, int ldc) {
-  int COLS = N / 16;
-  int j;
-  for (j = 0; j < COLS * 16; j += 16) {
-    __m512 float_bias = _mm512_loadu_ps(bias + j);
-    float* c = C + j;
-    for (int m = 0; m < M; m++) {
-      __m512 vc = _mm512_loadu_ps(c);
-      vc = _mm512_add_ps(vc, float_bias);
-      _mm512_storeu_ps(c, vc);
-      c += ldc;
-    }
-  }
-  if (j < N) {
-    const int res = N - j;
-    unsigned short mask = 0xffff;
-    mask = (1 << res) - 1;
-    __m512 float_bias = _mm512_maskz_loadu_ps(mask, bias + j);
-    float* c = C + j;
-    for (int m = 0; m < M; m++) {
-      __m512 vc = _mm512_maskz_loadu_ps(mask, c);
-      vc = _mm512_mask_add_ps(vc, mask, vc, float_bias);
-      _mm512_mask_storeu_ps(c, mask, vc);
-      c += ldc;
-    }
-  }
-}
-
-#else // not support AVX512
-
-// per channel
-template <int BLOCK_N, int BLOCK_K>
-void dequant(int8_t* B, float* b, float* scale, float* zero_point) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-void dequant(
-    int8_t* B,
-    float* b,
-    int K,
-    int N,
-    float* scale,
-    float* zero_point) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-void dequant(
-    int8_t* B,
-    BFloat16* b,
-    int K,
-    int N,
-    float* scale,
-    float* zero_point) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-// per tensor
-template <int BLOCK_N, int BLOCK_K>
-void dequant(int8_t* B, float* b, float scale, float zero_point) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-void dequant(int8_t* B, float* b, int K, int N, float scale, float zero_point) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-// per channel dequantize for int4
-// B is packed and not transposed, shape:[BLOCK_K x BLOCK_N]
-template <int BLOCK_K, int BLOCK_N>
-void dequant(uint8_t* B, float* b, float* scale, float* zero_point) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-// per channel dequantize for int4
-// handle edge cases
-void dequant(
-    uint8_t* B,
-    float* b,
-    int K,
-    int N,
-    float* scale,
-    float* zero_point) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-// dequant uint4 weight to bf16
-void dequant(
-    uint8_t* B,
-    BFloat16* b,
-    int K,
-    int N,
-    float* scale,
-    float* zero_point) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-void convert_bf16_to_fp32(
-    const BFloat16* src,
-    float* dst,
-    int M,
-    int K,
-    int ld_src) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-void convert_fp32_to_bf16(
-    const float* src,
-    BFloat16* dst,
-    int M,
-    int N,
-    int ld) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-void add_bias(float* C, float* bias, int M, int N, int ldc) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-template <
-    int LINES,
-    int N,
-    int PREFETCH_K_DIST,
-    bool ACC,
-    bool bias_add = false>
-void small_gemm_smallm(
-    const float* A,
-    const int8_t* B,
-    float* C,
-    int lda,
-    int ldb,
-    int ldc,
-    int actualN,
-    int K,
-    float* scale,
-    float* zero_point,
-    float* bias,
-    int rowOff = 0) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-// bf16 * int8 -> bf16
-template <
-    int LINES,
-    int N,
-    int PREFETCH_K_DIST,
-    bool ACC,
-    bool bias_add = false>
-void small_gemm_smallm(
-    const BFloat16* A,
-    const int8_t* B,
-    float* C,
-    int lda,
-    int ldb,
-    int ldc,
-    int actualN,
-    int K,
-    float* scale,
-    float* zero_point,
-    float* bias = NULL,
-    int rowOff = 0) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-inline void cvt_fp32_to_bf16(
-    BFloat16* dst,
-    const float* src,
-    int m,
-    int n,
-    int ld_dst,
-    int ld_src) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-
-inline void cvt_bf16_to_fp32(
-    float* dst,
-    const at::BFloat16* src,
-    int m,
-    int n,
-    int ld_dst,
-    int ld_src) {
-  AT_ASSERTM(false, "Unable to support AVX512!");
-}
-#endif
-
-static void print_mat(uint8_t* src, int size) {
-  std::cout << "B mat:" << std::endl;
-  for (int i = 0; i < size; i++) {
-    std::cout << (int)src[i] << " ";
-  }
-  std::cout << std::endl;
-  std::cout << "mat:" << std::endl;
-  for (int i = 0; i < size; i++) {
-    std::bitset<8> b(src[i]);
-    std::cout << b << " ";
-  }
-  std::cout << std::endl;
-}
-
-// fp32 * fp32 -> fp32
-template <int BLOCK_M, int BLOCK_N, int BLOCK_K>
-void dot_tile_update(
-    float* A,
-    float* B,
-    float* C,
-    bool trans_a,
-    bool trans_b,
-    int lda,
-    int ldb,
-    int ldc) {
-  auto&& kernel = create_or_get_dot_microkernel<BLOCK_M, BLOCK_N, BLOCK_K>(
-      trans_a, trans_b, lda, ldb, ldc); // nonblock
-  (*kernel)(A, B, C);
-}
-
-void dot_update(
-    float* A,
-    float* B,
-    float* C,
-    int M,
-    int N,
-    int K,
-    bool trans_a,
-    bool trans_b,
-    int lda,
-    int ldb,
-    int ldc) {
-  const char transa = trans_a ? 'Y' : 'N';
-  const char transb = trans_b ? 'Y' : 'N';
-  libxsmm_blasint BM = M, BN = N, BK = K, LDA = lda, LDB = ldb, LDC = ldc;
-  const float alpha = 1.0, beta = 1.0;
-  libxsmm_sgemm(
-      &transa,
-      &transb,
-      &BM,
-      &BN,
-      &BK,
-      &alpha,
-      A,
-      &LDA,
-      B,
-      &LDB,
-      &beta,
-      C,
-      &LDC);
-}
-
-template <typename T>
-void zero_fill(T* C, int M, int N, int stride) {
-  for (int m = 0; m < M; m++) {
-    memset(C + m * stride, 0, sizeof(T) * N);
-  }
-}
-
-// TODO: optimize with vectorized transposition
-void pack(
-    const int8_t* B,
-    int8_t* packed_B,
-    int K,
-    int N,
-    int ldb,
-    bool trans_B) {
-  AT_ASSERTM(
-      trans_B, "B must be transposed!"); // B must be transposed, shape:[N x K]
-  const int blks = (N + BLOCK_N - 1) / BLOCK_N;
-#pragma omp parallel for
-  for (int i = 0; i < blks; ++i) {
-    int rows = BLOCK_N; // each time pack BLOCK_N elements in N dimension
-    if (i == blks - 1) { // last block
-      rows = N - i * BLOCK_N;
-    }
-
-    const int8_t* psrc = B + i * BLOCK_N * ldb;
-    int8_t* pdst = packed_B + i * K * BLOCK_N;
-
-    for (int c = 0; c < K; ++c) {
-      for (int r = 0; r < rows; ++r) {
-        pdst[r] = psrc[r * ldb];
-      }
-      psrc += 1;
-      pdst += rows;
-    }
-  }
-}
-
-inline uint8_t extract_element(const uint8_t* src, int c, int r, int K) {
-  int offset_int4 = r * K + c;
-  int offset_int8 = offset_int4 / 2;
-  if (offset_int4 % 2 == 0) {
-    uint8_t elem = src[offset_int8] & 0xf;
-    return elem;
-  } else {
-    uint8_t elem = src[offset_int8] >> 4;
-    return elem;
-  }
-}
-
-inline void insert_element(uint8_t* dst, int c, int r, int rows, uint8_t elem) {
-  int offset_int4 = c * rows + r;
-  int offset_int8 = offset_int4 / 2;
-  if (offset_int4 % 2 == 0) { // in last 4 bits
-    dst[offset_int8] &= 0xf0;
-    dst[offset_int8] |= elem;
-
-  } else { // in first 4 bits
-    elem = elem << 4;
-    dst[offset_int8] &= 0xf;
-    dst[offset_int8] |= elem;
-  }
-}
-
-// TODO: optimize with vectorized transposition
-// pack int4 weight
-void pack(
-    const uint8_t* B,
-    uint8_t* packed_B,
-    int K,
-    int N,
-    int ldb,
-    bool trans_B) {
-  AT_ASSERTM(
-      trans_B, "B must be transposed!"); // B must be transposed, shape:[N x K]
-  static_assert(BLOCK_N == 64, "BLOCK_N must be 64");
-  const int blks = (N + BLOCK_N - 1) / BLOCK_N;
-#pragma omp parallel for
-  for (int i = 0; i < blks; i++) {
-    int rows = BLOCK_N;
-    if (i == blks - 1) {
-      rows = N - i * BLOCK_N;
-    }
-    const uint8_t* psrc = B + i * BLOCK_N * K / 2;
-    uint8_t* pdst = packed_B + i * K * BLOCK_N / 2;
-    for (int c = 0; c < K; c++) {
-      if (rows != BLOCK_N) {
-        for (int r = 0; r < rows; r++) {
-          uint8_t tmp = extract_element(psrc, c, r, K);
-          insert_element(pdst, c, r, rows, tmp);
-        }
-      } else {
-        for (int r = 0; r < BLOCK_N / 2; r++) {
-          uint8_t tmp = extract_element(psrc, c, r, K);
-          insert_element(pdst, c, r * 2, rows, tmp);
-          tmp = extract_element(psrc, c, r + BLOCK_N / 2, K);
-          insert_element(pdst, c, r * 2 + 1, rows, tmp);
-        }
-      }
-    }
-  }
-}
-
-// TODO: optimize with vectorized transposition
-void unpack(
-    const int8_t* packed_B,
-    int8_t* unpacked_B,
-    int K,
-    int N,
-    int ldb,
-    bool trans_B) {
-  AT_ASSERTM(
-      trans_B, "B must be transposed!"); // B must be transposed, shape:[N x K]
-  const int blks = (N + BLOCK_N - 1) / BLOCK_N;
-#pragma omp parallel for
-  for (int i = 0; i < blks; ++i) {
-    int rows = BLOCK_N; // each time pack BLOCK_N elements in N dimension
-    if (i == blks - 1) { // last block
-      rows = N - i * BLOCK_N;
-    }
-
-    const int8_t* psrc = packed_B + i * K * BLOCK_N;
-    int8_t* pdst = unpacked_B + i * BLOCK_N * ldb;
+  bool operator==(const DotMicroKernelKey& other) const {
+    return trans_a == other.trans_a && trans_b == other.trans_b &&
+        lda == other.lda && ldb == other.ldb && ldc == other.ldc;
+  }
+};
 
-    for (int c = 0; c < K; ++c) {
-      for (int r = 0; r < rows; ++r) {
-        pdst[r * ldb] = psrc[r];
-      }
-      psrc += rows;
-      pdst += 1;
-    }
+template <int BLOCK_M, int BLOCK_N, int BLOCK_K>
+class DotMicroKernel {
+ public:
+  DotMicroKernel(bool trans_a, bool trans_b, int lda, int ldb, int ldc) {
+    libxsmm_gemm_shape brshape = libxsmm_create_gemm_shape(
+        BLOCK_M,
+        BLOCK_N,
+        BLOCK_K,
+        lda,
+        ldb,
+        ldc,
+        /*type A*/ LIBXSMM_DATATYPE_F32,
+        /*type B*/ LIBXSMM_DATATYPE_F32,
+        /*type C*/ LIBXSMM_DATATYPE_F32,
+        /*acctype*/ LIBXSMM_DATATYPE_F32);
+    libxsmm_bitfield brflags =
+        (trans_a ? LIBXSMM_GEMM_FLAG_TRANS_A : LIBXSMM_GEMM_FLAG_NONE) |
+        (trans_b ? LIBXSMM_GEMM_FLAG_TRANS_B : LIBXSMM_GEMM_FLAG_NONE);
+    libxsmm_gemm_batch_reduce_config brconfig;
+    memset(&brconfig, 0, sizeof(libxsmm_gemm_batch_reduce_config));
+    brconfig.br_type = LIBXSMM_GEMM_BATCH_REDUCE_NONE;
+
+    kernel_func_ = libxsmm_dispatch_brgemm_v2(
+        brshape, brflags, /*prefetch_flags=*/0, brconfig);
+    memset(&gemm_param_, 0, sizeof(libxsmm_gemm_param));
   }
-}
 
-// TODO: rewrite this
-// unpack uint4 weight
-void unpack(
-    const uint8_t* packed_B,
-    uint8_t* unpacked_B,
-    int K,
-    int N,
-    int ldb,
-    bool trans_B) {
-  AT_ASSERTM(
-      trans_B, "B must be transposed!"); // B must be transposed, shape:[N x K]
-  static_assert(BLOCK_N == 64, "BLOCK_N must be 64");
-  const int blks = (N + BLOCK_N - 1) / BLOCK_N;
-#pragma omp parallel for
-  for (int i = 0; i < blks; i++) {
-    int rows = BLOCK_N;
-    if (i == blks - 1) {
-      rows = N - i * BLOCK_N;
-    }
-    uint8_t* pdst = unpacked_B + i * BLOCK_N * K / 2;
-    const uint8_t* psrc = packed_B + i * K * BLOCK_N / 2;
-    for (int c = 0; c < K; c++) {
-      if (rows != BLOCK_N) {
-        for (int r = 0; r < rows; r++) {
-          uint8_t tmp = extract_element(psrc, r, c, rows);
-          insert_element(pdst, r, c, K, tmp);
-        }
-      } else {
-        for (int r = 0; r < BLOCK_N / 2; r++) {
-          uint8_t tmp = extract_element(psrc, r * 2, c, rows);
-          insert_element(pdst, r, c, K, tmp);
-          tmp = extract_element(psrc, r * 2 + 1, c, rows);
-          insert_element(pdst, r + BLOCK_N / 2, c, K, tmp);
-        }
-      }
-    }
+  void operator()(void* A, void* B, void* C) {
+    gemm_param_.a.primary = (void*)A;
+    gemm_param_.b.primary = (void*)B;
+    gemm_param_.c.primary = (void*)C;
+    kernel_func_(&gemm_param_);
   }
-}
 
-// dequant per channel
-template <bool has_bias, int BLOCK_M>
-void woq_gemm_intrinsic(
-    float* A,
-    int8_t* B,
-    float* C,
-    int M,
-    int N,
-    int K,
-    int lda,
-    int ldb,
-    int ldc,
-    float* scale,
-    float* zero_point,
-    float* bias = NULL) {
-#define PTR_OFFSET(base, offset0, offset1, stride0) \
-  (base) + (offset0) * (stride0) + (offset1)
+ private:
+  libxsmm_gemmfunction kernel_func_;
+  libxsmm_gemm_param gemm_param_;
+};
 
-  const int MB = (M + BLOCK_M - 1) / BLOCK_M, NB = (N + BLOCK_N - 1) / BLOCK_N,
-            KB = (K + BLOCK_K - 1) / BLOCK_K;
+template <int BLOCK_M, int BLOCK_N, int BLOCK_K>
+using DotMicroKernelRef =
+    std::shared_ptr<DotMicroKernel<BLOCK_M, BLOCK_N, BLOCK_K>>;
 
-#pragma omp parallel for collapse(2)
-  for (int mb = 0; mb < MB; mb++) {
-    for (int nb = 0; nb < NB; nb++) {
-      int mb_start = mb * BLOCK_M;
-      int m_bs = std::min(BLOCK_M, M - mb_start);
-      int nb_start = nb * BLOCK_N;
-      int n_bs = std::min(BLOCK_N, N - nb_start);
-      float* C_offset = PTR_OFFSET(C, mb_start, nb_start, ldc);
-      float* bi_offset =
-          (float*)aligned_alloc(64, BLOCK_K * BLOCK_N * sizeof(float));
-      zero_fill(C_offset, m_bs, n_bs, ldc);
-      for (int kb = 0; kb < KB; kb++) {
-        int kb_start = kb * BLOCK_K;
-        int k_bs = std::min(BLOCK_K, K - kb_start);
-        float* A_offset = PTR_OFFSET(A, mb_start, kb_start, lda);
-        int8_t* B_offset = B + nb_start * K + kb_start * n_bs;
-        if (m_bs == BLOCK_M && n_bs == BLOCK_N) {
-          small_gemm_smallm<BLOCK_M, BLOCK_N, PREFETCH_K, true>(
-              A_offset,
-              B_offset,
-              C_offset,
-              lda,
-              n_bs,
-              ldc,
-              BLOCK_N,
-              k_bs,
-              scale + nb_start,
-              zero_point + nb_start);
-        } else { // edge case
-          dequant(
-              B_offset,
-              bi_offset,
-              k_bs,
-              n_bs,
-              scale + nb_start,
-              zero_point + nb_start);
-          dot_update( // libxsmm is col major
-              bi_offset,
-              A_offset,
-              C_offset,
-              n_bs,
-              m_bs,
-              k_bs,
-              false,
-              false,
-              n_bs,
-              lda,
-              ldc);
-        }
-      }
-      if constexpr (has_bias) {
-        add_bias(C_offset, bias + nb_start, m_bs, n_bs, ldc);
-      }
-      free(bi_offset);
-    }
+template <int BLOCK_M, int BLOCK_N, int BLOCK_K>
+DotMicroKernelRef<BLOCK_M, BLOCK_N, BLOCK_K> create_or_get_dot_microkernel(
+    bool trans_a,
+    bool trans_b,
+    int lda,
+    int ldb,
+    int ldc) {
+  thread_local std::unordered_map<
+      DotMicroKernelKey,
+      DotMicroKernelRef<BLOCK_M, BLOCK_N, BLOCK_K>>
+      cache;
+  DotMicroKernelKey key(trans_a, trans_b, lda, ldb, ldc);
+  auto search = cache.find(key);
+  if (search != cache.end()) {
+    return search->second;
+  } else {
+    cache.insert(
+        {key,
+         std::make_shared<DotMicroKernel<BLOCK_M, BLOCK_N, BLOCK_K>>(
+             trans_a, trans_b, lda, ldb, ldc)}); //
+    return cache[key];
   }
 }
 
-// dequant per channel
-template <bool has_bias, int BLOCK_M>
-void woq_gemm_intrinsic(
-    BFloat16* A,
-    int8_t* B,
-    BFloat16* C,
-    int M,
-    int N,
-    int K,
+template <int BLOCK_M, int BLOCK_N, int BLOCK_K>
+void dot_tile_update(
+    float* A,
+    float* B,
+    float* C,
+    bool trans_a,
+    bool trans_b,
     int lda,
     int ldb,
-    int ldc,
-    float* scale,
-    float* zero_point,
-    float* bias = NULL) {
-#define PTR_OFFSET(base, offset0, offset1, stride0) \
-  (base) + (offset0) * (stride0) + (offset1)
-
-  const int MB = (M + BLOCK_M - 1) / BLOCK_M, NB = (N + BLOCK_N - 1) / BLOCK_N,
-            KB = (K + BLOCK_K - 1) / BLOCK_K;
-
-#pragma omp parallel for collapse(2)
-  for (int mb = 0; mb < MB; mb++) {
-    for (int nb = 0; nb < NB; nb++) {
-      int mb_start = mb * BLOCK_M;
-      int m_bs = std::min(BLOCK_M, M - mb_start);
-      int nb_start = nb * BLOCK_N;
-      int n_bs = std::min(BLOCK_N, N - nb_start);
-      BFloat16* C_offset = PTR_OFFSET(C, mb_start, nb_start, ldc);
-      float* bi_offset =
-          (float*)aligned_alloc(64, BLOCK_K * BLOCK_N * sizeof(float));
-      float* ci_offset = (float*)aligned_alloc(64, m_bs * n_bs * sizeof(float));
-      zero_fill(ci_offset, m_bs, n_bs, n_bs);
-      if (m_bs == BLOCK_M && n_bs == BLOCK_N) {
-        for (int kb = 0; kb < KB; kb++) {
-          int kb_start = kb * BLOCK_K;
-          int k_bs = std::min(BLOCK_K, K - kb_start);
-          BFloat16* A_offset = PTR_OFFSET(A, mb_start, kb_start, lda);
-          int8_t* B_offset = B + nb_start * K + kb_start * n_bs;
-          if (kb != KB - 1) {
-            small_gemm_smallm<BLOCK_M, BLOCK_N, PREFETCH_K, true>(
-                A_offset,
-                B_offset,
-                ci_offset,
-                lda,
-                n_bs,
-                n_bs,
-                BLOCK_N,
-                k_bs,
-                scale + nb_start,
-                zero_point + nb_start);
-          } else { // last block in K, add bias
-            small_gemm_smallm<BLOCK_M, BLOCK_N, PREFETCH_K, true, has_bias>(
-                A_offset,
-                B_offset,
-                ci_offset,
-                lda,
-                n_bs,
-                n_bs,
-                BLOCK_N,
-                k_bs,
-                scale + nb_start,
-                zero_point + nb_start,
-                bias + nb_start);
-          }
-        }
-      } else { // edge case
-        float* ai_offset =
-            (float*)aligned_alloc(64, m_bs * BLOCK_K * sizeof(float));
-        for (int kb = 0; kb < KB; kb++) {
-          int kb_start = kb * BLOCK_K;
-          int k_bs = std::min(BLOCK_K, K - kb_start);
-          BFloat16* A_offset = PTR_OFFSET(A, mb_start, kb_start, lda);
-          int8_t* B_offset = B + nb_start * K + kb_start * n_bs;
-          cvt_bf16_to_fp32(ai_offset, A_offset, m_bs, k_bs, k_bs, lda);
-          dequant(
-              B_offset,
-              bi_offset,
-              k_bs,
-              n_bs,
-              scale + nb_start,
-              zero_point + nb_start);
-          dot_update( // libxsmm is col major
-              bi_offset,
-              ai_offset,
-              ci_offset,
-              n_bs,
-              m_bs,
-              k_bs,
-              false,
-              false,
-              n_bs,
-              k_bs,
-              n_bs);
-        }
-        if constexpr (has_bias) {
-          add_bias(ci_offset, bias + nb_start, m_bs, n_bs, n_bs);
-        }
-        free(ai_offset);
-      }
-      cvt_fp32_to_bf16(C_offset, ci_offset, m_bs, n_bs, ldc, n_bs);
-      free(ci_offset);
-      free(bi_offset);
-    }
-  }
+    int ldc) {
+  auto&& kernel = create_or_get_dot_microkernel<BLOCK_M, BLOCK_N, BLOCK_K>(
+      trans_a, trans_b, lda, ldb, ldc); // nonblock
+  (*kernel)(A, B, C);
 }
 
-// dequant per channel
-template <bool has_bias, int BLOCK_M>
-void woq_gemm_intrinsic(
+void dot_update(
     float* A,
-    uint8_t* B,
+    float* B,
     float* C,
     int M,
     int N,
     int K,
+    bool trans_a,
+    bool trans_b,
     int lda,
     int ldb,
-    int ldc,
-    float* scale,
-    float* zero_point,
-    float* bias = NULL) {
-#define PTR_OFFSET(base, offset0, offset1, stride0) \
-  (base) + (offset0) * (stride0) + (offset1)
+    int ldc) {
+  const char transa = trans_a ? 'Y' : 'N';
+  const char transb = trans_b ? 'Y' : 'N';
+  libxsmm_blasint BM = M, BN = N, BK = K, LDA = lda, LDB = ldb, LDC = ldc;
+  const float alpha = 1.0, beta = 1.0;
+  libxsmm_sgemm(
+      &transa,
+      &transb,
+      &BM,
+      &BN,
+      &BK,
+      &alpha,
+      A,
+      &LDA,
+      B,
+      &LDB,
+      &beta,
+      C,
+      &LDC);
+}
 
-  const int MB = (M + BLOCK_M - 1) / BLOCK_M, NB = (N + BLOCK_N - 1) / BLOCK_N,
-            KB = (K + BLOCK_K - 1) / BLOCK_K;
+void zero_fill(float* C, int M, int N, int stride) {
+  for (int m = 0; m < M; m++) {
+    memset(C + m * stride, 0, sizeof(float) * N);
+  }
+}
 
-#pragma omp parallel for collapse(2)
-  for (int mb = 0; mb < MB; mb++) {
-    for (int nb = 0; nb < NB; nb++) {
-      int mb_start = mb * BLOCK_M;
-      int m_bs = std::min(BLOCK_M, M - mb_start);
-      int nb_start = nb * BLOCK_N;
-      int n_bs = std::min(BLOCK_N, N - nb_start);
-      float* C_offset = PTR_OFFSET(C, mb_start, nb_start, ldc);
-      float* bi_offset =
-          (float*)aligned_alloc(64, BLOCK_K * BLOCK_N * sizeof(float));
-      zero_fill(C_offset, m_bs, n_bs, ldc);
-      for (int kb = 0; kb < KB; kb++) {
-        int kb_start = kb * BLOCK_K;
-        int k_bs = std::min(BLOCK_K, K - kb_start);
-        float* A_offset = PTR_OFFSET(A, mb_start, kb_start, lda);
-        uint8_t* B_offset = B + nb_start / 2 * K + kb_start * n_bs / 2;
-        if (m_bs == BLOCK_M && n_bs == BLOCK_N) {
-          small_gemm_smallm<BLOCK_M, BLOCK_N, PREFETCH_K, true>(
-              A_offset,
-              B_offset,
-              C_offset,
-              lda,
-              n_bs,
-              ldc,
-              BLOCK_N,
-              k_bs,
-              scale + nb_start,
-              zero_point + nb_start);
-        } else { // edge case
-          dequant(
-              B_offset,
-              bi_offset,
-              k_bs,
-              n_bs,
-              scale + nb_start,
-              zero_point + nb_start);
-          dot_update( // libxsmm is col major
-              bi_offset,
-              A_offset,
-              C_offset,
-              n_bs,
-              m_bs,
-              k_bs,
-              false,
-              false,
-              n_bs,
-              lda,
-              ldc);
-        }
-      }
-      if constexpr (has_bias) {
-        add_bias(C_offset, bias + nb_start, m_bs, n_bs, ldc);
+// TODO: optimize with vectorized transposition
+void pack(
+    const int8_t* B,
+    int8_t* packed_B,
+    int K,
+    int N,
+    int ldb,
+    bool trans_B) {
+  AT_ASSERTM(
+      trans_B, "B must be transposed!"); // B must be transposed, shape:[N x K]
+  const int blks = (N + BLOCK_N - 1) / BLOCK_N;
+#pragma omp parallel for
+  for (int i = 0; i < blks; ++i) {
+    int rows = BLOCK_N; // each time pack BLOCK_N elements in N dimension
+    if (i == blks - 1) { // last block
+      rows = N - i * BLOCK_N;
+    }
+
+    const int8_t* psrc = B + i * BLOCK_N * ldb;
+    int8_t* pdst = packed_B + i * K * BLOCK_N;
+
+    for (int c = 0; c < K; ++c) {
+      for (int r = 0; r < rows; ++r) {
+        pdst[r] = psrc[r * ldb];
       }
-      free(bi_offset);
+      psrc += 1;
+      pdst += rows;
     }
   }
 }
 
-// dequant per channel
-template <bool has_bias, int BLOCK_M>
-void woq_gemm_intrinsic(
-    BFloat16* A,
-    uint8_t* B,
-    BFloat16* C,
-    int M,
-    int N,
+// TODO: optimize with vectorized transposition
+void unpack(
+    const int8_t* packed_B,
+    int8_t* unpacked_B,
     int K,
-    int lda,
+    int N,
     int ldb,
-    int ldc,
-    float* scale,
-    float* zero_point,
-    float* bias = NULL) {
-#define PTR_OFFSET(base, offset0, offset1, stride0) \
-  (base) + (offset0) * (stride0) + (offset1)
+    bool trans_B) {
+  AT_ASSERTM(
+      trans_B, "B must be transposed!"); // B must be transposed, shape:[N x K]
+  const int blks = (N + BLOCK_N - 1) / BLOCK_N;
+#pragma omp parallel for
+  for (int i = 0; i < blks; ++i) {
+    int rows = BLOCK_N; // each time pack BLOCK_N elements in N dimension
+    if (i == blks - 1) { // last block
+      rows = N - i * BLOCK_N;
+    }
 
-  const int MB = (M + BLOCK_M - 1) / BLOCK_M, NB = (N + BLOCK_N - 1) / BLOCK_N,
-            KB = (K + BLOCK_K - 1) / BLOCK_K;
+    const int8_t* psrc = packed_B + i * K * BLOCK_N;
+    int8_t* pdst = unpacked_B + i * BLOCK_N * ldb;
 
-#pragma omp parallel for collapse(2)
-  for (int mb = 0; mb < MB; mb++) {
-    for (int nb = 0; nb < NB; nb++) {
-      int mb_start = mb * BLOCK_M;
-      int m_bs = std::min(BLOCK_M, M - mb_start);
-      int nb_start = nb * BLOCK_N;
-      int n_bs = std::min(BLOCK_N, N - nb_start);
-      BFloat16* C_offset = PTR_OFFSET(C, mb_start, nb_start, ldc);
-      float* bi_offset =
-          (float*)aligned_alloc(64, BLOCK_K * BLOCK_N * sizeof(float));
-      float* ci_offset = (float*)aligned_alloc(64, m_bs * n_bs * sizeof(float));
-      zero_fill(ci_offset, m_bs, n_bs, n_bs);
-      if (m_bs == BLOCK_M && n_bs == BLOCK_N) {
-        for (int kb = 0; kb < KB; kb++) {
-          int kb_start = kb * BLOCK_K;
-          int k_bs = std::min(BLOCK_K, K - kb_start);
-          BFloat16* A_offset = PTR_OFFSET(A, mb_start, kb_start, lda);
-          uint8_t* B_offset = B + nb_start / 2 * K + kb_start * n_bs / 2;
-          if (kb != KB - 1) {
-            small_gemm_smallm<BLOCK_M, BLOCK_N, PREFETCH_K, true>(
-                A_offset,
-                B_offset,
-                ci_offset,
-                lda,
-                n_bs,
-                n_bs,
-                BLOCK_N,
-                k_bs,
-                scale + nb_start,
-                zero_point + nb_start);
-          } else { // last block in K, add bias
-            small_gemm_smallm<BLOCK_M, BLOCK_N, PREFETCH_K, true, has_bias>(
-                A_offset,
-                B_offset,
-                ci_offset,
-                lda,
-                n_bs,
-                n_bs,
-                BLOCK_N,
-                k_bs,
-                scale + nb_start,
-                zero_point + nb_start,
-                bias + nb_start);
-          }
-        }
-      } else { // edge case
-        float* ai_offset =
-            (float*)aligned_alloc(64, m_bs * BLOCK_K * sizeof(float));
-        for (int kb = 0; kb < KB; kb++) {
-          int kb_start = kb * BLOCK_K;
-          int k_bs = std::min(BLOCK_K, K - kb_start);
-          BFloat16* A_offset = PTR_OFFSET(A, mb_start, kb_start, lda);
-          uint8_t* B_offset = B + nb_start / 2 * K + kb_start * n_bs / 2;
-          cvt_bf16_to_fp32(ai_offset, A_offset, m_bs, k_bs, k_bs, lda);
-          dequant(
-              B_offset,
-              bi_offset,
-              k_bs,
-              n_bs,
-              scale + nb_start,
-              zero_point + nb_start);
-          dot_update( // libxsmm is col major
-              bi_offset,
-              ai_offset,
-              ci_offset,
-              n_bs,
-              m_bs,
-              k_bs,
-              false,
-              false,
-              n_bs,
-              k_bs,
-              n_bs);
-        }
-        if constexpr (has_bias) {
-          add_bias(ci_offset, bias + nb_start, m_bs, n_bs, n_bs);
-        }
-        free(ai_offset);
+    for (int c = 0; c < K; ++c) {
+      for (int r = 0; r < rows; ++r) {
+        pdst[r * ldb] = psrc[r];
       }
-      cvt_fp32_to_bf16(C_offset, ci_offset, m_bs, n_bs, ldc, n_bs);
-      free(ci_offset);
-      free(bi_offset);
+      psrc += rows;
+      pdst += 1;
     }
   }
 }
 
 // dequant per channel
-// for large M
 template <bool has_bias, int BLOCK_M>
-void woq_gemm_brgemm(
+void woq_gemm_intrinsic(
     float* A,
     int8_t* B,
     float* C,
@@ -2061,8 +617,8 @@ void woq_gemm_brgemm(
     int lda,
     int ldb,
     int ldc,
-    float* scale,
     float* zero_point,
+    float* scale,
     float* bias = NULL) {
 #define PTR_OFFSET(base, offset0, offset1, stride0) \
   (base) + (offset0) * (stride0) + (offset1)
@@ -2086,17 +642,26 @@ void woq_gemm_brgemm(
         int k_bs = std::min(BLOCK_K, K - kb_start);
         float* A_offset = PTR_OFFSET(A, mb_start, kb_start, lda);
         int8_t* B_offset = B + nb_start * K + kb_start * n_bs;
-        dequant(
-            B_offset,
-            bi_offset,
-            k_bs,
-            n_bs,
-            scale + nb_start,
-            zero_point + nb_start);
-        if (m_bs == BLOCK_M && n_bs == BLOCK_N && k_bs == BLOCK_K) {
-          dot_tile_update<BLOCK_N, BLOCK_M, BLOCK_K>(
-              bi_offset, A_offset, C_offset, false, false, n_bs, lda, ldc);
-        } else {
+        if (m_bs == BLOCK_M && n_bs == BLOCK_N) {
+          small_gemm_smallm<BLOCK_M, BLOCK_N, PREFETCH_K, true>(
+              A_offset,
+              B_offset,
+              C_offset,
+              lda,
+              n_bs,
+              ldc,
+              BLOCK_N,
+              BLOCK_K,
+              zero_point + nb_start,
+              scale + nb_start);
+        } else { // edge case
+          dequant(
+              B_offset,
+              bi_offset,
+              k_bs,
+              n_bs,
+              zero_point + nb_start,
+              scale + nb_start);
           dot_update( // libxsmm is col major
               bi_offset,
               A_offset,
@@ -2120,83 +685,65 @@ void woq_gemm_brgemm(
 }
 
 // dequant per channel
-// for large M
-template <bool has_bias, int BLOCK_M>
+// for small M
+template <bool has_bias>
 void woq_gemm_brgemm(
-    BFloat16* A,
+    float* A,
     int8_t* B,
-    BFloat16* C,
+    float* C,
     int M,
     int N,
     int K,
     int lda,
     int ldb,
     int ldc,
-    float* scale,
     float* zero_point,
+    float* scale,
     float* bias = NULL) {
 #define PTR_OFFSET(base, offset0, offset1, stride0) \
   (base) + (offset0) * (stride0) + (offset1)
 
-  // Number of blocks
-  const int MB = (M + BLOCK_M - 1) / BLOCK_M;
-  const int NB = (N + BLOCK_N - 1) / BLOCK_N;
-  const int KB = (K + BLOCK_K - 1) / BLOCK_K;
-
-#pragma omp parallel for collapse(2)
-  for (int mb = 0; mb < MB; mb++) {
-    for (int nb = 0; nb < NB; nb++) {
-      int mb_start = mb * BLOCK_M;
-      int m_bs = std::min(BLOCK_M, M - mb_start);
-      int nb_start = nb * BLOCK_N;
-      int n_bs = std::min(BLOCK_N, N - nb_start);
-      BFloat16* C_offset = PTR_OFFSET(C, mb_start, nb_start, ldc);
-      BFloat16* bi_offset =
-          (BFloat16*)aligned_alloc(64, BLOCK_K * n_bs * sizeof(BFloat16));
-      float* ci_offset = (float*)aligned_alloc(64, m_bs * n_bs * sizeof(float));
-      zero_fill(ci_offset, m_bs, n_bs, n_bs);
-      auto compute_block = [&](int kb) {
-        int kb_start = kb * BLOCK_K;
-        int k_bs = std::min(BLOCK_K, K - kb_start);
-        BFloat16* A_offset = PTR_OFFSET(A, mb_start, kb_start, lda);
-        int8_t* B_offset = B + nb_start * K + kb_start * n_bs;
-        dequant(
-            B_offset,
-            bi_offset,
-            k_bs,
-            n_bs,
-            scale + nb_start,
-            zero_point + nb_start);
-        // MKL gemm
-        // C := alpha*op(A) *op(B) + beta*C
-        // op(A) is m-by-k, op(B) is k-by-n, C is m-by-n.
-        cblas_gemm_bf16bf16f32(
-            CblasRowMajor, // Row/col major
-            CblasNoTrans, // Trans A
-            CblasNoTrans, // Trans B
-            m_bs, // M
-            n_bs, // N
-            k_bs, // K
-            1.f, // alpha = 1.0
-            (const MKL_BF16*)A_offset, // A
-            lda, // lda
-            (const MKL_BF16*)bi_offset, // B
-            n_bs, // ldb
-            1.f, // beta = 1.0 as we need to accumulate
-            ci_offset, // C
-            n_bs); // ldc
-      };
-      int kb = 0;
-      for (; kb < KB; kb++) {
-        compute_block(kb);
-      }
-      if constexpr (has_bias) {
-        add_bias(ci_offset, bias + nb_start, m_bs, n_bs, n_bs);
-      }
-      cvt_fp32_to_bf16(C_offset, ci_offset, m_bs, n_bs, ldc, n_bs);
-      free(ci_offset);
-      free(bi_offset);
-    }
+  const int NB = (N + BLOCK_N - 1) / BLOCK_N, KB = (K + BLOCK_K - 1) / BLOCK_K;
+
+#pragma omp parallel for collapse(1)
+  for (int nb = 0; nb < NB; nb++) {
+    int mb_start = 0;
+    int m_bs = M;
+    int nb_start = nb * BLOCK_N;
+    int n_bs = std::min(BLOCK_N, N - nb_start);
+    float* C_offset = PTR_OFFSET(C, mb_start, nb_start, ldc);
+    zero_fill(C_offset, m_bs, n_bs, ldc);
+    float* bi_offset =
+        (float*)aligned_alloc(64, BLOCK_K * BLOCK_N * sizeof(float));
+    for (int kb = 0; kb < KB; kb++) {
+      int kb_start = kb * BLOCK_K;
+      int k_bs = std::min(BLOCK_K, K - kb_start);
+      float* A_offset = PTR_OFFSET(A, mb_start, kb_start, lda);
+      int8_t* B_offset = B + nb_start * K + kb_start * n_bs;
+      dequant(
+          B_offset,
+          bi_offset,
+          k_bs,
+          n_bs,
+          zero_point + nb_start,
+          scale + nb_start);
+      dot_update( // libxsmm is col major
+          bi_offset,
+          A_offset,
+          C_offset,
+          n_bs,
+          m_bs,
+          k_bs,
+          false,
+          false,
+          n_bs,
+          lda,
+          ldc);
+    }
+    if constexpr (has_bias) {
+      add_bias(C_offset, bias + nb_start, m_bs, n_bs, ldc);
+    }
+    free(bi_offset);
   }
 }
 
@@ -2205,7 +752,7 @@ void woq_gemm_brgemm(
 template <bool has_bias, int BLOCK_M>
 void woq_gemm_brgemm(
     float* A,
-    uint8_t* B,
+    int8_t* B,
     float* C,
     int M,
     int N,
@@ -2213,8 +760,8 @@ void woq_gemm_brgemm(
     int lda,
     int ldb,
     int ldc,
-    float* scale,
     float* zero_point,
+    float* scale,
     float* bias = NULL) {
 #define PTR_OFFSET(base, offset0, offset1, stride0) \
   (base) + (offset0) * (stride0) + (offset1)
@@ -2237,14 +784,14 @@ void woq_gemm_brgemm(
         int kb_start = kb * BLOCK_K;
         int k_bs = std::min(BLOCK_K, K - kb_start);
         float* A_offset = PTR_OFFSET(A, mb_start, kb_start, lda);
-        uint8_t* B_offset = B + nb_start / 2 * K + kb_start * n_bs / 2;
+        int8_t* B_offset = B + nb_start * K + kb_start * n_bs;
         dequant(
             B_offset,
             bi_offset,
             k_bs,
             n_bs,
-            scale + nb_start,
-            zero_point + nb_start);
+            zero_point + nb_start,
+            scale + nb_start);
         if (m_bs == BLOCK_M && n_bs == BLOCK_N && k_bs == BLOCK_K) {
           dot_tile_update<BLOCK_N, BLOCK_M, BLOCK_K>(
               bi_offset, A_offset, C_offset, false, false, n_bs, lda, ldc);
@@ -2271,87 +818,6 @@ void woq_gemm_brgemm(
   }
 }
 
-// dequant per channel
-// for large M
-template <bool has_bias, int BLOCK_M>
-void woq_gemm_brgemm(
-    BFloat16* A,
-    uint8_t* B,
-    BFloat16* C,
-    int M,
-    int N,
-    int K,
-    int lda,
-    int ldb,
-    int ldc,
-    float* scale,
-    float* zero_point,
-    float* bias = NULL) {
-#define PTR_OFFSET(base, offset0, offset1, stride0) \
-  (base) + (offset0) * (stride0) + (offset1)
-
-  // Number of blocks
-  const int MB = (M + BLOCK_M - 1) / BLOCK_M;
-  const int NB = (N + BLOCK_N - 1) / BLOCK_N;
-  const int KB = (K + BLOCK_K - 1) / BLOCK_K;
-
-#pragma omp parallel for collapse(2)
-  for (int mb = 0; mb < MB; mb++) {
-    for (int nb = 0; nb < NB; nb++) {
-      int mb_start = mb * BLOCK_M;
-      int m_bs = std::min(BLOCK_M, M - mb_start);
-      int nb_start = nb * BLOCK_N;
-      int n_bs = std::min(BLOCK_N, N - nb_start);
-      BFloat16* C_offset = PTR_OFFSET(C, mb_start, nb_start, ldc);
-      BFloat16* bi_offset =
-          (BFloat16*)aligned_alloc(64, BLOCK_K * n_bs * sizeof(BFloat16));
-      float* ci_offset = (float*)aligned_alloc(64, m_bs * n_bs * sizeof(float));
-      zero_fill(ci_offset, m_bs, n_bs, n_bs);
-      auto compute_block = [&](int kb) {
-        int kb_start = kb * BLOCK_K;
-        int k_bs = std::min(BLOCK_K, K - kb_start);
-        BFloat16* A_offset = PTR_OFFSET(A, mb_start, kb_start, lda);
-        uint8_t* B_offset = B + nb_start / 2 * K + kb_start * n_bs / 2;
-        dequant(
-            B_offset,
-            bi_offset,
-            k_bs,
-            n_bs,
-            scale + nb_start,
-            zero_point + nb_start);
-        // MKL gemm
-        // C := alpha*op(A) *op(B) + beta*C
-        // op(A) is m-by-k, op(B) is k-by-n, C is m-by-n.
-        cblas_gemm_bf16bf16f32(
-            CblasRowMajor, // Row/col major
-            CblasNoTrans, // Trans A
-            CblasNoTrans, // Trans B
-            m_bs, // M
-            n_bs, // N
-            k_bs, // K
-            1.f, // alpha = 1.0
-            (const MKL_BF16*)A_offset, // A
-            lda, // lda
-            (const MKL_BF16*)bi_offset, // B
-            n_bs, // ldb
-            1.f, // beta = 1.0 as we need to accumulate
-            ci_offset, // C
-            n_bs); // ldc
-      };
-      int kb = 0;
-      for (; kb < KB; kb++) {
-        compute_block(kb);
-      }
-      if constexpr (has_bias) {
-        add_bias(ci_offset, bias + nb_start, m_bs, n_bs, n_bs);
-      }
-      cvt_fp32_to_bf16(C_offset, ci_offset, m_bs, n_bs, ldc, n_bs);
-      free(ci_offset);
-      free(bi_offset);
-    }
-  }
-}
-
 // per tensor
 template <bool has_bias, int BLOCK_M>
 void woq_gemm_brgemm_per_tensor(
@@ -2364,8 +830,8 @@ void woq_gemm_brgemm_per_tensor(
     int lda,
     int ldb,
     int ldc,
-    float scale,
     float zero_point,
+    float scale,
     float* bias = NULL) {
 #define PTR_OFFSET(base, offset0, offset1, stride0) \
   (base) + (offset0) * (stride0) + (offset1)
@@ -2389,7 +855,7 @@ void woq_gemm_brgemm_per_tensor(
         int k_bs = std::min(BLOCK_K, K - kb_start);
         float* A_offset = PTR_OFFSET(A, mb_start, kb_start, lda);
         int8_t* B_offset = B + nb_start * K + kb_start * n_bs;
-        dequant(B_offset, bi_offset, k_bs, n_bs, scale, zero_point);
+        dequant(B_offset, bi_offset, k_bs, n_bs, zero_point, scale);
         if (m_bs == BLOCK_M && n_bs == BLOCK_N && k_bs == BLOCK_K) {
           dot_tile_update<BLOCK_N, BLOCK_M, BLOCK_K>(
               bi_offset, A_offset, C_offset, false, false, n_bs, lda, ldc);
@@ -2416,80 +882,31 @@ void woq_gemm_brgemm_per_tensor(
   }
 }
 
-template <typename T1, typename T2, bool has_bias, int BM>
-struct Function_matching {
-  static void func_match(
-      T1* A,
-      T2* B,
-      T1* C,
-      int M,
-      int N,
-      int K,
-      int lda,
-      int ldb,
-      int ldc,
-      float* scale,
-      float* zero_point,
-      float* bias = NULL) {
-    switch (M) {
-      case BM:
-        woq_gemm_intrinsic<has_bias, BM>(
-            A, B, C, M, N, K, K, K, N, scale, zero_point, bias);
-        break;
-      default:
-        Function_matching<T1, T2, has_bias, BM - 1>::func_match(
-            A, B, C, M, N, K, lda, ldb, ldc, scale, zero_point, bias);
-        break;
-    }
-  }
-};
-
-template <typename T1, typename T2, bool has_bias>
-struct Function_matching<T1, T2, has_bias, 0> {
-  static void func_match(
-      T1* A,
-      T2* B,
-      T1* C,
-      int M,
-      int N,
-      int K,
-      int lda,
-      int ldb,
-      int ldc,
-      float* scale,
-      float* zero_point,
-      float* bias = NULL) {
-    return; // do nothing
-  }
-};
-
 void woq_gemm_kernel_impl(
     const at::Tensor& self,
     const at::Tensor& weight,
-    const at::Tensor& scales_float,
     const at::Tensor& zero_points_float,
+    const at::Tensor& scales_float,
     const at::Tensor& bias,
-    int64_t lowp_mode,
     at::Tensor& output) {
+#if defined(CPU_CAPABILITY_AVX512)
   auto self_ = self.is_contiguous() ? self : self.contiguous();
   const int64_t dim = self.dim();
   auto self_reshaped =
       dim == 2 ? self_ : self_.reshape({-1, self.size(self.dim() - 1)});
   auto M = self_reshaped.size(0);
   auto K = self_reshaped.size(1);
+
+  auto in_ptr = self_.data_ptr<float>();
+  auto weight_ptr = weight.data_ptr<int8_t>();
+  auto out_ptr = output.data_ptr<float>();
+  auto zero_points_float_ptr = zero_points_float.data_ptr<float>();
+  auto scales_float_ptr = scales_float.data_ptr<float>();
   auto N = weight.size(0);
-#if defined(CPU_CAPABILITY_AVX512)
   const auto qtype = weight.qscheme();
 
   // TODO: per-tensor block size tuning
-  // dequant per tensor
-  // only fp32 activation and int8 weight is supported
-  if (qtype == c10::kPerTensorAffine) {
-    auto in_ptr = self_.data_ptr<float>();
-    auto weight_ptr = weight.data_ptr<int8_t>();
-    auto out_ptr = output.data_ptr<float>();
-    auto scales_float_ptr = scales_float.data_ptr<float>();
-    auto zero_points_float_ptr = zero_points_float.data_ptr<float>();
+  if (qtype == c10::kPerTensorAffine) { // per tensor
     if (bias.defined()) {
       auto bias_ = bias.is_contiguous() ? bias : bias.contiguous();
       auto bias_ptr = bias_.data_ptr<float>();
@@ -2503,8 +920,8 @@ void woq_gemm_kernel_impl(
           K,
           K,
           N,
-          scales_float_ptr[0],
           zero_points_float_ptr[0],
+          scales_float_ptr[0],
           bias_ptr);
     } else {
       return woq_gemm_brgemm_per_tensor<false, 24>(
@@ -2517,367 +934,227 @@ void woq_gemm_kernel_impl(
           K,
           K,
           N,
-          scales_float_ptr[0],
-          zero_points_float_ptr[0]);
-    }
-  }
-  // TODO: per-channel block size tuning
-  // dequant per channel
-  // activation of both fp32 and bf16, weight of int8 is supported
-  else if (qtype == c10::kPerChannelAffine) {
-    auto weight_ptr = weight.data_ptr<int8_t>();
-    auto scales_float_ptr = scales_float.data_ptr<float>();
-    auto zero_points_float_ptr = zero_points_float.data_ptr<float>();
-    if (self_.scalar_type() == at::kFloat) {
-      auto in_ptr = self_.data_ptr<float>();
-      auto out_ptr = output.data_ptr<float>();
-      if (bias.defined()) { // case with bias
-        auto bias_ = bias.is_contiguous() ? bias : bias.contiguous();
-        auto bias_ptr = bias_.data_ptr<float>();
-        if (M <= 4) { // small M
-          Function_matching<float, int8_t, true, 4>::func_match(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr,
-              bias_ptr);
-          return;
-        } else { // large M
-          return woq_gemm_brgemm<true, 196>(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr,
-              bias_ptr);
-        }
-      } else { // case without bias
-        if (M <= 4) { // small M
-          Function_matching<float, int8_t, false, 4>::func_match(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr);
-          return;
-        } else { // large M
-          return woq_gemm_brgemm<false, 196>(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr);
-        }
-      }
-    } else if (self_.scalar_type() == at::kBFloat16) {
-      auto in_ptr = self_.data_ptr<BFloat16>();
-      auto out_ptr = output.data_ptr<BFloat16>();
-      if (bias.defined()) { // case with bias
-        auto bias_ = bias.is_contiguous() ? bias : bias.contiguous();
-        auto bias_ptr = bias_.data_ptr<float>();
-        if (M <= 4) { // small M
-          return Function_matching<BFloat16, int8_t, true, 4>::func_match(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr,
-              bias_ptr);
-        } else { // large M
-          return woq_gemm_brgemm<true, 196>(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr,
-              bias_ptr);
-        }
-      } else { // case without bias
-        if (M <= 4) { // small M
-          return Function_matching<BFloat16, int8_t, false, 4>::func_match(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr);
-        } else { // large M
-          return woq_gemm_brgemm<false, 196>(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr);
-        }
-      }
+          zero_points_float_ptr[0],
+          scales_float_ptr[0]);
     }
-  } else { // kPerChannelAffineFloatQParams
-
-    auto weight_ptr = weight.data_ptr<uint8_t>();
-    auto scales_float_ptr = scales_float.data_ptr<float>();
-    auto zero_points_float_ptr = zero_points_float.data_ptr<float>();
-
-    if (self_.scalar_type() == at::kFloat) {
-      auto in_ptr = self_.data_ptr<float>();
-      auto out_ptr = output.data_ptr<float>();
-      if (bias.defined()) { // case with bias
-        auto bias_ = bias.is_contiguous() ? bias : bias.contiguous();
-        auto bias_ptr = bias_.data_ptr<float>();
-        if (M <= 4) { // small M
-          Function_matching<float, uint8_t, true, 4>::func_match(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr,
-              bias_ptr);
-          return;
-        } else { // large M
-          return woq_gemm_brgemm<true, 196>(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr,
-              bias_ptr);
-        }
-      } else { // case without bias
-        if (M <= 4) { // small M
-          Function_matching<float, uint8_t, false, 4>::func_match(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr);
-          return;
-        } else { // large M
-          return woq_gemm_brgemm<false, 196>(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr);
+  } else if (qtype == c10::kPerChannelAffine) { // per channel
+    // TODO: per-channel block size tuning
+    if (bias.defined()) { // case with bias
+      auto bias_ = bias.is_contiguous() ? bias : bias.contiguous();
+      auto bias_ptr = bias_.data_ptr<float>();
+      if (M <= 4) {
+        switch (M) {
+          case 1:
+            return woq_gemm_intrinsic<true, 1>(
+                in_ptr,
+                weight_ptr,
+                out_ptr,
+                M,
+                N,
+                K,
+                K,
+                K,
+                N,
+                zero_points_float_ptr,
+                scales_float_ptr,
+                bias_ptr);
+            break;
+          case 2:
+            return woq_gemm_intrinsic<true, 2>(
+                in_ptr,
+                weight_ptr,
+                out_ptr,
+                M,
+                N,
+                K,
+                K,
+                K,
+                N,
+                zero_points_float_ptr,
+                scales_float_ptr,
+                bias_ptr);
+            break;
+          case 3:
+            return woq_gemm_intrinsic<true, 3>(
+                in_ptr,
+                weight_ptr,
+                out_ptr,
+                M,
+                N,
+                K,
+                K,
+                K,
+                N,
+                zero_points_float_ptr,
+                scales_float_ptr,
+                bias_ptr);
+            break;
+          case 4:
+            return woq_gemm_intrinsic<true, 4>(
+                in_ptr,
+                weight_ptr,
+                out_ptr,
+                M,
+                N,
+                K,
+                K,
+                K,
+                N,
+                zero_points_float_ptr,
+                scales_float_ptr,
+                bias_ptr);
+            break;
         }
+      } else if (M < 196) { // small M
+        return woq_gemm_brgemm<true>(
+            in_ptr,
+            weight_ptr,
+            out_ptr,
+            M,
+            N,
+            K,
+            K,
+            K,
+            N,
+            zero_points_float_ptr,
+            scales_float_ptr,
+            bias_ptr);
+      } else { // large M
+        return woq_gemm_brgemm<true, 196>(
+            in_ptr,
+            weight_ptr,
+            out_ptr,
+            M,
+            N,
+            K,
+            K,
+            K,
+            N,
+            zero_points_float_ptr,
+            scales_float_ptr,
+            bias_ptr);
       }
-    } else if (self_.scalar_type() == at::kBFloat16) {
-      auto in_ptr = self_.data_ptr<BFloat16>();
-      auto out_ptr = output.data_ptr<BFloat16>();
-      if (bias.defined()) { // case with bias
-        auto bias_ = bias.is_contiguous() ? bias : bias.contiguous();
-        auto bias_ptr = bias_.data_ptr<float>();
-        if (M <= 4) { // small M
-          return Function_matching<BFloat16, uint8_t, true, 4>::func_match(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr,
-              bias_ptr);
-        } else { // large M
-          return woq_gemm_brgemm<true, 196>(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr,
-              bias_ptr);
-        }
-      } else { // case without bias
-        if (M <= 4) { // small M
-          return Function_matching<BFloat16, uint8_t, false, 4>::func_match(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr);
-        } else { // large M
-          return woq_gemm_brgemm<false, 196>(
-              in_ptr,
-              weight_ptr,
-              out_ptr,
-              M,
-              N,
-              K,
-              K,
-              K,
-              N,
-              scales_float_ptr,
-              zero_points_float_ptr);
+    } else { // case without bias
+      if (M <= 4) {
+        switch (M) {
+          case 1:
+            return woq_gemm_intrinsic<false, 1>(
+                in_ptr,
+                weight_ptr,
+                out_ptr,
+                M,
+                N,
+                K,
+                K,
+                K,
+                N,
+                zero_points_float_ptr,
+                scales_float_ptr);
+            break;
+          case 2:
+            return woq_gemm_intrinsic<false, 2>(
+                in_ptr,
+                weight_ptr,
+                out_ptr,
+                M,
+                N,
+                K,
+                K,
+                K,
+                N,
+                zero_points_float_ptr,
+                scales_float_ptr);
+            break;
+          case 3:
+            return woq_gemm_intrinsic<false, 3>(
+                in_ptr,
+                weight_ptr,
+                out_ptr,
+                M,
+                N,
+                K,
+                K,
+                K,
+                N,
+                zero_points_float_ptr,
+                scales_float_ptr);
+            break;
+          case 4:
+            return woq_gemm_intrinsic<false, 4>(
+                in_ptr,
+                weight_ptr,
+                out_ptr,
+                M,
+                N,
+                K,
+                K,
+                K,
+                N,
+                zero_points_float_ptr,
+                scales_float_ptr);
+            break;
         }
+      } else if (M < 196) {
+        return woq_gemm_brgemm<false>(
+            in_ptr,
+            weight_ptr,
+            out_ptr,
+            M,
+            N,
+            K,
+            K,
+            K,
+            N,
+            zero_points_float_ptr,
+            scales_float_ptr);
+      } else {
+        return woq_gemm_brgemm<false, 196>(
+            in_ptr,
+            weight_ptr,
+            out_ptr,
+            M,
+            N,
+            K,
+            K,
+            K,
+            N,
+            zero_points_float_ptr,
+            scales_float_ptr);
       }
     }
   }
 #else
-  if (self.scalar_type() == c10::ScalarType::Float) {
-    auto w = weight.dequantize();
-    if (bias.defined()) {
-      at::linear_out(output, self, w, bias.detach());
-    } else {
-      at::linear_out(output, self, w);
-    }
+  auto w = weight.dequantize();
+  if (bias.defined()) {
+    at::linear_out(output, self, w, bias.detach());
   } else {
-    auto w = weight.dequantize();
-    auto x = self.to(c10::ScalarType::Float);
-    auto out = at::linear(x, w);
-    if (bias.defined()) {
-      out = at::add(out, bias);
-    }
-    output = out.to(self.scalar_type());
+    at::linear_out(output, self, w);
   }
 
 #endif
 }
 
-void woq_gemm_eltwise_kernel_impl(
-    const at::Tensor& self,
-    const at::Tensor& weight,
-    const at::Tensor& scales_float,
-    const at::Tensor& zero_points_float,
-    const at::Tensor& bias,
-    const c10::string_view& post_op,
-    const torch::List<c10::optional<at::Scalar>>& scalars,
-    const c10::optional<c10::string_view>& algorithm,
-    int64_t lowp_mode,
-    at::Tensor& output) {
-  // TODO Postop not yet implemented in kernel
-  // Here we apply post op after GEMM
-  woq_gemm_kernel_impl(
-      self,
-      weight,
-      scales_float,
-      zero_points_float,
-      bias,
-      lowp_mode,
-      output);
-  auto postop_func = postop_func_map[post_op](scalars, algorithm);
-  postop_func(output);
-}
-
 at::Tensor woq_linear_packB_impl(
     const at::Tensor& weight,
-    const at::Tensor& scales,
-    const at::Tensor& zero_points) {
+    const at::Tensor& zero_points,
+    const at::Tensor& scales) {
 #if defined(CPU_CAPABILITY_AVX512)
   auto N = weight.size(0);
   auto K = weight.size(1);
   auto weight_size = weight.sizes().vec();
+  auto weight_packed = at::_empty_per_channel_affine_quantized(
+      weight_size,
+      scales,
+      zero_points,
+      1,
+      device(c10::kCPU).dtype(c10::kQInt8));
   auto weight_contig = weight.contiguous();
-  const auto qtype = weight.qscheme();
-  if (weight.scalar_type() == c10::ScalarType::QUInt4x2) { // int4 weight
-    auto weight_packed = at::_empty_per_channel_affine_quantized(
-        weight_size,
-        scales,
-        zero_points,
-        1,
-        device(c10::kCPU).dtype(c10::kQUInt4x2));
-    auto weight_ptr = weight_contig.data_ptr<uint8_t>();
-    auto weightpacked_ptr =
-        reinterpret_cast<uint8_t*>(weight_packed.data_ptr());
-    pack(weight_ptr, weightpacked_ptr, K, N, (K + 1) / 2, true);
-    return weight_packed;
-  } else { // int8 weight
-    auto weight_packed = at::_empty_per_channel_affine_quantized(
-        weight_size,
-        scales,
-        zero_points,
-        1,
-        device(c10::kCPU).dtype(c10::kQInt8));
-
-    auto weight_ptr = weight_contig.data_ptr<int8_t>();
-    auto weightpacked_ptr = reinterpret_cast<int8_t*>(weight_packed.data_ptr());
-    pack(weight_ptr, weightpacked_ptr, K, N, K, true);
-    return weight_packed;
-  }
+
+  auto weight_ptr = weight_contig.data_ptr<int8_t>();
+  auto weightpacked_ptr = reinterpret_cast<int8_t*>(weight_packed.data_ptr());
+  pack(weight_ptr, weightpacked_ptr, K, N, K, true);
+
+  return weight_packed;
 #else
   return weight;
 #endif
@@ -2887,117 +1164,81 @@ at::Tensor woq_linear_unpackB_impl(const at::Tensor& weight) {
 #if defined(CPU_CAPABILITY_AVX512)
   auto N = weight.size(0);
   auto K = weight.size(1);
+  std::vector<int32_t> weight_zero_points_int32(1, 0);
   const auto qtype = weight.qscheme();
-
-  if (weight.scalar_type() == c10::ScalarType::QUInt4x2) { // int4 weight
-    std::vector<float> weight_scales_float(1, 0.0);
-    if (qtype == c10::kPerChannelAffineFloatQParams) {
-      weight_scales_float.resize(N, 0.0);
-      for (const auto i : c10::irange(N)) {
-        weight_scales_float[i] = weight.q_per_channel_scales()[i].item<float>();
-      }
-    }
-    at::Tensor scales = at::empty(
-        {static_cast<long>(weight_scales_float.size())},
-        at::device(c10::kCPU).dtype(c10::kFloat));
-    std::copy(
-        weight_scales_float.begin(),
-        weight_scales_float.end(),
-        scales.data_ptr<float>());
-
-    std::vector<float> weight_zero_points_float(1, 0); // zero_points is float
-    if (qtype == c10::kPerChannelAffineFloatQParams) {
-      weight_zero_points_float.resize(N, 0);
-      for (const auto i : c10::irange(N)) {
-        weight_zero_points_float[i] =
-            weight.q_per_channel_zero_points()[i].item<float>();
-      }
-    }
-    at::Tensor zero_points = at::empty(
-        {static_cast<long>(weight_zero_points_float.size())},
-        at::device(c10::kCPU).dtype(c10::kFloat));
-    std::copy(
-        weight_zero_points_float.begin(),
-        weight_zero_points_float.end(),
-        zero_points.data_ptr<float>());
-
-    auto weight_size = weight.sizes().vec();
-    auto weight_unpacked = at::_empty_per_channel_affine_quantized(
-        weight_size,
-        scales,
-        zero_points,
-        0,
-        device(c10::kCPU).dtype(c10::kQUInt4x2));
-
-    auto weight_contig = weight.contiguous();
-
-    auto weight_ptr = weight_contig.data_ptr<uint8_t>();
-    auto weight_unpacked_ptr =
-        reinterpret_cast<uint8_t*>(weight_unpacked.data_ptr());
-    unpack(weight_ptr, weight_unpacked_ptr, K, N, K, true);
-    return weight_unpacked;
-  } else { // int8 weight
-    std::vector<float> weight_scales_float(1, 0.0);
-    if (qtype == c10::kPerTensorAffine) {
-      weight_scales_float[0] = weight.q_scale();
-    } else if (qtype == c10::kPerChannelAffine) {
-      weight_scales_float.resize(N, 0.0);
-      for (const auto i : c10::irange(N)) {
-        weight_scales_float[i] = weight.q_per_channel_scales()[i].item<float>();
-      }
+  if (qtype == c10::kPerTensorAffine) {
+    weight_zero_points_int32[0] = weight.q_zero_point();
+  } else if (qtype == c10::kPerChannelAffine) {
+    weight_zero_points_int32.resize(N, 0);
+    for (const auto i : c10::irange(N)) {
+      weight_zero_points_int32[i] =
+          weight.q_per_channel_zero_points()[i].item<int32_t>();
     }
-    at::Tensor scales = at::empty(
-        {static_cast<long>(weight_scales_float.size())},
-        at::device(c10::kCPU).dtype(c10::kFloat));
-    std::copy(
-        weight_scales_float.begin(),
-        weight_scales_float.end(),
-        scales.data_ptr<float>());
-
-    std::vector<int32_t> weight_zero_points_int32(1, 0); // zero points is int32
-    if (qtype == c10::kPerTensorAffine) {
-      weight_zero_points_int32[0] = weight.q_zero_point();
-    } else if (qtype == c10::kPerChannelAffine) {
-      weight_zero_points_int32.resize(N, 0);
-      for (const auto i : c10::irange(N)) {
-        weight_zero_points_int32[i] =
-            weight.q_per_channel_zero_points()[i].item<int32_t>();
-      }
+  }
+
+  at::Tensor zero_points = at::empty(
+      {static_cast<long>(weight_zero_points_int32.size())},
+      at::device(c10::kCPU).dtype(c10::kInt));
+  std::copy(
+      weight_zero_points_int32.begin(),
+      weight_zero_points_int32.end(),
+      zero_points.data_ptr<int32_t>());
+
+  std::vector<float> weight_scales_float(1, 0.0);
+  if (qtype == c10::kPerTensorAffine) {
+    weight_scales_float[0] = weight.q_scale();
+  } else if (qtype == c10::kPerChannelAffine) {
+    weight_scales_float.resize(N, 0.0);
+    for (const auto i : c10::irange(N)) {
+      weight_scales_float[i] = weight.q_per_channel_scales()[i].item<float>();
     }
-    at::Tensor zero_points = at::empty(
-        {static_cast<long>(weight_zero_points_int32.size())},
-        at::device(c10::kCPU).dtype(c10::kInt));
-    std::copy(
-        weight_zero_points_int32.begin(),
-        weight_zero_points_int32.end(),
-        zero_points.data_ptr<int32_t>());
-
-    auto weight_size = weight.sizes().vec();
-    auto weight_unpacked = at::_empty_per_channel_affine_quantized(
-        weight_size,
-        scales,
-        zero_points,
-        0,
-        device(c10::kCPU).dtype(c10::kQInt8));
-
-    auto weight_contig = weight.contiguous();
-
-    auto weight_ptr = weight_contig.data_ptr<int8_t>();
-    auto weight_unpacked_ptr =
-        reinterpret_cast<int8_t*>(weight_unpacked.data_ptr());
-    unpack(weight_ptr, weight_unpacked_ptr, K, N, K, true);
-    return weight_unpacked;
   }
 
+  at::Tensor scales = at::empty(
+      {static_cast<long>(weight_scales_float.size())},
+      at::device(c10::kCPU).dtype(c10::kFloat));
+  std::copy(
+      weight_scales_float.begin(),
+      weight_scales_float.end(),
+      scales.data_ptr<float>());
+
+  auto weight_size = weight.sizes().vec();
+  auto weight_unpacked = at::_empty_per_channel_affine_quantized(
+      weight_size,
+      scales,
+      zero_points,
+      1,
+      device(c10::kCPU).dtype(c10::kQInt8));
+
+  auto weight_contig = weight.contiguous();
+
+  auto weight_ptr = weight_contig.data_ptr<int8_t>();
+  auto weight_unpacked_ptr =
+      reinterpret_cast<int8_t*>(weight_unpacked.data_ptr());
+  unpack(weight_ptr, weight_unpacked_ptr, K, N, K, true);
+  return weight_unpacked;
 #else
   return weight;
 #endif
 }
-} // namespace
+} // anonymous namespace
 
 REGISTER_DISPATCH(woq_gemm_kernel_stub, &woq_gemm_kernel_impl);
-REGISTER_DISPATCH(woq_gemm_eltwise_kernel_stub, &woq_gemm_eltwise_kernel_impl);
 REGISTER_DISPATCH(woq_linear_unpackB_stub, &woq_linear_unpackB_impl);
 REGISTER_DISPATCH(woq_linear_packB_stub, &woq_linear_packB_impl);
 } // namespace cpu
-} // namespace torch_ipex
\ No newline at end of file
+} // namespace torch_ipex
+
+namespace std {
+template <>
+struct hash<torch_ipex::cpu::DotMicroKernelKey> {
+  std::size_t operator()(const torch_ipex::cpu::DotMicroKernelKey& key) const {
+    std::size_t h = std::hash<bool>()(key.trans_a);
+    h = std::hash<bool>()(key.trans_b) ^ (h << 1);
+    h = std::hash<int>()(key.lda) ^ (h << 1);
+    h = std::hash<int>()(key.ldb) ^ (h << 1);
+    h = std::hash<int>()(key.ldc) ^ (h << 1);
+    return h;
+  }
+};
+} // namespace std
\ No newline at end of file
diff --git a/csrc/cpu/aten/kernels/WoqTppKnl.cpp b/csrc/cpu/aten/kernels/WoqTppKnl.cpp
deleted file mode 100644
index ee782c6e..00000000
--- a/csrc/cpu/aten/kernels/WoqTppKnl.cpp
+++ /dev/null
@@ -1,1840 +0,0 @@
-// weight-only quantization gemm kernel (int8, int4 etc.)
-// #include <torch/extension.h>
-#include <ATen/ATen.h>
-#include <ATen/Tensor.h>
-#include <aten/Linear.h>
-#include <ATen/cpu/vec/vec.h>
-#include "csrc/cpu/tpp/woq/tla.h"
-
-namespace torch_ipex {
-namespace cpu {
-namespace {
-
-using namespace tpp;
-using TensorList = std::vector<at::Tensor>;
-
-#define SMALL_BATCH_THRESHOLD 32
-#define PARALLEL_M_THRESHOLD 128
-constexpr long PREFETCH_K_DIST = 64; // TODO(jgong5): do not hard-code
-constexpr long LOOP_K_UNROLL = 4; // TODO(jgong5): do not hard-code
-
-template <long N_GROUP_SIZE, typename VAT, typename LUT>
-inline VAT load_dequant_zp_only_int4(uint8_t *p, VAT vzps, LUT lut) {
-  TLA_ASSERT(false, "not implemented");
-}
-
-template <long N_GROUP_SIZE, typename VAT>
-inline VAT load_dequant_zp_only_int8(uint8_t *p, VAT vzps) {
-  TLA_ASSERT(false, "not implemented");
-}
-
-// TODO(jgong5): further simplify the dequant intrinsics below with VecOps
-#ifdef __AVX512F__
-template <>
-inline std::array<__m512, 4> load_dequant_zp_only_int4<64>(
-  uint8_t *p, std::array<__m512, 4> vzps, __m512 lut
-) {
-  using T = float;
-  using VA = VecArray<64, T>;
-  using VAT = typename VA::type;
-  constexpr long COLS = VA::num_vec;
-  auto packed = _mm256_loadu_si256((__m256i*)p);
-  __m512i int32[COLS];
-  {
-    auto low_4bit = _mm512_cvtepu8_epi32(_mm256_castsi256_si128(packed));
-    auto high_4bit = _mm512_srli_epi32(low_4bit, 4);
-    int32[0] = low_4bit;
-    int32[2] = high_4bit;
-  }
-  {
-    auto low_4bit =
-        _mm512_cvtepu8_epi32(_mm256_extracti128_si256(packed, 1));
-    auto high_4bit = _mm512_srli_epi32(low_4bit, 4);
-    int32[1] = low_4bit;
-    int32[3] = high_4bit;
-  }
-  VAT vbs;
-  compile_time_for<COLS>::op(
-    [&](auto idx) {
-      vbs[idx] = _mm512_permutexvar_ps(int32[idx], lut);
-      vbs[idx] = _mm512_sub_ps(vbs[idx], vzps[idx]);
-    }
-  );
-  return vbs;
-}
-
-template <>
-inline std::array<__m512, 2> load_dequant_zp_only_int4<32>(
-  uint8_t *p, std::array<__m512, 2> vzps, __m512 lut
-) {
-  using T = float;
-  using VA = VecArray<32, T>;
-  using VAT = typename VA::type;
-  constexpr long COLS = VA::num_vec;
-  auto packed = _mm_loadu_si128((__m128i*)p);
-  __m512i int32[COLS];
-  {
-    auto low_4bit = _mm512_cvtepu8_epi32(packed);
-    auto high_4bit = _mm512_srli_epi32(low_4bit, 4);
-    int32[0] = low_4bit;
-    int32[1] = high_4bit;
-  }
-  VAT vbs;
-  compile_time_for<COLS>::op(
-    [&](auto idx) {
-      vbs[idx] = _mm512_permutexvar_ps(int32[idx], lut);
-      vbs[idx] = _mm512_sub_ps(vbs[idx], vzps[idx]);
-    }
-  );
-  return vbs;
-}
-
-template <>
-inline std::array<__m512, 1> load_dequant_zp_only_int4<16>(
-  uint8_t *p, std::array<__m512, 1> vzps, __m512 lut
-) {
-  using T = float;
-  using VA = VecArray<16, T>;
-  using VAT = typename VA::type;
-  constexpr long COLS = VA::num_vec;
-  static_assert(COLS == 1, "COLS must be 1");
-  uint64_t packed = reinterpret_cast<uint64_t*>(p)[0];
-  uint64_t high = packed >> 4;
-  __m128i packed_128 = _mm_set_epi64x(high, packed);
-  __m512i int32 = _mm512_cvtepu8_epi32(packed_128);
-  VAT vbs;
-  vbs[0] = _mm512_permutexvar_ps(int32, lut);
-  vbs[0] = _mm512_sub_ps(vbs[0], vzps[0]);
-  return vbs;
-}
-
-template <>
-inline std::array<__m512, 4> load_dequant_zp_only_int8<64>(
-  uint8_t *p, std::array<__m512, 4> vzps
-) {
-  using T = float;
-  using VA = VecArray<64, T>;
-  using VAT = typename VA::type;
-  constexpr long COLS = VA::num_vec;
-  auto packed = _mm512_loadu_si512((__m512i*)p);
-  VAT vbs;
-  compile_time_for<COLS>::op(
-    [&](auto i) {
-      constexpr long imm = i;
-      auto int8 = _mm512_extracti32x4_epi32(packed, imm);
-      vbs[i] = _mm512_cvtepi32_ps(_mm512_cvtepi8_epi32(int8));
-      vbs[i] = _mm512_sub_ps(vbs[i], vzps[i]);
-    }
-  );
-  return vbs;
-}
-
-template <>
-inline std::array<__m512, 2> load_dequant_zp_only_int8<32>(
-  uint8_t *p, std::array<__m512, 2> vzps
-) {
-  using T = float;
-  using VA = VecArray<32, T>;
-  using VAT = typename VA::type;
-  constexpr long COLS = VA::num_vec;
-  auto packed = _mm256_loadu_si256((__m256i*)p);
-  VAT vbs;
-  compile_time_for<COLS>::op(
-    [&](auto i) {
-      constexpr long imm = i;
-      auto int8 = _mm256_extracti128_si256(packed, imm);
-      vbs[i] = _mm512_cvtepi32_ps(_mm512_cvtepi8_epi32(int8));
-      vbs[i] = _mm512_sub_ps(vbs[i], vzps[i]);
-    }
-  );
-  return vbs;
-}
-
-template <>
-inline std::array<__m512, 1> load_dequant_zp_only_int8<16>(
-  uint8_t *p, std::array<__m512, 1> vzps
-) {
-  using T = float;
-  using VA = VecArray<16, T>;
-  using VAT = typename VA::type;
-  constexpr long COLS = VA::num_vec;
-  static_assert(COLS == 1);
-  auto packed = _mm_loadu_si128((__m128i*)p);
-  VAT vbs;
-  vbs[0] = _mm512_cvtepi32_ps(_mm512_cvtepi8_epi32(packed));
-  vbs[0] = _mm512_sub_ps(vbs[0], vzps[0]);
-  return vbs;
-}
-
-inline __m512i combine_m256i(__m256i a, __m256i b) {
-  __m512i c = _mm512_castsi256_si512(a);
-  return _mm512_inserti64x4(c, b, 1);
-}
-
-inline __m512i combine_m256i(std::array<__m256i, 2> two_256) {
-  return combine_m256i(two_256[0], two_256[1]);
-}
-
-inline std::array<__m256i, 2> load_zps_4vnni(int8_t* zps) {
-  // broadcast 01234567 to
-  // 01234567012345670123456701234567
-  __m256i vzps_low = _mm256_set1_epi64x(*reinterpret_cast<long*>(zps));
-  __m256i vzps_high = _mm256_set1_epi64x(*reinterpret_cast<long*>(zps+8));
-  // shuffle from
-  // 01234567012345670123456701234567
-  // to
-  // 00001111222233334444555566667777
-  __m256i shuffle_mask = _mm256_set_epi8(
-    7, 7, 7, 7,
-    6, 6, 6, 6,
-    5, 5, 5, 5,
-    4, 4, 4, 4,
-    3, 3, 3, 3,
-    2, 2, 2, 2,
-    1, 1, 1, 1,
-    0, 0, 0, 0
-  );
-  vzps_low = _mm256_shuffle_epi8(vzps_low, shuffle_mask);
-  vzps_high = _mm256_shuffle_epi8(vzps_high, shuffle_mask);
-  return {vzps_low, vzps_high};
-}
-
-inline std::array<__m256i, 2> load_int4_as_int8(uint8_t* qB) {
-  __m256i packed = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(qB));
-  const __m256i low_mask = _mm256_set1_epi8(0x0f);
-  __m256i high = _mm256_srli_epi16(packed, 4);
-  high = _mm256_and_si256(high, low_mask);
-  __m256i low = _mm256_and_si256(packed, low_mask);
-  return {low, high};
-}
-
-#else
-inline std::array<__m256i, 2> load_zps_4vnni(int8_t* zps) {
-  TLA_ASSERT(false, "not implemented");
-  return std::array<__m256i, 2>();
-}
-
-inline std::array<__m256i, 2> load_int4_as_int8(uint8_t* qB) {
-  TLA_ASSERT(false, "not implemented");
-  return std::array<__m256i, 2>();
-}
-
-#endif
-
-#ifdef __AVX512FP16__
-template<>
-inline std::array<__m512h, 2> load_dequant_zp_only_int4<64>(
-  uint8_t* p, std::array<__m512h, 2> vzps, __m512h lut
-) {
-  using T = tpp::half;
-  using VA = VecArray<64, T>;
-  using VAT = typename VA::type;
-  constexpr long COLS = VA::num_vec;
-  auto packed = _mm256_loadu_si256((__m256i*)p);
-  __m512i int32[COLS];
-  {
-    auto low_4bit = _mm512_cvtepu8_epi16(packed);
-    auto high_4bit = _mm512_srli_epi16(low_4bit, 4);
-    int32[0] = low_4bit;
-    int32[1] = high_4bit;
-  }
-  VAT vbs;
-  compile_time_for<COLS>::op(
-    [&](auto idx) {
-      vbs[idx] = _mm512_permutexvar_ph(int32[idx], lut);
-      vbs[idx] = _mm512_sub_ph(vbs[idx], vzps[idx]);
-    }
-  );
-  return vbs;
-}
-
-template<>
-inline std::array<__m512h, 1> load_dequant_zp_only_int4<32>(
-  uint8_t* p, std::array<__m512h, 1> vzps, __m512h lut
-) {
-  using T = tpp::half;
-  using VA = VecArray<32, T>;
-  using VAT = typename VA::type;
-  constexpr long COLS = VA::num_vec;
-  auto packed = _mm_loadu_si128((__m128i*)p);
-  __m512i int32[COLS];
-  {
-    auto low_4bit = _mm256_cvtepu8_epi16(packed);
-    auto high_4bit = _mm256_srli_epi16(low_4bit, 4);
-    // combine low_4bit and high_4bit into __m512i
-    int32[0] = _mm512_inserti64x4(_mm512_castsi256_si512(low_4bit), high_4bit, 1);
-  }
-  VAT vbs;
-  compile_time_for<COLS>::op(
-    [&](auto idx) {
-      vbs[idx] = _mm512_permutexvar_ph(int32[idx], lut);
-      vbs[idx] = _mm512_sub_ph(vbs[idx], vzps[idx]);
-    }
-  );
-  return vbs;
-}
-
-template<>
-inline std::array<__m512h, 2> load_dequant_zp_only_int8<64>(uint8_t* p, std::array<__m512h, 2> vzps) {
-  using T = tpp::half;
-  using VA = VecArray<64, T>;
-  using VAT = typename VA::type;
-  constexpr long COLS = VA::num_vec;
-  auto packed = _mm512_loadu_si512((__m512i*)p);
-  VAT vbs;
-  compile_time_for<COLS>::op(
-    [&](auto i) {
-      constexpr long imm = i;
-      auto int8 = _mm512_extracti64x4_epi64(packed, imm);
-      vbs[i] = _mm512_cvtepi16_ph(_mm512_cvtepi8_epi16(int8));
-      vbs[i] = _mm512_sub_ph(vbs[i], vzps[i]);
-    }
-  );
-  return vbs;
-}
-
-template<>
-inline std::array<__m512h, 1> load_dequant_zp_only_int8<32>(
-  uint8_t* p, std::array<__m512h, 1> vzps
-) {
-  using T = tpp::half;
-  using VA = VecArray<32, T>;
-  using VAT = typename VA::type;
-  constexpr long COLS = VA::num_vec;
-  auto packed = _mm256_loadu_si256((__m256i*)p);
-  VAT vbs;
-  compile_time_for<COLS>::op(
-    [&](auto i) {
-      constexpr long imm = i;
-      vbs[i] = _mm512_cvtepi16_ph(_mm512_cvtepi8_epi16(packed));
-      vbs[i] = _mm512_sub_ph(vbs[i], vzps[i]);
-    }
-  );
-  return vbs;
-}
-#endif
-
-template<long N, typename T>
-struct load_dequant_int4 {
-  using VT = typename VecType<T>::type;
-  using V = VecOps<VT>;
-  using VA = VecArray<N, T>;
-  using VAT = typename VA::type;
-  constexpr static long COLS = VA::num_vec;
-
-  static inline VAT call(uint8_t *p, VAT vscales, VAT vzps, VT lut) {
-    auto vbs = load_dequant_zp_only_int4<N>(p, vzps, lut);
-    compile_time_for<COLS>::op(
-      [&](auto idx) {
-        vbs[idx] = V::mul(vbs[idx], vscales[idx]);
-      }
-    );
-    return vbs;
-  }
-};
-
-template<long N, typename T>
-struct load_dequant_int8 {
-  using VT = typename VecType<T>::type;
-  using V = VecOps<VT>;
-  using VA = VecArray<N, T>;
-  using VAT = typename VA::type;
-  constexpr static long COLS = VA::num_vec;
-
-  static inline VAT call(uint8_t *p, VAT vscales, VAT vzps) {
-    auto vbs = load_dequant_zp_only_int8<N>(p, vzps);
-    compile_time_for<COLS>::op(
-      [&](auto idx) {
-        vbs[idx] = V::mul(vbs[idx], vscales[idx]);
-      }
-    );
-    return vbs;
-  }
-};
-
-constexpr int get_n_group_size(int N) {
-  return N == 16 ? 16 : (N == 32 ? 32 : 64);
-}
-
-// TODO(jgong5): move to tpp.h
-// TODO(jgong5): add pre/post op fusion
-template <
-  typename T, typename Tout, typename TScale, typename TZero, long M, long N, long ldb,
-  bool transA=false, bool ACC=false, long PREFETCH_K_DIST=0, typename Enabled=void
->
-struct GemmMicroKernel {
-  template <bool is_int4>
-  static inline void call(long K, T* A, long lda, uint8_t* B, Tout* C, long ldc, TScale* scales, TZero* zps) {
-    TLA_ASSERT(false, "Not implemented");
-  }
-};
-
-template <
-  typename T, long M, long N, long ldb, bool transA, bool ACC, long PREFETCH_K_DIST
->
-struct GemmMicroKernel<
-  T, T, T, T, M, N, ldb, transA, ACC, PREFETCH_K_DIST,
-  typename std::enable_if_t<std::is_same<T, float>::value || std::is_same<T, half>::value>
-> {
-  // TODO(jgong5): generalize this with pre/post op handlers
-  template <bool is_int4>
-  static inline void call(long K, T* A, long lda, uint8_t* B, T* C, long ldc, T* scales, T* zps) {
-    #define INDEX(x, y, ld) ((x) * (ld) + (y))
-    #define ADDRESS(p, x, y, ld) ((p) + (x) * (ld) + (y))
-
-    static_assert(N % 16 == 0, "N must be a multiple of 16");
-    constexpr const int N_GROUP_SIZE = get_n_group_size(N);
-
-    using VT = typename VecType<T>::type;
-    using V = VecOps<VT>;
-    using ST = typename V::ST;
-    using VArray = VecArray<N_GROUP_SIZE, T>;
-    using VArrayT = typename VArray::type;
-
-    constexpr const int COLS = N / V::VLEN;
-    constexpr const int CBLOCK = N_GROUP_SIZE / V::VLEN;
-    constexpr const int CNBLOCKS = N / N_GROUP_SIZE;
-    VT va[M];
-    VArrayT vb[CNBLOCKS];
-    VT vc[M * COLS];
-    VArrayT vscales[CNBLOCKS];
-    VArrayT vzps[CNBLOCKS];
-
-    VT lut = V::set_0_to_15();
-
-    // Load scales and zps
-    compile_time_for<CNBLOCKS>::op(
-      [&](auto i) {
-        constexpr const int col = i * CBLOCK;
-        vscales[i] = VArray::load1d(scales + col * V::VLEN);
-        vzps[i] = VArray::load1d(zps + col * V::VLEN);
-      }
-    );
-
-    // NB: For fp16 in int8 woq, we do not delay the scale to the post-op but leave it
-    // to the dequant otherwise the weight value might be too large to overflow
-    // fp16 range.
-    constexpr bool scale_as_post_op = !std::is_same<T, half>() || is_int4;
-
-    compile_time_for<M * COLS>::op(
-      [&](auto i) { vc[i] = V::setzero(); }
-    );
-
-    auto compute = [&](auto i, int k) {
-      constexpr const int row = i / CNBLOCKS;
-      constexpr const int cbidx = i % CNBLOCKS;
-
-      if constexpr (cbidx == 0) {
-        if constexpr (transA) {
-          va[row] = V::set1(*(ST*)ADDRESS(A, k, row, lda));
-        } else {
-          va[row] = V::set1(*(ST*)ADDRESS(A, row, k, lda));
-        }
-      }
-
-      if constexpr (row == 0) {
-        constexpr const int col = cbidx * CBLOCK;
-        if constexpr (scale_as_post_op) {
-          if constexpr (is_int4) {
-            vb[cbidx] = load_dequant_zp_only_int4<N_GROUP_SIZE>(ADDRESS(B, k, col * V::VLEN / 2, ldb / 2), vzps[cbidx], lut);
-          } else {
-            vb[cbidx] = load_dequant_zp_only_int8<N_GROUP_SIZE>(ADDRESS(B, k, col * V::VLEN, ldb), vzps[cbidx]);
-          }
-        } else {
-          if constexpr (is_int4) {
-            vb[cbidx] = load_dequant_int4<N_GROUP_SIZE, T>::call(ADDRESS(B, k, col * V::VLEN / 2, ldb / 2), vscales[cbidx], vzps[cbidx], lut);
-          } else {
-            vb[cbidx] = load_dequant_int8<N_GROUP_SIZE, T>::call(ADDRESS(B, k, col * V::VLEN, ldb), vscales[cbidx], vzps[cbidx]);
-          }
-        }
-        if constexpr (PREFETCH_K_DIST > 0) {
-          if constexpr (is_int4) {
-            _mm_prefetch(ADDRESS(B, k + PREFETCH_K_DIST, col * V::VLEN / 2, ldb / 2), _MM_HINT_T0);
-          } else {
-            _mm_prefetch(ADDRESS(B, k + PREFETCH_K_DIST, col * V::VLEN, ldb), _MM_HINT_T0);
-          }
-        }
-      }
-
-      compile_time_for<CBLOCK>::op(
-        [&](auto col) {
-          constexpr const int idx = INDEX(row, INDEX(cbidx, col, CBLOCK), COLS);
-          vc[idx] = V::fmadd(va[row], vb[cbidx][col], vc[idx]);
-        }
-      );
-
-    };
-
-    // Accumulate along k
-    constexpr const int unroll = LOOP_K_UNROLL;
-    int k = 0;
-    for (; k < K / unroll; k++) {
-      compile_time_for<unroll>::op(
-        [&](auto i) {
-          compile_time_for<M * CNBLOCKS>::op(compute, k*unroll + i);
-        }
-      );
-    }
-    k *= unroll;
-    for (; k < K; k++) {
-      compile_time_for<M * CNBLOCKS>::op(compute, k);
-    }
-
-    // Store to C
-    auto store = [&](auto i) {
-      constexpr const int row = i / COLS;
-      constexpr const int col = i % COLS;
-      if constexpr (ACC) {
-        auto vc_old = V::loadu(ADDRESS(C, row, col * V::VLEN, ldc));
-        if constexpr (scale_as_post_op) {
-          vc[i] = V::fmadd(vscales[col/CBLOCK][col%CBLOCK], vc[i], vc_old);
-        } else {
-          vc[i] = V::fmadd(V::set1(1.0f), vc[i], vc_old);
-        }
-      } else if constexpr (scale_as_post_op) {
-        vc[i] = V::mul(vscales[col/CBLOCK][col%CBLOCK], vc[i]);
-      }
-      V::storeu(ADDRESS(C, row, col * V::VLEN, ldc), vc[i]);
-    };
-
-    compile_time_for<M * COLS>::op(store);
-  }
-};
-
-#ifdef __AVX512VNNI__
-template <
-  long M, long N, long ldb, bool transA, bool ACC, long PREFETCH_K_DIST
->
-struct GemmMicroKernel<
-  /*Tin*/uint8_t, /*Tout*/float, /*TScale*/float, /*TZero*/int8_t, M, N, ldb, transA, ACC, PREFETCH_K_DIST
-> {
-  template <bool is_int4>
-  static inline void call(
-    long K, uint8_t* A, long lda, uint8_t* B, float* C, long ldc,
-    float* scales, int8_t* zps, float scale_a, int32_t zp_a
-  ) {
-    auto pqB = GetVLAPtr<uint8_t>(B, {ldb, 2}); // [K/4,N,4] packed in 4-bit
-
-    static_assert(N % 16 == 0, "N must be a multiple of 16");
-    constexpr const int COLS = N / 16;
-
-    __m512i ones = _mm512_set1_epi8(1); // used for computing compensation
-    __m512i va;
-    __m512i vb[COLS];
-    __m512i vc[M * COLS];
-    __m512 vscales[COLS];
-    __m512i vzps[COLS];
-    __m512i vcompensate[COLS];
-
-    // Load scales and zps
-    compile_time_for<COLS>::op(
-      [&](auto i) {
-        vscales[i] = _mm512_loadu_ps(scales + i * 16);
-        // TODO(jgong5): should we use 512 or two 256 here?
-        vzps[i] = combine_m256i(load_zps_4vnni(zps + i * 16));
-        vcompensate[i] = _mm512_setzero_epi32();
-      }
-    );
-
-    compile_time_for<M * COLS>::op(
-      [&](auto i) { vc[i] = _mm512_setzero_epi32(); }
-    );
-
-    auto compute = [&](auto i, int k) {
-      constexpr const int row = i / COLS;
-      constexpr const int col = i % COLS;
-
-      if constexpr (col == 0) {
-        if constexpr (transA) {
-          va = _mm512_set1_epi32(*(int32_t*)ADDRESS(A, k, row, lda));
-        } else {
-          va = _mm512_set1_epi32(*(int32_t*)ADDRESS(A, row, k, lda));
-        }
-      }
-
-      if constexpr (row == 0) {
-        vb[col] = combine_m256i(load_int4_as_int8(pqB[k/4][col*16]));
-        vb[col] = _mm512_sub_epi8(vb[col], vzps[col]);
-        vcompensate[col] = _mm512_dpbusd_epi32(vcompensate[col], ones, vb[col]);
-        if constexpr (PREFETCH_K_DIST > 0) {
-          _mm_prefetch(pqB[(k + PREFETCH_K_DIST)/4][col*16], _MM_HINT_T0);
-        }
-      }
-
-      vc[i] = _mm512_dpbusd_epi32(vc[i], va, vb[col]);
-    };
-
-    // Accumulate along k
-    constexpr const int unroll = LOOP_K_UNROLL;
-    int k = 0;
-    for (; k < K / 4 / unroll; k++) {
-      compile_time_for<unroll>::op(
-        [&](auto i) {
-          compile_time_for<M * COLS>::op(compute, 4 * (k*unroll + i));
-        }
-      );
-    }
-    k *= 4 * unroll;
-    for (; k < K; k+=4) {
-      compile_time_for<M * COLS>::op(compute, k);
-    }
-
-    // Store to C
-    auto store = [&](auto i) {
-      constexpr const int row = i / COLS;
-      constexpr const int col = i % COLS;
-      // compute (qC - compensate * zp_a) * scale_a * scale_b
-      // where compensate = sum(qB)
-      vc[i] = _mm512_sub_epi32(
-        vc[i], _mm512_mullo_epi32(vcompensate[col], _mm512_set1_epi32(zp_a))
-      );
-      __m512 vc_float = _mm512_cvtepi32_ps(vc[i]);
-      vc_float = _mm512_mul_ps(vc_float, _mm512_set1_ps(scale_a));
-      vc_float = _mm512_mul_ps(vc_float, vscales[col]);
-      if constexpr (ACC) {
-        auto vc_old = _mm512_loadu_ps(C + row * ldc + col * 16);
-        vc_float = _mm512_add_ps(vc_float, vc_old);
-      }
-      _mm512_storeu_ps(C + row * ldc + col * 16, vc_float);
-    };
-    compile_time_for<M * COLS>::op(store);
-  }
-};
-#endif
-
-// a dequant function the requires N to be a multiple of N_GROUP_SIZE
-template <
-  typename Tin, long ldb, long N_GROUP_SIZE, bool is_int4
->
-struct dequant_n_grouped {
-  template <typename Lambda1, typename Lambda2, typename Lambda3>
-  static inline void call(
-    uint8_t* qB, long K, long N, Tin* scales, Tin* zps, Tin* B,
-    const Lambda1& load_qparam,
-    const Lambda2& load_qint_as_fp,
-    const Lambda3& store
-  ) {
-    for (int n = 0; n < N; n+=N_GROUP_SIZE) {
-      // load scales and zps
-      auto vscales = load_qparam(scales + n);
-      auto vzps = load_qparam(zps + n);
-      for (int k = 0; k < K; k++) {
-        // load and dequant qB to vb
-        auto vbs = load_qint_as_fp(is_int4 ? &qB[k*ldb/2 + n/2] : &qB[k*ldb + n], vscales, vzps);
-        // store vb to B
-        store(B + k*N + n, vbs);
-      }
-    }
-  }
-};
-
-#ifdef __AVX512F__
-template <
-  long ldb, long N_GROUP_SIZE, bool is_int4
->
-struct dequant_n_grouped<bfloat16, ldb, N_GROUP_SIZE, is_int4> {
-  template <typename Lambda1, typename Lambda2, typename Lambda3>
-  static inline void call(
-    uint8_t* qB, long K, long N, bfloat16* scales, bfloat16* zps, bfloat16* B,
-    const Lambda1& load_qparam,
-    const Lambda2& load_qint_as_fp,
-    const Lambda3& store
-  ) {
-    #define ADDRESS(p, x, y, ld) ((p) + (x) * (ld) + (y))
-
-    using VA = VecArray<N_GROUP_SIZE, float>;
-    constexpr long COLS = VA::num_vec;
-
-    for (int n = 0; n < N; n+=N_GROUP_SIZE) {
-      // load scales and zps
-      auto vscales = load_qparam(scales + n);
-      auto vzps = load_qparam(zps + n);
-      // convert to vnni: [K/2, N, 2]
-      for (int k = 0; k < K; k+=2) {
-        auto interleave = [](__m512 v0, __m512 v1) {
-          __m512i idx_low = _mm512_set_epi32(
-            0x17, 0x07, 0x16, 0x06,
-            0x15, 0x05, 0x14, 0x04,
-            0x13, 0x03, 0x12, 0x02,
-            0x11, 0x01, 0x10, 0x00
-          );
-          __m512i idx_high = _mm512_set_epi32(
-            0x1f, 0x0f, 0x1e, 0x0e,
-            0x1d, 0x0d, 0x1c, 0x0c,
-            0x1b, 0x0b, 0x1a, 0x0a,
-            0x19, 0x09, 0x18, 0x08
-          );
-          return std::array<__m512, 2>(
-            {_mm512_permutex2var_ps(v0, idx_low, v1), _mm512_permutex2var_ps(v0, idx_high, v1)}
-          );
-        };
-        // load and dequant qB to vb
-        auto vbs_k0 = load_qint_as_fp(is_int4 ? &qB[k*ldb/2 + n/2] : &qB[k*ldb + n], vscales, vzps);
-        auto vbs_k1 = load_qint_as_fp(is_int4 ? &qB[(k+1)*ldb/2 + n/2] : &qB[(k+1)*ldb + n], vscales, vzps);
-        typename VA::type vbs[2];
-        compile_time_for<COLS>::op(
-          [&](auto i) {
-            auto [low, high] = interleave(vbs_k0[i], vbs_k1[i]);
-            vbs[i*2/COLS][i*2%COLS] = low;
-            vbs[(i*2+1)/COLS][(i*2+1)%COLS] = high;
-          }
-        );
-        // store vb to B: low: [k + n*2 / N, n*2 % N], high: [k + (n*2+N_GROUP_SIZE) / N, (n*2+N_GROUP_SIZE) % N]
-        store(ADDRESS(B, k + (n*2) / N, (n*2) % N, N), vbs[0]);
-        store(ADDRESS(B, k + (n*2 + N_GROUP_SIZE) / N, (n*2 + N_GROUP_SIZE) % N, N), vbs[1]);
-      }
-    }
-  }
-};
-#endif
-
-template<typename Tin, long ldb, long N_GROUP_SIZE, bool is_int4>
-struct Dequantize {
-  static void call(uint8_t* qB, long K, long N, Tin* scales, Tin* zps, Tin* B);
-};
-
-template<long ldb, long N_GROUP_SIZE, bool is_int4>
-struct Dequantize<float, ldb, N_GROUP_SIZE, is_int4> {
-  static inline void call(uint8_t* qB, long K, long N, float* scales, float* zps, float* B) {
-#if defined(__AVX512F__)
-    using T = float;
-    using VA = VecArray<N_GROUP_SIZE, T>;
-    constexpr int VLEN = VA::vec_ops::VLEN;
-    constexpr long COLS = VA::num_vec;
-
-    __m512 lut = _mm512_set_ps(
-      15.0f, 14.0f, 13.0f, 12.0f, 11.0f, 10.0f, 9.0f, 8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f
-    );
-    dequant_n_grouped<float, ldb, N_GROUP_SIZE, is_int4>::call(
-      qB, K, N, scales, zps, B,
-      [&](float* p) {
-        return VA::load1d(p);
-      },
-      [&](uint8_t* p, auto vscales, auto vzps) {
-        if constexpr (is_int4) {
-          return load_dequant_int4<N_GROUP_SIZE, T>::call(p, vscales, vzps, lut);
-        } else {
-          return load_dequant_int8<N_GROUP_SIZE, T>::call(p, vscales, vzps);
-        }
-      },
-      [&](auto p, auto vbs) {
-        compile_time_for<COLS>::op(
-          [&](auto idx) {
-            _mm512_storeu_ps(p + idx*VLEN, vbs[idx]);
-          }
-        );
-      }
-    );
-#else
-    TLA_ASSERT(false, "not implemented");
-#endif
-  }
-};
-
-template<long ldb, long N_GROUP_SIZE, bool is_int4>
-struct Dequantize<bfloat16, ldb, N_GROUP_SIZE, is_int4> {
-  static inline void call(uint8_t* qB, long K, long N, bfloat16* scales, bfloat16* zps, bfloat16* B) {
-#ifdef __AVX512F__
-    using T = bfloat16;
-    using VA = VecArray<N_GROUP_SIZE, T>;
-    constexpr long COLS = VA::num_vec;
-
-    // lookup table converting uint8 to float, 15.0f - 0.0f
-    // _mm512_permutexvar_ph needs 5 bits while we only need 4 bits, init the table
-    // to honor the lower 4 bits regardless of the the highest bit, thus saving an "and" op
-     __m512 lut = _mm512_set_ps(
-      15.0f, 14.0f, 13.0f, 12.0f, 11.0f, 10.0f, 9.0f, 8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f
-    );
-    dequant_n_grouped<bfloat16, ldb, N_GROUP_SIZE, is_int4>::call(
-      qB, K, N, scales, zps, B,
-      [&](bfloat16* p) {
-        return VA::load1d(p);
-      },
-      [&](uint8_t* p, auto vscales, auto vzps) {
-        if constexpr (is_int4) {
-          return load_dequant_int4<N_GROUP_SIZE, float>::call(p, vscales, vzps, lut);
-        } else {
-          return load_dequant_int8<N_GROUP_SIZE, float>::call(p, vscales, vzps);
-        }
-      },
-      [&](auto p, auto vbs) {
-        compile_time_for<COLS/2>::op(
-          [&](auto idx) {
-            _vec_store_two_floats_as_bfloat16(p + idx*32, vbs[idx*2], vbs[idx*2+1]);
-          }
-        );
-      }
-    );
-#else
-    TLA_ASSERT(false, "not implemented");
-#endif
-  }
-};
-
-template<long ldb, long N_GROUP_SIZE, bool is_int4>
-struct Dequantize<half, ldb, N_GROUP_SIZE, is_int4> {
-  static inline void call(uint8_t* qB, long K, long N, half* scales, half* zps, half* B) {
-#ifdef __AVX512FP16__
-    using T = half;
-    using VA = VecArray<N_GROUP_SIZE, T>;
-    constexpr int VLEN = VA::vec_ops::VLEN;
-    constexpr long COLS = VA::num_vec;
-
-    // lookup table converting uint8 to float, 15.0f - 0.0f
-    // _mm512_permutexvar_ph needs 5 bits while we only need 4 bits, init the table
-    // to honor the lower 4 bits regardless of the the highest bit, thus saving an "and" op
-    __m512h lut = _mm512_set_ph(
-      15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.0,
-      15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.0
-    );
-    dequant_n_grouped<half, ldb, N_GROUP_SIZE, is_int4>::call(
-      qB, K, N, scales, zps, B,
-      [&](half* p) {
-        return VA::load1d(p);
-      },
-      [&](uint8_t* p, auto vscales, auto vzps) {
-        if constexpr (is_int4) {
-          return load_dequant_int4<N_GROUP_SIZE, T>::call(p, vscales, vzps, lut);
-        } else {
-          return load_dequant_int8<N_GROUP_SIZE, T>::call(p, vscales, vzps);
-        }
-      },
-      [&](auto p, auto vbs) {
-        compile_time_for<COLS>::op(
-          [&](auto idx) {
-            _mm512_storeu_ph(p + idx*VLEN, vbs[idx]);
-          }
-        );
-      }
-    );
-#else
-    TLA_ASSERT(false, "not implemented");
-#endif
-  }
-};
-
-template<long ldb>
-struct Dequantize<int8_t, ldb, /*N_GROUP_SIZE*/16, /*is_int4*/true> {
-  static inline void call(uint8_t* qB, long K, long N, int8_t* zps, int8_t* B, int32_t* compensation) {
-#ifdef __AVX512VNNI__
-    auto pqB = GetVLAPtr<uint8_t>(qB, {ldb, 2}); // [K/4,N,4] packed in 4-bit
-    auto pB = GetVLAPtr<int8_t>(B, {ldb, 4}); // [K/4,N,4]
-    __m256i ones = _mm256_set1_epi8(1);
-    for (int n = 0; n < N; n+=16) {
-      auto [vzps_low, vzps_high] = load_zps_4vnni(&zps[n]);
-      __m256i vcompensate[2];
-      vcompensate[0] = _mm256_setzero_si256();
-      vcompensate[1] = _mm256_setzero_si256();
-      // TODO(jgong5): unroll k?
-      for (int k = 0; k < K/4; k++) {
-        // TODO(jgong5): consider optimize the instruction sequence below, e.g, use avx512?
-        // load 64 (N:16, K:4) int4 values from qB
-        auto [low, high] = load_int4_as_int8(pqB[k][n]);
-        high = _mm256_sub_epi8(high, vzps_high);
-        low = _mm256_sub_epi8(low, vzps_low);
-        vcompensate[0] = _mm256_dpbusd_epi32(vcompensate[0], ones, low);
-        vcompensate[1] = _mm256_dpbusd_epi32(vcompensate[1], ones, high);
-        // store vb to B
-        _mm256_storeu_si256(reinterpret_cast<__m256i_u*>(pB[k][n]), low);
-        _mm256_storeu_si256(reinterpret_cast<__m256i_u*>(pB[k][n+8]), high);
-      }
-      _mm256_storeu_si256(reinterpret_cast<__m256i_u*>(&compensation[n]), vcompensate[0]);
-      _mm256_storeu_si256(reinterpret_cast<__m256i_u*>(&compensation[n+8]), vcompensate[1]);
-    }
-#else
-    TLA_ASSERT(false, "not implemented");
-#endif
-  }
-};
-
-// TODO(jgong5): move to tpp.h
-template <
-  typename Tin, typename Tout, typename TScale, typename TZero,
-  long BLOCK_M, long N, long ldb, bool transA, bool ACC, bool is_int4, long PREFETCH_K_DIST=0>
-class DequantGemmTPP {
-public:
-  DequantGemmTPP(
-      long M,
-      long K,
-      long lda,
-      long ldc
-  ) {
-    TLA_ASSERT(false, "not implemented");
-  }
-
-  void operator()(Tin* A, uint8_t* qB, TScale* scales, TZero* zps, Tout* C, bool no_tile_cfg = true, float scale_a = 1.0, int32_t zp_a = 0) {
-    TLA_ASSERT(false, "not implemented");
-  }
-
-  void config() {
-    TLA_ASSERT(false, "not implemented");
-  }
-
-  void release() {
-    TLA_ASSERT(false, "not implemented");
-  }
-};
-
-template <
-  typename Tin, typename Tout,
-  long BLOCK_M, long N, long ldb, bool transA, bool ACC, bool is_int4, long PREFETCH_K_DIST>
-class DequantGemmTPP<
-  Tin, Tout, Tin, Tin,
-  BLOCK_M, N, ldb, transA, ACC, is_int4, PREFETCH_K_DIST
-> {
-public:
-  DequantGemmTPP(
-      long M,
-      long K,
-      long lda,
-      long ldc
-  )
-  :
-  M(M),
-  K(K),
-  lda(lda),
-  ldc(ldc) {
-    static_assert(
-      N % 16 == 0,
-      "N must be a multiple of 16"
-    );
-    if (std::is_same<Tin, bfloat16>()) TLA_ASSERT(K % 2 == 0, "Kb must be a multiple of 2 for bfloat16");
-    pgemm = std::make_shared<BrgemmTPP<Tin, Tout>>(
-      M, N, K, 1, 1, lda, ldb, ldc, ACC ? 1 : 0, transA, 1, /*b_vnni*/std::is_same<Tin, bfloat16>()
-    );
-  }
-
-  inline void operator()(Tin* A, uint8_t* qB, Tin* scales, Tin* zps, Tout* C, bool no_tile_cfg = true, float scale_a = 1.0, int32_t zp_a = 0) {
-    if (
-      M < SMALL_BATCH_THRESHOLD &&
-      (
-        (std::is_same<Tin, half>() && std::is_same<Tout, half>()) ||
-        (std::is_same<Tin, float>() && std::is_same<Tout, float>())
-      )
-    ) {
-      for (long m = 0; m < M; m += BLOCK_M) {
-        long block_m = std::min(M - m, BLOCK_M);
-        enumerate_dispatcher<long, 4, BLOCK_M>::call(block_m,
-          [&](auto i) {
-            GemmMicroKernel<Tin, Tin, Tin, Tin, i, N, ldb, transA, ACC, PREFETCH_K_DIST>::template call<is_int4>(
-              K, transA ? (Tin*)A + m : (Tin*)A + m*lda, lda, qB, (Tin*)C + m*ldc, ldc, scales, zps
-            );
-          },
-          [&](auto i) {
-            range_dispatcher<long, 1, BLOCK_M-1>::call(i,
-              [&](auto j) {
-                GemmMicroKernel<Tin, Tin, Tin, Tin, j, N, ldb, transA, ACC, PREFETCH_K_DIST>::template call<is_int4>(
-                  K, transA ? (Tin*)A + m : (Tin*)A + m*lda, lda, qB, (Tin*)C + m*ldc, ldc, scales, zps
-                );
-              },
-              [&](auto j) {
-                failing_fallback();
-              }
-            );
-          }
-        );
-      }
-    } else {
-      constexpr const int N_GROUP_SIZE = get_n_group_size(N);
-      Tin B[K][N];
-      // TODO(jgong5): add prefetch
-      Dequantize<Tin, ldb, N_GROUP_SIZE, is_int4>::call(qB, K, N, scales, zps, B[0]);
-      (*pgemm)(A, B[0], C, 1, no_tile_cfg);
-    }
-  }
-
-  void config() {
-    if (pgemm) {
-      pgemm->config();
-    }
-  }
-
-  void release() {
-    if (pgemm) {
-      pgemm->release();
-    }
-  }
-
-private:
-  std::shared_ptr<BrgemmTPP<Tin, Tout>> pgemm;
-  long M;
-  long K;
-  long lda;
-  long ldc;
-};
-
-template <
-  long BLOCK_M, long N, long ldb, bool transA, bool ACC, long PREFETCH_K_DIST>
-class DequantGemmTPP<
-  /*Tin*/uint8_t, /*Tout*/float, /*TScale*/float, /*TZero*/int8_t,
-  BLOCK_M, N, ldb, transA, ACC, /*is_int4*/true, PREFETCH_K_DIST
-> {
-using TBrgemmTPP = BrgemmTPP<int8_t, int32_t>;
-
-public:
-  DequantGemmTPP(
-      long M,
-      long K,
-      long lda,
-      long ldc
-  )
-  :
-  M(M),
-  K(K),
-  lda(lda),
-  ldc(ldc) {
-    static_assert(
-      N % 16 == 0,
-      "N must be a multiple of 16"
-    );
-    TLA_ASSERT(K % 4 == 0, "Kb must be a multiple of 4 for int8 VNNI");
-    // TODO(jgong5): output fp32 directly
-    pgemm = std::make_shared<TBrgemmTPP>(
-      M, N, K, 1, 1, lda, N, N, /*ACC*/0, /*transA*/false, 1, /*b_vnni*/true
-    );
-  }
-
-  inline void operator()(uint8_t* A, uint8_t* qB, float* scales, int8_t* zps, float* C, bool no_tile_cfg = true, float scale_a = 1.0, int32_t zp_a = 0) {
-    auto qA = GetVLAPtr<uint8_t>(A, {lda});
-#ifdef __AVX512VNNI__
-    if (M < SMALL_BATCH_THRESHOLD) {
-      constexpr long PREFERRED_BLOCK_M = BLOCK_M * N / 16 >= 16 ? BLOCK_M / 2: BLOCK_M;
-      for (long m = 0; m < M; m += PREFERRED_BLOCK_M) {
-        long block_m = std::min(M - m, PREFERRED_BLOCK_M);
-        enumerate_dispatcher<long, 4, PREFERRED_BLOCK_M>::call(block_m,
-          [&](auto i) {
-            GemmMicroKernel<
-              /*Tin*/uint8_t, /*Tout*/float, /*TScale*/float, /*TZero*/int8_t, /*M*/i, N, ldb,
-              /*transA*/false, ACC, PREFETCH_K_DIST
-            >::template call<true>(
-              K, qA[m], lda, qB, C + m*ldc, ldc, scales, zps, scale_a, zp_a
-            );
-          },
-          [&](auto i) {
-            range_dispatcher<long, 1, PREFERRED_BLOCK_M-1>::call(i,
-              [&](auto j) {
-                GemmMicroKernel<
-                  /*Tin*/uint8_t, /*Tout*/float, /*TScale*/float, /*TZero*/int8_t, /*M*/j, N, ldb,
-                  /*transA*/false, ACC, PREFETCH_K_DIST
-                >::template call<true>(
-                  K, qA[m], lda, qB, C + m*ldc, ldc, scales, zps, scale_a, zp_a
-                );
-              },
-              [&](auto j) {
-                failing_fallback();
-              }
-            );
-          }
-        );
-      }
-    } else
-#endif
-    {
-      constexpr const int N_GROUP_SIZE = 16;
-      int8_t B[K/4][N][4];
-      int32_t qC[M][N];
-      int32_t compensation[N];
-      // TODO(jgong5): add prefetch
-      Dequantize<int8_t, ldb, N_GROUP_SIZE, /*is_int4*/true>::call(qB, K, N, zps, B[0][0], compensation);
-      (*pgemm)((int8_t*)qA[0], B[0][0], qC[0], 1, no_tile_cfg);
-      // post-op and convert back to C
-      for (long m = 0; m < M; ++m) {
-        #pragma omp simd
-        for (long n = 0; n < N; ++n) {
-          float c = (qC[m][n] - compensation[n] * zp_a) * scale_a * scales[n];
-          if constexpr (ACC) {
-            C[m*ldc + n] += c;
-          } else {
-            C[m*ldc + n] = c;
-          }
-        }
-      }
-    }
-  }
-
-  void config() {
-    if (pgemm) {
-      pgemm->config();
-    }
-  }
-
-  void release() {
-    if (pgemm) {
-      pgemm->release();
-    }
-  }
-
-private:
-  std::shared_ptr<TBrgemmTPP> pgemm;
-  long M;
-  long K;
-  long lda;
-  long ldc;
-};
-
-#define FUSE_GELU 1
-#define FUSE_ADD 2
-#define FUSE_ADD_ADD 3
-
-// If T != TComp
-//   T -> TComp -> GEMM -> TComp -> bias/PostOp -> Tout
-// If T == TComp (we can save intermediate output buffer and schedule M/N/K loops together)
-//   T -> GEMM -> T -> bias/PostOp -> Tout
-template <typename T, typename TComp, typename TGemmOut, typename Tout, typename TScale, typename TZero>
-void qlinear_woq_affine_impl(
-  const at::Tensor& x,
-  const at::Tensor& qw_packed,
-  const at::Tensor& scales, // dtype is TComp
-  const at::Tensor& zps, // dtype is TComp
-  const at::Tensor& b, // dtype is TComp
-  at::Tensor y,
-  bool is_int4,
-  int k_splits,
-  int num_concats,
-  int fusion_type,
-  const TensorList& others_list,
-  float scale_a = 1.0f,
-  int32_t zp_a = 0
-) {
-  auto x_sizes = x.sizes();
-  auto w_sizes = qw_packed.sizes();
-  auto M = x_sizes[0];
-  auto Nc = w_sizes[0];
-  auto Nb = is_int4 ? w_sizes[3] * 2 : w_sizes[3];
-  auto Kc = w_sizes[1];
-  auto Kb = w_sizes[2];
-  auto N = Nc * Nb;
-  auto K = Kc * Kb;
-
-  TLA_ASSERT(Nb % 16 == 0, "Nb must be a multiple of 16");
-  TLA_ASSERT(num_concats <= 1 || Nc % num_concats == 0, "Nc must be a multiple of num_concats");
-
-  // select BLOCK_M according to M
-  // TODO(jgong5): improve the heuristic
-  auto BLOCK_M = [&]() -> long {
-    if (M < 32) {
-      return M;
-    } else if (M < 64) {
-      return 32;
-    } else {
-      return 64;
-    }
-  }();
-
-  auto BLOCK_M_rem = M % BLOCK_M;
-
-  // TODO(jgong5): use heuristics to decide k_splits
-  if (k_splits <= 0 || num_concats > 1 || M >= 32 || BLOCK_M_rem) {
-    k_splits = 1;
-  }
-  TLA_ASSERT(Kc % k_splits == 0, "Kc must be a multiple of k_splits");
-  TLA_ASSERT(!(std::is_same<T, uint8_t>()) || (std::is_same<T, TComp>()), "T must be TComp if T is uint8_t");
-
-  bool no_x_buf = std::is_same<T, TComp>();
-  bool no_y_buf = std::is_same<T, TComp>() && std::is_same<Tout, TGemmOut>() && k_splits == 1;
-
-  auto lda = no_x_buf ? K : Kb;
-  auto ldy = num_concats <= 1 ? N : Nc/num_concats * Nb;
-  auto ldc = (no_y_buf || k_splits > 1) ? ldy : Nb;
-
-  auto px = GetVLAPtr<T>(x, {Kc,Kb});
-  auto pw = GetVLAPtr<uint8_t>((uint8_t*)qw_packed.data_ptr(), {Kc, Kb * (is_int4 ? Nb/2 : Nb)});
-  auto py = GetVLAPtr<Tout>(y, {Nc,Nb}); /*[M, Nc, Nb]*/
-  auto py_concat = GetVLAPtr<Tout>(y, {M,Nc/num_concats,Nb}); /*[num_concats, M, Nc/num_concats, Nb]*/
-  auto pscales = GetVLAPtr<TScale>(scales, {Nb});
-  auto pzps = GetVLAPtr<TZero>(zps, {Nb});
-  auto pb = GetVLAPtr<TGemmOut>(b, {Nb});
-  auto tin0 = others_list.size() > 0 ? others_list[0] : at::Tensor{};
-  auto pin0 = GetVLAPtr<Tout>(tin0, {Nc,Nb}); /*[M, Nc, Nb]*/
-  auto pin0_concat = GetVLAPtr<Tout>(tin0, {M,Nc/num_concats,Nb}); /*[num_concats, M, Nc/num_concats, Nb]*/
-  auto tin1 = others_list.size() > 1 ? others_list[1] : at::Tensor{};
-  auto pin1 = GetVLAPtr<Tout>(tin1, {Nc,Nb}); /*[M, Nc, Nb]*/
-  auto pin1_concat = GetVLAPtr<Tout>(tin1, {M,Nc/num_concats,Nb}); /*[num_concats, M, Nc/num_concats, Nb]*/
-
-  auto copy_bias_out_tpp = CpyBiasTPP<TGemmOut>(BLOCK_M, Nb, ldy);
-  auto copy_bias_buf_tpp = CpyBiasTPP<TGemmOut>(BLOCK_M, Nb, Nb);
-  auto copy_bias_out_rem_tpp = CpyBiasTPP<TGemmOut>(BLOCK_M_rem, Nb, ldy);
-  auto copy_bias_buf_rem_tpp = CpyBiasTPP<TGemmOut>(BLOCK_M_rem, Nb, Nb);
-  auto zero_out_tpp = SetZeroTPP<TGemmOut>(BLOCK_M, Nb, ldy);
-  auto zero_buf_tpp = SetZeroTPP<TGemmOut>(BLOCK_M, Nb, Nb);
-  auto zero_out_rem_tpp = SetZeroTPP<TGemmOut>(BLOCK_M_rem, Nb, ldy);
-  auto zero_buf_rem_tpp = SetZeroTPP<TGemmOut>(BLOCK_M_rem, Nb, Nb);
-  auto gelu_fwd_tpp = GeluFwdTPP<Tout>(BLOCK_M, Nb, ldy, ldy);
-  auto gelu_fwd_rem_tpp = GeluFwdTPP<Tout>(BLOCK_M_rem, Nb, ldy, ldy);
-  auto add_tpp = AddTPP<Tout>(BLOCK_M, Nb, ldy, ldy);
-  auto add_rem_tpp = AddTPP<Tout>(BLOCK_M_rem, Nb, ldy, ldy);
-  auto post_ops_fn = [&](int m, int nc){
-    Tout* y_ptr = num_concats <= 1 ? (Tout*)py[m][nc] : (Tout*)py_concat[nc/(Nc/num_concats)][m][nc%(Nc/num_concats)];
-    Tout* tin0_ptr = fusion_type > 1 ? num_concats <= 1 ? (Tout*)pin0[m][nc] : (Tout*)pin0_concat[nc/(Nc/num_concats)][m][nc%(Nc/num_concats)] : nullptr;
-    Tout* tin1_ptr = fusion_type > 2 ? num_concats <= 1 ? (Tout*)pin1[m][nc] : (Tout*)pin1_concat[nc/(Nc/num_concats)][m][nc%(Nc/num_concats)] : nullptr;
-    if (fusion_type == FUSE_GELU) {
-      gelu_fwd_tpp(y_ptr, y_ptr);
-    } else if (fusion_type == FUSE_ADD) {
-      add_tpp(y_ptr, tin0_ptr, y_ptr);
-    } else if (fusion_type == FUSE_ADD_ADD) {
-      add_tpp(y_ptr, tin0_ptr, y_ptr);
-      add_tpp(y_ptr, tin1_ptr, y_ptr);
-    }
-  };
-  auto post_ops_rem_fn = [&](int m, int nc){
-    Tout* y_ptr = num_concats <= 1 ? (Tout*)py[m][nc] : (Tout*)py_concat[nc/(Nc/num_concats)][m][nc%(Nc/num_concats)];
-    Tout* tin0_ptr = fusion_type > 1 ? num_concats <= 1 ? (Tout*)pin0[m][nc] : (Tout*)pin0_concat[nc/(Nc/num_concats)][m][nc%(Nc/num_concats)] : nullptr;
-    Tout* tin1_ptr = fusion_type > 2 ? num_concats <= 1 ? (Tout*)pin1[m][nc] : (Tout*)pin1_concat[nc/(Nc/num_concats)][m][nc%(Nc/num_concats)] : nullptr;
-    if (fusion_type == FUSE_GELU) {
-      gelu_fwd_rem_tpp(y_ptr, y_ptr);
-    } else if (fusion_type == FUSE_ADD) {
-      add_rem_tpp(y_ptr, tin0_ptr, y_ptr);
-    } else if (fusion_type == FUSE_ADD_ADD) {
-      add_rem_tpp(y_ptr, tin0_ptr, y_ptr);
-      add_rem_tpp(y_ptr, tin1_ptr, y_ptr);
-    }
-  };
-
-  constexpr long MICRO_BLOCK_M = 8;
-  product_dispatcher<
-    std::tuple</*BLOCK_N*/long, /*is_int4*/bool>,
-    std::tuple<
-      enumerate_dispatcher<long, 16, 32, 64, 128>,
-      boolean_dispatcher
-    >
-  >::call(
-    std::make_tuple(Nb, is_int4),
-    [&](auto tuple) {
-      auto BLOCK_N = std::get<0>(tuple);
-      auto is_int4 = std::get<1>(tuple);
-      // TODO(jgong5): design API to avoid duplicate code of defining similar kernel object
-      auto dequant_gemm_tpp = DequantGemmTPP<
-        TComp, TGemmOut, TScale, TZero, MICRO_BLOCK_M, BLOCK_N, /*ldb*/BLOCK_N, /*transA*/false, /*ACC*/true, is_int4, PREFETCH_K_DIST>(
-        /*M*/BLOCK_M, /*K*/Kb,
-        /*lda*/lda,
-        /*ldc*/ldc
-      );
-      auto dequant_gemm_no_prefetch_tpp = DequantGemmTPP<
-        TComp, TGemmOut, TScale, TZero, MICRO_BLOCK_M, BLOCK_N, /*ldb*/BLOCK_N, /*transA*/false, /*ACC*/true, is_int4, 0>(
-        /*M*/BLOCK_M, /*K*/Kb,
-        /*lda*/lda,
-        /*ldc*/ldc
-      );
-      auto dequant_gemm_rem_tpp = DequantGemmTPP<
-        TComp, TGemmOut, TScale, TZero, MICRO_BLOCK_M, BLOCK_N, /*ldb*/BLOCK_N, /*transA*/false, /*ACC*/true, is_int4, PREFETCH_K_DIST>(
-        /*M*/BLOCK_M_rem, /*K*/Kb,
-        /*lda*/lda,
-        /*ldc*/ldc
-      );
-      auto dequant_gemm_no_prefetch_rem_tpp = DequantGemmTPP<
-        TComp, TGemmOut, TScale, TZero, MICRO_BLOCK_M, BLOCK_N, /*ldb*/BLOCK_N, /*transA*/false, /*ACC*/true, is_int4, 0>(
-        /*M*/BLOCK_M_rem, /*K*/Kb,
-        /*lda*/lda,
-        /*ldc*/ldc
-      );
-
-      auto pcvt_x_tpp = std::is_same<T, uint8_t>() ? nullptr : std::make_shared<ConvertTPP<T, TComp>>(BLOCK_M, Kb, K, Kb);
-      auto pcvt_x_rem_tpp = std::is_same<T, uint8_t>() ? nullptr : std::make_shared<ConvertTPP<T, TComp>>(BLOCK_M_rem, Kb, K, Kb);
-      auto cvt_y_tpp = ConvertTPP<TGemmOut, Tout>(BLOCK_M, Nb, Nb, ldy);
-      auto cvt_y_rem_tpp = ConvertTPP<TGemmOut, Tout>(BLOCK_M_rem, Nb, Nb, ldy);
-      auto cvt_y_private_tpp = ConvertTPP<TGemmOut, Tout>(BLOCK_M, Nb, N, N);
-      auto add_y_tpp = BinaryTPP(
-        BLOCK_M, /*row*/
-        Nb, /*col*/
-        N, /*ldi0*/
-        N, /*ldi1*/
-        N, /*ldo*/
-        XsmmDtype<TGemmOut>(), /*dt_in0*/
-        XsmmDtype<Tout>(), /*dt_in1*/
-        XsmmDtype<Tout>(), /*dt_out*/
-        XsmmDtype<float>(), /*dt_compute*/
-        LIBXSMM_MELTW_FLAG_BINARY_NONE,
-        LIBXSMM_MELTW_TYPE_BINARY_ADD
-      );
-
-      // TODO(jgong5): parallelize over M on large BS
-      if (no_y_buf) {
-        auto loop_scheme = M >= PARALLEL_M_THRESHOLD ? "ACb" : "aCb";
-        auto gemm_loop = ThreadedLoop<3>({{0, M, BLOCK_M, false}, {Kc}, {Nc}}, loop_scheme);
-        gemm_loop(
-          [&](int *idx) {
-            int m = idx[0];
-            int kc = idx[1];
-            int nc = idx[2];
-            bool is_rem = (m + BLOCK_M > M);
-            TGemmOut* y_ptr = num_concats <= 1 ? (TGemmOut*)py[m][nc] : (TGemmOut*)py_concat[nc/(Nc/num_concats)][m][nc%(Nc/num_concats)];
-            if (!is_rem) {
-              if (kc == 0) {
-                if (b.defined()) {
-                  copy_bias_out_tpp(pb[nc], y_ptr);
-                } else {
-                  zero_out_tpp(y_ptr);
-                }
-              }
-              TComp* x_ptr = (TComp*)px[m][kc];
-              if (kc < Kc - 1) {
-                dequant_gemm_tpp(x_ptr, pw[nc][kc], pscales[nc], pzps[nc], y_ptr, true, scale_a, zp_a);
-              } else {
-                dequant_gemm_no_prefetch_tpp(x_ptr, pw[nc][kc], pscales[nc], pzps[nc], y_ptr, true, scale_a, zp_a);
-                if (fusion_type > 0) {
-                  post_ops_fn(m, nc);
-                }
-              }
-            } else {
-              if (kc == 0) {
-                if (b.defined()) {
-                  copy_bias_out_rem_tpp(pb[nc], y_ptr);
-                } else {
-                  zero_out_rem_tpp(y_ptr);
-                }
-              }
-              TComp* x_ptr = (TComp*)px[m][kc];
-              if (kc < Kc - 1) {
-                dequant_gemm_rem_tpp(x_ptr, pw[nc][kc], pscales[nc], pzps[nc], y_ptr, false, scale_a, zp_a);
-                dequant_gemm_tpp.config();                
-              } else {
-                dequant_gemm_no_prefetch_rem_tpp(x_ptr, pw[nc][kc], pscales[nc], pzps[nc], y_ptr, false, scale_a, zp_a);
-                dequant_gemm_no_prefetch_tpp.config();              
-                if (fusion_type > 0) {
-                  post_ops_rem_fn(m, nc);
-                }
-              }
-            }
-            // TODO(jgong5): post-op fusion
-          },
-          [&]() { dequant_gemm_tpp.config(); },
-          [&]() { dequant_gemm_tpp.release(); }
-        );
-      } else {
-        auto num_threads = omp_get_max_threads();
-        TGemmOut* y_private = nullptr;
-        bool* y_private_valid = nullptr;
-        if (k_splits > 1) {
-          // TODO(jgong5): if we know the thread decomposition, we can allocate a smaller buffer
-          y_private = (TGemmOut*)std::aligned_alloc(64, num_threads * M * N * sizeof(TGemmOut));
-          y_private_valid = (bool*)std::aligned_alloc(64, num_threads * (M/BLOCK_M) * Nc * sizeof(bool));
-          memset(y_private_valid, 0, sizeof(bool) * num_threads * (M/BLOCK_M) * Nc);
-        }
-        auto y_private_ptr = GetVLAPtr<TGemmOut>(y_private, {M,Nc,Nb});
-        auto y_private_valid_ptr = GetVLAPtr<bool>(y_private_valid, {M/BLOCK_M,Nc});
-        auto loop_scheme = M >= PARALLEL_M_THRESHOLD ? "CAB" : "ABc";
-        auto gemm_loop = ThreadedLoop<3>({{Nc}, {0, Kc, Kc/k_splits, true}, {0, M, BLOCK_M, false}}, loop_scheme);
-        gemm_loop(
-          [&](int *idx) {
-            int my_id = omp_get_thread_num();
-            int nc = idx[0];
-            int kc_start = idx[1];
-            int kc_end = kc_start + Kc/k_splits;
-            int m = idx[2];
-            bool is_rem = (m + BLOCK_M > M);
-            auto y_out_ptr = num_concats <= 1 ? py[m][nc] : py_concat[nc/(Nc/num_concats)][m][nc%(Nc/num_concats)];
-            alignas(64) TGemmOut y_buf[BLOCK_M][Nb];
-            TGemmOut* y_ptr = y_private_ptr[my_id][m][nc];
-            if (k_splits > 1) {
-              if (!y_private_valid_ptr[my_id][m/BLOCK_M][nc]) {
-                if (kc_start == 0 && b.defined()) {
-                  copy_bias_out_tpp(pb[nc], y_ptr);
-                } else {
-                  zero_out_tpp(y_ptr);
-                }
-                y_private_valid_ptr[my_id][m/BLOCK_M][nc] = true;
-              }
-            } else {
-              y_ptr = y_buf[0];
-              if (b.defined()) {
-                if(!is_rem){
-                  copy_bias_buf_tpp(pb[nc], y_buf[0]);
-                }else{
-                  copy_bias_buf_rem_tpp(pb[nc], y_buf[0]);
-                }
-              } else {
-                if(!is_rem){
-                  zero_buf_tpp(y_buf[0]);
-                }else{
-                  zero_buf_rem_tpp(y_buf[0]);
-                }
-              }
-            }
-            for (int kc = kc_start; kc < kc_end; kc++) {
-              TComp* x_ptr = (TComp*)px[m][kc];
-              if (!is_rem) {
-                alignas(64) TComp x_buf[BLOCK_M][Kb];
-                if (!no_x_buf) {
-                  (*pcvt_x_tpp)(px[m][kc], x_buf[0]);
-                  x_ptr = x_buf[0];
-                }
-                if (kc < Kc - 1) {
-                  dequant_gemm_tpp(x_ptr, pw[nc][kc], pscales[nc], pzps[nc], y_ptr, true, scale_a, zp_a);
-                } else {
-                  dequant_gemm_no_prefetch_tpp(x_ptr, pw[nc][kc], pscales[nc], pzps[nc], y_ptr, true, scale_a, zp_a);
-                }
-              } else {
-                alignas(64) TComp x_buf[BLOCK_M][Kb];
-                if (!no_x_buf) {
-                  (*pcvt_x_rem_tpp)(px[m][kc], x_buf[0]);
-                  x_ptr = x_buf[0];
-                }
-                if (kc < Kc - 1) {
-                  dequant_gemm_rem_tpp(x_ptr, pw[nc][kc], pscales[nc], pzps[nc], y_ptr, false, scale_a, zp_a);
-                  dequant_gemm_tpp.config();                  
-                } else {
-                  dequant_gemm_no_prefetch_rem_tpp(x_ptr, pw[nc][kc], pscales[nc], pzps[nc], y_ptr, false, scale_a, zp_a);
-                  dequant_gemm_no_prefetch_tpp.config();                
-                }
-              }
-            }
-            // TODO(jgong5): post-op fusion
-            if (k_splits <= 1) {
-              if (!is_rem) {
-                cvt_y_tpp(y_buf[0], y_out_ptr);
-                if (fusion_type > 0) {
-                  post_ops_fn(m, nc);               
-                }
-              }else {
-                cvt_y_rem_tpp(y_buf[0], y_out_ptr);
-                if (fusion_type > 0) {
-                  post_ops_rem_fn(m, nc);                
-                }
-              }
-            }
-          },
-          [&]() { dequant_gemm_tpp.config(); },
-          [&]() { dequant_gemm_tpp.release(); }
-        );
-        if (k_splits > 1) {
-          TLA_ASSERT(M % BLOCK_M == 0, "M must be divisible by BLOCK_M for k_splits > 1");
-          auto reduce_loop = ThreadedLoop<2>({{0, M, BLOCK_M, true}, {Nc}}, "AB");
-          reduce_loop(
-            [&](int *idx) {
-              int m = idx[0];
-              int nc = idx[1];
-              bool init = false;
-              for (int id = 0; id < num_threads; id++) {
-                if (y_private_valid_ptr[id][m/BLOCK_M][nc]) {
-                  if (!init) {
-                    cvt_y_private_tpp(y_private_ptr[id][m][nc], py[m][nc]);
-                    init = true;
-                  } else {
-                    add_y_tpp(y_private_ptr[id][m][nc], py[m][nc], py[m][nc]);
-                  }
-                }
-              }
-              if (fusion_type > 0) {
-                post_ops_fn(m, nc); 
-              }
-            }
-          );
-          std::free(y_private);
-          std::free(y_private_valid);
-        }
-      }
-    },
-    [](auto tuple) {
-      failing_fallback();
-    }
-  );
-}
-
-#define LOWP_MODE_NONE 0
-#define LOWP_MODE_FP16 1
-#define LOWP_MODE_BF16 2
-#define LOWP_MODE_INT8 3
-
-/**
- * @brief pack the weight in quantized format.
- * @param qw quantized weight with shape [N, K]
- * @param block_n block size along N, N % block_n == 0, block_n % 16 == 0
- * @param block_k block size along K, K % block_k == 0. block_k % 2 == 0 for bf16 compute_dtype.
- * false if activation is expected to be float32.
- */
-at::Tensor qlinear_woq_pack(const at::Tensor& qw, bool is_int4, size_t block_n, size_t block_k, int64_t lowp_mode) {
-  TLA_ASSERT(qw.is_contiguous(), "qw must be contiguous");
-  auto sizes = qw.sizes();
-  auto N = sizes[0];
-  auto K = is_int4 ? sizes[1] * 2 : sizes[1];
-  TLA_ASSERT(N % block_n == 0, "N must be multiple of block_n");
-  TLA_ASSERT(K % block_k == 0, "K must be multiple of block_k");
-  TLA_ASSERT(block_n % 16 == 0, "block_n must be multiple of 16 for int4");
-  if (lowp_mode == LOWP_MODE_INT8) {
-    TLA_ASSERT(block_k % 4 == 0, "block_k must be multiple of 4 for int8 for LOWP_MODE_INT8");
-  }
-  const int N_GROUP_SIZE = lowp_mode != LOWP_MODE_INT8 ? get_n_group_size(block_n) : 16;
-  const int Nc = N / block_n;
-  const int Kc = K / block_k;
-  if (is_int4) {
-    // TODO(jgong5): support lowp_mode == LOWP_MODE_INT8
-    auto result = at::empty({Nc, Kc, block_k, block_n/2}, qw.options());
-    // Pack weight in [N,K] to [N/block_n, K/block_k, block_k, block_n]
-    // And then, pre-shuffle per 32 or 64 4-bit values to save shuffle at runtime
-    // Take 32 4-bit values as an example below:
-    // x0 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 y0 y1 y2 y3 y4 y5 y6 y7 y8 y9 y10 y11 y12 y13 y14 y15
-    // becomes
-    // x0 y0 x1 y1 x2 y2 x3 y3 x4 y4 x5 y5 x6 y6 x7 y7 x8 y8 x9 y9 x10 y10 x11 y11 x12 y12 x13 y13 x14 y14 x15 y15
-    // Here, xi and yj are 4-bit values.
-    uint8_t* src_data = (uint8_t*)qw.data_ptr();
-    uint8_t* dst_data = (uint8_t*)result.data_ptr();
-    auto psrc = GetVLAPtr<uint8_t>(src_data, {block_n, Kc, block_k/2});
-    auto pdst = GetVLAPtr<uint8_t>(dst_data, {Kc, block_k, block_n/2});
-    auto pdst_4vnni = GetVLAPtr<uint8_t>(dst_data, {Kc, block_k/4, block_n/2, 4});
-    auto pack_loop = ThreadedLoop<3>({{Nc}, {Kc}, {0, block_n, N_GROUP_SIZE, false}}, "ABc");
-    pack_loop(
-      [&](int *idx) {
-        int nc = idx[0];
-        int kc = idx[1];
-        int nb = idx[2];
-        for (int i = 0; i < N_GROUP_SIZE/2; i++) {
-          for (int kb = 0; kb < block_k; kb+=2) {
-            auto src0 = psrc[nc][nb+i][kc][kb/2];
-            auto src1 = psrc[nc][nb+i+N_GROUP_SIZE/2][kc][kb/2];
-            auto dst0 = (src0 & 0xf) | ((src1 & 0xf) << 4);
-            auto dst1 = (src0 >> 4) | ((src1 >> 4) << 4);
-            if (lowp_mode != LOWP_MODE_INT8) {
-              pdst[nc][kc][kb][nb/2+i] = dst0;
-              pdst[nc][kc][kb+1][nb/2+i] = dst1;
-            } else {
-              pdst_4vnni[nc][kc][kb/4][nb/2+i][kb%4] = dst0;
-              pdst_4vnni[nc][kc][(kb+1)/4][nb/2+i][(kb+1)%4] = dst1;
-            }
-          }
-        }
-      }
-    );
-    return result;
-  } else {
-    TLA_ASSERT(lowp_mode != LOWP_MODE_INT8, "lowp mode int8 is not supported yet with int8 weight");
-    auto result = at::empty({Nc, Kc, block_k, block_n}, qw.options());
-    // Pack weight in [N,K] to [N/block_n, K/block_k, block_k, block_n]
-    int8_t* src_data = (int8_t*)qw.data_ptr();
-    int8_t* dst_data = (int8_t*)result.data_ptr();
-    auto psrc = GetVLAPtr<int8_t>(src_data, {block_n, Kc, block_k});
-    auto pdst = GetVLAPtr<int8_t>(dst_data, {Kc, block_k, block_n});
-    auto pack_loop = ThreadedLoop<3>({{Nc}, {Kc}, {0, block_n, N_GROUP_SIZE, false}}, "ABc");
-    pack_loop(
-      [&](int *idx) {
-        int nc = idx[0];
-        int kc = idx[1];
-        int nb = idx[2];
-        for (int i = 0; i < N_GROUP_SIZE; i++) {
-          for (int kb = 0; kb < block_k; kb++) {
-            pdst[nc][kc][kb][nb+i] = psrc[nc][nb+i][kc][kb];
-          }
-        }
-      }
-    );
-    return result;
-  }
-}
-
-at::Tensor qlinear_woq_unpack(const at::Tensor& qw_packed, bool is_int4, int64_t lowp_mode) {
-  if (qw_packed.dim() == 4) {
-    auto w_sizes = qw_packed.sizes();
-    auto Nc = w_sizes[0];
-    auto Nb = is_int4 ? w_sizes[3] * 2 : w_sizes[3];
-    auto Kc = w_sizes[1];
-    auto Kb = w_sizes[2];
-    auto N = Nc * Nb;
-    auto K = Kc * Kb;
-    const int N_GROUP_SIZE = lowp_mode != LOWP_MODE_INT8 ? get_n_group_size(Nb) : 16;
-    if (is_int4) {
-      // TODO: support lowp_mode == 3
-      auto result = at::empty({N, K/2}, qw_packed.options());
-      uint8_t* src_data = (uint8_t*)qw_packed.data_ptr();
-      uint8_t* dst_data = (uint8_t*)result.data_ptr();
-      auto psrc = GetVLAPtr<uint8_t>(src_data, {Kc, Kb, Nb/2});
-      auto psrc_4vnni = GetVLAPtr<uint8_t>(src_data, {Kc, Kb/4, Nb/2, 4});
-      auto pdst = GetVLAPtr<uint8_t>(dst_data, {Nb, Kc, Kb/2});
-      auto unpack_loop = ThreadedLoop<3>({{Nc}, {Kc}, {0, Nb, N_GROUP_SIZE, false}}, "ABc");
-      unpack_loop(
-        [&](int *idx) {
-          int nc = idx[0];
-          int kc = idx[1];
-          int nb = idx[2];
-          for (int kb = 0; kb < Kb; kb+=2) {
-            for (int i = 0; i < N_GROUP_SIZE/2; i++) {
-              uint8_t src0, src1;
-              if (lowp_mode != LOWP_MODE_INT8) {
-                src0 = psrc[nc][kc][kb][nb/2+i];
-                src1 = psrc[nc][kc][kb+1][nb/2+i];
-              } else {
-                src0 = psrc_4vnni[nc][kc][kb/4][nb/2+i][kb%4];
-                src1 = psrc_4vnni[nc][kc][(kb+1)/4][nb/2+i][(kb+1)%4];
-              }
-              pdst[nc][nb+i][kc][kb/2] = (src0 & 0xf) | ((src1 & 0xf) << 4);
-              pdst[nc][nb+i+N_GROUP_SIZE/2][kc][kb/2] = (src0 >> 4) | ((src1 >> 4) << 4);
-            }
-          }
-        }
-      );
-      return result;
-    } else {
-      TLA_ASSERT(lowp_mode != LOWP_MODE_INT8, "lowp mode int8 is not supported yet with int8 weight");
-      auto result = at::empty({N, K}, qw_packed.options());
-      int8_t* src_data = (int8_t*)qw_packed.data_ptr();
-      int8_t* dst_data = (int8_t*)result.data_ptr();
-      auto psrc = GetVLAPtr<int8_t>(src_data, {Kc, Kb, Nb});
-      auto pdst = GetVLAPtr<int8_t>(dst_data, {Nb, Kc, Kb});
-      auto unpack_loop = ThreadedLoop<3>({{Nc}, {Kc}, {0, Nb, N_GROUP_SIZE, false}}, "ABc");
-      unpack_loop(
-        [&](int *idx) {
-          int nc = idx[0];
-          int kc = idx[1];
-          int nb = idx[2];
-          for (int kb = 0; kb < Kb; kb++) {
-            for (int i = 0; i < N_GROUP_SIZE; i++) {
-              pdst[nc][nb+i][kc][kb] = psrc[nc][kc][kb][nb+i];
-            }
-          }
-        }
-      );
-      return result;
-    }
-  } else {
-    TLA_ASSERT(qw_packed.dim() == 2, "qw_packed must be 2D or 4D");
-    return qw_packed;
-  }
-}
-
-void compute_int8_qparams_per_tensor(const at::Tensor& t, float* scale, int32_t* zp) {
-  auto [t_min, t_max] = at::aminmax(t);
-  auto min = t_min.item<float>();
-  auto max = t_max.item<float>();
-  min = std::min(min, 0.0f);
-  max = std::max(max, 0.0f);
-  *scale = (max - min) / 255.0f;
-  *zp = (int32_t)(-std::nearbyint(min / *scale));
-}
-
-template <typename T>
-at::Tensor quantize_per_tensor(const at::Tensor& t, float scale, int32_t zp) {
-  // TODO(jgong5): optimize me
-  auto t_q = t / scale + zp;
-  t_q = at::clamp(at::round(t_q), 0, 255);
-  return t_q.to(at::kByte);
-}
-
-template <>
-at::Tensor quantize_per_tensor<bfloat16>(const at::Tensor& t, float scale, int32_t zp) {
-#ifdef __AVX512F__
-  // modified based on inductor codegen...
-  auto convert_float_to_uint8 = [](at::vec::Vectorized<float> src) -> at::vec::Vectorized<uint8_t> {
-    // Convert from float32 to int32
-    __m512i x_values_int32 = _mm512_cvtps_epi32(src);
-
-    // Convert from int32 to int16 using signed saturation
-    __m512i xy_packed_v = _mm512_packs_epi32(x_values_int32, x_values_int32);
-
-    constexpr auto min_val = std::numeric_limits<uint8_t>::min();
-    constexpr auto max_val = std::numeric_limits<uint8_t>::max();
-
-    // Convert from int16 to uint8 using unsigned saturation
-    __m512i packed_and_sat = _mm512_packus_epi16(xy_packed_v, xy_packed_v);
-    __m512i xyzw_clamped_v = _mm512_max_epu8(
-      _mm512_set1_epi8(min_val),
-      _mm512_min_epu8(packed_and_sat, _mm512_set1_epi8(max_val)));
-    __m512i permute_mask_v =
-        _mm512_set_epi32(0x0f, 0x0b, 0x07, 0x03, 0x0e, 0x0a, 0x06, 0x02,
-                        0x0d, 0x09, 0x05, 0x01, 0x0c, 0x08, 0x04, 0x00);
-    return _mm512_permutexvar_epi32(permute_mask_v, xyzw_clamped_v);
-  };
-  at::Tensor out = at::empty_like(t, at::kByte);
-  auto in_ptr0 = t.data_ptr<at::BFloat16>();
-  auto out_ptr0 = out.data_ptr<uint8_t>();
-  auto n = t.numel();
-  auto vecsize = at::vec::Vectorized<float>::size();
-  auto vec_end = 0;
-  #pragma omp parallel for
-  for(long i0 = 0; i0 < static_cast<long>(n)/vecsize*vecsize; i0+=static_cast<long>(vecsize))
-  {
-      auto tmp0 = at::vec::Vectorized<at::BFloat16>::loadu(in_ptr0 + static_cast<long>(i0), vecsize);
-      at::vec::Vectorized<float> res_vec1(0);
-      at::vec::Vectorized<float> res_vec2(0);
-      std::tie(res_vec1, res_vec2) = at::vec::convert_bfloat16_float(tmp0);
-      auto tmp1 = res_vec1;
-      // auto tmp1 = cvt_bf16_to_fp32(tmp0);
-      auto tmp2 = at::vec::Vectorized<float>(static_cast<float>(scale));
-      auto tmp3 = tmp1 / tmp2;
-      auto tmp4 = at::vec::Vectorized<float>(static_cast<float>(zp));
-      auto tmp5 = tmp3 + tmp4;
-      auto tmp6 = tmp5.round();
-      auto tmp7 = (tmp6);
-      auto tmp8 = at::vec::Vectorized<float>(static_cast<float>(0.0));
-      auto tmp9 = at::vec::maximum(tmp7, tmp8);
-      auto tmp10 = at::vec::Vectorized<float>(static_cast<float>(255.0));
-      auto tmp11 = at::vec::minimum(tmp9, tmp10);
-      auto tmp12 = (tmp11);
-      auto tmp13 = convert_float_to_uint8(tmp12);
-      tmp13.store(out_ptr0 + static_cast<long>(i0), vecsize);
-  }
-  for(long i0 = static_cast<long>(n)/vecsize*vecsize; i0<static_cast<long>(n); i0+=static_cast<long>(1))
-  {
-      auto tmp0 = in_ptr0[static_cast<long>(i0)];
-      auto tmp1 = static_cast<float>(tmp0);
-      auto tmp2 = static_cast<float>(0.05);
-      auto tmp3 = tmp1 / tmp2;
-      auto tmp4 = static_cast<float>(1.0);
-      auto tmp5 = tmp3 + tmp4;
-      auto tmp6 = std::nearbyint(tmp5);
-      auto tmp7 = static_cast<float>(tmp6);
-      auto tmp8 = static_cast<float>(0.0);
-      // auto tmp9 = max_propagate_nan(tmp7, tmp8);
-      auto tmp9 = 0;
-      if (at::_isnan(tmp7)) {
-          tmp9 = tmp7;
-      }
-      tmp9 = tmp7 > tmp8 ? tmp7 : tmp8;
-      auto tmp10 = static_cast<float>(255.0);
-      auto tmp11 = 0;
-      if (at::_isnan(tmp9)) {
-          tmp11 = tmp9;
-      }
-      tmp11 =  tmp9 < tmp10 ? tmp9 : tmp10;
-      // auto tmp11 = min_propagate_nan(tmp9, tmp10);
-      auto tmp12 = static_cast<float>(tmp11);
-      auto tmp13 = static_cast<unsigned char>(tmp12);
-      out_ptr0[static_cast<long>(i0)] = tmp13;
-  }
-  return out;
-#else
-  return at::quantize_per_tensor(t.to(c10::kFloat), scale, zp, c10::kQUInt8);
-#endif
-}
-
-/**
- * @brief quantized linear with weight in affine quantized format (scale + zero-point) but
- * activation in floating point format.
- * TODO(jgong5): support epilogue fusion
- * 
- * @param x input activation in floating point format, 2D plain format [M,K]
- * @param qw weight in affine quantized format, could be 4-bit or 8-bit quantized in
- * 4D blocked format [Nc,Kc,Kb,Nb] or 2D plain format [N,K].
- * @param scales_list a list of fp32/fp16/bf16 scales tensors
- * @param zp_list a list of fp32/fp16/bf16/int8 zero points tensors
- * @param bias_list a list of fp32/fp16/bf16 bias tensors
- * @param lowp_mode decide the compute dtype to use.
- *        LOWP_MODE_NONE: keep activation dtype
- *        LOWP_MODE_FP16: use FP16 or FP32 as compute dtype
- *        LOWP_MODE_BF16: use BF16, FP16 or FP32 as compute dtype
- * @return at::Tensor output activation in same dtype as `x`, 2D plain format [M,N]
- */
-at::Tensor qlinear_woq_affine(
-    const at::Tensor& x,
-    const at::Tensor& qw,
-    const TensorList& scales_list,
-    const TensorList& zp_list,
-    const TensorList& bias_list,
-    bool is_int4,
-    int64_t lowp_mode,
-    int64_t num_concats,
-    int64_t fusion_type,
-    const TensorList& others_list) {
-  const int64_t k_splits = 0;
-  // int8_idx is only valid with zp_list when lowp_mode == LOWP_MODE_INT8
-  constexpr size_t fp32_idx = 0, fp16_idx = 1, bf16_idx = 2, int8_idx = 3;
-  auto biases = bias_list.empty() ? TensorList({at::Tensor(), at::Tensor(), at::Tensor()}) : bias_list;
-  if (qw.dim() == 4) {
-    auto w_sizes = qw.sizes();
-    auto K = x.size(-1);
-    auto M = x.numel() / K;
-    auto N = w_sizes[0] * w_sizes[3];
-    if (is_int4) {
-      N *= 2;
-    }
-    auto out_sizes = x.sizes().vec();
-    out_sizes.back() = N;
-    auto y = at::empty(out_sizes, x.options());
-    auto x_reshape = x.reshape({M, K});
-    enumerate_dispatcher<at::ScalarType, at::kFloat, at::kBFloat16, at::kHalf>::call(x.scalar_type(),
-      [&](auto act_dtype) {
-        using act_type = typename c10::impl::ScalarTypeToCPPType<act_dtype>::type;
-        auto try_compute_in_half = [&]() {
-#ifdef __AVX512FP16__
-          qlinear_woq_affine_impl<act_type, half, /*TGemmOut*/half, act_type, half, half>(
-              x_reshape, qw, scales_list[fp16_idx], zp_list[fp16_idx], biases[fp16_idx], y, is_int4, k_splits, num_concats, fusion_type, others_list);
-#else
-          qlinear_woq_affine_impl<act_type, float, /*TGemmOut*/float, act_type, float, float>(
-              x_reshape, qw, scales_list[fp32_idx], zp_list[fp32_idx], biases[fp32_idx], y, is_int4, k_splits, num_concats, fusion_type, others_list);
-#endif
-        };
-        if (lowp_mode == LOWP_MODE_NONE) {
-          if (std::is_same<act_type, half>()) {
-            try_compute_in_half();
-          } else if (std::is_same<act_type, bfloat16>()) {
-            qlinear_woq_affine_impl<bfloat16, bfloat16, /*TGemmOut*/float, bfloat16, bfloat16, bfloat16>(
-              x_reshape, qw, scales_list[bf16_idx], zp_list[bf16_idx], biases[fp32_idx], y, is_int4, k_splits, num_concats, fusion_type, others_list
-            );
-          } else {
-            qlinear_woq_affine_impl<float, float, /*TGemmOut*/float, float, float, float>(
-              x_reshape, qw, scales_list[fp32_idx], zp_list[fp32_idx], biases[fp32_idx], y, is_int4, k_splits, num_concats, fusion_type, others_list
-            );
-          }
-        } else if (lowp_mode == LOWP_MODE_FP16) {
-          try_compute_in_half();
-        } else if (lowp_mode == LOWP_MODE_BF16) {
-          if (M >= SMALL_BATCH_THRESHOLD) {
-            // compute in bfloat16 for large bs
-            qlinear_woq_affine_impl<act_type, bfloat16, /*TGemmOut*/float, act_type, bfloat16, bfloat16>(
-                x_reshape, qw, scales_list[bf16_idx], zp_list[bf16_idx], biases[fp32_idx], y, is_int4, k_splits, num_concats, fusion_type, others_list);
-          } else {
-            try_compute_in_half();
-          }
-        } else {
-          TLA_ASSERT(lowp_mode == LOWP_MODE_INT8, "invalid lowp_mode");
-          TLA_ASSERT(is_int4, "LOWP_MODE_INT8 only support is_int4=true");
-          float scale_a;
-          int32_t zp_a;
-          auto x_reshape_contig = x_reshape.contiguous();
-          compute_int8_qparams_per_tensor(x_reshape_contig, &scale_a, &zp_a);
-          auto x_quantized = quantize_per_tensor<act_type>(x_reshape_contig, scale_a, zp_a);
-          qlinear_woq_affine_impl<uint8_t, uint8_t, /*TGemmOut*/float, act_type, float, int8_t>(
-              x_quantized, qw, scales_list[fp32_idx], zp_list[int8_idx], biases[fp32_idx], y, is_int4, k_splits, num_concats, fusion_type, others_list, scale_a, zp_a);
-        }
-      },
-      failing_fallback<at::ScalarType>
-    );
-    return y;
-  } else {
-    TLA_ASSERT(qw.dim() == 2, "weight must be in 4D blocked format or 2D plain format");
-    auto compute_dtype = x.scalar_type();
-    if (lowp_mode == LOWP_MODE_FP16) {
-      compute_dtype = at::kHalf;
-    } else if (lowp_mode == LOWP_MODE_BF16) {
-      compute_dtype = at::kBFloat16;
-    }
-    auto w = [&]() {
-      if (is_int4) {
-        using namespace at::indexing;
-        auto w_int8 = at::empty({qw.size(0), qw.size(1) * 2}, qw.options().dtype(at::kByte));
-        w_int8.index({Slice(), Slice(None, None, 2)}).copy_(qw.bitwise_and(0xf));
-        w_int8.index({Slice(), Slice(1, None, 2)}).copy_(qw.bitwise_right_shift(4));
-        return (w_int8.to(at::kFloat) - zp_list[fp32_idx]) * scales_list[fp32_idx];
-      } else {
-        return (qw.to(at::kFloat) - zp_list[fp32_idx]) * scales_list[fp32_idx];
-      }
-    }().to(compute_dtype);
-    auto x_fp = x.to(compute_dtype);
-    auto y = at::linear(x_fp, w);
-    if (biases[0].defined()) {
-      auto b_index = compute_dtype == at::kFloat ? fp32_idx :
-                     compute_dtype == at::kHalf ? fp16_idx : bf16_idx;
-      y = at::add(y, biases[b_index]);
-    }
-    if (fusion_type == FUSE_GELU) {
-      y = at::gelu(y);
-    } else if (fusion_type == FUSE_ADD || fusion_type == FUSE_ADD_ADD) {
-      for (auto& tin:others_list)
-        y = at::add(y, tin);
-    }
-    if (num_concats > 1) {
-      y = y.view({-1, num_concats, y.size(-1)/num_concats}).transpose(0, 1).contiguous().view({-1, y.size(-1)});
-    }
-    return y.to(x.scalar_type());
-  }
-}
-
-// TODO(jgong5): int4 quantized linear with lut
-// at::Tensor qlinear_woq_lut(at::Tensor x, at::Tensor qw_packed, at::Tensor lut, at::Tensor b, at::ScalarType compute_dtype) {
-// }
-
-} // namespace
-
-REGISTER_DISPATCH(woq_tpp_gemm_kernel_stub, &qlinear_woq_affine);
-REGISTER_DISPATCH(woq_tpp_gemm_packB_stub, &qlinear_woq_pack);
-REGISTER_DISPATCH(woq_tpp_gemm_unpackB_stub, &qlinear_woq_unpack);
-
-} // namespace cpu
-} // namespace torch_ipex
\ No newline at end of file
diff --git a/csrc/cpu/aten/kernels/optimizer/LambFusedStepKrnl.cpp b/csrc/cpu/aten/kernels/optimizer/LambFusedStepKrnl.cpp
index b4eec10c..560b646d 100644
--- a/csrc/cpu/aten/kernels/optimizer/LambFusedStepKrnl.cpp
+++ b/csrc/cpu/aten/kernels/optimizer/LambFusedStepKrnl.cpp
@@ -1,6 +1,6 @@
 #include <aten/optimizer/optimizer.h>
 #include "vec/vec.h"
-#include <immintrin.h>
+
 #include <torch/all.h>
 #include <torch/csrc/autograd/function.h>
 namespace torch_ipex {
diff --git a/csrc/cpu/aten/kernels/utils/woq_krnl.cpp b/csrc/cpu/aten/kernels/utils/woq_krnl.cpp
deleted file mode 100644
index 526ce89c..00000000
--- a/csrc/cpu/aten/kernels/utils/woq_krnl.cpp
+++ /dev/null
@@ -1 +0,0 @@
-#include <aten/utils/woq.h>
diff --git a/csrc/cpu/aten/utils/woq.cpp b/csrc/cpu/aten/utils/woq.cpp
deleted file mode 100644
index 46dd7193..00000000
--- a/csrc/cpu/aten/utils/woq.cpp
+++ /dev/null
@@ -1 +0,0 @@
-#include "woq.h"
diff --git a/csrc/cpu/aten/utils/woq.h b/csrc/cpu/aten/utils/woq.h
deleted file mode 100644
index 70e8afdf..00000000
--- a/csrc/cpu/aten/utils/woq.h
+++ /dev/null
@@ -1,162 +0,0 @@
-#pragma once
-
-#include <dyndisp/DispatchStub.h>
-#include <libxsmm.h>
-#include <torch/all.h>
-#include <cstdio>
-#include <cstdlib>
-#include <functional>
-#include <iostream>
-#include <memory>
-#include <unordered_map>
-
-namespace torch_ipex {
-namespace cpu {
-namespace {
-
-#define INDEX(x, y, ld) ((x) * (ld) + (y))
-#define ADDRESS(p, x, y, ld) ((p) + (x) * (ld) + (y))
-
-// A class for forced loop unrolling at compile time
-// These macro utils and the small gemm intrinsics kernels are implemented
-// based on the initial code by pujiang.he@intel.com.
-template <int i>
-struct compile_time_for {
-  template <typename Lambda, typename... Args>
-  inline static void op(const Lambda& function, Args... args) {
-    compile_time_for<i - 1>::op(function, args...);
-    function(std::integral_constant<int, i - 1>{}, args...);
-  }
-};
-template <>
-struct compile_time_for<1> {
-  template <typename Lambda, typename... Args>
-  inline static void op(const Lambda& function, Args... args) {
-    function(std::integral_constant<int, 0>{}, args...);
-  }
-};
-template <>
-struct compile_time_for<0> {
-  // 0 loops, do nothing
-  template <typename Lambda, typename... Args>
-  inline static void op(const Lambda& function, Args... args) {}
-};
-// Get mask for last column
-template <int EXPANDED_N, int col>
-constexpr inline unsigned short get_mask(unsigned short mask) {
-  // Not last column, return 0xffffff indicating load/store all 16 floats
-  if constexpr (col < EXPANDED_N / 16 - 1)
-    return (unsigned short)0xffff;
-  else
-    return mask;
-}
-template <int EXPANDED_N>
-constexpr inline unsigned short get_mask(int col, unsigned short mask) {
-  // Not last column, return 0xffffff indicating load/store all 16 floats
-  if (col < EXPANDED_N / 16 - 1)
-    return (unsigned short)0xffff;
-  else
-    return mask;
-}
-
-const int BLOCK_N = 64, BLOCK_K = 96, PREFETCH_K = 64;
-
-struct DotMicroKernelKey {
-  bool trans_a;
-  bool trans_b;
-  int lda;
-  int ldb;
-  int ldc;
-
-  DotMicroKernelKey(bool trans_a, bool trans_b, int lda, int ldb, int ldc)
-      : trans_a(trans_a), trans_b(trans_b), lda(lda), ldb(ldb), ldc(ldc) {}
-
-  bool operator==(const DotMicroKernelKey& other) const {
-    return trans_a == other.trans_a && trans_b == other.trans_b &&
-        lda == other.lda && ldb == other.ldb && ldc == other.ldc;
-  }
-};
-
-template <int BLOCK_M, int BLOCK_N, int BLOCK_K>
-class DotMicroKernel {
- public:
-  DotMicroKernel(bool trans_a, bool trans_b, int lda, int ldb, int ldc) {
-    libxsmm_gemm_shape brshape = libxsmm_create_gemm_shape(
-        BLOCK_M,
-        BLOCK_N,
-        BLOCK_K,
-        lda,
-        ldb,
-        ldc,
-        /*type A*/ LIBXSMM_DATATYPE_F32,
-        /*type B*/ LIBXSMM_DATATYPE_F32,
-        /*type C*/ LIBXSMM_DATATYPE_F32,
-        /*acctype*/ LIBXSMM_DATATYPE_F32);
-    libxsmm_bitfield brflags =
-        (trans_a ? LIBXSMM_GEMM_FLAG_TRANS_A : LIBXSMM_GEMM_FLAG_NONE) |
-        (trans_b ? LIBXSMM_GEMM_FLAG_TRANS_B : LIBXSMM_GEMM_FLAG_NONE);
-    libxsmm_gemm_batch_reduce_config brconfig;
-    memset(&brconfig, 0, sizeof(libxsmm_gemm_batch_reduce_config));
-    brconfig.br_type = LIBXSMM_GEMM_BATCH_REDUCE_NONE;
-
-    kernel_func_ = libxsmm_dispatch_brgemm_v2(
-        brshape, brflags, /*prefetch_flags=*/0, brconfig);
-    memset(&gemm_param_, 0, sizeof(libxsmm_gemm_param));
-  }
-
-  void operator()(void* A, void* B, void* C) {
-    gemm_param_.a.primary = (void*)A;
-    gemm_param_.b.primary = (void*)B;
-    gemm_param_.c.primary = (void*)C;
-    kernel_func_(&gemm_param_);
-  }
-
- private:
-  libxsmm_gemmfunction kernel_func_;
-  libxsmm_gemm_param gemm_param_;
-};
-
-template <int BLOCK_M, int BLOCK_N, int BLOCK_K>
-using DotMicroKernelRef =
-    std::shared_ptr<DotMicroKernel<BLOCK_M, BLOCK_N, BLOCK_K>>;
-
-template <int BLOCK_M, int BLOCK_N, int BLOCK_K>
-DotMicroKernelRef<BLOCK_M, BLOCK_N, BLOCK_K> create_or_get_dot_microkernel(
-    bool trans_a,
-    bool trans_b,
-    int lda,
-    int ldb,
-    int ldc) {
-  thread_local std::unordered_map<
-      DotMicroKernelKey,
-      DotMicroKernelRef<BLOCK_M, BLOCK_N, BLOCK_K>>
-      cache;
-  DotMicroKernelKey key(trans_a, trans_b, lda, ldb, ldc);
-  auto search = cache.find(key);
-  if (search != cache.end()) {
-    return search->second;
-  } else {
-    cache.insert(
-        {key,
-         std::make_shared<DotMicroKernel<BLOCK_M, BLOCK_N, BLOCK_K>>(
-             trans_a, trans_b, lda, ldb, ldc)});
-    return cache[key];
-  }
-}
-} // namespace
-} // namespace cpu
-} // namespace torch_ipex
-
-namespace std {
-template <>
-struct hash<torch_ipex::cpu::DotMicroKernelKey> {
-  std::size_t operator()(const torch_ipex::cpu::DotMicroKernelKey& key) const {
-    std::size_t h = std::hash<bool>()(key.trans_a);
-    h = std::hash<bool>()(key.trans_b) ^ (h << 1);
-    h = std::hash<int>()(key.lda) ^ (h << 1);
-    h = std::hash<int>()(key.ldb) ^ (h << 1);
-    h = std::hash<int>()(key.ldc) ^ (h << 1);
-    return h;
-  }
-};
-} // namespace std
\ No newline at end of file
diff --git a/csrc/cpu/autocast/autocast_mode.cpp b/csrc/cpu/autocast/autocast_mode.cpp
index 4c556fd8..b8153f04 100644
--- a/csrc/cpu/autocast/autocast_mode.cpp
+++ b/csrc/cpu/autocast/autocast_mode.cpp
@@ -208,8 +208,6 @@ IPEX_TORCH_LIBRARY_IMPL(aten, AutocastCPU, m) {
   MAKE_REGISTER_FUNC2_TWO_POLICIES(cumsum, dimname, fallthrough, fp32)
   MAKE_REGISTER_FUNC_TWO_POLICIES(
       scaled_dot_product_attention, fallthrough, fp32)
-  MAKE_REGISTER_FUNC_TWO_POLICIES(
-      _scaled_dot_product_attention, fallthrough, fp32)
   MAKE_REGISTER_FUNC_TWO_POLICIES(
       _scaled_dot_product_attention_math, fallthrough, fp32)
   MAKE_REGISTER_FUNC_TWO_POLICIES(addcdiv, fallthrough, fp32)
diff --git a/csrc/cpu/dyndisp/DispatchStub.h b/csrc/cpu/dyndisp/DispatchStub.h
index aa774fb1..9157e45e 100644
--- a/csrc/cpu/dyndisp/DispatchStub.h
+++ b/csrc/cpu/dyndisp/DispatchStub.h
@@ -274,6 +274,7 @@ struct RegisterHIPDispatch {
 
 #define DEFINE_DISPATCH(name) struct name name
 
+#undef REGISTER_ARCH_DISPATCH
 #define REGISTER_ARCH_DISPATCH(name, arch, fn) \
   template <>                                  \
   decltype(fn) DispatchStub<decltype(fn), struct name>::arch = fn;
@@ -291,6 +292,7 @@ struct RegisterHIPDispatch {
 #define REGISTER_AVX2_DISPATCH(name, fn)
 #endif
 
+#undef REGISTER_NO_CPU_DISPATCH
 #define REGISTER_NO_CPU_DISPATCH(name, fn_type)                        \
   REGISTER_ARCH_DISPATCH(name, DEFAULT, static_cast<fn_type>(nullptr)) \
   REGISTER_AVX512_DISPATCH(name, static_cast<fn_type>(nullptr))        \
@@ -321,6 +323,7 @@ ToDo: Fix warning: "REGISTER_HIP_DISPATCH" redefined to stock pytorch.
 #elif defined(CPU_CAPABILITY)
 #define REGISTER_DISPATCH(name, fn) \
   REGISTER_ARCH_DISPATCH(name, CPU_CAPABILITY, fn)
+#undef REGISTER_NO_AVX512_DISPATCH
 #define REGISTER_NO_AVX512_DISPATCH(name, fn_type) \
   REGISTER_AVX512_DISPATCH(name, static_cast<fn_type>(nullptr))
 #endif
diff --git a/csrc/cpu/isa/cpu_feature.cpp b/csrc/cpu/isa/cpu_feature.cpp
index e05a5949..e2c98c36 100644
--- a/csrc/cpu/isa/cpu_feature.cpp
+++ b/csrc/cpu/isa/cpu_feature.cpp
@@ -174,7 +174,6 @@ bool CPUFeature::os_avx2() {
   read_cpuid(0, &eax, &ebx, &ecx, &edx);
   uint32_t max_basic_id = eax;
   if (max_basic_id >= 0x00000007) {
-    uint32_t max_sub_leaf = 0;
     read_cpuidex(0x00000007, 0, &eax, &ebx, &ecx, &edx);
 
     support_avx2 = check_reg_bit(ebx, 5);
@@ -372,9 +371,7 @@ bool CPUFeature::isa_level_amx() {
 }
 
 bool CPUFeature::isa_level_avx512_fp16() {
-  // check and init in a funtion, avoid to double init.
-  static bool b_is_support =
-      isa_level_avx512_vnni() && isa_level_amx() && cpuid_avx512_fp16();
+  static bool b_is_support = isa_level_amx() && cpuid_avx512_fp16();
   return b_is_support;
 }
 
diff --git a/csrc/cpu/isa/cpu_feature.hpp b/csrc/cpu/isa/cpu_feature.hpp
index 8166fae2..e9e94843 100644
--- a/csrc/cpu/isa/cpu_feature.hpp
+++ b/csrc/cpu/isa/cpu_feature.hpp
@@ -149,7 +149,8 @@ class CPUFeature {
   bool isa_level_avx512_bf16();
 
   bool isa_level_amx();
+
   bool isa_level_avx512_fp16();
 };
 } // namespace cpu
-} // namespace torch_ipex
+} // namespace torch_ipex
\ No newline at end of file
diff --git a/csrc/cpu/jit/codegen/onednn/kernel.cpp b/csrc/cpu/jit/codegen/onednn/kernel.cpp
index 7f3519ac..8cf99bfd 100644
--- a/csrc/cpu/jit/codegen/onednn/kernel.cpp
+++ b/csrc/cpu/jit/codegen/onednn/kernel.cpp
@@ -21,9 +21,8 @@ using data_type = dnnl::graph::logical_tensor::data_type;
 
 thread_local std::list<LlgaKernel::key_value_pair_t>
     LlgaKernel::cache_items_list_;
-thread_local std::
-    unordered_map<std::vector<int64_t>, LlgaKernel::list_iterator_t>
-        LlgaKernel::cache_items_map_;
+thread_local std::unordered_map<size_t, LlgaKernel::list_iterator_t>
+    LlgaKernel::cache_items_map_;
 thread_local int LlgaKernel::capacity_ = 7500;
 
 LlgaKernel::LlgaKernel(const Node* fusionNode)
@@ -89,10 +88,16 @@ std::map<size_t, int64_t> LlgaKernel::initializeTensorIdToOccurence() const {
   return tensorIdToOccurence;
 }
 
-
 ArgSpecs LlgaKernel::initializeInputSpecs(const TensorArgs& inputs) {
   ArgSpecs inputSpecs;
   inputSpecs.reserve(nPartitionInputs_);
+  std::call_once(tracedInputShapesInitialized_, [&]() {
+    auto numInputs = inputs.size();
+    for (size_t i = 0; i < numInputs; i++) {
+      tracedInputShapes_.push_back(inputs[i].sizes().vec());
+      tracedInputStrides_.push_back(inputs[i].strides().vec());
+    }
+  });
   GRAPH_DEBUG("Initializing graph input logical tensors");
   // initializeTensorIdToOccurence can also be called just once for the first
   // input shape
@@ -147,11 +152,28 @@ ArgSpecs LlgaKernel::initializeInputSpecs(const TensorArgs& inputs) {
   return inputSpecs;
 }
 
-ArgSpecs LlgaKernel::initializeOutputSpecs() {
+ArgSpecs LlgaKernel::initializeOutputSpecs(
+    const TensorArgs& inputs,
+    bool convertDimsToUnknown = false) {
   ArgSpecs outputSpecs;
   outputSpecs.reserve(nOutputs_);
+
+  if (convertDimsToUnknown == false) {
+    auto numInputs = inputs.size();
+    for (auto i = 0; i < numInputs; i++) {
+      if (!((inputs[i].sizes().vec() == tracedInputShapes_[i]) &&
+            (inputs[i].strides().vec() == tracedInputStrides_[i]))) {
+        convertDimsToUnknown = true;
+        break;
+      }
+    }
+  }
+
   for (size_t i = 0; i < nOutputs_; i++) {
-    auto spec = ArgSpec(graph_->outputs()[i]).convertDimsToUnknown();
+    auto spec = ArgSpec(graph_->outputs()[i]);
+    if (convertDimsToUnknown) {
+      spec = spec.convertDimsToUnknown();
+    }
 
     if (spec.is_quantized())
       spec = getQuantizedSpec(spec, i);
@@ -193,20 +215,18 @@ void LlgaKernel::prepareAndCacheRunArgs(
   }
 
   outputTensorTypes_.reserve(nOutputs_);
-  inplacePairOffsets_.reserve(nOutputs_);
+  std::fill(outputTensorTypes_.begin(), outputTensorTypes_.end(), undefined);
   for (size_t i = 0; i < nOutputs_; i++) {
     auto& spec = outputSpecs[i];
     auto opt = c10::TensorOptions(spec.aten_scalar_type()).device(device_);
 
     auto outputId = spec.tid();
-    auto iter = inplacePairs_.find(outputId);
-    if (iter != inplacePairs_.end()) {
+    auto inputOffset = inplacePairOffsets_[i];
+    if (inputOffset != INT16_MIN) {
       // output reuses one of input tensors
 #ifdef GRAPH_DEBUG_ENABLED
       GRAPH_DEBUG("Inplace computation");
 #endif
-      auto inputOffset = iter->second;
-      inplacePairOffsets_[i] = static_cast<char>(inputOffset);
       GRAPH_DEBUG("INPUT INDEX OF INPLACE PAIR IS ", inputOffset);
       auto inputTensor = inputs[inputOffset];
       auto dataType = spec.dtype();
@@ -282,6 +302,11 @@ void LlgaKernel::prepareAndCacheRunArgs(
       }
     }
   }
+  TORCH_CHECK(
+      std::find(
+          outputTensorTypes_.begin(), outputTensorTypes_.end(), undefined) ==
+          outputTensorTypes_.end(),
+      "outputTensorTypes_ elements should not be undefined");
 }
 
 void LlgaKernel::prepareRunArgs(
@@ -303,13 +328,13 @@ void LlgaKernel::prepareRunArgs(
 
     switch (typeOfOutput) {
       case unwrappedInplaceCompute: {
-        auto inputTensor = inputs[static_cast<int>(inplacePairOffsets_[i])];
+        auto inputTensor = inputs[inplacePairOffsets_[i]];
         runOutputs[i].set_data_handle(inputTensor.data_ptr());
         outputs.push_back(std::move(inputTensor));
         break;
       }
       case quantizedInplaceCompute: {
-        auto inputTensor = inputs[static_cast<int>(inplacePairOffsets_[i])];
+        auto inputTensor = inputs[inplacePairOffsets_[i]];
         auto llgaImpl =
             static_cast<LlgaTensorImpl*>(inputTensor.unsafeGetTensorImpl());
         inputTensor =
@@ -319,7 +344,7 @@ void LlgaKernel::prepareRunArgs(
         break;
       }
       case unquantizedInplaceCompute: {
-        auto inputTensor = inputs[static_cast<int>(inplacePairOffsets_[i])];
+        auto inputTensor = inputs[inplacePairOffsets_[i]];
         auto llgaImpl =
             static_cast<LlgaTensorImpl*>(inputTensor.unsafeGetTensorImpl());
         inputTensor = LlgaTensorImpl::llga_to_aten_tensor(llgaImpl);
@@ -349,14 +374,42 @@ void LlgaKernel::prepareRunArgs(
   }
 }
 
-compiled_partition LlgaKernel::compile(
+std::pair<compiled_partition, ArgSpecs> LlgaKernel::compile(
     const partition& partition,
-    ArgSpecs& inputSpecs,
-    ArgSpecs& outputSpecs) {
+    const TensorArgs& inputs,
+    ArgSpecs& inputSpecs) {
   RECORD_FUNCTION("LLGA_bridge::compileKernel", c10::ArrayRef<c10::IValue>({}));
-  auto inputs = fmap(inputSpecs, toLogicalTensor);
-  auto outputs = fmap(outputSpecs, toLogicalTensor);
-  auto compilation = partition.compile(inputs, outputs, Engine::getEngine());
+  auto inputLogicalTensors = fmap(inputSpecs, toLogicalTensor);
+  auto outputSpecs = initializeOutputSpecs(inputs);
+  auto outputLogicalTensors = fmap(outputSpecs, toLogicalTensor);
+  compiled_partition compilation;
+  try {
+    compilation = partition.compile(
+        inputLogicalTensors, outputLogicalTensors, Engine::getEngine());
+  } catch (std::exception& e) {
+    // if partition compilation failed, check if outputSpecs had concrete shapes
+    // & strides
+    bool concreteOutputStrides = false;
+    for (auto outputSpec : outputSpecs) {
+      if (outputSpec.sizes().size()) {
+        if (outputSpec.sizes()[0] != INT64_MIN) {
+          concreteOutputStrides = true;
+          break;
+        }
+      }
+    }
+    if (concreteOutputStrides) {
+      // recompute outputSpecs and output logical tensors
+      // with INT64_MIN sizes & strides
+      outputSpecs = initializeOutputSpecs(inputs, true);
+      outputLogicalTensors = fmap(outputSpecs, toLogicalTensor);
+      compilation = partition.compile(
+          inputLogicalTensors, outputLogicalTensors, Engine::getEngine());
+    } else {
+      // there's nothing we can do
+      throw;
+    }
+  }
 
   // Since layouts of opaque outputs would be known after compilation,
   // we need to query them out from compilation and update outputSpecs
@@ -366,7 +419,10 @@ compiled_partition LlgaKernel::compile(
         outputSpecs[i].update_desc(compilation.query_logical_tensor(tid));
   }
 
-  // Build static mapping from output id to input offset
+  inplacePairOffsets_.resize(nOutputs_);
+  std::fill(inplacePairOffsets_.begin(), inplacePairOffsets_.end(), INT16_MIN);
+
+  // Build static mapping from output offset to input offset
   // in accordance with available inplace options
   for (auto&& option : compilation.get_inplace_ports()) {
     size_t inputId = option.first;
@@ -377,10 +433,17 @@ compiled_partition LlgaKernel::compile(
         });
     TORCH_CHECK(inputSpecIter != inputSpecs.end(), "In-place input not found");
     auto inputOffset = inputSpecIter - inputSpecs.begin();
-    inplacePairs_[outputId] = inputOffset;
+    auto outputSpecIter =
+        std::find_if(outputSpecs.begin(), outputSpecs.end(), [&](auto& spec) {
+          return spec.tid() == outputId;
+        });
+    TORCH_CHECK(
+        outputSpecIter != outputSpecs.end(), "In-place output not found");
+    auto outputOffset = outputSpecIter - outputSpecs.begin();
+    inplacePairOffsets_[outputOffset] = inputOffset;
   }
 
-  return compilation;
+  return std::make_pair(compilation, outputSpecs);
 }
 
 LlgaKernel::cp_entry& LlgaKernel::compileAndCache(
@@ -397,20 +460,27 @@ LlgaKernel::cp_entry& LlgaKernel::compileAndCache(
   std::vector<int64_t> key;
   key.reserve(1024);
   key.push_back(omp_get_max_threads());
+  // fusionNode_ may be reassigned to another LlgaFusionGroup after ~LlgaKernel
+  // would be called, and another LlgaFusionGroup may be created for another
+  // graph. But since JIT graphs have had a memory leak issue for years now,
+  // torch::jit::Graph::~Graph is not called after a model is traced.
+  // So we would use 2 pieces of info that make a partition unique.
   key.push_back((uintptr_t)((void*)fusionNode_));
+  key.push_back((uintptr_t)((void*)graph_.get()));
   for (auto& in : inputs) {
     auto shape_vec = in.sizes().vec();
     key.insert(key.end(), shape_vec.begin(), shape_vec.end());
   }
-  auto iter = cache_items_map_.find(key);
+  auto hashed_key = c10::get_hash(key);
+  auto iter = cache_items_map_.find(hashed_key);
   if (iter == cache_items_map_.end()) {
     GRAPH_DEBUG("Compiling partition");
     cp_entry compiledPartitionEntry;
     auto input_shape = inputs[0].sizes().vec();
     auto inputSpecs = initializeInputSpecs(inputs);
-    compiledPartitionEntry.outputSpecs_ = initializeOutputSpecs();
-    compiledPartitionEntry.cp_ = std::move(
-        compile(partition_, inputSpecs, compiledPartitionEntry.outputSpecs_));
+    auto compilationOutput = compile(partition_, inputs, inputSpecs);
+    compiledPartitionEntry.outputSpecs_ = std::move(compilationOutput.second);
+    compiledPartitionEntry.cp_ = std::move(compilationOutput.first);
     prepareAndCacheRunArgs(
         compiledPartitionEntry.inputLLGATensors_,
         compiledPartitionEntry.outputLLGATensors_,
@@ -419,8 +489,8 @@ LlgaKernel::cp_entry& LlgaKernel::compileAndCache(
         inputSpecs,
         compiledPartitionEntry.outputSpecs_);
     cache_items_list_.push_front(
-        key_value_pair_t(key, std::move(compiledPartitionEntry)));
-    cache_items_map_[key] = cache_items_list_.begin();
+        key_value_pair_t(hashed_key, std::move(compiledPartitionEntry)));
+    cache_items_map_[hashed_key] = cache_items_list_.begin();
     if (cache_items_map_.size() > capacity_) {
       auto last = cache_items_list_.end();
       last--;
@@ -429,8 +499,11 @@ LlgaKernel::cp_entry& LlgaKernel::compileAndCache(
     }
     // If hash computation cost is higher than copying this struct,
     // then remove std::move above & return compiledPartitionEntry instead
-    return cache_items_map_[key]->second;
+    return cache_items_map_[hashed_key]->second;
   } else {
+#ifdef GRAPH_DEBUG_ENABLED
+    GRAPH_DEBUG("Cached compiled partition is available");
+#endif
     cache_items_list_.splice(
         cache_items_list_.begin(), cache_items_list_, iter->second);
     prepareRunArgs(
@@ -448,9 +521,6 @@ void LlgaKernel::run(Stack& stack) {
   TensorArgs outputs;
   outputs.reserve(nOutputs_);
 
-#ifdef GRAPH_DEBUG_ENABLED
-  GRAPH_DEBUG("Cached compilation");
-#endif
   auto& compiledPartitionEntry = compileAndCache(stack, outputs);
 
 #ifdef GRAPH_DEBUG_ENABLED
diff --git a/csrc/cpu/jit/codegen/onednn/kernel.h b/csrc/cpu/jit/codegen/onednn/kernel.h
index 58c50f99..ef9840ba 100644
--- a/csrc/cpu/jit/codegen/onednn/kernel.h
+++ b/csrc/cpu/jit/codegen/onednn/kernel.h
@@ -11,34 +11,6 @@
 #include <torch/csrc/jit/jit_log.h>
 #include <torch/csrc/jit/runtime/interpreter.h>
 
-namespace std {
-template <>
-struct hash<std::vector<int64_t>> {
-  size_t operator()(const std::vector<int64_t>& key) const {
-    size_t total = key.size();
-    size_t sum = 0;
-    if (total < 64) {
-      for (size_t i = 0; i < total; i++) {
-        sum += key[i] << i;
-      }
-    } else {
-      size_t batch = total / 64;
-      size_t remain = total % 64;
-      for (size_t bs = 0; bs < batch; bs++) {
-        for (size_t i = 0; i < 64; i++) {
-          sum += key[bs * 64 + i] << i;
-        }
-      }
-      for (size_t i = 0; i < remain; i++) {
-        sum += key[batch * 64 + i] << i;
-      }
-    }
-    return sum;
-  }
-};
-
-} // namespace std
-
 namespace torch_ipex {
 namespace jit {
 namespace fuser {
@@ -70,6 +42,7 @@ class LlgaKernel {
   int64_t getOutputDtype(size_t offset) const;
 
   enum TypeOfOutputTensor {
+    undefined,
     unwrappedInplaceCompute,
     quantizedInplaceCompute,
     unquantizedInplaceCompute,
@@ -99,12 +72,14 @@ class LlgaKernel {
 
   ArgSpecs initializeInputSpecs(const TensorArgs& inputs);
 
-  ArgSpecs initializeOutputSpecs();
+  ArgSpecs initializeOutputSpecs(
+      const TensorArgs& inputs,
+      bool convertDimsToUnknown);
 
-  dnnl::graph::compiled_partition compile(
+  std::pair<dnnl::graph::compiled_partition, ArgSpecs> compile(
       const dnnl::graph::partition& partition,
-      ArgSpecs& inputSpecs,
-      ArgSpecs& outputSpecs);
+      const TensorArgs& inputs,
+      ArgSpecs& inputSpecs);
 
   cp_entry& compileAndCache(torch::jit::Stack& stack, TensorArgs& outputs);
 
@@ -147,6 +122,7 @@ class LlgaKernel {
   std::shared_ptr<torch::jit::Graph> graph_;
   int64_t nGraphInputs_ = 0; // number of inputs to graph_ on the IR
   int64_t nOutputs_ = 0;
+
   std::map<size_t, torch::jit::Value*> tensorIdToValue_;
   std::vector<int64_t> runArgsIdx_;
   dnnl::graph::partition partition_;
@@ -164,19 +140,20 @@ class LlgaKernel {
   // https://github.com/lamerman/cpp-lru-cache/blob/master/include/lrucache.hpp
   // LRU cache is per-thread, so as to enable weight sharing among groups of
   // threads.
-  using key_value_pair_t = std::pair<std::vector<int64_t>, cp_entry>;
+  using key_value_pair_t = std::pair<size_t, cp_entry>;
   using list_iterator_t = std::list<key_value_pair_t>::iterator;
   static thread_local std::list<key_value_pair_t> cache_items_list_;
-  static thread_local std::unordered_map<std::vector<int64_t>, list_iterator_t>
+  static thread_local std::unordered_map<size_t, list_iterator_t>
       cache_items_map_;
   static thread_local int capacity_;
-
-  std::unordered_map<size_t, size_t> inplacePairs_; // output id -> input offset
+  std::vector<std::vector<int64_t>> tracedInputShapes_;
+  std::vector<std::vector<int64_t>> tracedInputStrides_;
   std::string debugName_;
   std::string profileName_;
   std::vector<TypeOfOutputTensor> outputTensorTypes_;
   std::once_flag constantSpecInitializedFlag_;
-  std::vector<char> inplacePairOffsets_;
+  std::once_flag tracedInputShapesInitialized_;
+  std::vector<short> inplacePairOffsets_;
 };
 
 } // namespace onednn
diff --git a/csrc/cpu/jit/codegen/onednn/prepare_dequant.cpp b/csrc/cpu/jit/codegen/onednn/prepare_dequant.cpp
index 00a0311a..a9fc21f5 100644
--- a/csrc/cpu/jit/codegen/onednn/prepare_dequant.cpp
+++ b/csrc/cpu/jit/codegen/onednn/prepare_dequant.cpp
@@ -1,4 +1,5 @@
 #include "prepare_dequant.h"
+#include <torch/csrc/jit/jit_log.h>
 #include "operator.h"
 #include "utils.h"
 
@@ -62,13 +63,45 @@ class OpSplitter {
       input_values.push_back(nv);
     }
     at::ArrayRef<NamedValue> args(input_values);
+    Value* split_node = nullptr;
 
     for (int i = 1; i < nb_uses; i++) {
-      auto split_node = g->insert(node->kind(), args);
-      split_node->setType(output_type);
-
       auto output_user = output_users[i];
-      output_user->replaceInputWith(output_value, split_node);
+      if ((i > 1) &&
+          ((uintptr_t)(output_user->prev()) ==
+           (uintptr_t)(output_users[i - 1])) &&
+          (output_user->kind().toQualString() == std::string("aten::add")) &&
+          (output_users[i - 1]->kind().toQualString() ==
+           std::string("aten::mul"))) {
+        // hard-coding for DLRM duplicate input issue
+        // uses the new torch::jit::Value that was used for the use adjacent to
+        // it
+        output_user->replaceInputWith(output_value, split_node);
+      } else {
+        if (((uintptr_t)(output_user->prev()) ==
+             (uintptr_t)(output_users[i - 1]))) {
+          // Looks like they would land up in the same partition, so we should
+          // not make replacements. In particular, this step ensures that the
+          // special-case above is not "undone".
+          if ((i == 1) && (nb_uses == 2) &&
+              (output_user->kind().toQualString() ==
+               std::string("aten::add")) &&
+              (output_users[i - 1]->kind().toQualString() ==
+               std::string("aten::mul"))) {
+            return false;
+          }
+        }
+        split_node = g->insert(node->kind(), args);
+        split_node->setType(output_type);
+        output_user->replaceInputWith(output_value, split_node);
+      }
+      GRAPH_DEBUG(
+          "Replacing input ",
+          output_value->debugName(),
+          " of node ",
+          output_user->kind().toQualString(),
+          " with ",
+          split_node->debugName());
     }
     return true;
   }
diff --git a/csrc/cpu/jit/codegen/onednn/quantization_patterns.h b/csrc/cpu/jit/codegen/onednn/quantization_patterns.h
index 2ee5bd6a..d231d4d0 100644
--- a/csrc/cpu/jit/codegen/onednn/quantization_patterns.h
+++ b/csrc/cpu/jit/codegen/onednn/quantization_patterns.h
@@ -126,6 +126,7 @@ void IpexQuantFusion(std::shared_ptr<torch::jit::Graph>& graph) {
       "After replaceEmbeddingBagWithQEmbeddingBag. Before replaceInteractionWithQInteraction",
       graph);
   graph_rewrite::replaceInteractionWithQInteraction(graph);
+  graph_rewrite::replaceMergedEmbCatWithQmergedEmbCat(graph);
   GRAPH_DUMP(
       "After replaceInteractionWithQInteraction. Before preprocessSizeForQLstm",
       graph);
diff --git a/csrc/cpu/jit/cpu/kernels/ContextLinearWoq.h b/csrc/cpu/jit/cpu/kernels/ContextLinearWoq.h
index c7c5673f..00be4f86 100644
--- a/csrc/cpu/jit/cpu/kernels/ContextLinearWoq.h
+++ b/csrc/cpu/jit/cpu/kernels/ContextLinearWoq.h
@@ -8,55 +8,11 @@ namespace detail {
 struct ContextLinearWoq final {
   at::Tensor at_weight_;
   c10::optional<at::Tensor> at_bias_;
-  // The list contains three dtype versions of bias, scale and zp
-  // i.e., fp32, fp16, bf16
-  // If bias is not present, it contains empty tensors
-  std::vector<at::Tensor> bias_list_;
-  std::vector<at::Tensor> scales_list_;
-  std::vector<at::Tensor> zero_points_list_;
-  bool is_int4_;
-  int64_t lowp_mode_;
-  int64_t num_concats_;
-  // Original weight shape. Weight may be padded after packing
-  c10::optional<std::vector<int64_t>> orig_wei_shape_;
 
   ContextLinearWoq() = delete;
 
-  ContextLinearWoq(
-      at::Tensor&& at_weight,
-      at::Tensor&& scales_float,
-      at::Tensor&& zero_point_float,
-      c10::optional<at::Tensor>&& bias,
-      bool is_int4 = false,
-      int64_t lowp_mode = 0,
-      int64_t num_concats = 1,
-      c10::optional<std::vector<int64_t>>&& orig_wei_shape = c10::nullopt)
-      : at_weight_(std::move(at_weight)),
-        at_bias_(std::move(bias)),
-        is_int4_(is_int4),
-        lowp_mode_(lowp_mode),
-        num_concats_(num_concats),
-        orig_wei_shape_(std::move(orig_wei_shape)) {
-    // Make three dtype versions of scale, zp and bias
-    // There is one more dtype for zp
-    auto scales_fp16 = scales_float.to(c10::kHalf);
-    auto scales_bf16 = scales_float.to(c10::kBFloat16);
-    scales_list_ = {scales_float, scales_fp16, scales_bf16};
-    auto zp_fp16 = zero_point_float.to(c10::kHalf);
-    auto zp_bf16 = zero_point_float.to(c10::kBFloat16);
-    auto zp_int8 = zero_point_float.to(c10::kChar);
-    zero_points_list_ = {zero_point_float, zp_fp16, zp_bf16, zp_int8};
-    if (at_bias_.has_value() && at_bias_.value().defined()) {
-        auto bias_fp32 = at_bias_.value();
-        auto bias_fp16 = bias_fp32.to(c10::kHalf);
-        auto bias_bf16 = bias_fp32.to(c10::kBFloat16);
-        bias_list_ = {bias_fp32, bias_fp16, bias_bf16};
-    } else {
-        // bias tensor is empty (undefined). Leave the check to kernel.
-        auto bias_empty = at::Tensor();
-        bias_list_ = {bias_empty, bias_empty, bias_empty};
-    }
-  }
+  ContextLinearWoq(at::Tensor&& at_weight, c10::optional<at::Tensor>&& bias)
+      : at_weight_(std::move(at_weight)), at_bias_(std::move(bias)) {}
 
   ContextLinearWoq(ContextLinearWoq&&) = default;
   ContextLinearWoq& operator=(ContextLinearWoq&&) = default;
diff --git a/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.cpp b/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.cpp
index 8d83405b..5f09141b 100644
--- a/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.cpp
+++ b/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.cpp
@@ -12,77 +12,13 @@ namespace woq_linear {
 c10::intrusive_ptr<WoqLinearOpContext> createWoqLinearPrePackOpContext(
     at::Tensor&& weight,
     c10::optional<at::Tensor>&& bias,
-    c10::optional<int64_t> batch_size,
-    int64_t lowp_mode,
-    int64_t num_concats) {
+    c10::optional<int64_t> batch_size) {
   RECORD_FUNCTION(
       "ipex_prepack::createWoqLinearPrePackOpContext",
       c10::ArrayRef<c10::IValue>({}));
 
   return IpexWoqLinearOpContext::create_context(
-      std::move(weight), std::move(bias), batch_size, lowp_mode, num_concats);
-}
-
-c10::intrusive_ptr<WoqLinearOpContext> createWoqLinearPrePackOpContextInt4(
-    at::Tensor&& weight,
-    at::Tensor&& scales,
-    at::Tensor&& zero_points,
-    c10::optional<at::Tensor>&& bias,
-    c10::optional<int64_t> batch_size,
-    int64_t lowp_mode,
-    int64_t num_concats) {
-  RECORD_FUNCTION(
-      "ipex_prepack::createWoqLinearPrePackOpContextInt4",
-      c10::ArrayRef<c10::IValue>({}));
-  // From
-  // Weight dtype = int32 (uint4 * 8), scale dtype = fp16, zero points dtype = int32 (int4 * 8)
-  // To
-  // Weight dtype = quint4x2, scale dtype = fp32, zero points dtype = fp32
-  // There might be an extra output channel in weight and scales
-  // bool extra_o_channel = false; // scales.numel() > zero_points.numel() * 8;
-  auto scales_fp32 = scales.squeeze(0).to(c10::ScalarType::Float);
-
-  // Convert compressed zero points to float
-  auto zp_fp32 = at::empty_like(scales_fp32);
-  assert(zp_fp32.numel() == zero_points.numel() * 8);
-  float* zp_fp32_ptr = reinterpret_cast<float*>(zp_fp32.data_ptr());
-  int32_t* zp_int32_ptr = reinterpret_cast<int32_t*>(zero_points.data_ptr());
-  for (size_t i = 0; i < zero_points.numel(); ++i) {
-    int32_t zp_uint4x8 = zp_int32_ptr[i];
-    for (size_t j = 0; j < 8; ++j) {
-      zp_fp32_ptr[i * 8 + j] = (float)((zp_uint4x8 >> (j * 4)) & 0xf);
-    }
-  }
-  // Support two cases here:
-  // 1. bf16 weight after calibration
-  // 2. int4 weight after calibration, quantized and compressed, as int32
-  at::Tensor weight_int4;
-  if (weight.scalar_type() == c10::kInt) {
-    // Weight created by GPTQ and transposed
-    // Create empty weight with desired options then copy data
-    int64_t N = weight.size(1);
-    int64_t K_int32 = weight.size(0);
-    int64_t K = K_int32 * 8; // int32 = int4 * 8
-    std::vector<int64_t> weight_size = {N, K};
-    // Create an empty quint4x2 weight with scales and zero points
-    weight_int4 = at::_empty_per_channel_affine_quantized(
-        weight_size,
-        scales_fp32,
-        zp_fp32,
-        0,
-        device(c10::kCPU).dtype(c10::kQUInt4x2)
-    );
-    auto weight_t = weight.t().contiguous();
-    std::memcpy(weight_int4.data_ptr(), weight_t.data_ptr(), weight_t.numel() * sizeof(uint32_t));
-  } else if (weight.scalar_type() == c10::kBFloat16) {
-    // Load bf16 weight and quantize
-    auto weight_fp32 = weight.to(c10::kFloat);
-    weight_int4 = at::quantize_per_channel(weight_fp32, scales_fp32, zp_fp32, 0, c10::kQUInt4x2);
-  } else if (weight.scalar_type() == c10::kFloat) {
-    weight_int4 = at::quantize_per_channel(weight, scales_fp32, zp_fp32, 0, c10::kQUInt4x2);
-  }
-  return IpexWoqLinearOpContext::create_context(
-      std::move(weight_int4), std::move(bias), batch_size, lowp_mode, num_concats);
+      std::move(weight), std::move(bias), batch_size);
 }
 
 at::Tensor woq_linear_run(
@@ -96,222 +32,38 @@ at::Tensor woq_linear_run(
 
 ContextLinearWoq create(
     at::Tensor& weight,
-    at::Tensor& scales,
     at::Tensor& zero_points,
+    at::Tensor& scales,
     const c10::optional<at::Tensor>& bias,
-    const c10::optional<int64_t> batch_size,
-    int64_t lowp_mode,
-    int64_t num_concats) {
-  auto packed_weight = woq_linear_pack_weight(weight, scales, zero_points, lowp_mode);
-  bool is_int4 = weight.scalar_type() == c10::kQUInt4x2;
-  auto packed_shape = packed_weight.sizes();
-  int64_t N = weight.size(0);
-  int64_t K = weight.size(1);
-  bool weight_is_padded =
-      (packed_shape.size() == 4 && is_int4 && packed_shape[0] * packed_shape[3] * 2 != N) ||
-      (packed_shape.size() == 4 && !is_int4 && packed_shape[0] * packed_shape[3] != N) ||
-      (packed_shape.size() == 2 && packed_shape[0] != N);
-  auto zero_points_float = zero_points.to(c10::kFloat);
-  if (weight_is_padded) {
-    int64_t padded_N = packed_shape.size() == 4
-          ? (is_int4 ? packed_shape[0] * packed_shape[3] * 2 : packed_shape[0] * packed_shape[3])
-          : packed_shape[0];
-    auto scales_padded = at::pad(scales, {0, padded_N - N}, "constant", 1.f);
-    auto zero_points_padded = at::pad(zero_points_float, {0, padded_N - N}, "constant", 0.f);
-    if (bias.has_value()) {
-      auto bias_padded = at::pad(bias.value(), {0, padded_N - N}, "constant", 0.f);
-      return ContextLinearWoq(
-          std::move(packed_weight),
-          std::move(scales_padded),
-          std::move(zero_points_padded),
-          c10::make_optional(bias_padded),
-          is_int4,
-          lowp_mode,
-          num_concats,
-          c10::make_optional(weight.sizes().vec())
-      );
-    } else {
-      return ContextLinearWoq(
-          std::move(packed_weight),
-          std::move(scales_padded),
-          std::move(zero_points_padded),
-          c10::nullopt,
-          is_int4,
-          lowp_mode,
-          num_concats,
-          c10::make_optional(weight.sizes().vec())
-      );
-    }
+    const c10::optional<int64_t> batch_size) {
+  // TODO Will support optimized impl
+  if (weight.scalar_type() == c10::ScalarType::QUInt4x2) {
+    return ContextLinearWoq{
+        std::move(weight),
+        bias.has_value() ? c10::make_optional(*bias) : c10::nullopt,
+    };
   }
-  return ContextLinearWoq(
+  auto packed_weight = woq_linear_pack_weight(weight, zero_points, scales);
+  return ContextLinearWoq{
       std::move(packed_weight),
-      std::move(scales),
-      std::move(zero_points_float),
       bias.has_value() ? c10::make_optional(*bias) : c10::nullopt,
-      is_int4,
-      lowp_mode,
-      num_concats,
-      weight_is_padded ? c10::make_optional(weight.sizes().vec()) : c10::nullopt
-  );
+  };
 }
 
 at::Tensor run(
     ContextLinearWoq& context,
+    const at::Tensor& zero_points_float,
+    const at::Tensor& scales_float,
     const at::Tensor& input) {
-  // TPP kernel packs weight to 4d (Nc, Kc, block_k, block_n)
-  auto w_k = context.at_weight_.dim() == 2 ? context.at_weight_.size(1) :
-      context.at_weight_.size(1) * context.at_weight_.size(2);
   TORCH_CHECK(
-      input.size(input.dim() - 1) == w_k,
-      "WOQ linear: input and weight shapes do not match, got k = ", input.size(input.dim() - 1), " and ", w_k, " respectively.");
+      input.size(input.dim() - 1) == context.at_weight_.size(1),
+      "Check the shapes of mat1 and mat2, they cannot be multiplied!");
   auto input_ = input.contiguous();
-  // if weight is not padded, context.orig_wei_shape_ has no value
-  if (context.orig_wei_shape_.has_value()) {
-    auto res = woq_linear_kernel(
-        input_, context.at_weight_, context.scales_list_, context.zero_points_list_, context.bias_list_,
-        context.is_int4_, context.lowp_mode_, context.num_concats_);
-    // weight shape is [N by K], output shape is [M by N] or [batch by M by N]
-    int64_t N = context.orig_wei_shape_.value()[0];
-    return at::slice(res, /*dim*/-1, /*start*/0, /*end*/N, /*step*/1);
-  }
+  c10::MaybeOwned<at::Tensor> bias_maybe_owned =
+      at::borrow_from_optional_tensor(context.at_bias_);
+  const at::Tensor& bias = *bias_maybe_owned;
   return woq_linear_kernel(
-      input_, context.at_weight_, context.scales_list_, context.zero_points_list_, context.bias_list_,
-      context.is_int4_, context.lowp_mode_, context.num_concats_);
-}
-
-// Called by IpexWoqLinearOpContext::run_eltwise
-at::Tensor run_eltwise(
-    ContextLinearWoq& context,
-    const at::Tensor& input,
-    const c10::string_view& post_op,
-    const torch::List<c10::optional<at::Scalar>>& scalars,
-    const c10::optional<c10::string_view>& algorithm) {
-  // TPP kernel packs weight to 4d (Nc, Kc, block_k, block_n)
-  auto w_k = context.at_weight_.dim() == 2 ? context.at_weight_.size(1) : context.at_weight_.size(1) * context.at_weight_.size(2);
-  TORCH_CHECK(
-      input.size(input.dim() - 1) == w_k,
-      "WOQ linear: input and weight shapes do not match, got k = ", input.size(input.dim() - 1), " and ", w_k, " respectively.");
-  auto input_ = input.contiguous();
-  return woq_linear_eltwise_kernel(
-      input_, context.at_weight_, context.scales_list_, context.zero_points_list_, context.bias_list_,
-      post_op, scalars, algorithm, context.is_int4_, context.lowp_mode_, context.num_concats_);
-}
-
-// Registered as JIT op
-at::Tensor woq_linear_eltwise_run(
-    const at::Tensor& input,
-    const at::Tensor& op_context,
-    const c10::string_view& post_op,
-    const torch::List<c10::optional<at::Scalar>>& scalars,
-    const c10::optional<c10::string_view>& algorithm) {
-  static std::map<c10::string_view, std::string> postop_to_record_name_map = {
-    {"relu", "torch_ipex::woq_linear_relu_run"},
-    {"gelu", "torch_ipex::woq_linear_gelu_run"},
-  };
-  RECORD_FUNCTION(
-      postop_to_record_name_map[post_op], c10::ArrayRef<c10::IValue>({}));
-  return reinterpret_cast<IpexWoqLinearOpContext*>(
-             op_context.data_ptr<int64_t>()[0])
-      ->run_eltwise(input, post_op, scalars, algorithm);
-}
-
-// Called by IpexWoqLinearOpContext::run_add
-at::Tensor run_add(
-    ContextLinearWoq& context,
-    const at::Tensor& input,
-    at::Tensor& accumu,
-    const c10::optional<at::Scalar>& alpha) {
-  // TPP kernel packs weight to 4d (Nc, Kc, block_k, block_n)
-  auto w_k = context.at_weight_.dim() == 2 ? context.at_weight_.size(1) : context.at_weight_.size(1) * context.at_weight_.size(2);
-  TORCH_CHECK(
-      input.size(input.dim() - 1) == w_k,
-      "WOQ linear: input and weight shapes do not match, got k = ", input.size(input.dim() - 1), " and ", w_k, " respectively.");
-  auto input_ = input.contiguous();
-  return woq_linear_add_kernel(
-      input_, context.at_weight_, context.scales_list_, context.zero_points_list_, context.bias_list_,
-      context.is_int4_, context.lowp_mode_, context.num_concats_, accumu, alpha
-  );
-}
-
-// Called by IpexWoqLinearOpContext::run_add_relu
-at::Tensor run_add_relu(
-    ContextLinearWoq& context,
-    const at::Tensor& input,
-    at::Tensor& accumu,
-    const c10::optional<at::Scalar>& alpha) {
-  // TPP kernel packs weight to 4d (Nc, Kc, block_k, block_n)
-  auto w_k = context.at_weight_.dim() == 2 ? context.at_weight_.size(1) : context.at_weight_.size(1) * context.at_weight_.size(2);
-  TORCH_CHECK(
-      input.size(input.dim() - 1) == w_k,
-      "WOQ linear: input and weight shapes do not match, got k = ", input.size(input.dim() - 1), " and ", w_k, " respectively.");
-  auto input_ = input.contiguous();
-  auto output = woq_linear_kernel(
-      input_, context.at_weight_, context.scales_list_, context.zero_points_list_, context.bias_list_,
-      context.is_int4_, context.lowp_mode_, context.num_concats_);
-  at::add_out(accumu, output, accumu, alpha.value());
-  at::relu_(accumu);
-  return accumu;
-}
-
-// Called by IpexWoqLinearOpContext::run_add
-at::Tensor run_add(
-    ContextLinearWoq& context,
-    const at::Tensor& input,
-    const std::vector<at::Tensor>& others) {
-  // TPP kernel packs weight to 4d (Nc, Kc, block_k, block_n)
-  auto w_k = context.at_weight_.dim() == 2 ? context.at_weight_.size(1) : context.at_weight_.size(1) * context.at_weight_.size(2);
-  TORCH_CHECK(
-      input.size(input.dim() - 1) == w_k,
-      "WOQ linear: input and weight shapes do not match, got k = ", input.size(input.dim() - 1), " and ", w_k, " respectively.");
-  auto input_ = input.contiguous();
-  return woq_linear_add_kernel(
-      input_, context.at_weight_, context.scales_list_, context.zero_points_list_, context.bias_list_,
-      context.is_int4_, context.lowp_mode_, context.num_concats_, others
-  );
-}
-
-// Called by IpexWoqLinearOpContext::run_add_add
-at::Tensor run_add_add(
-    ContextLinearWoq& context,
-    const at::Tensor& input,
-    const std::vector<at::Tensor>& others) {
-  // TPP kernel packs weight to 4d (Nc, Kc, block_k, block_n)
-  auto w_k = context.at_weight_.dim() == 2 ? context.at_weight_.size(1) : context.at_weight_.size(1) * context.at_weight_.size(2);
-  TORCH_CHECK(
-      input.size(input.dim() - 1) == w_k,
-      "WOQ linear: input and weight shapes do not match, got k = ", input.size(input.dim() - 1), " and ", w_k, " respectively.");
-  auto input_ = input.contiguous();
-  return woq_linear_add_add_kernel(
-      input_, context.at_weight_, context.scales_list_, context.zero_points_list_, context.bias_list_,
-      context.is_int4_, context.lowp_mode_, context.num_concats_, others
-  );
-}
-
-
-// Registered as JIT op
-at::Tensor woq_linear_add_run(
-    const at::Tensor& input,
-    at::Tensor& accumu,
-    const c10::optional<at::Scalar>& alpha,
-    const at::Tensor& op_context) {
-  RECORD_FUNCTION(
-      "torch_ipex::woq_linear_add_run", c10::ArrayRef<c10::IValue>({}));
-  return reinterpret_cast<IpexWoqLinearOpContext*>(
-             op_context.data_ptr<int64_t>()[0])
-      ->run_add(input, accumu, alpha);
-}
-
-// Registered as JIT op
-at::Tensor woq_linear_add_relu_run(
-    const at::Tensor& input,
-    at::Tensor& accumu,
-    const c10::optional<at::Scalar>& alpha,
-    const at::Tensor& op_context) {
-  RECORD_FUNCTION(
-      "torch_ipex::woq_linear_add_relu_run", c10::ArrayRef<c10::IValue>({}));
-  return reinterpret_cast<IpexWoqLinearOpContext*>(
-            op_context.data_ptr<int64_t>()[0])
-      ->run_add_relu(input, accumu, alpha);
+      input_, context.at_weight_, zero_points_float, scales_float, bias);
 }
 
 at::Tensor pack(ContextLinearWoq& context, const at::Tensor& tensor) {
@@ -319,39 +71,7 @@ at::Tensor pack(ContextLinearWoq& context, const at::Tensor& tensor) {
 }
 
 at::Tensor unpack(ContextLinearWoq& context, const at::Tensor& tensor) {
-  // By using different kernels, the packed weight dim can be 2 or 4
-  // Return result directly if dim == 2
-  // For dim == 4, make a new quantized tensor and return.
-  // For padded weight (int4), make a slice of it.
-  auto unpacked_weight = woq_linear_unpack_weight(tensor, context.is_int4_, context.lowp_mode_);
-  if (tensor.dim() > 2) {
-    auto scales = context.scales_list_[0];
-    auto zero_points = context.zero_points_list_[0];
-    if (context.is_int4_) {
-      auto unpacked_shape = unpacked_weight.sizes().vec(); // = N * K/2
-      auto shape = context.orig_wei_shape_.has_value()
-          ? context.orig_wei_shape_.value()
-          : std::vector<int64_t>({unpacked_shape[0], unpacked_shape[1] * 2});
-      at::Tensor qweight = at::_empty_per_channel_affine_quantized(
-          shape,
-          scales,
-          zero_points,
-          0,
-          device(c10::kCPU).dtype(c10::kQUInt4x2)
-      );
-      assert(qweight.numel() % 2 == 0);
-      std::memcpy(qweight.data_ptr(), unpacked_weight.data_ptr(), qweight.numel() / 2);
-      return qweight;
-    } else { // int8
-      return at::_make_per_channel_quantized_tensor(
-        unpacked_weight.int_repr(),
-        scales,
-        zero_points.to(c10::kInt),
-        0
-      );
-    }
-  }
-  return unpacked_weight;
+  return woq_linear_unpack_weight(tensor);
 }
 
 } // namespace woq_linear
diff --git a/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.h b/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.h
index eaad71a7..39a08bc5 100644
--- a/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.h
+++ b/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.h
@@ -13,84 +13,27 @@ namespace woq_linear {
 c10::intrusive_ptr<WoqLinearOpContext> createWoqLinearPrePackOpContext(
     at::Tensor&& weight,
     c10::optional<at::Tensor>&& bias,
-    c10::optional<int64_t> batch_size,
-    int64_t lowp_mode,
-    int64_t num_concats);
-
-c10::intrusive_ptr<WoqLinearOpContext> createWoqLinearPrePackOpContextInt4(
-    at::Tensor&& weight,
-    at::Tensor&& scales,
-    at::Tensor&& zero_points,
-    c10::optional<at::Tensor>&& bias,
-    c10::optional<int64_t> batch_size,
-    int64_t lowp_mode,
-    int64_t num_concats);
+    c10::optional<int64_t> batch_size);
 
 at::Tensor woq_linear_run(
     const at::Tensor& input,
+    const at::Tensor& zero_points_int32,
+    const at::Tensor& scales_float,
     c10::intrusive_ptr<WoqLinearOpContext> op_context);
 
 ContextLinearWoq create(
     at::Tensor& weight,
-    at::Tensor& scales,
     at::Tensor& zero_points,
+    at::Tensor& scales,
     const c10::optional<at::Tensor>& bias,
-    const c10::optional<int64_t> batch_size,
-    int64_t lowp_mode,
-    int64_t num_concats);
+    const c10::optional<int64_t> batch_size);
 
 at::Tensor run(
     ContextLinearWoq& context,
+    const at::Tensor& zero_points_int32,
+    const at::Tensor& scales_float,
     const at::Tensor& input);
 
-at::Tensor run_eltwise(
-    ContextLinearWoq& context,
-    const at::Tensor& input,
-    const c10::string_view& post_op,
-    const torch::List<c10::optional<at::Scalar>>& scalars,
-    const c10::optional<c10::string_view>& algorithm);
-
-at::Tensor woq_linear_eltwise_run(
-    const at::Tensor& input,
-    const at::Tensor& op_context,
-    const c10::string_view& post_op,
-    const torch::List<c10::optional<at::Scalar>>& scalars,
-    const c10::optional<c10::string_view>& algorithm);
-
-at::Tensor run_add(
-    ContextLinearWoq& context,
-    const at::Tensor& input,
-    at::Tensor& accumu,
-    const c10::optional<at::Scalar>& alpha);
-
-at::Tensor run_add_relu(
-    ContextLinearWoq& context,
-    const at::Tensor& input,
-    at::Tensor& accumu,
-    const c10::optional<at::Scalar>& alpha);
-
-at::Tensor run_add(
-    ContextLinearWoq& context,
-    const at::Tensor& input,
-    const std::vector<at::Tensor>& others);
-
-at::Tensor run_add_add(
-    ContextLinearWoq& context,
-    const at::Tensor& input,
-    const std::vector<at::Tensor>& others);
-
-at::Tensor woq_linear_add_run(
-    const at::Tensor& input,
-    at::Tensor& accumu,
-    const c10::optional<at::Scalar>& alpha,
-    const at::Tensor& op_context);
-
-at::Tensor woq_linear_add_relu_run(
-    const at::Tensor& input,
-    at::Tensor& accumu,
-    const c10::optional<at::Scalar>& alpha,
-    const at::Tensor& op_context);
-
 at::Tensor pack(ContextLinearWoq& context, const at::Tensor& tensor);
 
 at::Tensor unpack(ContextLinearWoq& context, const at::Tensor& tensor);
@@ -98,4 +41,4 @@ at::Tensor unpack(ContextLinearWoq& context, const at::Tensor& tensor);
 } // namespace woq_linear
 } // namespace detail
 } // namespace cpu
-} // namespace torch_ipex
\ No newline at end of file
+} // namespace torch_ipex
diff --git a/csrc/cpu/jit/cpu/kernels/OpContext.cpp b/csrc/cpu/jit/cpu/kernels/OpContext.cpp
index 0114a9ed..71091320 100644
--- a/csrc/cpu/jit/cpu/kernels/OpContext.cpp
+++ b/csrc/cpu/jit/cpu/kernels/OpContext.cpp
@@ -355,114 +355,84 @@ void IpexConvTransposeOpContext::load_from_ctx(
 c10::intrusive_ptr<WoqLinearOpContext> IpexWoqLinearOpContext::create_context(
     at::Tensor&& weight,
     c10::optional<at::Tensor>&& bias,
-    c10::optional<int64_t> batch_size,
-    int64_t lowp_mode,
-    int64_t num_concats) {
+    c10::optional<int64_t> batch_size) {
+  // TODO Will support optimized impl
+  if (weight.scalar_type() == c10::ScalarType::QUInt4x2) {
+    at::Tensor scales, zero_points;
+    auto op_context = torch_ipex::cpu::detail::woq_linear::create(
+        weight, zero_points, scales, bias, batch_size);
+    return c10::make_intrusive<IpexWoqLinearOpContext>(
+        batch_size,
+        std::move(op_context),
+        std::move(zero_points),
+        std::move(scales));
+  }
   auto N = weight.size(0);
   const auto qtype = weight.qscheme();
-  if (weight.scalar_type() == c10::ScalarType::QInt8) {
-    // extract scales from weight
-    std::vector<float> weight_scales_float(1, 0.0);
-    if (qtype == c10::kPerTensorAffine) {
-      weight_scales_float[0] = weight.q_scale();
-    } else if (qtype == c10::kPerChannelAffine) {
-      weight_scales_float.resize(N, 0.0);
-      for (const auto i : c10::irange(N)) {
-        weight_scales_float[i] = weight.q_per_channel_scales()[i].item<float>();
-      }
-    }
 
-    at::Tensor scales = at::empty(
-        {static_cast<long>(weight_scales_float.size())},
-        at::device(c10::kCPU).dtype(c10::kFloat));
-    std::copy(
-        weight_scales_float.begin(),
-        weight_scales_float.end(),
-        scales.data_ptr<float>());
-
-    // extract zero_points from weight
-    std::vector<int32_t> weight_zero_points_int32(1, 0);
-    if (qtype == c10::kPerTensorAffine) {
-      weight_zero_points_int32[0] = weight.q_zero_point();
-    } else if (qtype == c10::kPerChannelAffine) {
-      weight_zero_points_int32.resize(N, 0);
-      for (const auto i : c10::irange(N)) {
-        weight_zero_points_int32[i] =
-            weight.q_per_channel_zero_points()[i].item<int32_t>();
-      }
-    }
-    at::Tensor zero_points_int32 = at::empty(
-        {static_cast<long>(weight_zero_points_int32.size())},
-        at::device(c10::kCPU).dtype(c10::kInt));
-    std::copy(
-        weight_zero_points_int32.begin(),
-        weight_zero_points_int32.end(),
-        zero_points_int32.data_ptr<int32_t>());
-
-    // convert zero_points from int32_t to float
-    std::vector<float> weight_zero_points_float(1, 0);
-    if (qtype == c10::kPerTensorAffine) {
-      weight_zero_points_float[0] = (float)weight.q_zero_point();
-    } else if (qtype == c10::kPerChannelAffine) {
-      weight_zero_points_float.resize(N, 0);
-      for (const auto i : c10::irange(N)) {
-        weight_zero_points_float[i] =
-            (float)weight.q_per_channel_zero_points()[i].item<int32_t>();
-      }
+  // extract zero_points from weight
+  std::vector<int32_t> weight_zero_points_int32(1, 0);
+  if (qtype == c10::kPerTensorAffine) {
+    weight_zero_points_int32[0] = weight.q_zero_point();
+  } else if (qtype == c10::kPerChannelAffine) {
+    weight_zero_points_int32.resize(N, 0);
+    for (const auto i : c10::irange(N)) {
+      weight_zero_points_int32[i] =
+          weight.q_per_channel_zero_points()[i].item<int32_t>();
     }
-    at::Tensor zero_points_float = at::empty(
-        {static_cast<long>(weight_zero_points_float.size())},
-        at::device(c10::kCPU).dtype(c10::kFloat));
-    std::copy(
-        weight_zero_points_float.begin(),
-        weight_zero_points_float.end(),
-        zero_points_float.data_ptr<float>());
-
-    auto op_context = torch_ipex::cpu::detail::woq_linear::create(
-        weight, scales, zero_points_int32, bias, batch_size, lowp_mode, num_concats);
-    return c10::make_intrusive<IpexWoqLinearOpContext>(
-        batch_size,
-        std::move(op_context));
-  } else {
-    // extract scales from weight
-    std::vector<float> weight_scales_float(1, 0.0);
-    if (qtype == c10::kPerChannelAffineFloatQParams) {
-      weight_scales_float.resize(N, 0.0);
-      for (const auto i : c10::irange(N)) {
-        weight_scales_float[i] = weight.q_per_channel_scales()[i].item<float>();
-      }
+  }
+  at::Tensor zero_points_int32 = at::empty(
+      {static_cast<long>(weight_zero_points_int32.size())},
+      at::device(c10::kCPU).dtype(c10::kInt));
+  std::copy(
+      weight_zero_points_int32.begin(),
+      weight_zero_points_int32.end(),
+      zero_points_int32.data_ptr<int32_t>());
+
+  // convert zero_points from int32_t to float
+  std::vector<float> weight_zero_points_float(1, 0);
+  if (qtype == c10::kPerTensorAffine) {
+    weight_zero_points_float[0] = (float)weight.q_zero_point();
+  } else if (qtype == c10::kPerChannelAffine) {
+    weight_zero_points_float.resize(N, 0);
+    for (const auto i : c10::irange(N)) {
+      weight_zero_points_float[i] =
+          (float)weight.q_per_channel_zero_points()[i].item<int32_t>();
     }
-
-    at::Tensor scales = at::empty(
-        {static_cast<long>(weight_scales_float.size())},
-        at::device(c10::kCPU).dtype(c10::kFloat));
-    std::copy(
-        weight_scales_float.begin(),
-        weight_scales_float.end(),
-        scales.data_ptr<float>());
-
-    // extract zero_points from weight
-    std::vector<float> weight_zero_points_float(1, 0);
-    if (qtype == c10::kPerChannelAffineFloatQParams) {
-      weight_zero_points_float.resize(N, 0);
-      for (const auto i : c10::irange(N)) {
-        weight_zero_points_float[i] =
-            weight.q_per_channel_zero_points()[i].item<float>();
-      }
+  }
+  at::Tensor zero_points_float = at::empty(
+      {static_cast<long>(weight_zero_points_float.size())},
+      at::device(c10::kCPU).dtype(c10::kFloat));
+  std::copy(
+      weight_zero_points_float.begin(),
+      weight_zero_points_float.end(),
+      zero_points_float.data_ptr<float>());
+
+  // extract scales from weight
+  std::vector<float> weight_scales_float(1, 0.0);
+  if (qtype == c10::kPerTensorAffine) {
+    weight_scales_float[0] = weight.q_scale();
+  } else if (qtype == c10::kPerChannelAffine) {
+    weight_scales_float.resize(N, 0.0);
+    for (const auto i : c10::irange(N)) {
+      weight_scales_float[i] = weight.q_per_channel_scales()[i].item<float>();
     }
-    at::Tensor zero_points_float = at::empty(
-        {static_cast<long>(weight_zero_points_float.size())},
-        at::device(c10::kCPU).dtype(c10::kFloat));
-    std::copy(
-        weight_zero_points_float.begin(),
-        weight_zero_points_float.end(),
-        zero_points_float.data_ptr<float>());
-    auto op_context = torch_ipex::cpu::detail::woq_linear::create(
-        weight, scales, zero_points_float, bias, batch_size, lowp_mode, num_concats);
-    return c10::make_intrusive<IpexWoqLinearOpContext>(
-        batch_size,
-        std::move(op_context));
   }
+  at::Tensor scales = at::empty(
+      {static_cast<long>(weight_scales_float.size())},
+      at::device(c10::kCPU).dtype(c10::kFloat));
+  std::copy(
+      weight_scales_float.begin(),
+      weight_scales_float.end(),
+      scales.data_ptr<float>());
+
+  auto op_context = torch_ipex::cpu::detail::woq_linear::create(
+      weight, zero_points_int32, scales, bias, batch_size);
+  return c10::make_intrusive<IpexWoqLinearOpContext>(
+      batch_size,
+      std::move(op_context),
+      std::move(zero_points_float),
+      std::move(scales));
 }
 
 at::Tensor IpexWoqLinearOpContext::get_data_handle() {
@@ -472,46 +442,8 @@ at::Tensor IpexWoqLinearOpContext::get_data_handle() {
 }
 
 at::Tensor IpexWoqLinearOpContext::run(const at::Tensor& input) {
-  return torch_ipex::cpu::detail::woq_linear::run(op_context_, input);
-}
-
-at::Tensor IpexWoqLinearOpContext::run_eltwise(
-    const at::Tensor& input,
-    const c10::string_view& post_op,
-    const torch::List<c10::optional<at::Scalar>>& scalars,
-    const c10::optional<c10::string_view>& algorithm) {
-  return torch_ipex::cpu::detail::woq_linear::run_eltwise(
-      op_context_, input, post_op, scalars, algorithm);
-}
-
-at::Tensor IpexWoqLinearOpContext::run_add(
-    const at::Tensor& input,
-    at::Tensor& accumu,
-    const c10::optional<at::Scalar>& alpha) {
-  return torch_ipex::cpu::detail::woq_linear::run_add(
-      op_context_, input, accumu, alpha);
-}
-
-at::Tensor IpexWoqLinearOpContext::run_add_relu(
-    const at::Tensor& input,
-    at::Tensor& accumu,
-    const c10::optional<at::Scalar>& alpha) {
-  return torch_ipex::cpu::detail::woq_linear::run_add_relu(
-      op_context_, input, accumu, alpha);
-}
-
-at::Tensor IpexWoqLinearOpContext::run_add(
-    const at::Tensor& input,
-    const std::vector<at::Tensor>& others) {
-  return torch_ipex::cpu::detail::woq_linear::run_add(
-      op_context_, input, others);
-}
-
-at::Tensor IpexWoqLinearOpContext::run_add_add(
-    const at::Tensor& input,
-    const std::vector<at::Tensor>& others) {
-  return torch_ipex::cpu::detail::woq_linear::run_add_add(
-      op_context_, input, others);
+  return torch_ipex::cpu::detail::woq_linear::run(
+      op_context_, zero_points_float_, scales_float_, input);
 }
 
 at::Tensor IpexWoqLinearOpContext::to_public(const at::Tensor& tensor) {
diff --git a/csrc/cpu/jit/cpu/kernels/OpContext.h b/csrc/cpu/jit/cpu/kernels/OpContext.h
index 1a9f7f5f..3d6a5923 100644
--- a/csrc/cpu/jit/cpu/kernels/OpContext.h
+++ b/csrc/cpu/jit/cpu/kernels/OpContext.h
@@ -351,7 +351,7 @@ class IpexLinearMKLOpContext final : public MKLOpContext {
 
 // Weight-only quantization
 using SerializationTypeWoqLinearPrePack =
-    std::tuple<at::Tensor, c10::optional<at::Tensor>, c10::optional<int64_t>, int64_t, int64_t>;
+    std::tuple<at::Tensor, c10::optional<at::Tensor>, c10::optional<int64_t>>;
 
 class WoqLinearOpContext : public torch::jit::CustomClassHolder {
  protected:
@@ -361,39 +361,13 @@ class WoqLinearOpContext : public torch::jit::CustomClassHolder {
   SerializationTypeWoqLinearPrePack unpack() {
     auto orig_weight_ = this->to_public(this->get_at_packed_weight());
     auto orig_bias_ = this->get_context().at_bias_;
-    return std::make_tuple(
-        orig_weight_, orig_bias_, batch_size_, this->get_context().lowp_mode_, this->get_context().num_concats_
-    );
+    return std::make_tuple(orig_weight_, orig_bias_, batch_size_);
   }
 
   virtual at::Tensor get_data_handle() = 0;
 
   virtual at::Tensor run(const at::Tensor& input) = 0;
 
-  virtual at::Tensor run_eltwise(
-      const at::Tensor& input,
-      const c10::string_view& post_op,
-      const torch::List<c10::optional<at::Scalar>>& scalars,
-      const c10::optional<c10::string_view>& algorithm) = 0;
-
-  virtual at::Tensor run_add(
-      const at::Tensor& input,
-      at::Tensor& accumu,
-      const c10::optional<at::Scalar>& alpha) = 0;
-
-  virtual at::Tensor run_add_relu(
-      const at::Tensor& input,
-      at::Tensor& accumu,
-      const c10::optional<at::Scalar>& alpha) = 0;
-
-  virtual at::Tensor run_add(
-      const at::Tensor& input,
-      const std::vector<at::Tensor>& others) = 0;
-
-  virtual at::Tensor run_add_add(
-      const at::Tensor& input,
-      const std::vector<at::Tensor>& others) = 0;
-
   virtual at::Tensor to_public(const at::Tensor& tensor) = 0;
 
   virtual at::Tensor get_at_packed_weight() = 0;
@@ -415,12 +389,18 @@ class WoqLinearOpContext : public torch::jit::CustomClassHolder {
 class IpexWoqLinearOpContext final : public WoqLinearOpContext {
  private:
   detail::ContextLinearWoq op_context_;
+  at::Tensor zero_points_float_;
+  at::Tensor scales_float_;
 
  public:
   IpexWoqLinearOpContext(
       c10::optional<int64_t> batch_size,
-      detail::ContextLinearWoq&& op_context)
-      : op_context_(std::move(op_context)) {
+      detail::ContextLinearWoq&& op_context,
+      at::Tensor&& zero_point_float,
+      at::Tensor&& scales_float)
+      : op_context_(std::move(op_context)),
+        zero_points_float_(std::move(zero_point_float)),
+        scales_float_(std::move(scales_float)) {
     batch_size_ = batch_size;
   }
 
@@ -428,30 +408,6 @@ class IpexWoqLinearOpContext final : public WoqLinearOpContext {
 
   virtual at::Tensor run(const at::Tensor& input) override;
 
-  virtual at::Tensor run_eltwise(
-      const at::Tensor& input,
-      const c10::string_view& post_op,
-      const torch::List<c10::optional<at::Scalar>>& scalars,
-      const c10::optional<c10::string_view>& algorithm) override;
-
-  virtual at::Tensor run_add(
-      const at::Tensor& input,
-      at::Tensor& accumu,
-      const c10::optional<at::Scalar>& alpha) override;
-
-  virtual at::Tensor run_add_relu(
-      const at::Tensor& input,
-      at::Tensor& accumu,
-      const c10::optional<at::Scalar>& alpha) override;
-
-  virtual at::Tensor run_add(
-      const at::Tensor& input,
-      const std::vector<at::Tensor>& others) override;
-
-  virtual at::Tensor run_add_add(
-      const at::Tensor& input,
-      const std::vector<at::Tensor>& others) override;
-
   virtual at::Tensor to_public(const at::Tensor& tensor) override;
 
   virtual at::Tensor get_at_packed_weight() override;
@@ -463,9 +419,7 @@ class IpexWoqLinearOpContext final : public WoqLinearOpContext {
   static c10::intrusive_ptr<WoqLinearOpContext> create_context(
       at::Tensor&& weight,
       c10::optional<at::Tensor>&& bias,
-      c10::optional<int64_t> batch_size,
-      int64_t lowp_mode,
-      int64_t num_concats);
+      c10::optional<int64_t> batch_size);
 
   virtual void load_from_ctx(
       c10::intrusive_ptr<WoqLinearOpContext> other) override;
diff --git a/csrc/cpu/jit/cpu/kernels/RegisterOpContextClass.cpp b/csrc/cpu/jit/cpu/kernels/RegisterOpContextClass.cpp
index fb6b6bab..acde860c 100644
--- a/csrc/cpu/jit/cpu/kernels/RegisterOpContextClass.cpp
+++ b/csrc/cpu/jit/cpu/kernels/RegisterOpContextClass.cpp
@@ -15,7 +15,6 @@ using detail::convolution::createConvolutionPrePackOpContext;
 using detail::linear::createLinearPrePackOpContext;
 using detail::mkl_sgemm::createLinearMKLPrePackOpContext;
 using detail::woq_linear::createWoqLinearPrePackOpContext;
-using detail::woq_linear::createWoqLinearPrePackOpContextInt4;
 
 TORCH_LIBRARY(ipex_prepack, m) {
   m.class_<ConvolutionOpContext>("ConvolutionOpContext")
@@ -130,9 +129,7 @@ TORCH_LIBRARY(ipex_prepack, m) {
             return createWoqLinearPrePackOpContext(
                 std::move(std::get<0>(state)),
                 std::move(std::get<1>(state)),
-                std::move(std::get<2>(state)),
-                std::move(std::get<3>(state)),
-                std::move(std::get<4>(state)));
+                std::move(std::get<2>(state)));
           })
       .def(
           "get_weight",
@@ -161,10 +158,7 @@ TORCH_LIBRARY(ipex_prepack, m) {
       "bool input_is_channels_last, int[] input_sizes) "
       "-> __torch__.torch.classes.ipex_prepack.ConvTransposeOpContext");
   m.def(
-      "weight_only_qlinear_prepack(Tensor W, Tensor? B, int? batch_size, int lowp_mode, int num_concats) "
-      "-> __torch__.torch.classes.ipex_prepack.WoqLinearOpContext");
-  m.def(
-      "weight_only_qlinear_prepack_int4(Tensor W, Tensor scales, Tensor zero_points, Tensor? B, int? batch_size, int lowp_mode, int num_concats) "
+      "weight_only_qlinear_prepack(Tensor W, Tensor? B, int? batch_size) "
       "-> __torch__.torch.classes.ipex_prepack.WoqLinearOpContext");
 }
 
@@ -179,10 +173,6 @@ TORCH_LIBRARY_IMPL(ipex_prepack, QuantizedCPU, m) {
   m.impl(
       "weight_only_qlinear_prepack", TORCH_FN(createWoqLinearPrePackOpContext));
 }
-TORCH_LIBRARY_IMPL(ipex_prepack, CPU, m) {
-  m.impl(
-      "weight_only_qlinear_prepack_int4", TORCH_FN(createWoqLinearPrePackOpContextInt4));
-}
 
 } // namespace cpu
 } // namespace torch_ipex
diff --git a/csrc/cpu/jit/fusion_pass.cpp b/csrc/cpu/jit/fusion_pass.cpp
index fee1cf3e..3f8b7790 100644
--- a/csrc/cpu/jit/fusion_pass.cpp
+++ b/csrc/cpu/jit/fusion_pass.cpp
@@ -155,8 +155,6 @@ void IPEXFusionPass(std::shared_ptr<Graph>& graph) {
   graph_rewrite::fuseLinearAddRelu(graph);
   GRAPH_DUMP("After fuseLinearAddRelu.", graph);
   graph_rewrite::FuseLinearSwishCustomized(graph);
-  // graph_rewrite::fuseWoqLinearWithEltwise(graph);
-  // graph_rewrite::fuseWoqLinearAddRelu(graph);
 
   // fuse rmsnorm
   graph_rewrite::FuseRMSNorm(graph);
@@ -195,8 +193,6 @@ void IPEXFusionPass(std::shared_ptr<Graph>& graph) {
   graph_rewrite::replaceAtenBatchNormWithIpexBatchNorm(graph);
   // TODO: Some post processing?? ECS/EDC/Peephole???
 
-  graph_rewrite::simplifyAllReduce(graph);
-
   // This path contains two functions:
   // 1. Fuse BF16 Mha for ViT because ViT has a special QKV split algorithm
   // 2. Replace the Matmul OP with MKL or DNNL Matmul kernels to enable
diff --git a/csrc/cpu/jit/passes/graph_rewrite.cpp b/csrc/cpu/jit/passes/graph_rewrite.cpp
index 7619279e..e35d9be8 100644
--- a/csrc/cpu/jit/passes/graph_rewrite.cpp
+++ b/csrc/cpu/jit/passes/graph_rewrite.cpp
@@ -309,26 +309,26 @@ void FuseMatmulDivOrMul(std::shared_ptr<Graph>& graph) {
 
 void PostScalarDivOrMul(std::shared_ptr<Graph>& graph) {
   std::string div_matmul = R"(
-      graph(%q: Tensor, %k: Tensor, %dim_per_head:float): 
+      graph(%q: Tensor, %k: Tensor, %dim_per_head:float):
         %_q = aten::div(%q, %dim_per_head)
-        %r = aten::matmul(%_q, %k) 
+        %r = aten::matmul(%_q, %k)
         return (%r) )";
 
   std::string mul_matmul = R"(
-      graph(%q: Tensor, %k: Tensor, %scale:float): 
+      graph(%q: Tensor, %k: Tensor, %scale:float):
         %_q = aten::mul(%q, %scale)
-        %r = aten::matmul(%_q, %k) 
+        %r = aten::matmul(%_q, %k)
         return (%r) )";
 
   std::string matmul_div = R"(
-      graph(%q: Tensor, %k: Tensor, %dim_per_head:float): 
-        %qk = aten::matmul(%q, %k) 
+      graph(%q: Tensor, %k: Tensor, %dim_per_head:float):
+        %qk = aten::matmul(%q, %k)
         %r = aten::div(%qk, %dim_per_head)
         return (%r) )";
 
   std::string matmul_mul = R"(
-      graph(%q: Tensor, %k: Tensor, %scale:float): 
-        %qk = aten::matmul(%q, %k) 
+      graph(%q: Tensor, %k: Tensor, %scale:float):
+        %qk = aten::matmul(%q, %k)
         %r = aten::mul(%qk, %scale)
         return (%r) )";
   auto filter_scalar = [](const Match& match,
@@ -748,6 +748,77 @@ void replaceInteractionWithQInteraction(std::shared_ptr<Graph>& graph) {
   }
 }
 
+void replaceMergedEmbCatWithQmergedEmbCat(std::shared_ptr<Graph>& graph) {
+  std::vector<std::string> patterns;
+  std::vector<std::string> replacements;
+  std::string graph_common_head = R"(graph()";
+  std::string graph_common_tail = R"(, %index, %qdense, %num_hot, %o_scale, %o_zp, %o_dtype):
+  )";
+  std::string list_construct_common_head =
+      R"(%weights : Tensor[] = prim::ListConstruct()";
+  std::string list_construct_common_tail = R"() )";
+  std::string replacement_common_tail =
+      R"(%out = ipex::qmerged_emb_with_cat(%weights, %index, %qdense, %num_hot, %o_scale, %o_zp, %o_dtype) return (%out) )";
+  std::string pattern_common_tail =
+      R"(%dense=aten::dequantize(%qdense)  %out = torch_ipex::merged_emb_with_cat(%weights, %index, %dense, %num_hot)  %qout = aten::quantize_per_tensor(%out, %o_scale, %o_zp, %o_dtype) return (%qout) )";
+
+  for (auto* n : graph->block()->nodes()) {
+    if (n->kind() ==
+        Symbol::fromQualString("torch_ipex::merged_emb_with_cat")) {
+      size_t id = 0;
+      auto weightslist = n->input(0)->node();
+      auto indexlist = n->input(1)->node();
+
+      bool is_quantized = std::any_of(
+          weightslist->inputs().begin(),
+          weightslist->inputs().end(),
+          [](auto& v) {
+            return v->node()->kind() == Symbol::aten("dequantize");
+          });
+
+      if (!is_quantized)
+        return;
+
+      std::string pattern = R"()";
+      std::string replacement = R"()";
+      std::string dequantizes = R"()";
+      std::vector<std::string> qinputs;
+      std::vector<std::string> dqinputs;
+      for (auto input : weightslist->inputs()) {
+        if (input->node()->kind() == Symbol::aten("dequantize")) {
+          qinputs.push_back("%q" + std::to_string(id));
+          dqinputs.push_back("%dq" + std::to_string(id));
+          std::string dequantize = "%dq" + std::to_string(id) +
+              " : Tensor = aten::dequantize(" + "%q" + std::to_string(id) + ")";
+          dequantizes.append(dequantize);
+          ++id;
+        }
+      }
+
+      std::string header =
+          graph_common_head + c10::Join(", ", qinputs) + graph_common_tail;
+      pattern += header;
+      pattern += dequantizes;
+      pattern += list_construct_common_head + c10::Join(", ", dqinputs) +
+          list_construct_common_tail;
+      pattern += pattern_common_tail;
+      patterns.push_back(pattern);
+
+      replacement = header;
+      replacement += list_construct_common_head + c10::Join(", ", qinputs) +
+          list_construct_common_tail;
+      replacement += replacement_common_tail;
+      replacements.push_back(replacement);
+    }
+  }
+
+  SubgraphRewriter rewriter;
+  for (size_t i = 0; i < patterns.size(); i++) {
+    rewriter.RegisterRewritePattern(patterns[i], replacements[i]);
+    rewriter.runOnGraph(graph);
+  }
+}
+
 // When converting LSTM to int8 LSTM, IPEX will pre-hook the LSTM forward
 // function to insert quant and dequant node. After converting the model, when
 // entering the forward function, if the hidden state and cell state are empty,
@@ -771,11 +842,11 @@ void replaceInteractionWithQInteraction(std::shared_ptr<Graph>& graph) {
 // %y, %hy, %cy = aten::lstm(%ret, ...)
 void preprocessSizeForQLstm(std::shared_ptr<Graph>& graph) {
   const static std::string op_list_construct_same_states = R"(
-%hx.1 = aten::zeros(%sizes, %scalar_type, %layout, %device, %pin_memory) 
+%hx.1 = aten::zeros(%sizes, %scalar_type, %layout, %device, %pin_memory)
 %state : Tensor[] = prim::ListConstruct(%hx.1, %hx.1) )";
 
   const static std::string op_list_construct_diff_states = R"(
-%hx.1 = aten::zeros(%sizes, %scalar_type, %layout, %device, %pin_memory) 
+%hx.1 = aten::zeros(%sizes, %scalar_type, %layout, %device, %pin_memory)
 %hx = aten::zeros(%sizes, %scalar_type, %layout, %device, %pin_memory)
 %state : Tensor[] = prim::ListConstruct(%hx.1, %hx) )";
 
@@ -864,7 +935,7 @@ void replaceLstmWithQLstm(std::shared_ptr<Graph>& graph) {
       std::string QLstmPattern = complete_header + R"(
               %input : Tensor = aten::dequantize(%quantized_input) )" +
           weight_pattern + complete_LC + R"(
-              %output, %hy, %cy = aten::lstm(%input, %h, %weights, %has_biases, %num_layers, %dropout_p, %train, %bidirectional, %batch_fist) 
+              %output, %hy, %cy = aten::lstm(%input, %h, %weights, %has_biases, %num_layers, %dropout_p, %train, %bidirectional, %batch_fist)
               %quantized_output = aten::quantize_per_tensor(%output, %scale, %zp, %dtype)
               return (%quantized_output, %hy, %cy) )";
 
@@ -1239,58 +1310,6 @@ void replaceAtenMaxPool2dWithIpexMaxPool2d(std::shared_ptr<Graph>& graph) {
   rewriter_max_pool2d.runOnGraph(graph, filter);
 }
 
-void simplifyAllReduce(std::shared_ptr<Graph>& graph) {
-  std::string all_reduce_v1 = R"(
-    graph(%a, %weight, %reduceop, %tag, %ranks, %group_size, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha, %idx, %no, %dtype, %zero):
-      %r1 = torch_ipex::tpp_linear(%a, %weight)
-      %r2 = deepspeed_comm::all_reduce(%r1, %reduceop, %tag, %ranks, %group_size)
-      %r3 = torch_ipex::tpp_linear_gelu(%b, %fc_in_weight, %fc_in_bias)
-      %r4 = aten::to(%r3, %idx, %no, %no, %dtype)
-      %r5 = aten::contiguous(%r4, %zero)
-      %r6 = torch_ipex::tpp_linear(%r5, %fc_out_weight)
-      %r7 = deepspeed_comm::all_reduce(%r6, %reduceop, %tag, %ranks, %group_size)
-      %r8 = aten::add_(%r7, %fc_out_bias, %alpha)
-      %r = aten::add(%r2, %r8, %alpha)
-      return (%r) )";
-  std::string all_reduce_repl_v1 = R"(
-    graph(%a, %weight, %reduceop, %tag, %ranks, %group_size, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha, %idx, %no, %dtype, %zero):
-      %r1 = torch_ipex::tpp_linear(%a, %weight)
-      %r2 = torch_ipex::tpp_linear_gelu(%b, %fc_in_weight, %fc_in_bias)
-      %r3 = aten::to(%r2, %idx, %no, %no, %dtype)
-      %r4 = aten::contiguous(%r3, %zero)
-      %r5 = torch_ipex::tpp_linear(%r4, %fc_out_weight)
-      %r6 = aten::add(%r1, %r5, %alpha)
-      %r7 = deepspeed_comm::all_reduce(%r6, %reduceop, %tag, %ranks, %group_size)
-      %r = aten::add_(%r7, %fc_out_bias, %alpha)
-      return (%r) )";
-
-  std::string all_reduce_v2 = R"(
-    graph(%a, %weight, %reduceop, %tag, %ranks, %group_size, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha):
-      %r1 = ipex_prepack::linear_run(%a, %weight)
-      %r2 = deepspeed_comm::all_reduce(%r1, %reduceop, %tag, %ranks, %group_size)
-      %r3 = ipex_prepack::linear_gelu_run(%b, %fc_in_weight, %fc_in_bias)
-      %r4 = ipex_prepack::linear_run(%r3, %fc_out_weight)
-      %r5 = deepspeed_comm::all_reduce(%r4, %reduceop, %tag, %ranks, %group_size)
-      %r6 = aten::add_(%r5, %fc_out_bias, %alpha)
-      %r = aten::add(%r2, %r6, %alpha)
-      return (%r) )";
-  std::string all_reduce_repl_v2 = R"(
-    graph(%a, %weight, %reduceop, %tag, %ranks, %group_size, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha):
-      %r1 = ipex_prepack::linear_run(%a, %weight)
-      %r2 = ipex_prepack::linear_gelu_run(%b, %fc_in_weight, %fc_in_bias)
-      %r3 = ipex_prepack::linear_run(%r2, %fc_out_weight)
-      %r4 = aten::add(%r1, %r3, %alpha)
-      %r5 = deepspeed_comm::all_reduce(%r4, %reduceop, %tag, %ranks, %group_size)
-      %r = aten::add_(%r5, %fc_out_bias, %alpha)
-      return (%r) )";
-
-  SubgraphRewriter rewriter_v1, rewriter_v2;
-  rewriter_v1.RegisterRewritePattern(all_reduce_v1, all_reduce_repl_v1);
-  rewriter_v2.RegisterRewritePattern(all_reduce_v2, all_reduce_repl_v2);
-  rewriter_v1.runOnGraph(graph);
-  rewriter_v2.runOnGraph(graph);
-}
-
 } // namespace graph_rewrite
 } // namespace jit
 } // namespace torch_ipex
diff --git a/csrc/cpu/jit/passes/graph_rewrite.h b/csrc/cpu/jit/passes/graph_rewrite.h
index 1ac98431..47f14817 100644
--- a/csrc/cpu/jit/passes/graph_rewrite.h
+++ b/csrc/cpu/jit/passes/graph_rewrite.h
@@ -35,10 +35,12 @@ void replaceEmbeddingBagWithQEmbeddingBag(
     std::shared_ptr<torch::jit::Graph>& graph);
 void replaceInteractionWithQInteraction(
     std::shared_ptr<torch::jit::Graph>& graph);
+void replaceMergedEmbCatWithQmergedEmbCat(
+    std::shared_ptr<torch::jit::Graph>& graph);
 void preprocessSizeForQLstm(std::shared_ptr<torch::jit::Graph>& graph);
 void replaceLstmWithQLstm(std::shared_ptr<torch::jit::Graph>& graph);
 void replaceAddWithQAdd(std::shared_ptr<torch::jit::Graph>& graph);
-void simplifyAllReduce(std::shared_ptr<torch::jit::Graph>& graph);
+
 void replaceFrozenIPEXConvWithAtenConv(
     std::shared_ptr<torch::jit::Graph>& graph);
 void replaceFrozenIPEXLinearWithAtenLinear(
@@ -58,8 +60,6 @@ void insertPrePackedLinearOp(
     const bool& use_mkl_sgemm);
 void fuseLinearWithEltwise(std::shared_ptr<torch::jit::Graph>& graph);
 void fuseLinearAddRelu(std::shared_ptr<torch::jit::Graph>& graph);
-void fuseWoqLinearWithEltwise(std::shared_ptr<torch::jit::Graph>& graph);
-void fuseWoqLinearAddRelu(std::shared_ptr<torch::jit::Graph>& graph);
 
 void FuseRMSNorm(std::shared_ptr<torch::jit::Graph>& graph);
 void FuseAddLayerNorm(std::shared_ptr<torch::jit::Graph>& graph);
diff --git a/csrc/cpu/jit/passes/graph_rewrite_linear.cpp b/csrc/cpu/jit/passes/graph_rewrite_linear.cpp
index 3f38edd5..dbf60bd7 100644
--- a/csrc/cpu/jit/passes/graph_rewrite_linear.cpp
+++ b/csrc/cpu/jit/passes/graph_rewrite_linear.cpp
@@ -414,155 +414,6 @@ void fuseLinearAddRelu(std::shared_ptr<Graph>& graph) {
   rewriter_add_relu.runOnGraph(graph);
 }
 
-void fuseWoqLinearWithEltwise(std::shared_ptr<Graph>& graph) {
-  using torch_ipex::jit::graph_rewrite::utils::PostOp;
-  using torch_ipex::jit::graph_rewrite::utils::NonUnaryPostOp;
-  // For unary post OPs:
-  auto linear_op_rstring = at::jit::CodeTemplate(R"(
-     graph(%input, %packed_weight):
-        %x : Tensor = torch_ipex::ipex_woq_linear(%input, %packed_weight)
-        %res = ${op}(%x)
-        return (%res))");
-
-  auto linear_op_fused_rstring = at::jit::CodeTemplate(R"(
-    graph(%input, %packed_weight):
-        %res = torch_ipex::woq_linear_${op}_run(%input, %packed_weight)
-        return (%res))");
-
-  static const std::map<std::string, PostOp> unary_fusion_map{
-      {"aten::relu_", {"relu"}},
-      {"aten::relu", {"relu"}},
-  };
-  for (auto const& it : unary_fusion_map) {
-    std::string op = it.first;
-    std::string ipex_op_name = it.second.ipex_op_name;
-
-    at::jit::TemplateEnv env;
-    env.s("op", op);
-
-    at::jit::TemplateEnv env_fused;
-    env_fused.s("op", ipex_op_name);
-
-    SubgraphRewriter rewriter;
-    rewriter.RegisterRewritePattern(
-        linear_op_rstring.format(env),
-        linear_op_fused_rstring.format(env_fused));
-
-    auto filters = it.second.filters;
-    rewriter.runOnGraph(graph, filters);
-  }
-
-  // For non-unary post OPs:
-  auto linear_op_non_unary_rstring = at::jit::CodeTemplate(R"(
-     graph(%input, %packed_weight, ${op_input_str}):
-        %x : Tensor = torch_ipex::ipex_woq_linear(%input, %packed_weight)
-        %res = ${op}(%x, ${op_input_str})
-        return (%res))");
-
-  auto linear_op_non_unary_fused_rstring = at::jit::CodeTemplate(R"(
-    graph(%input, %packed_weight, ${op_input_str}):
-        %res = torch_ipex::woq_linear_${op}_run(%input, %packed_weight, ${op_input_str})
-        return (%res))");
-
-  static const std::map<std::string, NonUnaryPostOp> non_unary_fusion_map{
-      {"aten::gelu",
-       {"gelu",
-        std::vector<std::string>({"%approximate"})}},
-      {"aten::gelu_",
-       {"gelu",
-        std::vector<std::string>({"%approximate"})}},
-  };
-  for (auto const& it : non_unary_fusion_map) {
-    std::string op = it.first;
-    std::string ipex_op_name = it.second.ipex_op_name;
-    std::vector<std::string> op_input_list = it.second.op_input_list;
-    std::string op_input_str = c10::Join(", ", op_input_list);
-
-    at::jit::TemplateEnv env;
-    env.s("op", op);
-    env.s("op_input_str", op_input_str);
-
-    at::jit::TemplateEnv env_fused;
-    env_fused.s("op", ipex_op_name);
-    env_fused.s("op_input_str", op_input_str);
-
-    SubgraphRewriter rewriter;
-    rewriter.RegisterRewritePattern(
-        linear_op_non_unary_rstring.format(env),
-        linear_op_non_unary_fused_rstring.format(env_fused));
-
-    auto filters = it.second.filters;
-    rewriter.runOnGraph(graph, filters);
-  }
-}
-
-void fuseWoqLinearAddRelu(std::shared_ptr<Graph>& graph) {
-  SubgraphRewriter rewriter_add_accumu_on_the_right,
-      rewriter_add_accumu_on_the_left, rewriter_add_relu;
-  std::array<std::string, 2> add_operators = {"add", "add_"};
-  std::array<std::string, 2> relu_operators = {"relu", "relu_"};
-
-  // linear   Y
-  //   \   /
-  //    add
-  // output = linear_output + alpha*Y
-  auto woq_linear_add_accumu_on_the_right_rstring = CodeTemplate(R"(
-    graph(%input, %accumu, %alpha, %packed_weight):
-        %x = torch_ipex::ipex_woq_linear(%input, %packed_weight)
-        %res = aten::${add}(%x, %accumu, %alpha)
-        return (%res))");
-
-  //  Y     linear
-  //   \   /
-  //    add
-  // output = Y + alpha*linear_output, alpha must be one or none.
-  auto woq_linear_add_accumu_on_the_left_rstring = CodeTemplate(R"(
-    graph(%input, %accumu, %alpha, %packed_weight):
-        %x = torch_ipex::ipex_woq_linear(%input, %packed_weight)
-        %res = aten::${add}(%accumu, %x, %alpha)
-        return (%res))");
-
-  std::string woq_linear_add_fused = R"(
-    graph(%input, %accumu, %alpha, %packed_weight):
-        %res = torch_ipex::woq_linear_add_run(%input, %accumu, %alpha, %packed_weight)
-        return (%res))";
-
-  auto woq_linear_add_relu_rstring = CodeTemplate(R"(
-    graph(%input, %accumu, %alpha, %packed_weight):
-        %x = torch_ipex::woq_linear_add_run(%input, %accumu, %alpha, %packed_weight)
-        %res = aten::${relu}(%x)
-        return (%res))");
-
-  std::string woq_linear_add_relu_fused = R"(
-    graph(%input, %accumu, %alpha, %packed_weight):
-        %res = torch_ipex::woq_linear_add_relu_run(%input, %accumu, %alpha, %packed_weight)
-        return (%res))";
-
-  // linear + add
-  for (const auto& add : add_operators) {
-    TemplateEnv env;
-    env.s("add", add);
-    rewriter_add_accumu_on_the_right.RegisterRewritePattern(
-        woq_linear_add_accumu_on_the_right_rstring.format(env), woq_linear_add_fused);
-    rewriter_add_accumu_on_the_left.RegisterRewritePattern(
-        woq_linear_add_accumu_on_the_left_rstring.format(env), woq_linear_add_fused);
-  }
-
-  // linear + add + relu
-  for (const auto& relu : relu_operators) {
-    TemplateEnv env;
-    env.s("relu", relu);
-    rewriter_add_relu.RegisterRewritePattern(
-        woq_linear_add_relu_rstring.format(env), woq_linear_add_relu_fused);
-  }
-
-  rewriter_add_accumu_on_the_right.runOnGraph(
-      graph, fuse_add_filter_accumu_on_the_right);
-  rewriter_add_accumu_on_the_left.runOnGraph(
-      graph, fuse_add_filter_accumu_on_the_left);
-  rewriter_add_relu.runOnGraph(graph);
-}
-
 } // namespace graph_rewrite
 } // namespace jit
 } // namespace torch_ipex
diff --git a/csrc/cpu/jit/passes/register_dnnl_jit_ops.cpp b/csrc/cpu/jit/passes/register_dnnl_jit_ops.cpp
index 3aa7a506..9039672a 100644
--- a/csrc/cpu/jit/passes/register_dnnl_jit_ops.cpp
+++ b/csrc/cpu/jit/passes/register_dnnl_jit_ops.cpp
@@ -6,6 +6,7 @@
 #include "aten/AddLayerNorm.h"
 #include "aten/ConcatBnRelu.h"
 #include "aten/RMSNorm.h"
+#include "aten/MergedEmbWithCat.h"
 #include "cpu/kernels/ConvPacked.h"
 #include "cpu/kernels/ConvTransposePacked.h"
 #include "cpu/kernels/Einsum.h"
@@ -14,7 +15,6 @@
 #include "cpu/kernels/LinearMKLPacked.h"
 #include "cpu/kernels/LinearPacked.h"
 #include "cpu/kernels/LinearSwishCustomized.h"
-#include "cpu/kernels/LinearWoqPacked.h"
 #include "cpu/kernels/Matmul.h"
 #include "cpu/kernels/MaxPool2D.h"
 #include "cpu/kernels/Mha.h"
@@ -33,7 +33,6 @@ using namespace torch_ipex::cpu::detail::convolution;
 using namespace torch_ipex::cpu::detail::linear;
 using namespace torch_ipex::cpu::detail::conv_transpose;
 using namespace torch_ipex::cpu::detail::mkl_sgemm;
-using namespace torch_ipex::cpu::detail::woq_linear;
 
 c10::AliasAnalysisKind aliasAnalysisFromSchema() {
   return c10::AliasAnalysisKind::FROM_SCHEMA;
@@ -663,71 +662,6 @@ torch::jit::RegisterOperators op({
           };
         },
         aliasAnalysisFromSchema()),
-    Operator(
-        "torch_ipex::woq_linear_relu_run(Tensor input, Tensor W_prepack) -> Tensor",
-        [](const Node* node) -> Operation {
-          return [](Stack* stack) {
-            auto result = woq_linear_eltwise_run(
-                (std::move(peek(stack, 0, 2))).toTensor(),
-                (std::move(peek(stack, 1, 2))).toTensor(),
-                "relu",
-                c10::List<c10::optional<c10::Scalar>>(),
-                c10::optional<c10::string_view>());
-            drop(stack, 2);
-            torch::jit::pack(stack, std::move(result));
-            return 0;
-          };
-        },
-        aliasAnalysisFromSchema()),
-    Operator(
-        "torch_ipex::woq_linear_gelu_run(Tensor input, Tensor W_prepack, str? algorithm) -> Tensor",
-        [](const Node* node) -> Operation {
-          return [](Stack* stack) {
-            auto algo = (std::move(peek(stack, 2, 3))).toOptional<c10::string_view>();
-            auto result = woq_linear_eltwise_run(
-                (std::move(peek(stack, 0, 3))).toTensor(),
-                (std::move(peek(stack, 1, 3))).toTensor(),
-                "gelu",
-                c10::List<c10::optional<c10::Scalar>>(),
-                algo);
-            drop(stack, 3);
-            torch::jit::pack(stack, std::move(result));
-            return 0;
-          };
-        },
-        aliasAnalysisFromSchema()),
-    // Operator(
-    //     "torch_ipex::woq_linear_add_run(Tensor input, Tensor(a!) accumu, Scalar? alpha, Tensor W_prepack) -> Tensor",
-    //     [](const Node* node) -> Operation {
-    //       return [](Stack* stack) {
-    //         auto output = (std::move(peek(stack, 1, 4))).toTensor();
-    //         auto result = woq_linear_add_run(
-    //             (std::move(peek(stack, 0, 4))).toTensor(),
-    //             output,
-    //             (std::move(peek(stack, 2, 4))).toOptional<at::Scalar>(),
-    //             (std::move(peek(stack, 3, 4))).toTensor());
-    //         drop(stack, 4);
-    //         torch::jit::pack(stack, std::move(output));
-    //         return 0;
-    //       };
-    //     },
-    //     aliasAnalysisFromSchema()),
-    // Operator(
-    //     "torch_ipex::woq_linear_add_relu_run(Tensor input, Tensor(a!) accumu, Scalar? alpha, Tensor W_prepack) -> Tensor",
-    //     [](const Node* node) -> Operation {
-    //       return [](Stack* stack) {
-    //         auto output = (std::move(peek(stack, 1, 4))).toTensor();
-    //         auto result = woq_linear_add_relu_run(
-    //             (std::move(peek(stack, 0, 4))).toTensor(),
-    //             output,
-    //             (std::move(peek(stack, 2, 4))).toOptional<at::Scalar>(),
-    //             (std::move(peek(stack, 3, 4))).toTensor());
-    //         drop(stack, 4);
-    //         torch::jit::pack(stack, std::move(result));
-    //         return 0;
-    //       };
-    //     },
-    //     aliasAnalysisFromSchema()),
 
     // ConvTranspose fusion run OP
     CreateConvTransposeUnaryPostOpRun(run),
@@ -1409,6 +1343,26 @@ torch::jit::RegisterOperators op({
         },
         aliasAnalysisFromSchema()),
 
+    Operator(
+        "ipex::qmerged_emb_with_cat(Tensor[] weights,  Tensor[] index, Tensor dense, int[] num_hots, float o_scale, int o_zp, "
+        "ScalarType o_dtype) -> Tensor",
+        [](const Node* node) -> Operation {
+          return [](Stack* stack) {
+            auto result = dil_qmerged_emb_with_cat(
+                (std::move(peek(stack, 0, 7))).toTensorVector(),
+                (std::move(peek(stack, 1, 7))).toTensorVector(),
+                (std::move(peek(stack, 2, 7))).toTensor(),
+                (std::move(peek(stack, 3, 7))).toIntVector(),
+                (std::move(peek(stack, 4, 7))).toDouble(),
+                (std::move(peek(stack, 5, 7))).toInt(),
+                (std::move(peek(stack, 6, 7))).toScalarType());
+            drop(stack, 7);
+            torch::jit::pack(stack, std::move(result));
+            return 0;
+          };
+        },
+        aliasAnalysisFromSchema()),
+
     Operator(
         "ipex::quantized_lstm(Tensor quantized_input, Tensor[] hx, Tensor [] quantized_weights, bool has_biases, int num_layers, float dropout_p, bool train, bool bidirectional, bool batch_first, float scale, int zp, int dtype) -> (Tensor, Tensor, Tensor)",
         [](const Node* node) -> Operation {
diff --git a/csrc/cpu/jit/register_interface.cpp b/csrc/cpu/jit/register_interface.cpp
index 1055c881..023565f9 100644
--- a/csrc/cpu/jit/register_interface.cpp
+++ b/csrc/cpu/jit/register_interface.cpp
@@ -38,6 +38,8 @@ bool canFuseNode(const Node* node) {
       node->kind() == Symbol::aten("masked_fill") ||
       node->kind() == Symbol::aten("masked_fill_") ||
       node->kind() == Symbol::aten("pad") ||
+      node->kind() == Symbol::aten("mul") ||
+      node->kind() == Symbol::aten("flatten") ||
       node->kind() ==
           Symbol::fromQualString("torch_ipex::convolution_forward") ||
       node->kind() == Symbol::fromQualString("torch_ipex::ipex_linear") ||
diff --git a/csrc/cpu/tpp/common_loops.cpp b/csrc/cpu/tpp/common_loops.cpp
index e0266af1..ee57d60d 100644
--- a/csrc/cpu/tpp/common_loops.cpp
+++ b/csrc/cpu/tpp/common_loops.cpp
@@ -95,29 +95,6 @@ void par_nested_loops_bA(
   }
 }
 
-void par_nested_loops_Ba(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)> body_func,
-    std::function<void()> init_func,
-    std::function<void()> fini_func) {
-#pragma omp parallel
-  {
-    if (init_func)
-      init_func();
-#pragma omp for nowait
-    for (int b0 = loopSpecs[1].start; b0 < loopSpecs[1].end;
-         b0 += loopSpecs[1].step) {
-      for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end;
-           a0 += loopSpecs[0].step) {
-        int ind[2] = {a0, b0};
-        body_func(ind);
-      }
-    }
-    if (fini_func)
-      fini_func();
-  }
-}
-
 void par_nested_loops_BA(
     LoopSpecs* loopSpecs,
     std::function<void(int*)> body_func,
@@ -219,70 +196,15 @@ void par_nested_loops_acB(
   }
 }
 
-void par_nested_loops_aCb(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)> body_func,
-    std::function<void()> init_func,
-    std::function<void()> fini_func) {
-#pragma omp parallel
-  {
-    if (init_func)
-      init_func();
-    for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end;
-         a0 += loopSpecs[0].step) {
-#pragma omp for nowait
-      for (int c0 = loopSpecs[2].start; c0 < loopSpecs[2].end;
-           c0 += loopSpecs[2].step) {
-        for (int b0 = loopSpecs[1].start; b0 < loopSpecs[1].end;
-             b0 += loopSpecs[1].step) {
-          int ind[3] = {a0, b0, c0};
-          body_func(ind);
-        }
-      }
-    }
-    if (fini_func)
-      fini_func();
-  }
-}
-
-void par_nested_loops_aCB(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)> body_func,
-    std::function<void()> init_func,
-    std::function<void()> fini_func) {
-#pragma omp parallel
-  {
-    if (init_func)
-      init_func();
-    for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end;
-         a0 += loopSpecs[0].step) {
-#pragma omp for collapse(2) nowait
-      for (int c0 = loopSpecs[2].start; c0 < loopSpecs[2].end;
-           c0 += loopSpecs[2].step) {
-        for (int b0 = loopSpecs[1].start; b0 < loopSpecs[1].end;
-             b0 += loopSpecs[1].step) {
-          int ind[3] = {a0, b0, c0};
-          body_func(ind);
-        }
-      }
-    }
-    if (fini_func)
-      fini_func();
-  }
-}
-
 std::unordered_map<std::string, par_loop_kernel> pre_defined_loops = {
     {"A", par_nested_loops_A},
     {"AB", par_nested_loops_AB},
     {"BA", par_nested_loops_BA},
     {"bA", par_nested_loops_bA},
-    {"Ba", par_nested_loops_Ba},
     {"aB", par_nested_loops_aB},
     {"ABC", par_nested_loops_ABC},
     {"aBC", par_nested_loops_aBC},
     {"acB", par_nested_loops_acB},
-    {"aCb", par_nested_loops_aCb},
-    {"aCB", par_nested_loops_aCB},
 };
 } // namespace tpp
 } // namespace torch_ipex
\ No newline at end of file
diff --git a/csrc/cpu/tpp/kernels/TPPGEMMKrnl.h b/csrc/cpu/tpp/kernels/TPPGEMMKrnl.h
deleted file mode 100644
index 43d1bdb8..00000000
--- a/csrc/cpu/tpp/kernels/TPPGEMMKrnl.h
+++ /dev/null
@@ -1,773 +0,0 @@
-
-#include <ATen/record_function.h>
-#include <aten/TPPGEMM.h>
-#include <torch/all.h>
-#include <iostream>
-#include <vector>
-#include "tpp/ext_tpp.h"
-#include "tpp/utils.h"
-#ifndef NO_PARLOOPER
-#include "tpp/threaded_loops.h"
-#endif
-#include <cstdint>
-#include "tpp/tensor_helper.h"
-#include "tpp/xsmm_functors.h"
-
-namespace torch_ipex {
-namespace tpp {
-
-static int large_cache_opt = false;
-static int use_at_vnni = false; // env2int("USE_AT_VNNI");
-static int FT_OPT_SIZE = env2int("FT_OPT_SIZE", 256);
-static int NCB_BLOCK_SIZE = env2int("NCB_BLOCK_SIZE", 64);
-static const char* GEMM_LOOP_SCHEME =
-    getenv("GEMM_LOOP_SCHEME") ? getenv("GEMM_LOOP_SCHEME") : "aCB";
-
-REGISTER_LOCAL_SCOPE(
-    tpp_linear_krnl,
-    "tpp_linear_krnl"); //  linear W/ and W/O bias
-REGISTER_LOCAL_SCOPE(
-    tpp_linear_add_add_krnl,
-    "tpp_linear_add_add_krnl"); // linear bias + add + add
-REGISTER_LOCAL_SCOPE(
-    tpp_linear_gelu_krnl,
-    "tpp_linear_gelu_krnl"); // linear bias + gelu
-
-REGISTER_LOCAL_SCOPE(
-    tpp_linear_mul_krnl,
-    "tpp_linear_mul_krnl"); // linear bias + mul
-REGISTER_LOCAL_SCOPE(
-    tpp_linear_add_krnl,
-    "tpp_linear_add_krnl"); // linear bias + add
-REGISTER_LOCAL_SCOPE(
-    tpp_linear_silu_krnl,
-    "tpp_linear_silu_krnl"); // linear bias + silu
-REGISTER_LOCAL_SCOPE(
-    tpp_linear_relu_krnl,
-    "tpp_linear_relu_krnl"); // linear bias + relu
-
-REGISTER_LOCAL_SCOPE(fftkn, "fftkn");
-
-template <typename T>
-inline at::Tensor wt_tensor_for_first_token(at::Tensor& t) {
-  RECORD_SCOPE(fftkn, {t});
-  auto dim = t.dim();
-  if (dim < 5)
-    return t;
-  auto sizes = t.sizes();
-  constexpr long RBS = 2;
-  auto K1 = sizes[0];
-  if (K1 % RBS != 0)
-    return t;
-  auto C1 = sizes[1];
-  auto C2 = sizes[2];
-  auto K2 = sizes[3];
-  auto C3 = sizes[4];
-#if 0
-  auto t_new = t.view({K1/RBS, RBS, C1, C2, K2, C3}).permute({0, 2, 3, 1, 4, 5}).contiguous().view({K1/RBS, C1, C2, RBS*K2, C3});
-#else
-  auto t_new = t.new_empty({K1 / RBS, C1, C2, RBS * K2, C3});
-  auto in = GetVLAPtr<T>(t, {RBS, C1, C2, K2 * C3});
-  auto out = GetVLAPtr<T>(t_new, {C1, C2, RBS, K2 * C3});
-
-#if 1
-  auto cpy_tpp =
-      SCOPEIT(CpyTPP<T>(C2, K2 * C3, K2 * C3, RBS * K2 * C3), EW_COPY);
-
-#pragma omp parallel for collapse(2)
-  for (int i = 0; i < K1 / RBS; i++) {
-    for (int j = 0; j < C1; j++) {
-      for (int k = 0; k < RBS; k++) {
-        cpy_tpp(in[i][k][j][0], out[i][j][0][k]);
-      }
-    }
-  }
-#else
-  auto cpy_tpp =
-      SCOPEIT(CpyTPP<T>(RBS, K2 * C3, C1 * C2 * K2 * C3, K2 * C3), EW_COPY);
-
-#pragma omp parallel for collapse(2)
-  for (int i = 0; i < K1 / RBS; i++) {
-    for (int j = 0; j < C1; j++) {
-      for (int k = 0; k < C2; k++) {
-        cpy_tpp(in[i][0][j][k], out[i][j][k][0]);
-      }
-    }
-  }
-#endif
-
-#endif
-  return t_new;
-}
-
-template <typename T>
-inline void tpp_linear_bias(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias,
-    at::Tensor& t_out) {
-  auto in_sizes = t_in.sizes();
-  auto wt_sizes = t_wt.sizes();
-  auto BS = in_sizes[0] * in_sizes[1];
-  auto C = in_sizes[2];
-
-  auto Nc = wt_sizes[1];
-  auto Hc = C / Nc;
-  auto Nk = wt_sizes[0];
-  auto Hk = wt_sizes[3];
-  auto K = Nk * Hk;
-
-  auto t_wt_V = torch_ipex::tpp::wt_tensor_for_fwd(Nk, Hk, Nc, Hc, t_wt);
-  // std::cout << "XXX " << t_in.dtype() << "  " << t_wt_V.dtype() << std::endl;
-
-  // printf("reached at %s:%d\n", __func__, __LINE__);
-  auto in = GetVLAPtr<T>(t_in, {Nc, Hc});
-  // printf("reached at %s:%d\n", __func__, __LINE__);
-  auto wt_V = GetVLAPtr<T>(t_wt_V, {Nc, Hc * Hk});
-  // printf("reached at %s:%d\n", __func__, __LINE__);
-  auto bias = GetVLAPtr<T>(t_bias, {Hk});
-  // printf("reached at %s:%d\n", __func__, __LINE__);
-  auto out = GetVLAPtr<T>(t_out, {Nk, Hk});
-  // printf("reached at %s:%d\n", __func__, __LINE__);
-
-  auto Ncb = Nc;
-  auto BSb = 64L;
-  auto rem = BS % 64;
-  if (large_cache_opt)
-    Ncb = NCB_BLOCK_SIZE;
-
-  bool with_bias = (t_bias.numel() > 0);
-  auto copy_bias_tpp = SCOPEIT(CpyBiasTPP<T>(BSb, Hk, K), BIAS);
-  auto copy_bias_tpp_rem = SCOPEIT(CpyBiasTPP<T>(rem, Hk, K), BIAS);
-  auto zero_tpp = SCOPEIT(SetZeroTPP<T>(BSb, Hk, K), EW_ZERO);
-  auto zero_tpp_rem = SCOPEIT(SetZeroTPP<T>(rem, Hk, K), EW_ZERO);
-  auto brgemm_tpp = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto brgemm_tpp_rem = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-
-  {
-    RECORD_SCOPE(tpp_linear_krnl, {t_in, t_wt_V});
-    // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
-    auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
-    auto ogemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
-        {{0, Nc, Ncb, false}, {0L, BS, BSb}, {Nk}}, loop_scheme);
-    ogemm_loop(
-        [&](int* ind) {
-          int nc = ind[0], s1 = ind[1], nk = ind[2];
-          auto count = nc + Ncb < Nc ? Ncb : Nc - nc;
-          bool is_rem = (s1 + BSb > BS);
-          if (!is_rem) {
-            if (nc == 0) {
-              if (with_bias) {
-                copy_bias_tpp(bias[nk], out[s1][nk]);
-              } else {
-                zero_tpp(out[s1][nk]);
-              }
-            }
-            brgemm_tpp(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, true);
-          } else {
-            if (nc == 0) {
-              if (with_bias) {
-                copy_bias_tpp_rem(bias[nk], out[s1][nk]);
-              } else {
-                zero_tpp_rem(out[s1][nk]);
-              }
-            }
-            brgemm_tpp_rem(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, false);
-            brgemm_tpp.config();
-          }
-        },
-        [&]() { brgemm_tpp.config(); },
-        [&]() { brgemm_tpp.release(); });
-  }
-}
-
-template <typename T, typename Tout = T>
-inline void tpp_linear_no_bias(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_out) {
-  auto in_sizes = t_in.sizes();
-  auto BS = in_sizes[0] * in_sizes[1];
-  if (BS > FT_OPT_SIZE) { // first token compute
-    t_wt = wt_tensor_for_first_token<T>(t_wt);
-  }
-  auto wt_sizes = t_wt.sizes();
-  auto C = in_sizes[2];
-
-  auto Nc = wt_sizes[1];
-  auto Hc = C / Nc;
-  auto Nk = wt_sizes[0];
-  auto Hk = wt_sizes[3];
-  auto K = Nk * Hk;
-  auto t_wt_V = torch_ipex::tpp::wt_tensor_for_fwd(Nk, Hk, Nc, Hc, t_wt);
-
-  auto in = GetVLAPtr<T>(t_in, {Nc, Hc});
-  auto wt_V = GetVLAPtr<T>(t_wt_V, {Nc, Hc * Hk});
-  auto out = GetVLAPtr<Tout>(t_out, {Nk, Hk});
-
-  auto Ncb = Nc;
-  auto BSb = 64L;
-  auto rem = BS % BSb;
-  if (large_cache_opt)
-    Ncb = NCB_BLOCK_SIZE;
-
-  auto zero_tpp = SCOPEIT(SetZeroTPP<Tout>(BSb, Hk, K), EW_ZERO);
-  auto zero_tpp_rem = SCOPEIT(SetZeroTPP<Tout>(rem, Hk, K), EW_ZERO);
-  auto brgemm_tpp = SCOPEITGEMM(
-      (BrgemmTPP<T, Tout>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto brgemm_tpp_rem = SCOPEITGEMM(
-      (BrgemmTPP<T, Tout>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-
-  {
-    RECORD_SCOPE(tpp_linear_krnl, {t_in, t_wt_V});
-    // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
-    auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
-    auto gemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
-        {{0, Nc, Ncb, false}, {0, BS, BSb}, {Nk}}, loop_scheme);
-    gemm_loop(
-        [&](int* ind) {
-          int nc = ind[0], s1 = ind[1], nk = ind[2];
-          auto count = nc + Ncb < Nc ? Ncb : Nc - nc;
-          bool is_rem = (s1 + BSb > BS);
-          if (!is_rem) {
-            if (nc == 0) {
-              zero_tpp(out[s1][nk]);
-            }
-            brgemm_tpp(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, true);
-          } else {
-            if (nc == 0) {
-              zero_tpp_rem(out[s1][nk]);
-            }
-            brgemm_tpp_rem(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, false);
-            brgemm_tpp.config();
-          }
-        },
-        [&]() { brgemm_tpp.config(); },
-        [&]() { brgemm_tpp.release(); });
-  }
-}
-
-template <typename T>
-inline void tpp_linear_mul(
-    at::Tensor t_in,
-    at::Tensor t_in1,
-    at::Tensor t_wt,
-    at::Tensor t_bias,
-    at::Tensor t_out) {
-  auto in_sizes = t_in.sizes();
-  auto BS = in_sizes[0] * in_sizes[1];
-  if (BS > FT_OPT_SIZE) { // first token compute
-    t_wt = wt_tensor_for_first_token<T>(t_wt);
-  }
-  auto wt_sizes = t_wt.sizes();
-  auto C = in_sizes[2];
-
-  auto Nc = wt_sizes[1];
-  auto Hc = C / Nc;
-  auto Nk = wt_sizes[0];
-  auto Hk = wt_sizes[3];
-  auto K = Nk * Hk;
-
-  auto t_wt_V = torch_ipex::tpp::wt_tensor_for_fwd(Nk, Hk, Nc, Hc, t_wt);
-
-  auto in = GetVLAPtr<T>(t_in, {Nc, Hc});
-  auto in1 = GetVLAPtr<T>(t_in1, {Nk, Hk});
-  auto wt_V = GetVLAPtr<T>(t_wt_V, {Nc, Hc * Hk});
-  auto bias = GetVLAPtr<T>(t_bias, {Hk});
-  auto out = GetVLAPtr<T>(t_out, {Nk, Hk});
-
-  auto Ncb = Nc;
-  auto BSb = 64L;
-  auto rem = BS % 64;
-  if (large_cache_opt)
-    Ncb = NCB_BLOCK_SIZE;
-
-  bool with_bias = (t_bias.numel() > 0);
-  auto copy_bias_tpp = SCOPEIT(CpyBiasTPP<T>(BSb, Hk, K), BIAS);
-  auto copy_bias_tpp_rem = SCOPEIT(CpyBiasTPP<T>(rem, Hk, K), BIAS);
-  auto zero_tpp = SCOPEIT(SetZeroTPP<T>(BSb, Hk, K), EW_ZERO);
-  auto zero_tpp_rem = SCOPEIT(SetZeroTPP<T>(rem, Hk, K), EW_ZERO);
-  auto brgemm_tpp = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto brgemm_tpp_rem = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto mul_tpp = SCOPEIT((MulTPP<T, T>(BSb, Hk, K, K)), EW_MUL);
-  auto mul_tpp_rem = SCOPEIT((MulTPP<T, T>(rem, Hk, K, K)), EW_MUL);
-
-  {
-    RECORD_SCOPE(tpp_linear_mul_krnl, {t_in, t_wt_V});
-    // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
-    auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
-    auto ogemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
-        {{0, Nc, Ncb, false}, {0L, BS, BSb}, {Nk}}, loop_scheme);
-    ogemm_loop(
-        [&](int* ind) {
-          int nc = ind[0], s1 = ind[1], nk = ind[2];
-          auto count = nc + Ncb < Nc ? Ncb : Nc - nc;
-          bool is_rem = (s1 + BSb > BS);
-          if (!is_rem) {
-            if (nc == 0) {
-              if (with_bias) {
-                copy_bias_tpp(bias[nk], out[s1][nk]);
-              } else {
-                zero_tpp(out[s1][nk]);
-              }
-            }
-            brgemm_tpp(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, true);
-            if (!(nc + Ncb < Nc)) { // last nc iter
-              mul_tpp(in1[s1][nk], out[s1][nk], out[s1][nk]);
-            }
-          } else {
-            if (nc == 0) {
-              if (with_bias) {
-                copy_bias_tpp_rem(bias[nk], out[s1][nk]);
-              } else {
-                zero_tpp_rem(out[s1][nk]);
-              }
-            }
-            brgemm_tpp_rem(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, false);
-            brgemm_tpp.config();
-            if (!(nc + Ncb < Nc)) { // last nc iter
-              mul_tpp_rem(in1[s1][nk], out[s1][nk], out[s1][nk]);
-            }
-          }
-        },
-        [&]() { brgemm_tpp.config(); },
-        [&]() { brgemm_tpp.release(); });
-  }
-}
-
-template <typename T>
-inline void tpp_linear_add_add(
-    at::Tensor& t_in,
-    at::Tensor& t_in1,
-    at::Tensor& t_in2,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias,
-    at::Tensor& t_out,
-    double scale) {
-  auto in_sizes = t_in.sizes();
-  auto BS = in_sizes[0] * in_sizes[1];
-  if (BS > FT_OPT_SIZE) { // first token compute
-    t_wt = wt_tensor_for_first_token<T>(t_wt);
-  }
-  auto wt_sizes = t_wt.sizes();
-  auto C = in_sizes[2];
-
-  auto Nc = wt_sizes[1];
-  auto Hc = C / Nc;
-  auto Nk = wt_sizes[0];
-  auto Hk = wt_sizes[3];
-  auto K = Nk * Hk;
-
-  auto t_wt_V = torch_ipex::tpp::wt_tensor_for_fwd(Nk, Hk, Nc, Hc, t_wt);
-
-  auto in = GetVLAPtr<T>(t_in, {Nc, Hc});
-  auto in1 = GetVLAPtr<T>(t_in1, {Nk, Hk});
-  auto in2 = GetVLAPtr<T>(t_in2, {Nk, Hk});
-  auto wt_V = GetVLAPtr<T>(t_wt_V, {Nc, Hc * Hk});
-  auto bias = GetVLAPtr<T>(t_bias, {Hk});
-  auto out = GetVLAPtr<T>(t_out, {Nk, Hk});
-
-  auto Ncb = Nc;
-  auto BSb = 64L;
-  auto rem = BS % 64;
-  if (large_cache_opt)
-    Ncb = NCB_BLOCK_SIZE;
-
-  auto copy_bias_tpp = SCOPEIT(CpyBiasTPP<T>(BSb, Hk, K), BIAS);
-  auto copy_bias_tpp_rem = SCOPEIT(CpyBiasTPP<T>(rem, Hk, K), BIAS);
-  auto brgemm_tpp = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto brgemm_tpp_rem = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto add_tpp = SCOPEIT((AddTPP<T, T>(BSb, Hk, K, K)), EW_ADD);
-  auto add_tpp_rem = SCOPEIT((AddTPP<T, T>(rem, Hk, K, K)), EW_ADD);
-  auto sadd_tpp = SCOPEIT((ScaleAddTPP<T, T>(BSb, Hk, K, K)), EW_ADD);
-  auto sadd_tpp_rem = SCOPEIT((ScaleAddTPP<T, T>(rem, Hk, K, K)), EW_ADD);
-
-  {
-    RECORD_SCOPE(tpp_linear_add_add_krnl, {t_in, t_wt_V});
-    // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
-    auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
-    auto ogemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
-        {{0, Nc, Ncb, false}, {0L, BS, BSb}, {Nk}}, loop_scheme);
-    ogemm_loop(
-        [&](int* ind) {
-          int nc = ind[0], s1 = ind[1], nk = ind[2];
-          auto count = nc + Ncb < Nc ? Ncb : Nc - nc;
-          bool is_rem = (s1 + BSb > BS);
-          if (!is_rem) {
-            if (nc == 0) {
-              copy_bias_tpp(bias[nk], out[s1][nk]);
-            }
-            brgemm_tpp(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, true);
-            if (!(nc + Ncb < Nc)) { // last nc iter
-              add_tpp(out[s1][nk], in1[s1][nk], out[s1][nk]);
-              sadd_tpp(in2[s1][nk], out[s1][nk], scale);
-            }
-          } else {
-            if (nc == 0) {
-              copy_bias_tpp_rem(bias[nk], out[s1][nk]);
-            }
-            brgemm_tpp_rem(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, false);
-            brgemm_tpp.config();
-            if (!(nc + Ncb < Nc)) { // last nc iter
-              add_tpp_rem(out[s1][nk], in1[s1][nk], out[s1][nk]);
-              sadd_tpp_rem(in2[s1][nk], out[s1][nk], scale);
-            }
-          }
-        },
-        [&]() { brgemm_tpp.config(); },
-        [&]() { brgemm_tpp.release(); });
-  }
-}
-
-template <typename T>
-inline void tpp_linear_gelu(
-    at::Tensor& t_in,
-    at::Tensor& t_wt,
-    at::Tensor& t_bias,
-    at::Tensor& t_out) {
-  auto in_sizes = t_in.sizes();
-  auto BS = in_sizes[0] * in_sizes[1];
-  if (BS > FT_OPT_SIZE) { // first token compute
-    t_wt = wt_tensor_for_first_token<T>(t_wt);
-  }
-  auto wt_sizes = t_wt.sizes();
-  auto C = in_sizes[2];
-
-  auto Nc = wt_sizes[1];
-  auto Hc = C / Nc;
-  auto Nk = wt_sizes[0];
-  auto Hk = wt_sizes[3];
-  auto K = Nk * Hk;
-
-  auto t_wt_V = torch_ipex::tpp::wt_tensor_for_fwd(Nk, Hk, Nc, Hc, t_wt);
-
-  auto in = GetVLAPtr<T>(t_in, {Nc, Hc});
-  auto wt_V = GetVLAPtr<T>(t_wt_V, {Nc, Hc * Hk});
-  auto bias = GetVLAPtr<T>(t_bias, {Hk});
-  auto out = GetVLAPtr<T>(t_out, {Nk, Hk});
-
-  auto Ncb = Nc;
-  auto BSb = 64L;
-  auto rem = BS % 64;
-  if (large_cache_opt)
-    Ncb = NCB_BLOCK_SIZE;
-
-  auto copy_bias_tpp = SCOPEIT(CpyBiasTPP<T>(BSb, Hk, K), BIAS);
-  auto copy_bias_tpp_rem = SCOPEIT(CpyBiasTPP<T>(rem, Hk, K), BIAS);
-  auto brgemm_tpp = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto brgemm_tpp_rem = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto gelu_fwd_tpp = SCOPEIT(GeluFwdTPP<T>(BSb, Hk, K, K), ACT);
-  auto gelu_fwd_tpp_rem = SCOPEIT(GeluFwdTPP<T>(rem, Hk, K, K), ACT);
-
-  {
-    RECORD_SCOPE(tpp_linear_gelu_krnl, {t_in, t_wt_V});
-    // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
-    auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
-    auto igemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
-        {{0, Nc, Ncb, false}, {0, BS, BSb}, {Nk}}, loop_scheme);
-    igemm_loop(
-        [&](int* ind) {
-          int nc = ind[0], s1 = ind[1], nk = ind[2];
-          auto count = nc + Ncb < Nc ? Ncb : Nc - nc;
-          bool is_rem = (s1 + BSb > BS);
-          if (!is_rem) {
-            if (nc == 0) {
-              copy_bias_tpp(bias[nk], out[s1][nk]);
-            }
-            brgemm_tpp(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, true);
-            if (!(nc + Ncb < Nc)) { // last nc iter
-              gelu_fwd_tpp(out[s1][nk], out[s1][nk]);
-            }
-          } else {
-            if (nc == 0) {
-              copy_bias_tpp_rem(bias[nk], out[s1][nk]);
-            }
-            brgemm_tpp_rem(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, false);
-            brgemm_tpp.config();
-            if (!(nc + Ncb < Nc)) { // last nc iter
-              gelu_fwd_tpp_rem(out[s1][nk], out[s1][nk]);
-            }
-          }
-        },
-        [&]() { brgemm_tpp.config(); },
-        [&]() { brgemm_tpp.release(); });
-  }
-}
-
-template <typename T>
-inline void tpp_linear_add(
-    at::Tensor t_in,
-    at::Tensor t_in1,
-    at::Tensor t_wt,
-    at::Tensor t_bias,
-    at::Tensor t_out,
-    float scale) {
-  auto in_sizes = t_in.sizes();
-  auto BS = in_sizes[0] * in_sizes[1];
-  if (BS > FT_OPT_SIZE) { // first token compute
-    t_wt = wt_tensor_for_first_token<T>(t_wt);
-  }
-  auto wt_sizes = t_wt.sizes();
-  auto C = in_sizes[2];
-
-  auto Nc = wt_sizes[1];
-  auto Hc = C / Nc;
-  auto Nk = wt_sizes[0];
-  auto Hk = wt_sizes[3];
-  auto K = Nk * Hk;
-
-  auto t_wt_V = torch_ipex::tpp::wt_tensor_for_fwd(Nk, Hk, Nc, Hc, t_wt);
-
-  auto in = GetVLAPtr<T>(t_in, {Nc, Hc});
-  auto in1 = GetVLAPtr<T>(t_in1, {Nk, Hk});
-  auto wt_V = GetVLAPtr<T>(t_wt_V, {Nc, Hc * Hk});
-  auto bias = GetVLAPtr<T>(t_bias, {Hk});
-  auto out = GetVLAPtr<T>(t_out, {Nk, Hk});
-
-  auto Ncb = Nc;
-  auto BSb = 64L;
-  auto rem = BS % 64;
-  if (large_cache_opt)
-    Ncb = NCB_BLOCK_SIZE;
-
-  bool with_bias = (t_bias.numel() > 0);
-  auto copy_bias_tpp = SCOPEIT(CpyBiasTPP<T>(BSb, Hk, K), BIAS);
-  auto copy_bias_tpp_rem = SCOPEIT(CpyBiasTPP<T>(rem, Hk, K), BIAS);
-  auto zero_tpp = SCOPEIT(SetZeroTPP<T>(BSb, Hk, K), EW_ZERO);
-  auto zero_tpp_rem = SCOPEIT(SetZeroTPP<T>(rem, Hk, K), EW_ZERO);
-  auto brgemm_tpp = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto brgemm_tpp_rem = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto sadd_tpp = SCOPEIT((ScaleAddTPP<T, T>(BSb, Hk, K, K)), EW_ADD);
-  auto sadd_tpp_rem = SCOPEIT((ScaleAddTPP<T, T>(rem, Hk, K, K)), EW_ADD);
-
-  {
-    RECORD_SCOPE(tpp_linear_add_krnl, {t_in, t_wt_V});
-    // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
-    auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
-    auto ogemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
-        {{0, Nc, Ncb, false}, {0L, BS, BSb}, {Nk}}, loop_scheme);
-    ogemm_loop(
-        [&](int* ind) {
-          int nc = ind[0], s1 = ind[1], nk = ind[2];
-          auto count = nc + Ncb < Nc ? Ncb : Nc - nc;
-          bool is_rem = (s1 + BSb > BS);
-          if (!is_rem) {
-            if (nc == 0) {
-              if (with_bias) {
-                copy_bias_tpp(bias[nk], out[s1][nk]);
-              } else {
-                zero_tpp(out[s1][nk]);
-              }
-            }
-            brgemm_tpp(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, true);
-            if (!(nc + Ncb < Nc)) { // last nc iter
-              sadd_tpp(in1[s1][nk], out[s1][nk], scale);
-            }
-          } else {
-            if (nc == 0) {
-              if (with_bias) {
-                copy_bias_tpp_rem(bias[nk], out[s1][nk]);
-              } else {
-                zero_tpp_rem(out[s1][nk]);
-              }
-            }
-            brgemm_tpp_rem(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, false);
-            brgemm_tpp.config();
-            if (!(nc + Ncb < Nc)) { // last nc iter
-              sadd_tpp_rem(in1[s1][nk], out[s1][nk], scale);
-            }
-          }
-        },
-        [&]() { brgemm_tpp.config(); },
-        [&]() { brgemm_tpp.release(); });
-  }
-}
-
-template <typename T>
-inline void tpp_linear_silu(
-    at::Tensor t_in,
-    at::Tensor t_wt,
-    at::Tensor t_bias,
-    at::Tensor t_out) {
-  auto in_sizes = t_in.sizes();
-  auto BS = in_sizes[0] * in_sizes[1];
-  if (BS > FT_OPT_SIZE) { // first token compute
-    t_wt = wt_tensor_for_first_token<T>(t_wt);
-  }
-  auto wt_sizes = t_wt.sizes();
-  auto C = in_sizes[2];
-
-  auto Nc = wt_sizes[1];
-  auto Hc = C / Nc;
-  auto Nk = wt_sizes[0];
-  auto Hk = wt_sizes[3];
-  auto K = Nk * Hk;
-
-  auto t_wt_V = torch_ipex::tpp::wt_tensor_for_fwd(Nk, Hk, Nc, Hc, t_wt);
-
-  auto in = GetVLAPtr<T>(t_in, {Nc, Hc});
-  auto wt_V = GetVLAPtr<T>(t_wt_V, {Nc, Hc * Hk});
-  auto bias = GetVLAPtr<T>(t_bias, {Hk});
-  auto out = GetVLAPtr<T>(t_out, {Nk, Hk});
-
-  auto Ncb = Nc;
-  auto BSb = 64L;
-  auto rem = BS % 64;
-  if (large_cache_opt)
-    Ncb = NCB_BLOCK_SIZE;
-
-  bool with_bias = (t_bias.numel() > 0);
-  auto copy_bias_tpp = SCOPEIT(CpyBiasTPP<T>(BSb, Hk, K), BIAS);
-  auto copy_bias_tpp_rem = SCOPEIT(CpyBiasTPP<T>(rem, Hk, K), BIAS);
-  auto zero_tpp = SCOPEIT(SetZeroTPP<T>(BSb, Hk, K), EW_ZERO);
-  auto zero_tpp_rem = SCOPEIT(SetZeroTPP<T>(rem, Hk, K), EW_ZERO);
-  auto brgemm_tpp = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto brgemm_tpp_rem = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto silu_fwd_tpp = SCOPEIT(SiLUFwdTPP<T>(BSb, Hk, K, K), ACT);
-  auto silu_fwd_tpp_rem = SCOPEIT(SiLUFwdTPP<T>(rem, Hk, K, K), ACT);
-
-  {
-    RECORD_SCOPE(tpp_linear_silu_krnl, {t_in, t_wt_V});
-    // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
-    auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
-    auto igemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
-        {{0, Nc, Ncb, false}, {0, BS, BSb}, {Nk}}, loop_scheme);
-    igemm_loop(
-        [&](int* ind) {
-          int nc = ind[0], s1 = ind[1], nk = ind[2];
-          auto count = nc + Ncb < Nc ? Ncb : Nc - nc;
-          bool is_rem = (s1 + BSb > BS);
-          if (!is_rem) {
-            if (nc == 0) {
-              if (with_bias) {
-                copy_bias_tpp(bias[nk], out[s1][nk]);
-              } else {
-                zero_tpp(out[s1][nk]);
-              }
-            }
-            brgemm_tpp(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, true);
-            if (!(nc + Ncb < Nc)) { // last nc iter
-              silu_fwd_tpp(out[s1][nk], out[s1][nk]);
-            }
-          } else {
-            if (nc == 0) {
-              if (with_bias) {
-                copy_bias_tpp_rem(bias[nk], out[s1][nk]);
-              } else {
-                zero_tpp_rem(out[s1][nk]);
-              }
-            }
-            brgemm_tpp_rem(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, false);
-            brgemm_tpp.config();
-            if (!(nc + Ncb < Nc)) { // last nc iter
-              silu_fwd_tpp_rem(out[s1][nk], out[s1][nk]);
-            }
-          }
-        },
-        [&]() { brgemm_tpp.config(); },
-        [&]() { brgemm_tpp.release(); });
-  }
-}
-
-template <typename T>
-inline void tpp_linear_relu(
-    at::Tensor t_in,
-    at::Tensor t_wt,
-    at::Tensor t_bias,
-    at::Tensor t_out) {
-  auto in_sizes = t_in.sizes();
-  auto BS = in_sizes[0] * in_sizes[1];
-  if (BS > FT_OPT_SIZE) { // first token compute
-    t_wt = wt_tensor_for_first_token<T>(t_wt);
-  }
-  auto wt_sizes = t_wt.sizes();
-  auto C = in_sizes[2];
-
-  auto Nc = wt_sizes[1];
-  auto Hc = C / Nc;
-  auto Nk = wt_sizes[0];
-  auto Hk = wt_sizes[3];
-  auto K = Nk * Hk;
-
-  auto t_wt_V = torch_ipex::tpp::wt_tensor_for_fwd(Nk, Hk, Nc, Hc, t_wt);
-
-  auto in = GetVLAPtr<T>(t_in, {Nc, Hc});
-  auto wt_V = GetVLAPtr<T>(t_wt_V, {Nc, Hc * Hk});
-  auto bias = GetVLAPtr<T>(t_bias, {Hk});
-  auto out = GetVLAPtr<T>(t_out, {Nk, Hk});
-
-  auto Ncb = Nc;
-  auto BSb = 64L;
-  auto rem = BS % 64;
-  if (large_cache_opt)
-    Ncb = NCB_BLOCK_SIZE;
-
-  bool with_bias = (t_bias.numel() > 0);
-  auto copy_bias_tpp = SCOPEIT(CpyBiasTPP<T>(BSb, Hk, K), BIAS);
-  auto copy_bias_tpp_rem = SCOPEIT(CpyBiasTPP<T>(rem, Hk, K), BIAS);
-  auto zero_tpp = SCOPEIT(SetZeroTPP<T>(BSb, Hk, K), EW_ZERO);
-  auto zero_tpp_rem = SCOPEIT(SetZeroTPP<T>(rem, Hk, K), EW_ZERO);
-  auto brgemm_tpp = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(BSb, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto brgemm_tpp_rem = SCOPEITGEMM(
-      (BrgemmTPP<T, T>(rem, Hk, Hc, Hc, Hk * Hc, C, Hk, K, 1.0, 0, Ncb)));
-  auto relu_fwd_tpp = SCOPEIT(ReLUFwdTPP<T>(BSb, Hk, K, K, false), ACT);
-  auto relu_fwd_tpp_rem = SCOPEIT(ReLUFwdTPP<T>(rem, Hk, K, K, false), ACT);
-
-  {
-    RECORD_SCOPE(tpp_linear_relu_krnl, {t_in, t_wt_V});
-    // auto loop_scheme = large_cache_opt ? "acB" : "aBC";
-    auto loop_scheme = large_cache_opt ? GEMM_LOOP_SCHEME : "aCb";
-    auto igemm_loop = torch_ipex::tpp::ThreadedLoop<3>(
-        {{0, Nc, Ncb, false}, {0, BS, BSb}, {Nk}}, loop_scheme);
-    igemm_loop(
-        [&](int* ind) {
-          int nc = ind[0], s1 = ind[1], nk = ind[2];
-          auto count = nc + Ncb < Nc ? Ncb : Nc - nc;
-          bool is_rem = (s1 + BSb > BS);
-          if (!is_rem) {
-            if (nc == 0) {
-              if (with_bias) {
-                copy_bias_tpp(bias[nk], out[s1][nk]);
-              } else {
-                zero_tpp(out[s1][nk]);
-              }
-            }
-            brgemm_tpp(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, true);
-            if (!(nc + Ncb < Nc)) { // last nc iter
-              relu_fwd_tpp(out[s1][nk], out[s1][nk]);
-            }
-          } else {
-            if (nc == 0) {
-              if (with_bias) {
-                copy_bias_tpp_rem(bias[nk], out[s1][nk]);
-              } else {
-                zero_tpp_rem(out[s1][nk]);
-              }
-            }
-            brgemm_tpp_rem(in[s1][nc], wt_V[nk][nc], out[s1][nk], count, false);
-            brgemm_tpp.config();
-            if (!(nc + Ncb < Nc)) { // last nc iter
-              relu_fwd_tpp_rem(out[s1][nk], out[s1][nk]);
-            }
-          }
-        },
-        [&]() { brgemm_tpp.config(); },
-        [&]() { brgemm_tpp.release(); });
-  }
-}
-
-} // namespace tpp
-} // namespace torch_ipex
diff --git a/csrc/cpu/tpp/tensor_helper.h b/csrc/cpu/tpp/tensor_helper.h
index c5d5c75f..2b4438d8 100644
--- a/csrc/cpu/tpp/tensor_helper.h
+++ b/csrc/cpu/tpp/tensor_helper.h
@@ -6,24 +6,23 @@
 namespace torch_ipex {
 namespace tpp {
 
-template <typename T>
 inline at::Tensor wt_tensor_n2v(
     long Nk,
     long Hk,
     long Nc,
     long Hc,
     at::Tensor& input) {
-  const int BS = get_vnni_block_size<T>();
 #if 0
-  AT_ASSERT(Hc % BS == 0, "Uneven number for Hc\n");
-  return input.view({Nk, Nc, Hc/BS, BS, Hk}).permute({0, 1, 2, 4, 3}).contiguous();
+  PCL_ASSERT(Hc % 2 == 0, "Uneven number for Hc\n");
+  return input.view({Nk, Nc, Hc/2, 2, Hk}).permute({0, 1, 2, 4, 3}).contiguous();
 #else
-  auto Hcp2 = (Hc + BS - 1) / BS;
-  auto output = input.new_empty({Nk, Nc, Hcp2, Hk, BS});
-  auto out = GetVLAPtr<T>(output, {Hcp2 * Hk * BS});
-  auto in = GetVLAPtr<T>(input, {Hc * Hk});
+  auto Hcp2 = (Hc + 1) / 2;
+  auto output = input.new_empty({Nk, Nc, Hcp2, Hk, 2});
+  DECL_VLA_PTR_PT(bfloat16, out, [Hcp2 * Hk * 2], output);
+  DECL_VLA_PTR_PT(bfloat16, in, [Hc * Hk], input);
   auto n2v_tpp = SCOPEIT(
-      XformExtTPP<T>(Hc, Hk, Hcp2 * BS, Hk, XformTPP::XFORM_N2V_TPP), VNNI);
+      XformExtTPP<bfloat16>(Hc, Hk, Hcp2 * 2, Hk, XformTPP::XFORM_N2V_TPP),
+      VNNI);
   RECORD_FUNCTION("parallel_for", std::vector<c10::IValue>());
 #pragma omp parallel for
   for (int n = 0; n < Nk * Nc; n++) {
@@ -158,15 +157,11 @@ inline at::Tensor wt_tensor_for_fwd(
     long Hc,
     at::Tensor& input) {
   RECORD_SCOPE(w_vnni, {input});
-  if (input.dtype() != at::kFloat) {
+  if (input.dtype() == at::kBFloat16) {
     if (input.dim() == 5) {
       return input;
     } else {
-      if (input.dtype() == at::kBFloat16) {
-        return wt_tensor_n2v<bfloat16>(Nk, Hk, Nc, Hc, input);
-      }else {
-        AT_ASSERT(false, "Unsupported datatype!");
-      }
+      return wt_tensor_n2v(Nk, Hk, Nc, Hc, input);
     }
   } else {
     return input;
diff --git a/csrc/cpu/tpp/threaded_loops.h b/csrc/cpu/tpp/threaded_loops.h
index ce8cba11..a5465695 100644
--- a/csrc/cpu/tpp/threaded_loops.h
+++ b/csrc/cpu/tpp/threaded_loops.h
@@ -247,7 +247,7 @@ inline LoopingScheme* getLoopingScheme(std::string scheme) {
 template <int N>
 class ThreadedLoop {
  public:
-  ThreadedLoop(const LoopSpecs (&bounds)[N], std::string scheme = "")
+  ThreadedLoop(std::array<LoopSpecs, N> bounds, std::string scheme = "")
       : bounds(bounds), scheme(scheme) {
     if (scheme == "")
       scheme = getDefaultScheme();
@@ -256,11 +256,11 @@ class ThreadedLoop {
 
   template <class T>
   void operator()(T func) {
-    loopScheme->call(bounds, func, NULL, NULL);
+    loopScheme->call(bounds.data(), func, NULL, NULL);
   }
   template <class T, class Ti, class Tf>
   void operator()(T func, Ti init, Tf fini) {
-    loopScheme->call(bounds, func, init, fini);
+    loopScheme->call(bounds.data(), func, init, fini);
   }
 
   std::string getDefaultScheme() {
@@ -275,7 +275,7 @@ class ThreadedLoop {
   }
 
  private:
-  LoopSpecs bounds[N];
+  std::array<LoopSpecs, N> bounds;
   std::string scheme;
   LoopingScheme* loopScheme;
 };
diff --git a/csrc/cpu/tpp/timing.h b/csrc/cpu/tpp/timing.h
index f2f7cf68..e399d038 100644
--- a/csrc/cpu/tpp/timing.h
+++ b/csrc/cpu/tpp/timing.h
@@ -158,16 +158,6 @@ class ScopedTPP {
   template <typename... Types>
   void operator()(Types... vars) {
     ScopedTimer _t(t);
-#ifdef DEBUG_TRACE_TPP
-    if (omp_get_thread_num() == 0) {
-      auto cur_class_name = get_class_name<T>();
-      if (cur_class_name != prev_class_name) {
-        std::cout << "Calling impl " << impl << " for " << cur_class_name
-                  << std::endl;
-        prev_class_name = cur_class_name;
-      }
-    }
-#endif
     if (impl == 0) {
       func(vars...);
     } else if (impl == 1) {
diff --git a/csrc/cpu/tpp/utils.h b/csrc/cpu/tpp/utils.h
index 695e203e..391d2288 100644
--- a/csrc/cpu/tpp/utils.h
+++ b/csrc/cpu/tpp/utils.h
@@ -124,151 +124,6 @@ class SafePrint {
   char buf[2 * maxlen];
   int len = 0;
 };
-
-template <typename T, typename index_t = int64_t>
-class VLAAccessorBase {
- public:
-  typedef T* PtrType;
-
-  VLAAccessorBase(PtrType data_, const index_t* strides_)
-      : data_(data_), strides_(strides_) {}
-
- protected:
-  PtrType data_;
-  const index_t* strides_;
-};
-
-template <typename T, std::size_t N, typename index_t = int64_t>
-class VLAAccessor : public VLAAccessorBase<T, index_t> {
- public:
-  typedef T* PtrType;
-
-  VLAAccessor(PtrType data_, const index_t* strides_)
-      : VLAAccessorBase<T, index_t>(data_, strides_) {}
-
-  VLAAccessor<T, N - 1, index_t> operator[](index_t i) {
-    return VLAAccessor<T, N - 1, index_t>(
-        this->data_ + this->strides_[0] * i, this->strides_ + 1);
-  }
-
-  const VLAAccessor<T, N - 1, index_t> operator[](index_t i) const {
-    return VLAAccessor<T, N - 1, index_t>(
-        this->data_ + this->strides_[0] * i, this->strides_ + 1);
-  }
-};
-
-#if 1
-template <typename T, typename index_t>
-class VLAAccessor<T, 1, index_t> : public VLAAccessorBase<T, index_t> {
- public:
-  typedef T* PtrType;
-
-  VLAAccessor(PtrType data_, const index_t* strides_)
-      : VLAAccessorBase<T, index_t>(data_, strides_) {}
-  T* operator[](index_t i) {
-    return this->data_ + i * this->strides_[0];
-  }
-  const T* operator[](index_t i) const {
-    return this->data_ + i * this->strides_[0];
-  }
-};
-#endif
-template <typename T, typename index_t>
-class VLAAccessor<T, 0, index_t> : public VLAAccessorBase<T, index_t> {
- public:
-  typedef T* PtrType;
-
-  VLAAccessor(PtrType data_, const index_t* strides_)
-      : VLAAccessorBase<T, index_t>(data_, strides_) {}
-  T& operator[](index_t i) {
-    return this->data_[i];
-  }
-  const T& operator[](index_t i) const {
-    return this->data_[i];
-  }
-  operator T*() {
-    return this->data_;
-  }
-  operator const T*() const {
-    return this->data_;
-  }
-};
-
-template <typename T, std::size_t N, typename index_t = int64_t>
-class VLAPtr {
- public:
-  VLAPtr(T* data_, const index_t (&sizes)[N]) : data_(data_) {
-    strides[N - 1] = sizes[N - 1];
-    for (long i = N - 2; i >= 0; i--)
-      strides[i] = strides[i + 1] * sizes[i];
-  }
-  VLAAccessor<T, N - 1, index_t> operator[](index_t i) {
-    return VLAAccessor<T, N - 1, index_t>(data_ + i * strides[0], strides + 1);
-  }
-  operator bool() {
-    return data_ != nullptr;
-  }
-
- protected:
-  index_t strides[N];
-  T* data_;
-};
-
-#if 1
-template <typename T>
-class VLAPtr<T, 1, int64_t> {
- public:
-  typedef int64_t index_t;
-  VLAPtr(T* data_, const index_t (&sizes)[1]) : data_(data_) {
-    strides[0] = sizes[0];
-  }
-  T* operator[](index_t i) {
-    return data_ + i * strides[0];
-  }
-  operator bool() {
-    return data_ != nullptr;
-  }
-
- protected:
-  index_t strides[1];
-  T* data_;
-};
-#endif
-
-template <typename T, std::size_t N, typename index_t = int64_t>
-VLAPtr<T, N, index_t> GetVLAPtr(T* data_, const index_t (&list)[N]) {
-  return VLAPtr<T, N, index_t>(data_, list);
-}
-
-template <typename T>
-inline T* pt_get_data_ptr(at::Tensor t) {
-  if (!t.is_contiguous()) {
-    std::cout << "Warning: Tensor t " << t.sizes() << " is not contiguous"
-              << std::endl;
-  }
-  return t.data_ptr<T>();
-}
-
-typedef int64_t index_t;
-template <typename T, std::size_t N> //, typename index_t = int64_t>
-VLAPtr<T, N, index_t> GetVLAPtr(at::Tensor t, const index_t (&sizes)[N]) {
-  return VLAPtr<T, N, index_t>(pt_get_data_ptr<T>(t), sizes);
-}
-template <typename T>
-T* GetVLAPtr(at::Tensor t) {
-  return pt_get_data_ptr<T>(t);
-}
-
-inline int env2int(const char* env_name, int def_val = 0) {
-  int val = def_val;
-  auto env = getenv(env_name);
-  if (env)
-    val = atoi(env);
-  // printf("Using %s = %d\n", env_name, val);
-  return val;
-}
-
-
 } // namespace tpp
 } // namespace torch_ipex
 #endif //_PCL_UTILS_H_
diff --git a/csrc/cpu/tpp/woq/debug.h b/csrc/cpu/tpp/woq/debug.h
deleted file mode 100644
index 365ed462..00000000
--- a/csrc/cpu/tpp/woq/debug.h
+++ /dev/null
@@ -1,69 +0,0 @@
-#ifndef _TLA_DEBUG_H_
-#define _TLA_DEBUG_H_
-
-#include <cstdint>
-#include <cstdio>
-#include <iomanip>
-#include <iostream>
-
-template <int maxlen>
-class SafePrint {
- public:
-  SafePrint() {}
-  template <typename... Types>
-  int operator()(Types... vars) {
-    if (len < maxlen) {
-      int l = snprintf(&buf[len], 2 * maxlen - len, vars...);
-      len += l;
-      if (len > maxlen) {
-        print();
-      }
-      return l;
-    }
-    return 0;
-  }
-  void print() {
-    printf("%s", buf);
-    len = 0;
-    buf[0] = 0;
-  }
-
- private:
-  char buf[2 * maxlen];
-  int len = 0;
-};
-
-template <typename T>
-inline void print_matrix(T* mat, int m, int n, int ldm, const char* name=nullptr, int ldn=1) {
-  if (omp_get_thread_num() != 0) return;
-  std::cout << "\"" << (name ? name : (const char*)("noname")) << "\"" << "\n";
-  for (int i = 0; i < m; i++) {
-    std::cout << "[";
-    for (int j = 0; j < n; j++) {
-      // for floating point, keep four digits after decimal point
-      // for integers, print as is but with the same width by adding leading spaces
-      if (std::is_integral<T>::value) {
-        std::cout << std::setw(5) << (long)(mat[i * ldm + j * ldn]);
-      } else {
-        std::cout << std::fixed << std::setprecision(4) << std::setw(10) << (float)(mat[i * ldm + j * ldn]);
-      }
-    }
-    std::cout << "]\n";
-  }
-}
-
-inline void print_matrix_int4(uint8_t* mat, int m, int n, int ldm, const char* name=nullptr, int ldn=1) {
-  std::cout << "\"" << (name ? name : (const char*)("noname")) << "\"" << "\n";
-  for (int i = 0; i < m; i++) {
-    std::cout << "[";
-    for (int j = 0; j < n; j+=2) {
-      // for floating point, keep four digits after decimal point
-      // for integers, print as is but with the same width by adding leading spaces
-      std::cout << std::setw(5) << (long)(mat[i * ldm/2 + j * ldn/2] & 0xf);
-      std::cout << std::setw(5) << (long)(mat[i * ldm/2 + j * ldn/2] >> 4);
-    }
-    std::cout << "]\n";
-  }
-}
-
-#endif
\ No newline at end of file
diff --git a/csrc/cpu/tpp/woq/dispatcher.h b/csrc/cpu/tpp/woq/dispatcher.h
deleted file mode 100644
index b1c87029..00000000
--- a/csrc/cpu/tpp/woq/dispatcher.h
+++ /dev/null
@@ -1,248 +0,0 @@
-#ifndef _TLA_DISPATCHER_H_
-#define _TLA_DISPATCHER_H_
-
-#include <cstddef>
-#include <utility>
-
-template <typename... Args>
-void failing_fallback(Args... args) {
-  TLA_ASSERT(false, "should not reach here");
-}
-
-// TODO(jgong5): use switch-case to speed up range and enumerate dispatchers
-
-// dispatch a range of integers to a lambda function
-template <typename IntegralType, IntegralType start, IntegralType end>
-struct range_dispatcher {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(IntegralType i, const Lambda1& function, const Lambda2& fallback, Args... args) {
-    static_assert(start < end, "Start must be less than end");
-    if (i == start)
-      function(std::integral_constant<IntegralType, start>{}, std::forward<Args>(args)...);
-    else
-      range_dispatcher<IntegralType, start + 1, end>::call(i, function, fallback, std::forward<Args>(args)...);
-  }
-};
-
-template <typename IntegralType, IntegralType start>
-struct range_dispatcher<IntegralType, start, start> {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(IntegralType i, const Lambda1& function, const Lambda2& fallback, Args... args) {
-    if (i == start) {
-      function(std::integral_constant<IntegralType, start>{}, std::forward<Args>(args)...);
-    } else {
-      fallback(i, std::forward<Args>(args)...);
-    }
-  }
-};
-
-template <typename IntegralType, int n, IntegralType First, IntegralType ...Rest>
-struct enumerate_dispatcher_helper {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(
-    IntegralType i,
-    const std::function<bool(IntegralType, IntegralType)>& comparator,
-    const Lambda1& function,
-    const Lambda2& fallback,
-    Args... args
-  ) {
-    if (comparator(i, First))
-      function(std::integral_constant<IntegralType, First>{}, std::forward<Args>(args)...);
-    else
-      enumerate_dispatcher_helper<IntegralType, n-1, Rest...>::call(i, comparator, function, fallback, std::forward<Args>(args)...);
-  }
-};
-
-template <typename IntegralType, IntegralType First>
-struct enumerate_dispatcher_helper<IntegralType, 0, First> {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(
-    IntegralType i,
-    const std::function<bool(IntegralType, IntegralType)>& comparator,
-    const Lambda1& function,
-    const Lambda2& fallback,
-    Args... args
-  ) {
-    if (comparator(i, First))
-      function(std::integral_constant<IntegralType, First>{}, std::forward<Args>(args)...);
-    else
-      fallback(i, std::forward<Args>(args)...);
-  }
-};
-
-// dispatch a list of integers to a lambda function
-template <typename IntegralType, IntegralType... ints>
-struct enumerate_dispatcher {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(IntegralType i, const Lambda1& function, const Lambda2& fallback, Args... args) {
-    enumerate_dispatcher_helper<IntegralType, sizeof...(ints)-1, ints...>::call(
-      i,
-      [&](IntegralType a, IntegralType b) { return a == b; },
-      function,
-      fallback,
-      std::forward<Args>(args)...
-      );
-  }
-};
-
-// dispatch a list of integers to a lambda function based on divisibility
-template <typename IntegralType, IntegralType... ints>
-struct divisible_dispatcher {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(IntegralType i, const Lambda1& function, const Lambda2& fallback, Args... args) {
-    enumerate_dispatcher_helper<IntegralType, sizeof...(ints)-1, ints...>::call(
-      i,
-      [&](IntegralType a, IntegralType b) { return a % b == 0; },
-      function,
-      fallback,
-      std::forward<Args>(args)...
-      );
-  }
-};
-
-// dispatch a list of integers to a lambda function based on given condition
-template <typename IntegralType, IntegralType... ints>
-struct conditional_enumerate_dispatcher {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(
-    IntegralType i,
-    const std::function<bool(IntegralType, IntegralType)>& comparator,
-    const Lambda1& function,
-    const Lambda2& fallback, Args... args
-  ) {
-    enumerate_dispatcher_helper<IntegralType, sizeof...(ints)-1, ints...>::call(
-      i,
-      comparator,
-      function,
-      fallback,
-      std::forward<Args>(args)...
-      );
-  }
-};
-
-// dispatch boolean to a lambda function
-struct boolean_dispatcher {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(bool i, const Lambda1& function, const Lambda2& fallback, Args... args) {
-    enumerate_dispatcher<bool, true, false>::call(i, function, fallback, std::forward<Args>(args)...);
-  }
-};
-
-// A helper function that returns the last N-1 items of a tuple as a new tuple
-template <typename TupleType, std::size_t... Is>
-auto get_last_n_minus_one_impl(TupleType&& t, std::index_sequence<Is...>)
-{
-  return std::tuple_cat(std::make_tuple(std::get<Is + 1>(t))...);
-}
-
-// A function that returns the last N-1 items of a tuple as a new tuple
-template <typename TupleType>
-auto get_last_n_minus_one(TupleType&& t)
-{
-  // Get the size of the tuple
-  constexpr auto size = std::tuple_size<typename std::remove_reference<TupleType>::type>::value;
-  // Check if the size is greater than one
-  return get_last_n_minus_one_impl(std::forward<TupleType>(t), std::make_index_sequence<size - 1>{});
-}
-
-template <typename TupleType, std::enable_if_t<std::tuple_size<TupleType>::value == 1, bool> = true>
-auto get_last_n_minus_one(TupleType&& t)
-{
-  return std::make_tuple();
-}
-
-template <typename IntegralTypesProcessed, typename IntegralTypesToProcess, typename Dispatchers>
-struct product_dispatcher_helper;
-
-template <typename... IntegralTypeProcessed>
-struct product_dispatcher_helper<std::tuple<IntegralTypeProcessed...>, std::tuple<>, std::tuple<>> {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(std::tuple<>, std::tuple<IntegralTypeProcessed...> constants, std::tuple<>, const Lambda1& function, const Lambda2& fallback, Args... args) {
-    function(constants, std::forward<Args>(args)...);
-  }
-};
-
-template <typename... IntegralTypeProcessed, typename... IntegeralTypeToProcess, typename... Dispatcher>
-struct product_dispatcher_helper<std::tuple<IntegralTypeProcessed...>, std::tuple<IntegeralTypeToProcess...>, std::tuple<Dispatcher...>> {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(std::tuple<Dispatcher...> dispatchers, std::tuple<IntegralTypeProcessed...> constants, std::tuple<IntegeralTypeToProcess...> integrals, const Lambda1& function, const Lambda2& fallback, Args... args) {
-    std::get<0>(dispatchers).call(
-      std::get<0>(integrals),
-      [&](auto i, Args... args) {
-        auto new_dispatchers = get_last_n_minus_one(dispatchers);
-        auto new_constants = std::tuple_cat(constants, std::tuple<decltype(i)>(i));
-        auto new_integrals = get_last_n_minus_one(integrals);
-        product_dispatcher_helper<decltype(new_constants), decltype(new_integrals), decltype(new_dispatchers)>::call(
-          new_dispatchers,
-          new_constants,
-          new_integrals,
-          function,
-          fallback,
-          std::forward<Args>(args)...
-        );
-      },
-      [&](auto i, Args... args) {
-        fallback(std::tuple_cat(constants, integrals), std::forward<Args>(args)...);
-      },
-      std::forward<Args>(args)...
-    );
-  }
-};
-
-template <typename IntegralTypes, typename Dispatchers>
-struct product_dispatcher;
-
-// dispatch to a carsian product of a list of integers to a lambda function
-template <typename... IntegeralType, typename... Dispatcher>
-struct product_dispatcher<std::tuple<IntegeralType...>, std::tuple<Dispatcher...>> {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(std::tuple<IntegeralType...> integrals, const Lambda1& function, const Lambda2& fallback, Args... args) {
-    static auto dispatchers = std::tuple<Dispatcher...>{};
-    product_dispatcher_helper<std::tuple<>, std::tuple<IntegeralType...>, std::tuple<Dispatcher...>>::call(
-      dispatchers, std::tuple<>{}, integrals, function, fallback, std::forward<Args>(args)...
-    );
-  }
-};
-
-template <size_t N, typename IntegralType, typename Dispatcher, typename... OtherDispatchers>
-struct union_dispatcher_helper {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(IntegralType i, const Lambda1& function, const Lambda2& fallback, Args... args) {
-    Dispatcher::call(
-      i,
-      function,
-      [&](auto i, Args... args) {
-        union_dispatcher_helper<N-1, IntegralType, OtherDispatchers...>::call(
-          i, function, fallback, std::forward<Args>(args)...
-        );
-      },
-      std::forward<Args>(args)...
-    );
-  }
-};
-
-template <typename IntegralType, typename Dispatcher>
-struct union_dispatcher_helper<1, IntegralType, Dispatcher> {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(IntegralType i, const Lambda1& function, const Lambda2& fallback, Args... args) {
-    Dispatcher::call(
-      i,
-      function,
-      fallback,
-      std::forward<Args>(args)...
-    );
-  }
-};
-
-// dispatch to a combination of a list of integers to a lambda function
-template <typename IntegralType, typename... Dispatchers>
-struct union_dispatcher {
-  template <typename Lambda1, typename Lambda2, typename... Args>
-  inline static void call(IntegralType i, const Lambda1& function, const Lambda2& fallback, Args... args) {
-    union_dispatcher_helper<sizeof...(Dispatchers), IntegralType, Dispatchers...>::call(
-      i, function, fallback, std::forward<Args>(args)...
-    );
-  }
-};
-
-#endif
\ No newline at end of file
diff --git a/csrc/cpu/tpp/woq/par_loop_generator.h b/csrc/cpu/tpp/woq/par_loop_generator.h
deleted file mode 100644
index f075155e..00000000
--- a/csrc/cpu/tpp/woq/par_loop_generator.h
+++ /dev/null
@@ -1,896 +0,0 @@
-#ifndef _PAR_LOOP_GENERATOR_H_
-#define _PAR_LOOP_GENERATOR_H_
-
-#include <ctype.h>
-#include <dlfcn.h>
-#include <stdio.h>
-#include <stdlib.h>
-#include <string.h>
-#include <functional>
-#include <string>
-
-#define MAX_CODE_SIZE 1048576
-//#define STAND_ALONE
-
-static void* jit_compile_and_load(
-    const std::string filename,
-    const std::string flags) {
-  char libname[] = "/tmp/ppx_XXXXXX";
-  int fd = mkstemp(libname);
-  unlink(libname);
-  char fdname[50];
-  sprintf(fdname, "/proc/self/fd/%d", fd);
-  auto cmd = std::string("g++ -shared -fPIC -x c++ ") + flags;
-  cmd = cmd + " -o " + fdname + " " + filename;
-  printf("JIT COMPILE: %s\n", cmd.c_str());
-  int ret = system(cmd.c_str());
-  if (ret != 0)
-    return NULL;
-  auto handle = dlopen(fdname, RTLD_LAZY | RTLD_NODELETE);
-  if (!handle) {
-    fputs(dlerror(), stderr);
-    return NULL;
-  }
-  return handle;
-}
-
-static void* jit_from_file(
-    const std::string filename,
-    const std::string flags,
-    const std::string func_name) {
-  void* handle = jit_compile_and_load(filename, flags);
-  if (handle == NULL)
-    return NULL;
-  void* func = dlsym(handle, func_name.c_str());
-  if (func == NULL) {
-    printf("Unable to find '%s' symbol in JIT COMPILE\n", func_name.c_str());
-  }
-  dlclose(handle);
-  return func;
-}
-
-static void* jit_from_str(
-    const std::string src,
-    const std::string flags,
-    const std::string func_name) {
-  char filename[] = "/tmp/ppx_XXXXXX";
-  int fd = mkstemp(filename);
-  unlink(filename);
-  char fdname[50];
-  sprintf(fdname, "/proc/self/fd/%d", fd);
-  write(fd, src.c_str(), src.length());
-  return jit_from_file(fdname, flags, func_name);
-}
-
-typedef struct {
-  long start;
-  long end;
-  long step;
-  long block_size[8];
-} loop_rt_spec_t;
-
-typedef struct {
-  int idx_id;
-  char idx_name[256];
-  char start_var_name[256];
-  char end_var_name[256];
-  char step_var_name[256];
-  int jit_start;
-  int jit_step;
-  int jit_end;
-  int jit_block_sizes;
-  long start;
-  long end;
-  long step;
-  int pos_in_loopnest;
-  int is_parallelizable;
-  int is_blocked;
-  int is_blocked_outer;
-  long block_size[8];
-  int is_par_across_col_teams;
-  int is_par_across_row_teams;
-  int n_col_teams;
-  int n_row_teams;
-} loop_param_t;
-
-typedef struct {
-  char* buf;
-  int cur_nest_level;
-  int cur_pos;
-  int n_loops;
-  loop_param_t* loop_params;
-  int n_logical_loops;
-  char occurence_map[256];
-  int jit_loop_spec;
-  int use_2d_par;
-  int n_row_teams;
-  int n_col_teams;
-} loop_code;
-
-static loop_param_t find_loop_param_at_pos(loop_param_t* i_loop_params, int pos) {
-  loop_param_t res;
-  int i = 0;
-  int found = 0;
-  while (!found) {
-    if (i_loop_params[i].pos_in_loopnest == pos) {
-      found = 1;
-      res = i_loop_params[i];
-    } else {
-      i++;
-    }
-  }
-  return res;
-}
-
-static void add_buf_to_code(loop_code* i_code, char* buf) {
-  sprintf(i_code->buf + i_code->cur_pos, "%s", buf);
-  i_code->cur_pos += strlen(buf);
-}
-
-static void align_line(loop_code* i_code) {
-  char tmp_buf[512];
-  int i;
-  for (i = 0; i < 2 * i_code->cur_nest_level; i++) {
-    tmp_buf[i] = ' ';
-  }
-  tmp_buf[2 * i_code->cur_nest_level] = '\0';
-  add_buf_to_code(i_code, tmp_buf);
-  return;
-}
-
-static void increase_nest_level(loop_code* i_code) {
-  i_code->cur_nest_level = i_code->cur_nest_level + 1;
-  return;
-}
-
-static void decrease_nest_level(loop_code* i_code) {
-  i_code->cur_nest_level = i_code->cur_nest_level - 1;
-  return;
-}
-
-static void emit_parallel_for(loop_code* i_code, int collapse_level) {
-  char tmp_buf[512];
-  align_line(i_code);
-  if (collapse_level > 1) {
-    sprintf(tmp_buf, "#pragma omp for collapse(%d) nowait\n", collapse_level);
-  } else {
-    sprintf(tmp_buf, "#pragma omp for nowait\n");
-  }
-  add_buf_to_code(i_code, tmp_buf);
-  return;
-}
-
-static void emit_loop_header(loop_code* i_code) {
-  char tmp_buf[512];
-  align_line(i_code);
-  sprintf(tmp_buf, "#pragma omp parallel\n");
-}
-
-static void emit_parallel_region(loop_code* i_code) {
-  char tmp_buf[512];
-  align_line(i_code);
-  sprintf(tmp_buf, "#pragma omp parallel\n");
-  add_buf_to_code(i_code, tmp_buf);
-  align_line(i_code);
-  sprintf(tmp_buf, "{\n");
-  add_buf_to_code(i_code, tmp_buf);
-  increase_nest_level(i_code);
-  if (i_code->use_2d_par > 0) {
-    align_line(i_code);
-    sprintf(tmp_buf, "int tid = omp_get_thread_num();\n");
-    add_buf_to_code(i_code, tmp_buf);
-    align_line(i_code);
-    sprintf(tmp_buf, "int row_teams = %d;\n", i_code->n_row_teams);
-    add_buf_to_code(i_code, tmp_buf);
-    align_line(i_code);
-    sprintf(tmp_buf, "int col_teams = %d;\n", i_code->n_col_teams);
-    add_buf_to_code(i_code, tmp_buf);
-    align_line(i_code);
-    sprintf(tmp_buf, "int row_id = tid/col_teams;\n");
-    add_buf_to_code(i_code, tmp_buf);
-    align_line(i_code);
-    sprintf(tmp_buf, "int col_id = tid%%col_teams;\n");
-    add_buf_to_code(i_code, tmp_buf);
-    align_line(i_code);
-    sprintf(tmp_buf, "if (tid < row_teams * col_teams) {\n");
-    add_buf_to_code(i_code, tmp_buf);
-    increase_nest_level(i_code);
-  }
-  return;
-}
-
-static void close_parallel_region(loop_code* i_code) {
-  char tmp_buf[512];
-  decrease_nest_level(i_code);
-  align_line(i_code);
-  sprintf(tmp_buf, "}\n");
-  add_buf_to_code(i_code, tmp_buf);
-  return;
-}
-
-static void emit_loop_header(loop_code* i_code, loop_param_t* i_loop_param) {
-  char tmp_buf[512];
-  char str_idx[512];
-  char str_start[512];
-  char str_end[512];
-  char str_step[512];
-
-  if (strcmp(i_loop_param->idx_name, "") == 0) {
-    sprintf(str_idx, "i%d", i_loop_param->idx_id);
-  } else {
-    sprintf(str_idx, "%s", i_loop_param->idx_name);
-  }
-
-  if (strcmp(i_loop_param->start_var_name, "") == 0) {
-    sprintf(str_start, "%ld", i_loop_param->start);
-  } else {
-    sprintf(str_start, "%s", i_loop_param->start_var_name);
-  }
-
-  if (strcmp(i_loop_param->end_var_name, "") == 0) {
-    sprintf(str_end, "%ld", i_loop_param->end);
-  } else {
-    sprintf(str_end, "%s", i_loop_param->end_var_name);
-  }
-
-  if (strcmp(i_loop_param->step_var_name, "") == 0) {
-    sprintf(str_step, "%ld", i_loop_param->step);
-  } else {
-    sprintf(str_step, "%s", i_loop_param->step_var_name);
-  }
-
-  if ((i_loop_param->is_par_across_col_teams > 0) ||
-      (i_loop_param->is_par_across_row_teams > 0)) {
-    char prefix[16];
-    if (i_loop_param->is_par_across_col_teams > 0) {
-      sprintf(prefix, "col");
-    } else {
-      sprintf(prefix, "row");
-    }
-    align_line(i_code);
-    sprintf(
-        tmp_buf,
-        "int %s_tasks = ((%s) - (%s) + ((%s) - 1))/(%s);\n",
-        prefix,
-        str_end,
-        str_start,
-        str_step,
-        str_step);
-    add_buf_to_code(i_code, tmp_buf);
-    align_line(i_code);
-    sprintf(
-        tmp_buf,
-        "int %s_tasks_chunksize = (%s_tasks + %s_teams - 1)/%s_teams;\n",
-        prefix,
-        prefix,
-        prefix,
-        prefix);
-    add_buf_to_code(i_code, tmp_buf);
-    align_line(i_code);
-    sprintf(
-        tmp_buf,
-        "int my_%s_start = (%s_id * %s_tasks_chunksize < %s_tasks) ? %s + (%s_id * %s_tasks_chunksize) * %s : %s;\n",
-        prefix,
-        prefix,
-        prefix,
-        prefix,
-        str_start,
-        prefix,
-        prefix,
-        str_step,
-        str_end);
-    add_buf_to_code(i_code, tmp_buf);
-    align_line(i_code);
-    sprintf(
-        tmp_buf,
-        "int my_%s_end = ((%s_id+1) * %s_tasks_chunksize < %s_tasks) ? %s + ((%s_id+1) * %s_tasks_chunksize) * %s : %s;\n",
-        prefix,
-        prefix,
-        prefix,
-        prefix,
-        str_start,
-        prefix,
-        prefix,
-        str_step,
-        str_end);
-    add_buf_to_code(i_code, tmp_buf);
-    align_line(i_code);
-    sprintf(
-        tmp_buf,
-        "for (int %s = my_%s_start; %s < my_%s_end; %s += %s) {\n",
-        str_idx,
-        prefix,
-        str_idx,
-        prefix,
-        str_idx,
-        str_step);
-    add_buf_to_code(i_code, tmp_buf);
-    increase_nest_level(i_code);
-  } else {
-    align_line(i_code);
-    sprintf(
-        tmp_buf,
-        "for (int %s = %s; %s < %s; %s += %s) {\n",
-        str_idx,
-        str_start,
-        str_idx,
-        str_end,
-        str_idx,
-        str_step);
-    add_buf_to_code(i_code, tmp_buf);
-    increase_nest_level(i_code);
-  }
-
-  return;
-}
-
-static void emit_func_signature(
-    loop_code* i_code,
-    char* spec_func_name,
-    char* body_func_name,
-    char* init_func_name,
-    char* term_func_name) {
-  char tmp_buf[512];
-  // int i;
-  align_line(i_code);
-  sprintf(
-      tmp_buf,
-      "#include <omp.h>\nextern \"C\" void par_nested_loops(loop_rt_spec_t *%s, std::function<void(int *)> %s, std::function<void()> %s, std::function<void()> %s) {\n",
-      spec_func_name,
-      body_func_name,
-      init_func_name,
-      term_func_name);
-  add_buf_to_code(i_code, tmp_buf);
-  increase_nest_level(i_code);
-}
-
-static void emit_func_termination(loop_code* i_code) {
-  char tmp_buf[512];
-  decrease_nest_level(i_code);
-  align_line(i_code);
-  sprintf(tmp_buf, "}\n");
-  add_buf_to_code(i_code, tmp_buf);
-  if (i_code->use_2d_par > 0) {
-    decrease_nest_level(i_code);
-    align_line(i_code);
-    sprintf(tmp_buf, "}\n");
-    add_buf_to_code(i_code, tmp_buf);
-  }
-  return;
-}
-
-static void emit_void_function(loop_code* i_code, char* func_name) {
-  char tmp_buf[512];
-  align_line(i_code);
-  sprintf(tmp_buf, "if (%s) %s();\n", func_name, func_name);
-  add_buf_to_code(i_code, tmp_buf);
-  return;
-}
-
-static void emit_loop_body(loop_code* i_code, char* body_func_name) {
-  char tmp_buf[512];
-  int i;
-  align_line(i_code);
-  sprintf(tmp_buf, "int idx[%d];\n", i_code->n_logical_loops);
-  add_buf_to_code(i_code, tmp_buf);
-  /* Here we set the idx array to be used by function called */
-  for (i = 0; i < i_code->n_logical_loops; i++) {
-    char str_idx[64];
-    sprintf(str_idx, "%c%d", 'a' + i, i_code->occurence_map['a' + i] - 1);
-    align_line(i_code);
-    sprintf(tmp_buf, "idx[%d] = %s;\n", i, str_idx);
-    add_buf_to_code(i_code, tmp_buf);
-  }
-  align_line(i_code);
-  sprintf(tmp_buf, "%s(idx);\n", body_func_name);
-  add_buf_to_code(i_code, tmp_buf);
-  return;
-}
-
-static void emit_loop_termination(loop_code* i_code) {
-  char tmp_buf[512];
-  decrease_nest_level(i_code);
-  align_line(i_code);
-  sprintf(tmp_buf, "}\n");
-  add_buf_to_code(i_code, tmp_buf);
-  return;
-}
-
-static void emit_barrier(loop_code* i_code) {
-  char tmp_buf[512];
-  align_line(i_code);
-  sprintf(tmp_buf, "#pragma omp barrier\n");
-  add_buf_to_code(i_code, tmp_buf);
-  return;
-}
-
-static void set_loop_param(
-    loop_param_t* io_param,
-    const char* idx_name,
-    const char* s_name,
-    const char* e_name,
-    const char* step_name,
-    int pos) {
-  io_param->pos_in_loopnest = pos;
-  sprintf(io_param->idx_name, "%s", idx_name);
-  sprintf(io_param->start_var_name, "%s", s_name);
-  sprintf(io_param->end_var_name, "%s", e_name);
-  sprintf(io_param->step_var_name, "%s", step_name);
-  return;
-}
-
-static int is_simple_char(char cur) {
-  int result = 0;
-  if ((cur >= 'a' && cur <= 'z') || (cur >= 'A' && cur <= 'Z') ||
-      (cur == '|')) {
-    result = 1;
-  }
-  return result;
-}
-
-static void parse_jit_info(char* jit_info_str, loop_param_t* loop_param) {
-  char cur_token[512];
-  char token_start[512];
-  char token_end[512];
-  char token_step[512];
-  char token_bs[512];
-  int i = 0;
-  int j = 0;
-  int token_id = 0;
-  char* bs_str;
-  int bs_index = 0;
-
-  /* First extract the BS token */
-  while (jit_info_str[i] != '(') {
-    i++;
-  }
-  jit_info_str[i] = '\0';
-  i++;
-  while (jit_info_str[i] != ')') {
-    token_bs[j] = jit_info_str[i];
-    j++;
-    i++;
-  }
-  token_bs[j] = '\0';
-
-  /* Now extract rest token */
-  i = 0;
-  j = 0;
-  while (jit_info_str[i] != '\0') {
-    if (jit_info_str[i] == ',') {
-      if (i == 0) {
-        /* Empty token */
-        if (token_id == 0) {
-          sprintf(token_start, "");
-        } else if (token_id == 1) {
-          sprintf(token_end, "");
-        } else if (token_id == 2) {
-          sprintf(token_step, "");
-        }
-        token_id++;
-      } else if (jit_info_str[i - 1] == ',') {
-        /* Empty token */
-        if (token_id == 0) {
-          sprintf(token_start, "");
-        } else if (token_id == 1) {
-          sprintf(token_end, "");
-        } else if (token_id == 2) {
-          sprintf(token_step, "");
-        }
-        token_id++;
-      } else {
-        /* Finalize current token */
-        cur_token[j] = '\0';
-        j = 0;
-        if (token_id == 0) {
-          sprintf(token_start, "%s", cur_token);
-        } else if (token_id == 1) {
-          sprintf(token_end, "%s", cur_token);
-        } else if (token_id == 2) {
-          sprintf(token_step, "%s", cur_token);
-        }
-        token_id++;
-      }
-    } else {
-      cur_token[j] = jit_info_str[i];
-      j++;
-    }
-    i++;
-  }
-
-  /* Now based on token parse info... */
-  if (strlen(token_start) > 0) {
-    loop_param->jit_start = 1;
-    loop_param->start = atoi(token_start);
-  }
-
-  if (strlen(token_end) > 0) {
-    loop_param->jit_end = 1;
-    loop_param->end = atoi(token_end);
-  }
-
-  if (strlen(token_step) > 0) {
-    loop_param->jit_step = 1;
-    loop_param->step = atoi(token_step);
-  }
-
-  if (strlen(token_bs) > 0) {
-    bs_str = strtok(token_bs, ",");
-    bs_index = 0;
-    while (bs_str != NULL) {
-      loop_param->jit_block_sizes = 1;
-      loop_param->block_size[bs_index] = atoi(bs_str);
-      bs_index++;
-      bs_str = strtok(NULL, ",");
-    }
-  }
-}
-
-static void extract_jit_info(
-    const char* in_desc,
-    char* out_desc,
-    loop_param_t* loop_params) {
-  int i = 0, k = 0;
-  char jit_params_str[512];
-  char loop_id;
-
-  while (i < strlen(in_desc)) {
-    char cur = in_desc[i];
-    if (is_simple_char(cur)) {
-      out_desc[k] = cur;
-      k++;
-      i++;
-      if (cur != '|') {
-        loop_id = tolower(cur);
-      }
-    } else {
-      /* Start reading specs string [ .. ] */
-      if (cur == '[') {
-        int j = 0;
-        while (cur != ']') {
-          i++;
-          cur = in_desc[i];
-          if (cur != ']') {
-            jit_params_str[j] = cur;
-            j++;
-          } else {
-            i++;
-          }
-        }
-        jit_params_str[j] = '\0';
-        parse_jit_info(jit_params_str, &loop_params[loop_id - 'a']);
-      }
-    }
-  }
-  out_desc[k] = '\0';
-}
-
-static void extract_2d_par_info(
-    const char* in_desc,
-    char* out_desc,
-    loop_param_t* loop_params,
-    loop_code* i_code) {
-  int i = 0, k = 0;
-  char jit_params_str[512];
-  char loop_id = 0;
-
-  while (i < strlen(in_desc)) {
-    char cur = in_desc[i];
-    if (cur != '{') {
-      out_desc[k] = cur;
-      k++;
-      i++;
-      if (is_simple_char(cur) && cur != '|') {
-        loop_id++;
-      }
-    } else {
-      /* Start reading parallelization string {R or C:parallelization degree}]
-       */
-      int j = 0;
-      i++;
-      /* Consume par dimension */
-      cur = in_desc[i];
-      if (cur == 'R' || cur == 'r') {
-        loop_params[loop_id - 1].is_par_across_row_teams = 1;
-        loop_params[loop_id - 1].is_par_across_col_teams = 0;
-      } else if (cur == 'C' || cur == 'c') {
-        loop_params[loop_id - 1].is_par_across_col_teams = 1;
-        loop_params[loop_id - 1].is_par_across_row_teams = 0;
-      }
-      /* Consume :  */
-      i += 2;
-      cur = in_desc[i];
-      while (cur != '}') {
-        jit_params_str[j] = cur;
-        j++;
-        i++;
-        cur = in_desc[i];
-      }
-      i++;
-      jit_params_str[j] = '\0';
-      if (loop_params[loop_id - 1].is_par_across_row_teams == 1) {
-        loop_params[loop_id - 1].n_row_teams = atoi(jit_params_str);
-        i_code->n_row_teams = loop_params[loop_id - 1].n_row_teams;
-      } else {
-        loop_params[loop_id - 1].n_col_teams = atoi(jit_params_str);
-        i_code->n_col_teams = loop_params[loop_id - 1].n_col_teams;
-      }
-    }
-  }
-  out_desc[k] = '\0';
-}
-
-// void loop_generator( FILE *fp_out, const char *__loop_nest_desc_extended ) {
-static std::string loop_generator(const char* __loop_nest_desc_extended) {
-  char body_func_name[64] = "body_func";
-  char init_func_name[64] = "init_func";
-  char term_func_name[64] = "term_func";
-  char spec_func_name[64] = "loop_rt_spec";
-  char loop_map[256];
-  char occurence_map[256];
-  loop_code l_code;
-  char* result_code;
-  loop_param_t loop_params[256], cur_loop, loop_params_map[256];
-  int n_loops, n_logical_loops, i, k, have_emitted_parallel_for = 0,
-                                      n_parallel_loops = 0;
-  char loop_nest_desc[256];
-  char barrier_positions[256];
-  int jit_loop_spec = 0;
-  int use_2d_par = 0;
-  char _loop_nest_desc_extended[strlen(__loop_nest_desc_extended)];
-  char loop_nest_desc_extended[strlen(_loop_nest_desc_extended)];
-
-  /* Extract explicit 2D parallelization info */
-  for (i = 0; i < strlen(__loop_nest_desc_extended); i++) {
-    if (__loop_nest_desc_extended[i] == '{') {
-      use_2d_par = 1;
-      break;
-    }
-  }
-  l_code.use_2d_par = use_2d_par;
-  if (use_2d_par > 0) {
-    l_code.n_col_teams = 1;
-    l_code.n_row_teams = 1;
-    extract_2d_par_info(
-        __loop_nest_desc_extended,
-        _loop_nest_desc_extended,
-        loop_params,
-        &l_code);
-  } else {
-    strcpy(_loop_nest_desc_extended, __loop_nest_desc_extended);
-  }
-
-  /* Check if we have to jit the loop specs  */
-  for (i = 0; i < strlen(_loop_nest_desc_extended); i++) {
-    if (_loop_nest_desc_extended[i] == '[') {
-      jit_loop_spec = 1;
-      break;
-    }
-  }
-  l_code.jit_loop_spec = jit_loop_spec;
-
-  memset(loop_params_map, 0, 256 * sizeof(loop_param_t));
-  if (jit_loop_spec > 0) {
-    extract_jit_info(
-        _loop_nest_desc_extended, loop_nest_desc_extended, loop_params_map);
-  } else {
-    strcpy(loop_nest_desc_extended, _loop_nest_desc_extended);
-  }
-
-  /* Cleanup input descriptor to exclude barriers */
-  k = 0;
-  memset(barrier_positions, 0, 256);
-  for (i = 0; i < strlen(loop_nest_desc_extended); i++) {
-    if (loop_nest_desc_extended[i] == '|') {
-      if (k - 1 >= 0) {
-        barrier_positions[k - 1] = 1;
-      }
-    } else {
-      loop_nest_desc[k] = loop_nest_desc_extended[i];
-      k++;
-    }
-  }
-  loop_nest_desc[k] = '\0';
-
-  n_loops = strlen(loop_nest_desc);
-  result_code = (char*)malloc(MAX_CODE_SIZE * sizeof(char));
-
-  l_code.buf = result_code;
-  l_code.cur_nest_level = 0;
-  l_code.n_loops = n_loops;
-  l_code.loop_params = loop_params;
-  l_code.cur_pos = 0;
-
-  /* Find number of parallel loops */
-  for (i = 0; i < n_loops; i++) {
-    if (tolower(loop_nest_desc[i]) != loop_nest_desc[i]) {
-      n_parallel_loops++;
-    }
-  }
-
-  /* Count how many times each loop occurs (lower case and upper case are
-   * equivalent for that matter) */
-  memset(loop_map, 0, 256 * sizeof(char));
-  for (i = 0; i < n_loops; i++) {
-    loop_map[tolower(loop_nest_desc[i])]++;
-  }
-
-  /* Set up loop properties */
-  memset(occurence_map, 0, 256 * sizeof(char));
-  for (i = 0; i < n_loops; i++) {
-    int is_blocked = (loop_map[tolower(loop_nest_desc[i])] > 1) ? 1 : 0;
-    int is_parallelizable =
-        (tolower(loop_nest_desc[i]) != loop_nest_desc[i]) ? 1 : 0;
-    int occurence_id, is_blocked_outer;
-    char idx_name[16];
-    char spec_array_name[512];
-    char start_var_name[512];
-    char end_var_name[512];
-    char step_var_name[512];
-    int loop_abs_index = tolower(loop_nest_desc[i]) - 'a';
-
-    occurence_id = occurence_map[tolower(loop_nest_desc[i])];
-    is_blocked_outer = (occurence_id == 0) ? 1 : 0;
-    occurence_map[tolower(loop_nest_desc[i])]++;
-
-    sprintf(spec_array_name, "%s", spec_func_name);
-
-    sprintf(idx_name, "%c%d", tolower(loop_nest_desc[i]), occurence_id);
-
-    if (occurence_id == 0) {
-      if (loop_params_map[loop_abs_index].jit_start > 0) {
-        sprintf(start_var_name, "%d", loop_params_map[loop_abs_index].start);
-      } else {
-        sprintf(
-            start_var_name, "%s[%d].start", spec_array_name, loop_abs_index);
-      }
-    } else {
-      sprintf(
-          start_var_name, "%c%d", tolower(loop_nest_desc[i]), occurence_id - 1);
-    }
-
-    if (occurence_id == 0) {
-      if (loop_params_map[loop_abs_index].jit_end > 0) {
-        sprintf(end_var_name, "%d", loop_params_map[loop_abs_index].end);
-      } else {
-        sprintf(end_var_name, "%s[%d].end", spec_array_name, loop_abs_index);
-      }
-    } else {
-      if (loop_params_map[loop_abs_index].jit_block_sizes > 0) {
-        sprintf(
-            end_var_name,
-            "%c%d + %d",
-            tolower(loop_nest_desc[i]),
-            occurence_id - 1,
-            loop_params_map[loop_abs_index].block_size[occurence_id - 1]);
-      } else {
-        sprintf(
-            end_var_name,
-            "%c%d + %s[%d].block_size[%d]",
-            tolower(loop_nest_desc[i]),
-            occurence_id - 1,
-            spec_array_name,
-            loop_abs_index,
-            occurence_id - 1);
-      }
-    }
-
-    if (is_blocked) {
-      if (occurence_id == loop_map[tolower(loop_nest_desc[i])] - 1) {
-        if (loop_params_map[loop_abs_index].jit_step > 0) {
-          sprintf(step_var_name, "%d", loop_params_map[loop_abs_index].step);
-        } else {
-          sprintf(
-              step_var_name, "%s[%d].step", spec_array_name, loop_abs_index);
-        }
-      } else {
-        if (loop_params_map[loop_abs_index].jit_block_sizes > 0) {
-          sprintf(
-              step_var_name,
-              "%d",
-              loop_params_map[loop_abs_index].block_size[occurence_id]);
-        } else {
-          sprintf(
-              step_var_name,
-              "%s[%d].block_size[%d]",
-              spec_array_name,
-              loop_abs_index,
-              occurence_id);
-        }
-      }
-    } else {
-      if (loop_params_map[loop_abs_index].jit_step > 0) {
-        sprintf(step_var_name, "%d", loop_params_map[loop_abs_index].step);
-      } else {
-        sprintf(step_var_name, "%s[%d].step", spec_array_name, loop_abs_index);
-      }
-    }
-
-    set_loop_param(
-        &loop_params[i],
-        idx_name,
-        start_var_name,
-        end_var_name,
-        step_var_name,
-        i);
-    loop_params[i].is_parallelizable = is_parallelizable;
-    loop_params[i].is_blocked = is_blocked;
-    loop_params[i].is_blocked_outer = is_blocked_outer;
-  }
-
-  /* Setup number of logical loops and the ocurence map */
-  n_logical_loops = 0;
-  for (i = 0; i < 256; i++) {
-    if (occurence_map[i] > 0) {
-      n_logical_loops++;
-    }
-  }
-  l_code.n_logical_loops = n_logical_loops;
-
-  memcpy(&l_code.occurence_map[0], occurence_map, 256);
-
-  /* Emit function signature  */
-  emit_func_signature(
-      &l_code, spec_func_name, body_func_name, init_func_name, term_func_name);
-
-  /* Emit loop function header */
-  if (n_parallel_loops > 0) {
-    emit_parallel_region(&l_code);
-  }
-
-  /* Emit init function */
-  emit_void_function(&l_code, init_func_name);
-
-  for (i = 0; i < n_loops; i++) {
-    cur_loop = loop_params[i];
-    /* Emit parallel for if need be*/
-    if ((cur_loop.is_parallelizable == 1) && (have_emitted_parallel_for == 0) &&
-        (cur_loop.is_par_across_col_teams == 0) &&
-        (cur_loop.is_par_across_row_teams == 0)) {
-      int collapse_level = 1;
-      int j = i + 1;
-      int is_parallel = 1;
-      while ((is_parallel > 0) && (j < n_loops)) {
-        loop_param_t tmp_loop = loop_params[j];
-        if (tmp_loop.is_parallelizable > 0) {
-          collapse_level++;
-          j++;
-        } else {
-          is_parallel = 0;
-        }
-      }
-      emit_parallel_for(&l_code, collapse_level);
-      have_emitted_parallel_for = 1;
-    }
-    emit_loop_header(&l_code, &cur_loop);
-  }
-
-  emit_loop_body(&l_code, body_func_name);
-
-  for (i = n_loops - 1; i >= 0; i--) {
-    emit_loop_termination(&l_code);
-    if (barrier_positions[i] > 0) {
-      emit_barrier(&l_code);
-    }
-  }
-
-  /* Emit term function */
-  emit_void_function(&l_code, term_func_name);
-
-  if (n_parallel_loops > 0) {
-    close_parallel_region(&l_code);
-  }
-
-  emit_func_termination(&l_code);
-
-  // fprintf(fp_out, "%s", result_code);
-  // fprintf(stderr, "%s", result_code);
-  std::string outstr = std::string(result_code);
-
-  if (result_code)
-    free(result_code);
-
-  return outstr;
-}
-
-#endif
\ No newline at end of file
diff --git a/csrc/cpu/tpp/woq/threaded_loops.h b/csrc/cpu/tpp/woq/threaded_loops.h
deleted file mode 100644
index e7bd6c83..00000000
--- a/csrc/cpu/tpp/woq/threaded_loops.h
+++ /dev/null
@@ -1,616 +0,0 @@
-/******************************************************************************
- * Copyright (c) 2022 Intel Corporation - All rights reserved.                *
- *                                                                            *
- * For information on the license, see the LICENSE file.                      *
- * Further information: https://github.com/libxsmm/tpp-pytorch-extension/     *
- * SPDX-License-Identifier: BSD-3-Clause                                      *
- ******************************************************************************/
-/* Author: Dhiraj Kalamkar (Intel Corp.)
- ******************************************************************************/
-
-#ifndef _TPP_THREADED_LOOPS_H_
-#define _TPP_THREADED_LOOPS_H_
-
-#include <stdio.h>
-#include <array>
-#include <cassert>
-#include <fstream>
-#include <functional>
-#include <initializer_list>
-#include <iostream>
-#include <sstream>
-#include <string>
-#include <unordered_map>
-#include "par_loop_generator.h"
-
-typedef std::function<void()> init_func;
-typedef std::function<void()> fini_func;
-typedef std::function<void(int*)> loop_func;
-
-constexpr int MAX_BLOCKING_LEVELS = 5;
-constexpr int MAX_LOGICAL_LOOPS = 10;
-constexpr int MAX_LOOPS = MAX_LOGICAL_LOOPS * MAX_BLOCKING_LEVELS;
-
-class LoopSpecs {
- public:
-  LoopSpecs(long end, std::initializer_list<long> block_sizes = {})
-      : LoopSpecs(0L, end, 1L, block_sizes) {}
-  LoopSpecs(
-      long end,
-      bool isParallel,
-      std::initializer_list<long> block_sizes = {})
-      : LoopSpecs(0L, end, 1L, isParallel, block_sizes) {}
-  LoopSpecs(long start, long end, std::initializer_list<long> block_sizes = {})
-      : LoopSpecs(start, end, 1L, block_sizes) {}
-  LoopSpecs(
-      long start,
-      long end,
-      bool isParallel,
-      std::initializer_list<long> block_sizes = {})
-      : LoopSpecs(start, end, 1L, isParallel, block_sizes) {}
-  LoopSpecs(
-      long start,
-      long end,
-      long step,
-      std::initializer_list<long> block_sizes = {})
-      : LoopSpecs(start, end, step, true, block_sizes) {}
-  LoopSpecs(
-      long start,
-      long end,
-      long step,
-      bool isParallel,
-      std::initializer_list<long> block_sizes = {})
-      : start(start),
-        end(end),
-        step(step),
-        isParallel(isParallel),
-        nBlockingLevels(block_sizes.size()),
-        block_size{0} {
-    assert(nBlockingLevels <= MAX_BLOCKING_LEVELS);
-    int i = 0;
-    for (auto x : block_sizes)
-      block_size[i++] = x;
-  }
-  long start;
-  long end;
-  long step;
-  bool isParallel;
-  long nBlockingLevels;
-  long block_size[MAX_BLOCKING_LEVELS];
-};
-
-typedef void (*par_loop_kernel)(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)>,
-    std::function<void()>,
-    std::function<void()>);
-
-#if 0
-void par_nested_loops(LoopSpecs *loopSpecs, std::function<void(int*)> body_func, std::function<void()> init_func, std::function<void()> fini_func)
-{
-#pragma omp parallel
-  {
-    if (init_func) init_func();
-    for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end; a0 += loopSpecs[0].step) {
-#pragma omp for collapse(2) nowait
-      for (int b0 = loopSpecs[1].start; b0 < loopSpecs[1].end; b0 += loopSpecs[1].step) {
-        for (int c0 = loopSpecs[2].start; c0 < loopSpecs[2].end; c0 += loopSpecs[2].step) {
-          int ind[3] = {a0, b0, c0};
-          body_func(ind);
-        }
-      }
-    }
-    if (fini_func) fini_func();
-  }
-}
-#endif
-
-static void par_nested_loops_A(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)> body_func,
-    std::function<void()> init_func,
-    std::function<void()> fini_func) {
-#pragma omp parallel
-  {
-    if (init_func)
-      init_func();
-#pragma omp for nowait
-    for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end;
-         a0 += loopSpecs[0].step) {
-      int ind[1] = {a0};
-      body_func(ind);
-    }
-    if (fini_func)
-      fini_func();
-  }
-}
-
-static void par_nested_loops_AB(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)> body_func,
-    std::function<void()> init_func,
-    std::function<void()> fini_func) {
-#pragma omp parallel
-  {
-    if (init_func)
-      init_func();
-#pragma omp for collapse(2) nowait
-    for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end;
-         a0 += loopSpecs[0].step) {
-      for (int b0 = loopSpecs[1].start; b0 < loopSpecs[1].end;
-           b0 += loopSpecs[1].step) {
-        int ind[2] = {a0, b0};
-        body_func(ind);
-      }
-    }
-    if (fini_func)
-      fini_func();
-  }
-}
-
-static void par_nested_loops_aB(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)> body_func,
-    std::function<void()> init_func,
-    std::function<void()> fini_func) {
-#pragma omp parallel
-  {
-    if (init_func)
-      init_func();
-    for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end;
-         a0 += loopSpecs[0].step) {
-#pragma omp for nowait
-      for (int b0 = loopSpecs[1].start; b0 < loopSpecs[1].end;
-           b0 += loopSpecs[1].step) {
-        int ind[2] = {a0, b0};
-        body_func(ind);
-      }
-    }
-    if (fini_func)
-      fini_func();
-  }
-}
-
-static void par_nested_loops_bA(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)> body_func,
-    std::function<void()> init_func,
-    std::function<void()> fini_func) {
-#pragma omp parallel
-  {
-    if (init_func)
-      init_func();
-    for (int b0 = loopSpecs[1].start; b0 < loopSpecs[1].end;
-         b0 += loopSpecs[1].step) {
-#pragma omp for nowait
-      for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end;
-           a0 += loopSpecs[0].step) {
-        int ind[2] = {a0, b0};
-        body_func(ind);
-      }
-    }
-    if (fini_func)
-      fini_func();
-  }
-}
-
-static void par_nested_loops_BA(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)> body_func,
-    std::function<void()> init_func,
-    std::function<void()> fini_func) {
-#pragma omp parallel
-  {
-    if (init_func)
-      init_func();
-#pragma omp for collapse(2) nowait
-    for (int b0 = loopSpecs[1].start; b0 < loopSpecs[1].end;
-         b0 += loopSpecs[1].step) {
-      for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end;
-           a0 += loopSpecs[0].step) {
-        int ind[2] = {a0, b0};
-        body_func(ind);
-      }
-    }
-    if (fini_func)
-      fini_func();
-  }
-}
-
-static void par_nested_loops_ABC(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)> body_func,
-    std::function<void()> init_func,
-    std::function<void()> fini_func) {
-#pragma omp parallel
-  {
-    if (init_func)
-      init_func();
-#pragma omp for collapse(3) nowait
-    for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end;
-         a0 += loopSpecs[0].step) {
-      for (int b0 = loopSpecs[1].start; b0 < loopSpecs[1].end;
-           b0 += loopSpecs[1].step) {
-        for (int c0 = loopSpecs[2].start; c0 < loopSpecs[2].end;
-             c0 += loopSpecs[2].step) {
-          int ind[3] = {a0, b0, c0};
-          body_func(ind);
-        }
-      }
-    }
-    if (fini_func)
-      fini_func();
-  }
-}
-
-static void par_nested_loops_aBC(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)> body_func,
-    std::function<void()> init_func,
-    std::function<void()> fini_func) {
-#pragma omp parallel
-  {
-    if (init_func)
-      init_func();
-    for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end;
-         a0 += loopSpecs[0].step) {
-#pragma omp for collapse(2) nowait
-      for (int b0 = loopSpecs[1].start; b0 < loopSpecs[1].end;
-           b0 += loopSpecs[1].step) {
-        for (int c0 = loopSpecs[2].start; c0 < loopSpecs[2].end;
-             c0 += loopSpecs[2].step) {
-          int ind[3] = {a0, b0, c0};
-          body_func(ind);
-        }
-      }
-    }
-    if (fini_func)
-      fini_func();
-  }
-}
-
-static void par_nested_loops_acB(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)> body_func,
-    std::function<void()> init_func,
-    std::function<void()> fini_func) {
-#pragma omp parallel
-  {
-    if (init_func)
-      init_func();
-    for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end;
-         a0 += loopSpecs[0].step) {
-      for (int c0 = loopSpecs[2].start; c0 < loopSpecs[2].end;
-           c0 += loopSpecs[2].step) {
-#pragma omp for nowait
-        for (int b0 = loopSpecs[1].start; b0 < loopSpecs[1].end;
-             b0 += loopSpecs[1].step) {
-          int ind[3] = {a0, b0, c0};
-          body_func(ind);
-        }
-      }
-    }
-    if (fini_func)
-      fini_func();
-  }
-}
-
-static void par_nested_loops_ABc(LoopSpecs *loop_rt_spec, std::function<void(int *)> body_func, std::function<void()> init_func, std::function<void()> term_func) {
-  #pragma omp parallel
-  {
-    if (init_func) init_func();
-    #pragma omp for collapse(2) nowait
-    for (int a0 = loop_rt_spec[0].start; a0 < loop_rt_spec[0].end; a0 += loop_rt_spec[0].step) {
-      for (int b0 = loop_rt_spec[1].start; b0 < loop_rt_spec[1].end; b0 += loop_rt_spec[1].step) {
-        for (int c0 = loop_rt_spec[2].start; c0 < loop_rt_spec[2].end; c0 += loop_rt_spec[2].step) {
-          int idx[3];
-          idx[0] = a0;
-          idx[1] = b0;
-          idx[2] = c0;
-          body_func(idx);
-        }
-      }
-    }
-    if (term_func) term_func();
-  }
-}
-
-static void par_nested_loops_aCb(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)> body_func,
-    std::function<void()> init_func,
-    std::function<void()> fini_func) {
-#pragma omp parallel
-  {
-    if (init_func)
-      init_func();
-    for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end;
-         a0 += loopSpecs[0].step) {
-#pragma omp for nowait
-      for (int c0 = loopSpecs[2].start; c0 < loopSpecs[2].end;
-           c0 += loopSpecs[2].step) {
-        for (int b0 = loopSpecs[1].start; b0 < loopSpecs[1].end;
-             b0 += loopSpecs[1].step) {
-          int ind[3] = {a0, b0, c0};
-          body_func(ind);
-        }
-      }
-    }
-    if (fini_func)
-      fini_func();
-  }
-}
-
-static void par_nested_loops_aCB(
-    LoopSpecs* loopSpecs,
-    std::function<void(int*)> body_func,
-    std::function<void()> init_func,
-    std::function<void()> fini_func) {
-#pragma omp parallel
-  {
-    if (init_func)
-      init_func();
-    for (int a0 = loopSpecs[0].start; a0 < loopSpecs[0].end;
-         a0 += loopSpecs[0].step) {
-#pragma omp for collapse(2) nowait
-      for (int c0 = loopSpecs[2].start; c0 < loopSpecs[2].end;
-           c0 += loopSpecs[2].step) {
-        for (int b0 = loopSpecs[1].start; b0 < loopSpecs[1].end;
-             b0 += loopSpecs[1].step) {
-          int ind[3] = {a0, b0, c0};
-          body_func(ind);
-        }
-      }
-    }
-    if (fini_func)
-      fini_func();
-  }
-}
-
-static void par_nested_loops_CAB(
-  LoopSpecs *loop_rt_spec,
-  std::function<void(int *)> body_func,
-  std::function<void()> init_func,
-  std::function<void()> term_func
-) {
-  #pragma omp parallel
-  {
-    if (init_func) init_func();
-    #pragma omp for collapse(3) nowait
-    for (int c0 = loop_rt_spec[2].start; c0 < loop_rt_spec[2].end; c0 += loop_rt_spec[2].step) {
-      for (int a0 = loop_rt_spec[0].start; a0 < loop_rt_spec[0].end; a0 += loop_rt_spec[0].step) {
-        for (int b0 = loop_rt_spec[1].start; b0 < loop_rt_spec[1].end; b0 += loop_rt_spec[1].step) {
-          int idx[3];
-          idx[0] = a0;
-          idx[1] = b0;
-          idx[2] = c0;
-          body_func(idx);
-        }
-      }
-    }
-    if (term_func) term_func();
-  }
-}
-
-static void par_nested_loops_ACb(
-  LoopSpecs *loop_rt_spec,
-  std::function<void(int *)> body_func,
-  std::function<void()> init_func,
-  std::function<void()> term_func
-) {
-  #pragma omp parallel
-  {
-    if (init_func) init_func();
-    #pragma omp for collapse(2) nowait
-    for (int a0 = loop_rt_spec[0].start; a0 < loop_rt_spec[0].end; a0 += loop_rt_spec[0].step) {
-      for (int c0 = loop_rt_spec[2].start; c0 < loop_rt_spec[2].end; c0 += loop_rt_spec[2].step) {
-        for (int b0 = loop_rt_spec[1].start; b0 < loop_rt_spec[1].end; b0 += loop_rt_spec[1].step) {
-          int idx[3];
-          idx[0] = a0;
-          idx[1] = b0;
-          idx[2] = c0;
-          body_func(idx);
-        }
-      }
-    }
-    if (term_func) term_func();
-  }
-}
-
-class LoopingScheme {
- public:
-  LoopingScheme(std::string scheme)
-      : scheme(scheme),
-        nLogicalLoops(0),
-        nLoops(0),
-        barrierAfter(0),
-        ompforBefore(-1),
-        nCollapsed(0),
-        nLLBL{0},
-        test_kernel(NULL) {
-    static std::unordered_map<std::string, par_loop_kernel> pre_defined_loops = {
-        {"A", par_nested_loops_A},
-        {"AB", par_nested_loops_AB},
-        {"BA", par_nested_loops_BA},
-        {"bA", par_nested_loops_bA},
-        {"aB", par_nested_loops_aB},
-        {"ABC", par_nested_loops_ABC},
-        {"aBC", par_nested_loops_aBC},
-        {"acB", par_nested_loops_acB},
-        {"aCb", par_nested_loops_aCb},
-        {"aCB", par_nested_loops_aCB},
-        {"ABc", par_nested_loops_ABc},
-        {"CAB", par_nested_loops_CAB},
-        {"ACb", par_nested_loops_ACb}
-    };
-
-    static std::string code_str = R"(
-    #include <stdio.h>
-    #include <cassert>
-    #include <functional>
-    #include <initializer_list>
-    #include <string>
-
-    constexpr int MAX_BLOCKING_LEVELS = 5;
-    class LoopSpecs {
-    public:
-      LoopSpecs(long end, std::initializer_list<long> block_sizes = {}) : LoopSpecs(0L, end, 1L, block_sizes) {}
-      LoopSpecs(long end, bool isParallel, std::initializer_list<long> block_sizes = {}) : LoopSpecs(0L, end, 1L, isParallel, block_sizes) {}
-      LoopSpecs(long start, long end, std::initializer_list<long> block_sizes = {}) : LoopSpecs(start, end, 1L, block_sizes) {}
-      LoopSpecs(long start, long end, bool isParallel, std::initializer_list<long> block_sizes = {}) : LoopSpecs(start, end, 1L, isParallel, block_sizes) {}
-      LoopSpecs(long start, long end, long step, std::initializer_list<long> block_sizes = {}) :  LoopSpecs(start, end, step, true, block_sizes) {}
-      LoopSpecs(long start, long end, long step, bool isParallel, std::initializer_list<long> block_sizes = {}) : start(start), end(end), step(step), isParallel(isParallel), nBlockingLevels(block_sizes.size()), block_size{0} {
-        assert(nBlockingLevels <= MAX_BLOCKING_LEVELS);
-        int i = 0;
-        for (auto x : block_sizes) block_size[i++] = x;
-      }
-      long start;
-      long end;
-      long step;
-      bool isParallel;
-      long nBlockingLevels;
-      long block_size[MAX_BLOCKING_LEVELS];
-    };
-
-    using loop_rt_spec_t = LoopSpecs;
-
-    )";
-
-    int curLoop = 0;
-    for (int i = 0; i < (int)scheme.length(); i++) {
-      char c = scheme[i];
-      if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {
-        int l;
-        assert(curLoop < MAX_LOOPS);
-        if ((i >= 1) && (scheme[i - 1] == '{')) {
-          printf(
-              "LoopingScheme: '%s': Ignoring unknown scheme character: '%c' at position %d\n",
-              scheme.c_str(),
-              scheme[i],
-              i);
-        } else {
-          if (c >= 'a' && c <= 'z') {
-            isParallel[curLoop] = false;
-            l = c - 'a';
-          } else {
-            isParallel[curLoop] = true;
-            l = c - 'A';
-            if (ompforBefore == -1)
-              ompforBefore = curLoop;
-            if (ompforBefore + nCollapsed == curLoop)
-              nCollapsed++;
-          }
-          p2lMap[curLoop] = l;
-          curLoop++;
-        }
-      } else if (c == '|') {
-        barrierAfter = curLoop;
-      } else {
-        printf(
-            "LoopingScheme: '%s': Ignoring unknown scheme character: '%c' at position %d\n",
-            scheme.c_str(),
-            scheme[i],
-            i);
-      }
-    }
-    nLoops = curLoop;
-    for (int i = 0; i < nLoops; i++) {
-      int ll = p2lMap[i];
-      assert(ll < MAX_LOGICAL_LOOPS);
-      if (nLogicalLoops <= ll)
-        nLogicalLoops = ll + 1;
-      nLLBL[ll]++;
-    }
-    for (int i = 0; i < nLogicalLoops; i++) {
-      assert(nLLBL[i] > 0);
-    }
-    auto search = pre_defined_loops.find(scheme);
-    if (search != pre_defined_loops.end()) {
-      test_kernel = search->second;
-    } else {
-      std::string gen_code = loop_generator(scheme.c_str());
-      std::ofstream ofs("debug.cpp", std::ofstream::out);
-      ofs << code_str + gen_code;
-      ofs.close();
-      std::cout << "Scheme: " << scheme << std::endl;
-      std::cout << "Generated code:" << std::endl << gen_code;
-
-      test_kernel = (par_loop_kernel)jit_from_str(
-          code_str + gen_code, " -fopenmp ", "par_nested_loops");
-    }
-  }
-
-  void call(
-      LoopSpecs* loopSpecs,
-      std::function<void(int*)> body_func,
-      std::function<void()> init_func,
-      std::function<void()> fini_func) {
-    test_kernel(loopSpecs, body_func, init_func, fini_func);
-  }
-
-  const std::string getKernelCode() {
-    return "test";
-  }
-  std::string scheme;
-  int nLogicalLoops;
-  int nLoops;
-  int barrierAfter;
-  int ompforBefore;
-  int nCollapsed;
-  int nLLBL[MAX_LOGICAL_LOOPS]; // LogicalLoopBlockingLevels - 1 as no blocking
-  bool isParallel[MAX_LOOPS];
-  int p2lMap[MAX_LOOPS];
-  par_loop_kernel test_kernel;
-};
-
-inline LoopingScheme* getLoopingScheme(std::string scheme) {
-  static std::unordered_map<std::string, LoopingScheme*> kernel_cache;
-
-  LoopingScheme* kernel = NULL;
-  auto search = kernel_cache.find(scheme);
-  if (search != kernel_cache.end())
-    kernel = search->second;
-  if (kernel == NULL) {
-    kernel = new LoopingScheme(scheme);
-    kernel_cache[scheme] = kernel;
-  }
-  return kernel;
-}
-
-template <int N>
-class ThreadedLoop {
- public:
-  ThreadedLoop(const LoopSpecs (&bounds)[N], std::string scheme = "")
-      : bounds(bounds), scheme(scheme) {
-    if (scheme == "")
-      scheme = getDefaultScheme();
-    loopScheme = getLoopingScheme(scheme);
-  }
-
-  template <class T>
-  void operator()(T func) {
-    loopScheme->call(bounds, func, NULL, NULL);
-  }
-  template <class T, class Ti, class Tf>
-  void operator()(T func, Ti init, Tf fini) {
-    loopScheme->call(bounds, func, init, fini);
-  }
-
-  std::string getDefaultScheme() {
-    std::string scheme;
-    for (int i = 0; i < N; i++) {
-      if (bounds[i].isParallel)
-        scheme.append(std::to_string('A' + i));
-      else
-        scheme.append(std::to_string('a' + i));
-    }
-    return scheme;
-  }
-
- private:
-  LoopSpecs bounds[N];
-  std::string scheme;
-  LoopingScheme* loopScheme;
-};
-
-#endif // _TPP_THREADED_LOOPS_H_
\ No newline at end of file
diff --git a/csrc/cpu/tpp/woq/timing.h b/csrc/cpu/tpp/woq/timing.h
deleted file mode 100644
index a652c91d..00000000
--- a/csrc/cpu/tpp/woq/timing.h
+++ /dev/null
@@ -1,209 +0,0 @@
-/******************************************************************************
- * Copyright (c) 2022 Intel Corporation - All rights reserved.                *
- *                                                                            *
- * For information on the license, see the LICENSE file.                      *
- * Further information: https://github.com/libxsmm/tpp-pytorch-extension/     *
- * SPDX-License-Identifier: BSD-3-Clause                                      *
- ******************************************************************************/
-/* Author: Dhiraj Kalamkar (Intel Corp.)
- ******************************************************************************/
-
-#ifndef _TPP_BERT_TIMING_H_
-#define _TPP_BERT_TIMING_H_
-
-#include <omp.h>
-#include "utils.h"
-
-enum DebugTimer {
-  BRGEMM,
-  XPOSE,
-  DROPOUT,
-  LAYER_NORM,
-  SOFTMAX,
-  ACT,
-  BIAS,
-  VNNI,
-  EW_COPY,
-  EW_ADD,
-  EW_SCL,
-  EW_RCP,
-  EW_RSQRT,
-  EW_MUL,
-  EW_ZERO,
-  EW_RED,
-  ROW_GT,
-  ROW_ST,
-  OPTIM,
-  LAST_TIMER
-};
-
-inline const char* DebugTimerName(int t) {
-  const char* names[] = {
-      "BRGEMM", "XPOSE",  "DROPOUT", "LYR_NRM", "SOFTMAX", "ACT",       "BIAS",
-      "VNNI",   "COPY",   "ADD",     "SCALE",   "RCP",     "RSQRT",     "MUL",
-      "ZERO",   "REDUCE", "ROW_GT",  "ROW_ST",  "OPTIM",   "LAST_TIMER"};
-  return names[t];
-}
-
-enum PassType { OTH, FWD, BWD, UPD };
-
-extern PassType globalPass;
-extern int globalScope;
-constexpr int NUM_TIMERS = ((LAST_TIMER + 7) / 8) * 8;
-extern double pass_timers[MAX_THREADS][3][NUM_TIMERS];
-extern double master_pass_timers[3];
-struct Scope {
-  Scope(std::string const& name)
-      : name(name), master_timer(0.0), detailed_timers{0.0}, flops{0.0} {}
-  const std::string name;
-  double master_timer;
-  double detailed_timers[MAX_THREADS][NUM_TIMERS];
-  double flops[MAX_THREADS][8];
-};
-
-inline std::vector<Scope>& get_scope_list() {
-  static std::vector<Scope> _scope_list{Scope("Reserved")};
-  return _scope_list;
-}
-
-inline std::vector<Scope>& get_pass_list() {
-  static std::vector<Scope> _pass_list{
-      Scope("OTH"), Scope("FWD"), Scope("BWD"), Scope("UPD")};
-  return _pass_list;
-}
-
-inline int register_scope(std::string name) {
-  auto& _scope_list = get_scope_list();
-  _scope_list.emplace_back(name);
-  int idx = _scope_list.size() - 1;
-  // printf("Registering %s scope @%d\n", name.c_str(), idx);
-  return idx;
-}
-
-#ifdef PROFILE_TPP
-#define REGISTER_LOCAL_SCOPE(id, name) static int sc_##id = register_scope(name)
-#define REGISTER_SCOPE(id, name) int sc_##id = register_scope(name)
-#define USING_SCOPE(id) extern int sc_##id
-#else
-#define REGISTER_LOCAL_SCOPE(id, name)
-#define REGISTER_SCOPE(id, name)
-#define USING_SCOPE(id)
-#endif
-
-class ScopedTimer {
- public:
-  ScopedTimer(DebugTimer t, long f = 0) : type(t), flops(f), start(getTime()) {}
-  ~ScopedTimer() {
-    auto time = getTime() - start;
-    int tid = omp_get_thread_num();
-    auto& pass = get_pass_list()[globalPass];
-    pass.detailed_timers[tid][type] += time;
-    if (type == BRGEMM)
-      pass.flops[tid][0] += flops;
-    if (globalPass == 0 && tid == 0)
-      pass.master_timer += time;
-
-    auto& scope = get_scope_list()[globalScope];
-    scope.detailed_timers[tid][type] += time;
-    if (type == BRGEMM)
-      scope.flops[tid][0] += flops;
-    if (globalScope == 0 && tid == 0)
-      scope.master_timer += time;
-  }
-  DebugTimer type;
-  long flops;
-  double start;
-};
-
-class GlobalScope {
- public:
-  GlobalScope(int t) : oldScope(globalScope), start(getTime()) {
-    TPP_ASSERT(t < (int)get_scope_list().size(), "Invalid scope initialized");
-    globalScope = t;
-  }
-  ~GlobalScope() {
-    auto time = getTime() - start;
-    auto& scope = get_scope_list()[globalScope];
-    scope.master_timer += time;
-    if (oldScope != 0) {
-      // Remove time from outer scope
-      auto& outer_scope = get_scope_list()[oldScope];
-      outer_scope.master_timer -= time;
-    }
-    globalScope = oldScope;
-  }
-  int oldScope;
-  double start;
-};
-
-class GlobalPass {
- public:
-  GlobalPass(PassType p) : oldPass(globalPass), start(getTime()) {
-    globalPass = p;
-  }
-  ~GlobalPass() {
-    auto time = getTime() - start;
-    auto& pass = get_pass_list()[globalPass];
-    pass.master_timer += time;
-    if (oldPass != 0) {
-      auto& outer_pass = get_pass_list()[oldPass];
-      outer_pass.master_timer -= time;
-    }
-    globalPass = oldPass;
-  }
-  PassType oldPass;
-  double start;
-};
-
-// #define DEBUG_TRACE_TPP
-#ifdef DEBUG_TRACE_TPP
-static thread_local std::string prev_class_name = "";
-#endif
-template <typename T, int impl = 0>
-class ScopedTPP {
- public:
-  ScopedTPP(T func, DebugTimer t) : func(std::move(func)), t(t) {}
-  template <typename... Types>
-  void operator()(Types... vars) {
-    ScopedTimer _t(t);
-#ifdef DEBUG_TRACE_TPP
-    if (omp_get_thread_num() == 0) {
-      auto cur_class_name = get_class_name<T>();
-      if (cur_class_name != prev_class_name) {
-        std::cout << "Calling impl " << impl << " for " << cur_class_name
-                  << std::endl;
-        prev_class_name = cur_class_name;
-      }
-    }
-#endif
-    if (impl == 0) {
-      func(vars...);
-    } else if (impl == 1) {
-      func.ref(vars...);
-    } else {
-      printf("invalid impl requested\n");
-      exit(1);
-    }
-  }
-
- private:
-  T func;
-  DebugTimer t;
-};
-
-// Keeping below two definitions for backward compatibility for now
-#define SCOPEITGEMM SCOPEIT
-#define SCOPEITGEMM2 SCOPEIT
-
-#ifdef PROFILE_TPP
-#define SCOPEIT(f, ...) ScopedTPP<decltype(f), 0>(f, ##__VA_ARGS__)
-#define SCOPEIT_REF(f, ...) ScopedTPP<decltype(f), 1>(f, ##__VA_ARGS__)
-#define RECORD_SCOPE(scope, ...) \
-  GlobalScope gs_(sc_##scope);   \
-  RECORD_FUNCTION(#scope, std::vector<c10::IValue>(__VA_ARGS__))
-#else
-#define SCOPEIT(f, ...) f
-#define RECORD_SCOPE(scope, ...)
-#endif
-
-#endif //_TPP_BERT_TIMING_H_
\ No newline at end of file
diff --git a/csrc/cpu/tpp/woq/tla.h b/csrc/cpu/tpp/woq/tla.h
deleted file mode 100644
index c3605b79..00000000
--- a/csrc/cpu/tpp/woq/tla.h
+++ /dev/null
@@ -1,11 +0,0 @@
-// clang-format off
-#include "tpp.h"
-#include "par_loop_generator.h"
-#include "threaded_loops.h"
-#include "vla.h"
-#include "utils.h"
-#include "timing.h"
-#include "vec.h"
-#include "dispatcher.h"
-#include "debug.h"
-// clang-format on
\ No newline at end of file
diff --git a/csrc/cpu/tpp/woq/tpp.h b/csrc/cpu/tpp/woq/tpp.h
deleted file mode 100644
index 09e73f7f..00000000
--- a/csrc/cpu/tpp/woq/tpp.h
+++ /dev/null
@@ -1,5499 +0,0 @@
-/******************************************************************************
- * Copyright (c) 2022 Intel Corporation - All rights reserved.                *
- *                                                                            *
- * For information on the license, see the LICENSE file.                      *
- * Further information: https://github.com/libxsmm/tpp-pytorch-extension/     *
- * SPDX-License-Identifier: BSD-3-Clause                                      *
- ******************************************************************************/
-/* Author: Dhiraj Kalamkar (Intel Corp.)
- ******************************************************************************/
-
-#ifndef _TPP_XSMM_FUNCTORS_H_
-#define _TPP_XSMM_FUNCTORS_H_
-
-#ifdef __x86_64__
-#include <immintrin.h>
-#endif
-
-#include <libxsmm_source.h>
-#include <stdexcept>
-#include <string>
-#include <unordered_map>
-
-#define TPP_ASSERT(cond, x...) \
-  do {                         \
-    if (!(cond)) {             \
-      printf(x);               \
-      fflush(stdout);          \
-      exit(1);                 \
-    }                          \
-  } while (0)
-#define DECL_VLA_PTR(type, name, dims, ptr) type(*name) dims = (type(*) dims)ptr
-#define ALIGNDOWN(N, A) ((N) & ~((A)-1))
-namespace tpp {
-typedef at::BFloat16 bfloat16;
-typedef at::Half half;
-typedef struct bfloat8 {
-  uint8_t data;
-} bfloat8;
-
-inline float upconvert_to_float(float val) {
-  return val;
-}
-// inline float upconvert_to_float(bfloat16 val) {
-//   return (float)val;
-// }
-// inline float upconvert_to_float(half val) {
-//   return (float)val;
-// }
-// inline float upconvert_to_float(bfloat8 val) {
-//   return (float)val;
-// }
-template <typename T>
-inline libxsmm_datatype XsmmDtype();
-template <>
-inline libxsmm_datatype XsmmDtype<int64_t>() {
-  return LIBXSMM_DATATYPE_I64;
-}
-template <>
-inline libxsmm_datatype XsmmDtype<int32_t>() {
-  return LIBXSMM_DATATYPE_I32;
-}
-template <>
-inline libxsmm_datatype XsmmDtype<int8_t>() {
-  return LIBXSMM_DATATYPE_I8;
-}
-template <>
-inline libxsmm_datatype XsmmDtype<uint8_t>() {
-  return LIBXSMM_DATATYPE_U8;
-}
-template <>
-inline libxsmm_datatype XsmmDtype<float>() {
-  return LIBXSMM_DATATYPE_F32;
-}
-template <>
-inline libxsmm_datatype XsmmDtype<bfloat16>() {
-  return LIBXSMM_DATATYPE_BF16;
-}
-template <>
-inline libxsmm_datatype XsmmDtype<half>() {
-  return LIBXSMM_DATATYPE_F16;
-}
-template <>
-inline libxsmm_datatype XsmmDtype<bfloat8>() {
-  return LIBXSMM_DATATYPE_BF8;
-}
-
-#ifdef __AVX512F__
-inline __m512 _mm512_loadu_ps_auto(float const* mem_addr) {
-  return _mm512_loadu_ps(mem_addr);
-}
-inline __m512 _mm512_maskz_loadu_ps_auto(__mmask16 k, float const* mem_addr) {
-  return _mm512_maskz_loadu_ps(k, mem_addr);
-}
-inline void _mm512_storeu_ps_auto(float* mem_addr, __m512 a) {
-  _mm512_storeu_ps(mem_addr, a);
-}
-inline void _mm512_mask_storeu_ps_auto(float* mem_addr, __mmask16 k, __m512 a) {
-  _mm512_mask_storeu_ps(mem_addr, k, a);
-}
-
-inline __m512 _mm512_loadu_ps_auto(half const* mem_addr) {
-  return _mm512_cvtph_ps(_mm256_loadu_si256((__m256i*)mem_addr));
-}
-inline __m512 _mm512_maskz_loadu_ps_auto(__mmask16 k, half const* mem_addr) {
-  return _mm512_cvtph_ps(_mm256_maskz_loadu_epi16(k, (__m256i*)mem_addr));
-}
-inline void _mm512_storeu_ps_auto(half* mem_addr, __m512 a) {
-  _mm256_storeu_si256(
-      (__m256i*)mem_addr,
-      _mm512_cvtps_ph(a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
-}
-inline void _mm512_mask_storeu_ps_auto(half* mem_addr, __mmask16 k, __m512 a) {
-  _mm256_mask_storeu_epi16(
-      (__m256i*)mem_addr,
-      k,
-      _mm512_cvtps_ph(a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
-}
-
-inline __m512 _mm512_convert_bf_ps(__m256i a) {
-  return _mm512_castsi512_ps(_mm512_slli_epi32(_mm512_cvtepi16_epi32(a), 16));
-}
-inline __m256i _mm256_convert_ps_bf(__m512 a) {
-  return _mm512_cvtepi32_epi16(
-      _mm512_srai_epi32(LIBXSMM_INTRINSICS_MM512_ROUNDNE_BF16(a), 16));
-}
-
-inline __m512 _mm512_loadu_ps_auto(bfloat16 const* mem_addr) {
-  return _mm512_convert_bf_ps(_mm256_loadu_si256((__m256i*)mem_addr));
-}
-inline __m512 _mm512_maskz_loadu_ps_auto(
-    __mmask16 k,
-    bfloat16 const* mem_addr) {
-  return _mm512_convert_bf_ps(_mm256_maskz_loadu_epi16(k, (__m256i*)mem_addr));
-}
-inline void _mm512_storeu_ps_auto(bfloat16* mem_addr, __m512 a) {
-  _mm256_storeu_si256((__m256i*)mem_addr, _mm256_convert_ps_bf(a));
-}
-inline void _mm512_mask_storeu_ps_auto(
-    bfloat16* mem_addr,
-    __mmask16 k,
-    __m512 a) {
-  _mm256_mask_storeu_epi16((__m256i*)mem_addr, k, _mm256_convert_ps_bf(a));
-}
-
-inline __m512 _mm512_split_loadu_ps(bfloat16 const* hi, bfloat16 const* lo) {
-  auto yh = _mm512_cvtepu16_epi32(_mm256_loadu_si256((__m256i*)hi));
-  auto yl = _mm512_cvtepu16_epi32(_mm256_loadu_si256((__m256i*)lo));
-  return _mm512_castsi512_ps(_mm512_add_epi32(_mm512_bslli_epi128(yh, 2), yl));
-}
-inline __m512 _mm512_maskz_split_loadu_ps(
-    __mmask16 k,
-    bfloat16 const* hi,
-    bfloat16 const* lo) {
-  auto yh = _mm512_cvtepu16_epi32(_mm256_maskz_loadu_epi16(k, (__m256i*)hi));
-  auto yl = _mm512_cvtepu16_epi32(_mm256_maskz_loadu_epi16(k, (__m256i*)lo));
-  return _mm512_castsi512_ps(_mm512_add_epi32(_mm512_bslli_epi128(yh, 2), yl));
-}
-inline void _mm512_split_storeu_ps(bfloat16* hi, bfloat16* lo, __m512 a) {
-  //_mm512_storeu_ps_auto(hi, a);
-  _mm256_storeu_si256(
-      (__m256i*)hi,
-      _mm512_cvtepi32_epi16(_mm512_bsrli_epi128(_mm512_castps_si512(a), 2)));
-  _mm256_storeu_si256(
-      (__m256i*)lo, _mm512_cvtepi32_epi16(_mm512_castps_si512(a)));
-}
-inline void _mm512_mask_split_storeu_ps(
-    bfloat16* hi,
-    bfloat16* lo,
-    __mmask16 k,
-    __m512 a) {
-  //_mm512_mask_storeu_ps_auto(hi, k, a);
-  _mm256_mask_storeu_epi16(
-      (__m256i*)hi,
-      k,
-      _mm512_cvtepi32_epi16(_mm512_bsrli_epi128(_mm512_castps_si512(a), 2)));
-  _mm256_mask_storeu_epi16(
-      (__m256i*)lo, k, _mm512_cvtepi32_epi16(_mm512_castps_si512(a)));
-}
-inline __m512 _mm512_convert_bf8_ps(__m128i a) {
-  return _mm512_cvtph_ps(_mm256_slli_epi16(_mm256_cvtepi8_epi16(a), 8));
-}
-inline __m128i _mm_convert_ps_bf8(__m512 a) {
-  return _mm256_cvtepi16_epi8(_mm256_srai_epi16(
-      _mm512_cvtps_ph(a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC), 8));
-}
-
-inline __m512 _mm512_loadu_ps_auto(bfloat8 const* mem_addr) {
-  return _mm512_convert_bf8_ps(_mm_loadu_si128((__m128i const*)mem_addr));
-}
-inline __m512 _mm512_maskz_loadu_ps_auto(__mmask16 k, bfloat8 const* mem_addr) {
-  return _mm512_convert_bf8_ps(
-      _mm_maskz_loadu_epi8(k, (__m128i const*)mem_addr));
-}
-inline void _mm512_storeu_ps_auto(bfloat8* mem_addr, __m512 a) {
-  _mm_storeu_si128((__m128i*)mem_addr, _mm_convert_ps_bf8(a));
-}
-inline void _mm512_mask_storeu_ps_auto(
-    bfloat8* mem_addr,
-    __mmask16 k,
-    __m512 a) {
-  _mm_mask_storeu_epi8((__m128i*)mem_addr, k, _mm_convert_ps_bf8(a));
-}
-#endif
-
-// inline libxsmm_datatype convert_dtype_pt2xsmm(at::ScalarType dtype) {
-//   static const std::map<at::ScalarType, libxsmm_datatype> pt2xsmmDtypes = {
-//       {at::kDouble, LIBXSMM_DATATYPE_F64},
-//       {at::kFloat, LIBXSMM_DATATYPE_F32},
-//       {at::kHalf, LIBXSMM_DATATYPE_F16},
-//       {at::kBFloat16, LIBXSMM_DATATYPE_BF16},
-//       {at::kByte, LIBXSMM_DATATYPE_I8},
-//       {at::kChar, LIBXSMM_DATATYPE_I8},
-//       {at::kShort, LIBXSMM_DATATYPE_I16},
-//       {at::kInt, LIBXSMM_DATATYPE_I32},
-//       {at::kLong, LIBXSMM_DATATYPE_I64}};
-
-//   return pt2xsmmDtypes.at(dtype);
-// }
-
-inline int xsmm_get_vnni_block_size(libxsmm_datatype dtype) {
-  int bs = libxsmm_cpuid_dot_pack_factor(dtype);
-  if (bs <= 0) {
-    throw std::invalid_argument("Unsupported datatype");
-  }
-  return bs;
-}
-
-// inline int get_vnni_block_size(at::ScalarType dtype) {
-//   auto xsmm_dtype = convert_dtype_pt2xsmm(dtype);
-//   return xsmm_get_vnni_block_size(xsmm_dtype);
-// }
-
-// inline int get_vnni_block_size(caffe2::TypeMeta dtype_) {
-//   at::ScalarType dtype = dtype_.toScalarType();
-//   auto xsmm_dtype = convert_dtype_pt2xsmm(dtype);
-//   return xsmm_get_vnni_block_size(xsmm_dtype);
-// }
-
-template <typename T>
-inline int get_vnni_block_size() {
-  auto xsmm_dtype = XsmmDtype<T>();
-  return xsmm_get_vnni_block_size(xsmm_dtype);
-}
-
-inline void debug_print_eqn_tree(libxsmm_blasint eqn_no) {
-  if (false) {
-    libxsmm_matrix_eqn_tree_print(eqn_no);
-    libxsmm_matrix_eqn_rpn_print(eqn_no);
-  }
-}
-
-inline int meqn_push_arg(
-    const libxsmm_blasint idx,
-    const libxsmm_blasint m,
-    const libxsmm_blasint n,
-    const libxsmm_blasint ld,
-    const libxsmm_blasint in_pos,
-    const libxsmm_blasint offs_in_pos,
-    const libxsmm_datatype dtype) {
-  // This "singular" type dictates that the arg is a regular tensor (and not a
-  // set of tensors)
-  libxsmm_matrix_arg_attributes arg_singular_attr =
-      libxsmm_create_matrix_arg_attributes(
-          LIBXSMM_MATRIX_ARG_TYPE_SINGULAR,
-          LIBXSMM_MATRIX_ARG_SET_TYPE_NONE,
-          0,
-          0);
-  // Arg metadata include equation id and pos in arg array at runtime
-  libxsmm_matrix_eqn_arg_metadata arg_metadata =
-      libxsmm_create_matrix_eqn_arg_metadata(idx, in_pos);
-  libxsmm_meqn_arg_shape arg_shape =
-      libxsmm_create_meqn_arg_shape(m, n, ld, dtype);
-  return libxsmm_matrix_eqn_push_back_arg_v2(
-      arg_metadata, arg_shape, arg_singular_attr);
-}
-
-inline libxsmm_matrix_eqn_function meqn_dispatch(
-    const libxsmm_blasint m,
-    const libxsmm_blasint n,
-    const libxsmm_blasint* ldo,
-    const libxsmm_datatype out_type,
-    const unsigned int idx) {
-  libxsmm_meqn_arg_shape arg_shape =
-      libxsmm_create_meqn_arg_shape(m, n, *ldo, out_type);
-  return libxsmm_dispatch_matrix_eqn_v2(idx, arg_shape);
-}
-
-inline int meqn_push_unary_op(
-    const libxsmm_blasint idx,
-    const libxsmm_meltw_unary_type type,
-    const libxsmm_bitfield flags = LIBXSMM_MELTW_FLAG_UNARY_NONE,
-    const libxsmm_datatype dtype = LIBXSMM_DATATYPE_F32) {
-  // OP metadata include equation id and an integer dictating where the op
-  // metadata at runtime (if any) are located in the op arg array. -1 dictates
-  // there are no op metadata needed
-  libxsmm_matrix_eqn_op_metadata op_metadata =
-      libxsmm_create_matrix_eqn_op_metadata(idx, -1);
-  return libxsmm_matrix_eqn_push_back_unary_op_v2(
-      op_metadata, type, dtype, flags);
-}
-inline int meqn_push_binary_op(
-    const libxsmm_blasint idx,
-    const libxsmm_meltw_binary_type type,
-    const libxsmm_bitfield flags = LIBXSMM_MELTW_FLAG_BINARY_NONE,
-    const libxsmm_datatype dtype = LIBXSMM_DATATYPE_F32) {
-  libxsmm_matrix_eqn_op_metadata op_metadata =
-      libxsmm_create_matrix_eqn_op_metadata(idx, -1);
-  return libxsmm_matrix_eqn_push_back_binary_op_v2(
-      op_metadata, type, dtype, flags);
-}
-inline int meqn_push_ternary_op(
-    const libxsmm_blasint idx,
-    const libxsmm_meltw_ternary_type type,
-    const libxsmm_bitfield flags = LIBXSMM_MELTW_FLAG_TERNARY_NONE,
-    const libxsmm_datatype dtype = LIBXSMM_DATATYPE_F32) {
-  libxsmm_matrix_eqn_op_metadata op_metadata =
-      libxsmm_create_matrix_eqn_op_metadata(idx, -1);
-  return libxsmm_matrix_eqn_push_back_ternary_op_v2(
-      op_metadata, type, dtype, flags);
-}
-
-class BaseTPP {
- public:
-  void* get_kernel() {
-    auto& kernel_cache = get_kernel_cache();
-    void* kernel = NULL;
-    if (hash == "")
-      hash = hash_str();
-    auto search = kernel_cache.find(hash);
-    if (search != kernel_cache.end())
-      kernel = search->second;
-    if (kernel == NULL) {
-      kernel = build_kernel();
-      if (kernel == NULL) {
-        fprintf(stderr, "Unable to get JIT kernel for %s\n", hash.c_str());
-        exit(1);
-      }
-      // printf("TPP: %s @ %p\n", hash.c_str(), kernel);
-      kernel_cache[hash] = kernel;
-    }
-    return kernel;
-  }
-  // We should make hash_str() public
-  std::string get_hash_str() {
-    return hash_str();
-  }
-
- protected:
-  std::unordered_map<std::string, void*>& get_kernel_cache() {
-    static std::unordered_map<std::string, void*> kernel_cache;
-    return kernel_cache;
-  }
-  virtual std::string hash_str() = 0;
-  virtual void* build_kernel() = 0;
-  std::string hash = "";
-  bool initialized = false;
-};
-
-class UnaryTPP : public BaseTPP {
- public:
-  UnaryTPP() {}
-  UnaryTPP(
-      libxsmm_blasint rows,
-      libxsmm_blasint cols,
-      libxsmm_blasint ldi,
-      libxsmm_blasint ldo,
-      libxsmm_datatype dt_in,
-      libxsmm_datatype dt_out,
-      libxsmm_datatype dt_compute,
-      libxsmm_bitfield flags,
-      libxsmm_meltw_unary_type type)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        dt_in(dt_in),
-        dt_out(dt_out),
-        dt_compute(dt_compute),
-        flags(flags),
-        type(type) {
-    kernel = (libxsmm_meltwfunction_unary)get_kernel();
-    if (kernel)
-      initialized = true;
-  }
-
-  void operator()(void* in, void* out) {
-    if (!initialized)
-      return;
-    libxsmm_meltw_unary_param unary_param;
-    unary_param.in.primary = in;
-    unary_param.out.primary = out;
-    kernel(&unary_param);
-  }
-  void operator()(void* in, void* out, void* out2) {
-    if (!initialized)
-      return;
-    libxsmm_meltw_unary_param unary_param;
-    unary_param.in.primary = in;
-    unary_param.out.primary = out;
-    unary_param.out.secondary = out2;
-    kernel(&unary_param);
-  }
-  void operator()(void* in, void* in2, void* in3, void* out, void* out2) {
-    if (!initialized)
-      return;
-    libxsmm_meltw_unary_param unary_param;
-    unary_param.in.primary = in;
-    unary_param.in.secondary = in2;
-    unary_param.in.tertiary = in3;
-    unary_param.out.primary = out;
-    unary_param.out.secondary = out2;
-    kernel(&unary_param);
-  }
-
-  void operator()(
-      void* in,
-      void* in2,
-      void* in3,
-      void* op,
-      void* op2,
-      void* op3,
-      void* out,
-      void* out2) {
-    if (!initialized)
-      return;
-    libxsmm_meltw_unary_param unary_param;
-    unary_param.in.primary = in;
-    unary_param.in.secondary = in2;
-    unary_param.in.tertiary = in3;
-    unary_param.op.primary = op;
-    unary_param.op.secondary = op2;
-    unary_param.op.tertiary = op3;
-    unary_param.out.primary = out;
-    unary_param.out.secondary = out2;
-    kernel(&unary_param);
-  }
-
- protected:
-  std::string hash_str() override {
-    char hash[200];
-    snprintf(
-        hash,
-        200,
-        "unary_r%d_c%d_i%d_o%d_di%d_do%d_dc%d_f%d_t%d",
-        rows,
-        cols,
-        ldi,
-        ldo,
-        dt_in,
-        dt_out,
-        dt_compute,
-        flags,
-        type);
-    return std::string(hash);
-  }
-  void* build_kernel() override {
-    libxsmm_meltw_unary_shape shape = libxsmm_create_meltw_unary_shape(
-        cols, rows, ldi, ldo, dt_in, dt_out, dt_compute);
-    return (void*)libxsmm_dispatch_meltw_unary_v2(type, shape, flags);
-  }
-
-  libxsmm_blasint rows = 0;
-  libxsmm_blasint cols = 0;
-  libxsmm_blasint ldi = 0;
-  libxsmm_blasint ldo = 0;
-  libxsmm_datatype dt_in = LIBXSMM_DATATYPE_F32;
-  libxsmm_datatype dt_out = LIBXSMM_DATATYPE_F32;
-  libxsmm_datatype dt_compute = LIBXSMM_DATATYPE_F32;
-  libxsmm_bitfield flags = LIBXSMM_MELTW_FLAG_UNARY_NONE;
-  libxsmm_meltw_unary_type type = LIBXSMM_MELTW_TYPE_UNARY_IDENTITY;
-  libxsmm_meltwfunction_unary kernel = NULL;
-};
-
-class BinaryTPP : public BaseTPP {
- public:
-  BinaryTPP() {}
-  BinaryTPP(
-      libxsmm_blasint rows,
-      libxsmm_blasint cols,
-      libxsmm_blasint ldi,
-      libxsmm_blasint ldo,
-      libxsmm_datatype dt_in,
-      libxsmm_datatype dt_out,
-      libxsmm_datatype dt_compute,
-      libxsmm_bitfield flags,
-      libxsmm_meltw_binary_type type)
-      : BinaryTPP(
-            rows,
-            cols,
-            ldi,
-            ldi,
-            ldo,
-            dt_in,
-            dt_in,
-            dt_out,
-            dt_compute,
-            flags,
-            type) {}
-  BinaryTPP(
-      libxsmm_blasint rows,
-      libxsmm_blasint cols,
-      libxsmm_blasint ldi0,
-      libxsmm_blasint ldi1,
-      libxsmm_blasint ldo,
-      libxsmm_datatype dt_in0,
-      libxsmm_datatype dt_in1,
-      libxsmm_datatype dt_out,
-      libxsmm_datatype dt_compute,
-      libxsmm_bitfield flags,
-      libxsmm_meltw_binary_type type)
-      : rows(rows),
-        cols(cols),
-        ldi0(ldi0),
-        ldi1(ldi1),
-        ldo(ldo),
-        dt_in0(dt_in0),
-        dt_in1(dt_in1),
-        dt_out(dt_out),
-        dt_compute(dt_compute),
-        flags(flags),
-        type(type) {
-    kernel = (libxsmm_meltwfunction_binary)get_kernel();
-    if (kernel)
-      initialized = true;
-  }
-
-  void operator()(void* in0, void* in1, void* out) {
-    if (!initialized)
-      return;
-    libxsmm_meltw_binary_param binary_param;
-    binary_param.in0.primary = in0;
-    binary_param.in1.primary = in1;
-    binary_param.out.primary = out;
-    kernel(&binary_param);
-  }
-
- protected:
-  std::string hash_str() override {
-    char hash[200];
-    snprintf(
-        hash,
-        200,
-        "binary_r%d_c%d_i0%d_i1%d_o%d_di0%d_di1%d_do%d_dc%d_f%d_t%d",
-        rows,
-        cols,
-        ldi0,
-        ldi1,
-        ldo,
-        dt_in0,
-        dt_in1,
-        dt_out,
-        dt_compute,
-        flags,
-        type);
-    return std::string(hash);
-  }
-  void* build_kernel() override {
-    libxsmm_meltw_binary_shape shape = libxsmm_create_meltw_binary_shape(
-        cols, rows, ldi0, ldi1, ldo, dt_in0, dt_in1, dt_out, dt_compute);
-    return (void*)libxsmm_dispatch_meltw_binary_v2(type, shape, flags);
-  }
-
-  libxsmm_blasint rows = 0;
-  libxsmm_blasint cols = 0;
-  libxsmm_blasint ldi0;
-  libxsmm_blasint ldi1;
-  libxsmm_blasint ldo;
-  libxsmm_datatype dt_in0;
-  libxsmm_datatype dt_in1;
-  libxsmm_datatype dt_out;
-  libxsmm_datatype dt_compute;
-  libxsmm_bitfield flags;
-  libxsmm_meltw_binary_type type;
-  libxsmm_meltwfunction_binary kernel = NULL;
-};
-
-template <typename T>
-class SetZeroTPP {
- public:
-  SetZeroTPP() {}
-  SetZeroTPP(int N) : SetZeroTPP(1, N) {}
-  SetZeroTPP(int rows, int cols) : SetZeroTPP(rows, cols, cols) {}
-  SetZeroTPP(int rows, int cols, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            ldo,
-            ldo,
-            XsmmDtype<T>(),
-            XsmmDtype<T>(),
-            XsmmDtype<T>(),
-            LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_XOR) {}
-  void operator()(T* buf) {
-    kernel((void*)buf, (void*)buf);
-  }
-  void ref(T* buf) {
-    for (int i = 0; i < rows; i++) {
-      for (int j = 0; j < cols; j++) {
-        buf[i * ldo + j] = 0;
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldo;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tout>
-class ConvertTPP {
- public:
-  ConvertTPP() {}
-  ConvertTPP(int N) : ConvertTPP(1, N) {}
-  ConvertTPP(int rows, int cols) : ConvertTPP(rows, cols, cols, cols) {}
-  ConvertTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            XsmmDtype<Tin>() == XsmmDtype<Tout>() ? XsmmDtype<Tout>()
-                                                  : LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_IDENTITY),
-        init_done(true) {}
-  void operator()(Tin* in, Tout* out) {
-    if (!(XsmmDtype<Tin>() == LIBXSMM_DATATYPE_F32 &&
-          XsmmDtype<Tout>() == LIBXSMM_DATATYPE_F32) ||
-        ((void*)in != (void*)out))
-      kernel((void*)in, (void*)out);
-  }
-  void ref(Tin* in, Tout* out) {
-    for (int i = 0; i < rows; i++) {
-      for (int j = 0; j < cols; j++) {
-        out[i * ldo + j] = (Tout)in[i * ldi + j];
-      }
-    }
-  }
-  bool initialized() {
-    return init_done;
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  UnaryTPP kernel;
-  bool init_done = false;
-};
-
-template <typename T>
-class CpyTPP {
- public:
-  CpyTPP() {}
-  CpyTPP(int N) : CpyTPP(1, N) {}
-  CpyTPP(int rows, int cols) : CpyTPP(rows, cols, cols, cols) {}
-  CpyTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<T>(),
-            XsmmDtype<T>(),
-            XsmmDtype<T>(),
-            LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_IDENTITY) {}
-  void operator()(T* in, T* out) {
-    kernel((void*)in, (void*)out);
-  }
-  void ref(T* in, T* out) {
-    for (int i = 0; i < rows; i++) {
-      for (int j = 0; j < cols; j++) {
-        out[i * ldo + j] = in[i * ldi + j];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  UnaryTPP kernel;
-};
-
-template <typename T>
-class PadTPP {
- public:
-  PadTPP() {}
-  PadTPP(int in_rows, int in_cols, int out_rows, int out_cols)
-      : PadTPP(in_rows, in_cols, out_rows, out_cols, in_cols, out_cols) {}
-  PadTPP(int in_rows, int in_cols, int out_rows, int out_cols, int ldi, int ldo)
-      : in_rows(in_rows),
-        in_cols(in_cols),
-        out_rows(out_rows),
-        out_cols(out_cols),
-        ldi(ldi),
-        ldo(ldo),
-        cpy(),
-        zero() {
-    if (out_rows > in_rows || out_cols > in_cols) {
-      TPP_ASSERT(
-          out_rows == in_rows || out_cols == in_cols,
-          "PadTPP can pad only 1 dim at a time");
-      cpy = CpyTPP<T>(in_rows, in_cols, ldi, ldo);
-      if (out_rows > in_rows) {
-        zero = SetZeroTPP<T>(out_rows - in_rows, out_cols, ldo);
-        zero_offset = in_rows * ldo;
-      } else {
-        zero = SetZeroTPP<T>(out_rows, out_cols - in_cols, ldo);
-        zero_offset = in_cols;
-      }
-    }
-  }
-  void operator()(T* in, T* out) {
-    cpy(in, out);
-    zero(out);
-  }
-  void ref(T* in, T* out) {
-    cpy.ref(in, out);
-    zero.ref(out);
-  }
-
- private:
-  int in_rows = 0;
-  int in_cols = 0;
-  int out_rows = 0;
-  int out_cols = 0;
-  int ldi;
-  int ldo;
-  int zero_offset = 0;
-  CpyTPP<T> cpy;
-  SetZeroTPP<T> zero;
-};
-
-template <typename Tin, typename Tout = Tin>
-class CpyBiasTPP {
- public:
-  CpyBiasTPP() {}
-  CpyBiasTPP(int rows, int cols) : CpyBiasTPP(rows, cols, cols) {}
-  CpyBiasTPP(int rows, int cols, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            cols,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            XsmmDtype<Tin>() == XsmmDtype<Tout>() ? XsmmDtype<Tout>()
-                                                  : LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_BCAST_COL,
-            LIBXSMM_MELTW_TYPE_UNARY_IDENTITY) {}
-  void operator()(Tin* in, Tout* out) {
-    kernel((void*)in, (void*)out);
-  }
-  void ref(Tin* in, Tout* out) {
-    for (int i = 0; i < rows; i++) {
-      for (int j = 0; j < cols; j++) {
-        out[i * ldo + j] = (Tout)in[j];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldo;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tout = Tin>
-class CpyBcastTPP {
- public:
-  CpyBcastTPP() {}
-  CpyBcastTPP(int rows, int cols) : CpyBcastTPP(rows, cols, cols) {}
-  CpyBcastTPP(int rows, int cols, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            1,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            XsmmDtype<Tin>() == XsmmDtype<Tout>() ? XsmmDtype<Tout>()
-                                                  : LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_BCAST_ROW,
-            LIBXSMM_MELTW_TYPE_UNARY_IDENTITY) {}
-  void operator()(Tin* in, Tout* out) {
-    kernel((void*)in, (void*)out);
-  }
-  void ref(Tin* in, Tout* out) {
-    for (int i = 0; i < rows; i++) {
-      for (int j = 0; j < cols; j++) {
-        out[i * ldo + j] = (Tout)in[i];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldo;
-  UnaryTPP kernel;
-};
-template <typename T>
-class AddBiasTPP {
- public:
-  AddBiasTPP() {}
-  AddBiasTPP(int rows, int cols) : AddBiasTPP(rows, cols, cols) {}
-  AddBiasTPP(int rows, int cols, int ld)
-      : rows(rows),
-        cols(cols),
-        ld(ld),
-        kernel(
-            rows,
-            cols,
-            ld,
-            ld,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_COL_IN_0,
-            LIBXSMM_MELTW_TYPE_BINARY_ADD),
-        cvt() {
-    if (!std::is_same<T, float>::value)
-      cvt = ConvertTPP<T, float>(1, cols);
-  }
-  void operator()(T* in, float* out) {
-    if (std::is_same<T, float>::value) {
-      kernel((void*)in, (void*)out, (void*)out);
-    } else {
-      float tmp[cols];
-      cvt(in, tmp);
-      kernel((void*)tmp, (void*)out, (void*)out);
-    }
-  }
-  void ref(T* in, float* out) {
-    for (int r = 0; r < rows; r++) {
-      for (int c = 0; c < cols; c++) {
-        out[r * ld + c] += (float)in[c];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ld;
-  BinaryTPP kernel;
-  ConvertTPP<T, float> cvt;
-};
-
-template <typename Tin, typename Tout = Tin>
-class AddTPP {
- public:
-  AddTPP() {}
-  AddTPP(int N) : AddTPP(1, N) {}
-  AddTPP(int rows, int cols) : AddTPP(rows, cols, cols, cols) {}
-  AddTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_NONE,
-            LIBXSMM_MELTW_TYPE_BINARY_ADD) {}
-  void operator()(Tin* in0, Tin* in1, Tout* out) {
-    kernel((void*)in0, (void*)in1, (void*)out);
-  }
-  void ref(Tin* in0, Tin* in1, Tout* out) {
-    for (int r = 0; r < rows; r++) {
-      for (int c = 0; c < cols; c++) {
-        out[r * ldo + c] = (float)in0[r * ldi + c] + (float)in1[r * ldi + c];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  BinaryTPP kernel;
-};
-
-template <typename Tin, typename Tout = Tin>
-class MulTPP {
- public:
-  MulTPP() {}
-  MulTPP(int N) : MulTPP(1, N) {}
-  MulTPP(int rows, int cols) : MulTPP(rows, cols, cols, cols) {}
-  MulTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_NONE,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL) {}
-  void operator()(Tin* in0, Tin* in1, Tout* out) {
-    kernel((void*)in0, (void*)in1, (void*)out);
-  }
-  void ref(Tin* in0, Tin* in1, Tout* out) {
-    for (int r = 0; r < rows; r++) {
-      for (int c = 0; c < cols; c++) {
-        out[r * ldo + c] = (float)in0[r * ldi + c] * (float)in1[r * ldi + c];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  BinaryTPP kernel;
-};
-
-template <typename Tin>
-class GradBiasTPP {
- public:
-  GradBiasTPP() {}
-  GradBiasTPP(int rows, int cols) : GradBiasTPP(rows, cols, cols) {}
-  GradBiasTPP(int rows, int cols, int ldi)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        reduce(
-            rows,
-            cols,
-            ldi,
-            cols,
-            XsmmDtype<Tin>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_COLS,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD),
-        add(cols) {}
-  void operator()(Tin* in, float* out) {
-    float tmp[cols];
-    reduce((void*)in, (void*)tmp);
-    add(tmp, out, out);
-  }
-  void ref(Tin* in, float* out) {
-    for (int r = 0; r < rows; r++) {
-      for (int c = 0; c < cols; c++) {
-        out[c] += (float)in[r * ldi + c];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-
-  UnaryTPP reduce;
-  AddTPP<float, float> add;
-};
-
-// ############################# Mul & Reduction TPP
-// #####################################
-
-template <typename T1, typename T2 = T1, typename T3 = T1>
-class MulReduceTPP : public BaseTPP {
- public:
-  MulReduceTPP() {}
-  MulReduceTPP(int N, int M) : N(N), M(M) {
-    kernel = (libxsmm_matrix_eqn_function)get_kernel();
-    initialized = true;
-  }
-
-  void operator()(T1* in0, T2* in1, T3* out) {
-    if (!initialized)
-      return;
-    libxsmm_matrix_eqn_param eqn_param;
-    libxsmm_matrix_arg arg_array[2];
-    arg_array[0].primary = (void*)in0;
-    arg_array[1].primary = (void*)in1;
-    eqn_param.inputs = arg_array;
-    eqn_param.output.primary = (void*)out;
-
-    kernel(&eqn_param);
-  }
-
-  void ref(T1* in0, T2* in1, T3* out) {
-    for (int r = 0; r < N; r++) {
-      for (int c = 0; c < M; c++) {
-        out[r] += (float)in0[r * M + c] * (float)in1[r * M + c];
-      }
-    }
-  }
-
- protected:
-  std::string hash_str() override {
-    char hash[200];
-    snprintf(
-        hash,
-        200,
-        "mul_reduce_eqn_t%d_%d_%d_r%d_c%d",
-        XsmmDtype<T1>(),
-        XsmmDtype<T2>(),
-        XsmmDtype<T3>(),
-        N, //);
-        M);
-    return std::string(hash);
-  }
-  void* build_kernel() override {
-    auto dt1 = XsmmDtype<T1>();
-    auto dt2 = XsmmDtype<T2>();
-    auto dt3 = XsmmDtype<T3>();
-    libxsmm_blasint ld = 1;
-    libxsmm_blasint my_eqn0 = libxsmm_matrix_eqn_create();
-    meqn_push_unary_op(
-        my_eqn0,
-        LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD,
-        LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS,
-        LIBXSMM_DATATYPE_F32);
-    // libxsmm_matrix_eqn_push_back_arg(my_eqn0, M, N, M, 0, 0, dt1);
-    meqn_push_binary_op(
-        my_eqn0,
-        LIBXSMM_MELTW_TYPE_BINARY_MUL,
-        LIBXSMM_MELTW_FLAG_BINARY_NONE,
-        LIBXSMM_DATATYPE_F32);
-    meqn_push_arg(my_eqn0, M, N, M, 0, 0, dt1);
-    meqn_push_arg(my_eqn0, M, N, M, 1, 0, dt2);
-    debug_print_eqn_tree(my_eqn0);
-    return (void*)meqn_dispatch(1, N, &ld, dt3, my_eqn0);
-  }
-
- private:
-  int N = 0;
-  int M = 0;
-  libxsmm_matrix_eqn_function kernel = NULL;
-};
-template <typename Tin, typename Tout = Tin>
-class ReduceAddColTPP {
- public:
-  ReduceAddColTPP() {}
-  ReduceAddColTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        reduce(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_COLS,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD) {}
-  void operator()(Tin* in, float* out) {
-    reduce(in, out);
-  }
-  void ref(Tin* in, float* out) {
-    for (int r = 0; r < rows; r++) {
-      for (int c = 0; c < cols; c++) {
-        if (r == 0)
-          out[c] = 0;
-        out[c] += (float)in[r * ldi + c];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi, ldo;
-
-  UnaryTPP reduce;
-};
-
-template <typename Tin, typename Tout = Tin>
-class ReduceAddRowTPP {
- public:
-  ReduceAddRowTPP() {}
-  ReduceAddRowTPP(int rows, int cols, bool acc)
-      : ReduceAddRowTPP(rows, cols, cols, acc) {}
-  ReduceAddRowTPP(int rows, int cols, int ldi, bool acc)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        acc(acc),
-        reduce(
-            rows,
-            cols,
-            ldi,
-            cols,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD),
-        add(rows) {}
-  void operator()(Tin* in, Tout* out) {
-    if (acc) {
-      Tout tmp[rows];
-      reduce((void*)in, (void*)tmp);
-      add(tmp, out, out);
-    } else {
-      reduce((void*)in, (void*)out);
-    }
-  }
-  void ref(Tin* in, Tout* out) {
-    for (int r = 0; r < rows; r++) {
-      if (!acc) {
-        out[r] = 0;
-      }
-      for (int c = 0; c < cols; c++) {
-        out[r] += (float)in[r * ldi + c];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  bool acc;
-  UnaryTPP reduce;
-  AddTPP<Tout, Tout> add;
-};
-
-// ############################# Broadcast & Multiplication TPP
-// #####################################
-template <typename Tin, typename Tout = Tin>
-class BCastMulTPP {
- public:
-  BCastMulTPP() {}
-  BCastMulTPP(int rows, int cols) : BCastMulTPP(rows, cols, cols, cols) {}
-  BCastMulTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            1,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_ROW_IN_0, // Broadcast in Row
-                                                      // Dimension
-            LIBXSMM_MELTW_TYPE_BINARY_MUL) // Multiplication
-  {}
-  void operator()(Tin* in0, Tin* in1, Tout* out) {
-    kernel((void*)in0, (void*)in1, (void*)out);
-  }
-  void ref(Tin* in0, Tin* in1, Tout* out) {
-    for (int r = 0; r < rows; r++) {
-      for (int c = 0; c < cols; c++) {
-        out[c * ldo + r] = (Tin)in0[r] * in1[c * ldi + r];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  BinaryTPP kernel;
-};
-
-// ############################# Broadcast & Multiplication Addition TPP
-// #####################################
-template <typename Tin, typename Tout = Tin>
-class BCastMulAddTPP {
- public:
-  BCastMulAddTPP() {}
-  BCastMulAddTPP(int rows, int cols) : BCastMulAddTPP(rows, cols, cols, cols) {}
-  BCastMulAddTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            1,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_ROW_IN_0, // Broadcast in Row
-                                                      // Dimension
-            LIBXSMM_MELTW_TYPE_BINARY_MULADD) // Multiplication
-  {}
-  void operator()(Tin* in0, Tin* in1, Tout* out) {
-    kernel((void*)in0, (void*)in1, (void*)out);
-  }
-
-  void ref(Tin* in0, Tin* in1, Tout* out) {
-    for (int r = 0; r < rows; r++) {
-      for (int c = 0; c < cols; c++) {
-        out[c * ldo + r] += (Tin)in0[r] * (Tin)in1[c * ldi + r];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  BinaryTPP kernel;
-};
-
-template <typename Tin, typename Tout>
-class ScaleTPP {
- public:
-  ScaleTPP() {}
-  ScaleTPP(int N)
-      : N(N),
-        kernel(
-            1,
-            N,
-            N,
-            N,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_0,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL) {}
-  void operator()(Tin* in, Tout* out, float scale) {
-    Tin alpha = scale;
-    kernel((void*)&alpha, (void*)in, (void*)out);
-  }
-  void ref(Tin* in, Tout* out, float scale) {
-    Tin alpha = scale;
-    for (int i = 0; i < N; i++) {
-      out[i] = (float)in[i] * (float)alpha;
-    }
-  }
-
- private:
-  int N = 0;
-  BinaryTPP kernel;
-};
-
-template <typename T, typename TN = float>
-class Norm2TPP {
- public:
-  Norm2TPP() {}
-  Norm2TPP(int N)
-      : N(N),
-        kernel(
-            1,
-            N,
-            N,
-            N,
-            XsmmDtype<T>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X2_OP_ADD) {}
-  void operator()(T* in, TN* sum) {
-    float lsum = 0.0f;
-    kernel((void*)in, (void*)&lsum);
-    *sum += (TN)lsum;
-  }
-  void ref(T* in, TN* sum) {
-    float lsum = 0.0f;
-    for (int i = 0; i < N; i++) {
-      lsum += (float)in[i] * (float)in[i];
-    }
-    *sum += (TN)lsum;
-  }
-
- private:
-  int N = 0;
-  UnaryTPP kernel;
-};
-
-template <typename T>
-class RecpTPP {
- public:
-  RecpTPP() {}
-  RecpTPP(int N)
-      : N(N),
-        kernel(
-            1,
-            N,
-            N,
-            N,
-            XsmmDtype<T>(),
-            XsmmDtype<T>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_RECIPROCAL) {}
-  void operator()(T* in, T* out) {
-    kernel((void*)in, (void*)out);
-  }
-  void ref(T* in, T* out) {
-    for (int i = 0; i < N; i++)
-      out[i] = 1.0 / in[i];
-  }
-
- private:
-  int N = 0;
-  UnaryTPP kernel;
-};
-
-template <typename T>
-class RecpSqrtTPP {
- public:
-  RecpSqrtTPP() {}
-  RecpSqrtTPP(int N)
-      : N(N),
-        kernel(
-            1,
-            N,
-            N,
-            N,
-            XsmmDtype<T>(),
-            XsmmDtype<T>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_RECIPROCAL_SQRT) {}
-  void operator()(T* in, T* out) {
-    kernel((void*)in, (void*)out);
-  }
-  void ref(T* in, T* out) {
-    for (int i = 0; i < N; i++)
-      out[i] = 1.0 / sqrt(in[i]);
-  }
-
- private:
-  int N = 0;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tout = Tin>
-class MulNormTPP {
- public:
-  MulNormTPP() {}
-  MulNormTPP(int rows, int cols) : MulNormTPP(rows, cols, cols, cols) {}
-  MulNormTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            1, // ldi0
-            ldi, // ldi1
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_ROW_IN_0,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL) {}
-  void operator()(Tin* in, Tin* in2, Tout* out) {
-    kernel((void*)in, (void*)in2, (void*)out);
-  }
-  void ref(Tin* in, Tin* in2, Tout* out) {
-    for (int r = 0; r < rows; r++)
-      for (int c = 0; c < cols; c++)
-        out[r * ldo + c] = in[r] * in2[r * ldi + c];
-  }
-
- private:
-  int rows, cols;
-  int ldi, ldo;
-  BinaryTPP kernel;
-};
-
-template <typename Tin, typename Tout>
-class ScaleAddTPP {
- public:
-  ScaleAddTPP() {}
-  ScaleAddTPP(int N)
-      : N(N),
-        kernel(
-            1,
-            N,
-            N,
-            N,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_0,
-            LIBXSMM_MELTW_TYPE_BINARY_MULADD) {}
-  void operator()(Tin* in, Tout* out, float scale) {
-    Tin alpha = scale;
-    kernel((void*)&alpha, (void*)in, (void*)out);
-  }
-  void ref(Tin* in, Tout* out, float scale) {
-    Tin alpha = scale;
-    for (int i = 0; i < N; i++) {
-      out[i] += (float)in[i] * (float)alpha;
-    }
-  }
-
- private:
-  int N = 0;
-  BinaryTPP kernel;
-};
-
-template <typename Tin, typename Tind, typename Tout>
-class EmbeddingFwdTPP {
- public:
-  EmbeddingFwdTPP() {}
-  EmbeddingFwdTPP(int rows, int cols, int ldi)
-      : EmbeddingFwdTPP(rows, cols, ldi, ldi) {}
-  EmbeddingFwdTPP(int rows, int cols)
-      : EmbeddingFwdTPP(rows, cols, cols, cols) {}
-  EmbeddingFwdTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            (LIBXSMM_MELTW_FLAG_UNARY_GS_COLS |
-             (sizeof(Tind) == 8 ? LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_8BYTES
-                                : LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_4BYTES)),
-            LIBXSMM_MELTW_TYPE_UNARY_GATHER) {}
-  void operator()(Tin* in0, Tind* in1, Tout* out) {
-    kernel((void*)in0, (void*)in1, NULL, (void*)out, NULL);
-  }
-  void ref(Tin* in0, Tind* in1, Tout* out) {
-    for (int r = 0; r < rows; r++) {
-      auto ind = in1[r];
-      for (int c = 0; c < cols; c++) {
-        out[r * ldo + c] = in0[ind * ldi + c];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi = 0;
-  int ldo = 0;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tind, typename Tout>
-class EmbeddingBwdTPP {
- public:
-  EmbeddingBwdTPP() {}
-  EmbeddingBwdTPP(int E)
-      : E(E),
-        kernel(
-            0,
-            E,
-            E,
-            E,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            (libxsmm_meltw_unary_flags)(
-                sizeof(Tind) == 8 ? LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_8BYTES
-                                  : LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_4BYTES),
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_COLS_IDX_OP_ADD) {}
-  void operator()(Tin* in0, Tind* in1, Tout* out, int N) {
-    unsigned long long _N = N;
-    kernel((void*)in0, (void*)in1, (void*)&_N, (void*)out, NULL);
-  }
-  void ref(Tin* in0, Tind* in1, Tout* out, int N) {
-    for (long v = 0; v < E; v++)
-      out[v] = 0;
-    for (long s = 0; s < N; s++) {
-      auto ind = in1[s];
-      for (long v = 0; v < E; v++)
-        out[v] += in0[ind * E + v];
-    }
-  }
-
- private:
-  int E = 0;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tind, typename Tout>
-class ScatterTPP {
- public:
-  ScatterTPP() {}
-  ScatterTPP(int rows, int cols, int ldi) : ScatterTPP(rows, cols, ldi, ldi) {}
-  ScatterTPP(int rows, int cols) : ScatterTPP(rows, cols, cols, cols) {}
-  ScatterTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            (LIBXSMM_MELTW_FLAG_UNARY_GS_COLS |
-             (sizeof(Tind) == 8 ? LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_8BYTES
-                                : LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_4BYTES)),
-            LIBXSMM_MELTW_TYPE_UNARY_SCATTER) {}
-  void operator()(Tin* in, Tind* out1, Tout* out) {
-    kernel((void*)in, NULL, NULL, (void*)out, (void*)out1);
-  }
-  void ref(Tin* in, Tind* out1, Tout* out) {
-    for (int r = 0; r < rows; r++) {
-      auto ind = out1[r];
-      for (int c = 0; c < cols; c++) {
-        out[ind * ldo + c] = in[r * ldi + c];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi = 0;
-  int ldo = 0;
-  UnaryTPP kernel;
-};
-
-class XformTPP {
- public:
-  XformTPP() {}
-  XformTPP(
-      libxsmm_blasint rows_i,
-      libxsmm_blasint cols_i,
-      libxsmm_blasint ldi,
-      libxsmm_blasint ldo,
-      libxsmm_datatype dtype,
-      libxsmm_meltw_unary_type type)
-      : rows(rows_i),
-        cols(cols_i),
-        ldi(ldi),
-        ldo(ldo),
-        dtype(dtype),
-        type(type),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            dtype,
-            dtype,
-            dtype,
-            LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            type) {}
-  void operator()(void* in, void* out) {
-    kernel(in, out);
-  }
-  typedef enum XFORM_TYPE {
-    XFORM_NONE_TPP = 0,
-    XFORM_XPOSE_TPP = 1,
-    XFORM_N2V_TPP = 2,
-    XFORM_XPOSE_N2V_TPP = 3,
-    XFORM_XPOSE_V2V_TPP = 4
-  } XFORM_TYPE;
-
- private:
-  libxsmm_blasint rows = 0;
-  libxsmm_blasint cols = 0;
-  libxsmm_blasint ldi;
-  libxsmm_blasint ldo;
-  libxsmm_datatype dtype;
-  libxsmm_meltw_unary_type type;
-  UnaryTPP kernel;
-};
-
-template <typename T>
-class XformExtTPP {
- public:
-  XformExtTPP() {}
-  XformExtTPP(
-      /* rows and cols as for input tensor */
-      int rows,
-      int cols,
-      XformTPP::XFORM_TYPE xtype,
-      bool ignore_vnni_for_fp32 = false)
-      : XformExtTPP(
-            rows,
-            cols,
-            (xtype == XformTPP::XFORM_N2V_TPP ? rows : cols),
-            (xtype == XformTPP::XFORM_N2V_TPP ? cols : rows),
-            xtype,
-            ignore_vnni_for_fp32) {}
-  XformExtTPP(
-      int in_rows,
-      int in_cols,
-      int out_rows,
-      int out_cols,
-      XformTPP::XFORM_TYPE xtype,
-      bool ignore_vnni_for_fp32 = false)
-      : XformExtTPP(
-            in_rows,
-            in_cols,
-            out_rows,
-            out_cols,
-            in_cols,
-            out_cols,
-            xtype,
-            ignore_vnni_for_fp32) {}
-  XformExtTPP(
-      int in_rows,
-      int in_cols,
-      int out_rows,
-      int out_cols,
-      int ldi,
-      int ldo,
-      XformTPP::XFORM_TYPE xtype,
-      bool ignore_vnni_for_fp32 = false)
-      : in_rows(in_rows),
-        in_cols(in_cols),
-        out_rows(out_rows),
-        out_cols(out_cols),
-        ldi(ldi),
-        ldo(ldo),
-        xtype(xtype),
-        dtype(XsmmDtype<T>()),
-        kernel(),
-        cvt(),
-        cpy(),
-        zero() {
-    libxsmm_meltw_unary_type unary_type = LIBXSMM_MELTW_TYPE_UNARY_IDENTITY;
-    if (ignore_vnni_for_fp32 == false) {
-      TPP_ASSERT(
-          (xtype == XformTPP::XFORM_XPOSE_TPP || dtype != LIBXSMM_DATATYPE_F32),
-          "Only Transpose Xofrm supportd for FP32 datatype, specified %d\n",
-          (int)xtype);
-    }
-    const int BS = xsmm_get_vnni_block_size(dtype);
-    if (xtype == XformTPP::XFORM_N2V_TPP) {
-      in_rows_p = out_rows;
-      in_cols_p = out_cols;
-      TPP_ASSERT(in_rows_p % BS == 0, "N2VTPP: unaligned number of rows\n");
-      if (BS == 1) {
-        unary_type = LIBXSMM_MELTW_TYPE_UNARY_IDENTITY;
-      } else if (BS == 2) {
-        unary_type = LIBXSMM_MELTW_TYPE_UNARY_TRANSFORM_NORM_TO_VNNI2;
-      } else if (BS == 4) {
-        unary_type = LIBXSMM_MELTW_TYPE_UNARY_TRANSFORM_NORM_TO_VNNI4;
-      } else {
-        TPP_ASSERT(false, "N2VTPP: unsupported packing size (%d)\n", BS);
-      }
-    } else {
-      in_rows_p = out_cols;
-      in_cols_p = out_rows;
-      if (dtype != LIBXSMM_DATATYPE_F32) {
-        if (xtype == XformTPP::XFORM_XPOSE_TPP) {
-          unary_type = LIBXSMM_MELTW_TYPE_UNARY_TRANSFORM_NORM_TO_NORMT;
-        } else if (xtype == XformTPP::XFORM_XPOSE_N2V_TPP) {
-          // unary_type = LIBXSMM_MELTW_TYPE_UNARY_TRANSFORM_NORM_TO_VNNIT;
-          unary_type = LIBXSMM_MELTW_TYPE_UNARY_TRANSFORM_NORM_TO_NORMT;
-          TPP_ASSERT(
-              in_cols_p % BS == 0, "XposeN2VTPP: uneven number of cols\n");
-        } else {
-          if (BS == 2) {
-            unary_type = LIBXSMM_MELTW_TYPE_UNARY_TRANSFORM_VNNI2_TO_VNNI2T;
-          } else if (BS == 4) {
-            unary_type = LIBXSMM_MELTW_TYPE_UNARY_TRANSFORM_VNNI4_TO_VNNI4T;
-          } else {
-            TPP_ASSERT(false, "V2VTPP: unsupported packing size (%d)\n", BS);
-          }
-          TPP_ASSERT(in_rows % BS == 0, "XposeV2VTPP: uneven number of rows\n");
-          TPP_ASSERT(
-              in_cols_p % BS == 0, "XposeV2VTPP: uneven number of cols\n");
-        }
-      } else {
-        unary_type = LIBXSMM_MELTW_TYPE_UNARY_TRANSFORM_NORM_TO_NORMT;
-      }
-    }
-    TPP_ASSERT(
-        (in_rows_p >= in_rows && in_cols_p >= in_cols),
-        "Invalid output rows or cols value\n");
-    TPP_ASSERT(
-        in_rows_p == in_rows || in_cols_p == in_cols,
-        "Padding can only be done in rows or cols\n");
-
-    if (xtype != XformTPP::XFORM_XPOSE_N2V_TPP) {
-      int ld = (in_rows_p != in_rows || in_cols_p != in_cols) ? in_cols_p : ldi;
-      kernel = XformTPP(in_rows_p, in_cols_p, ld, ldo, dtype, unary_type);
-    } else {
-      // LIBXSMM_MELTW_TYPE_UNARY_TRANSFORM_NORM_TO_VNNIT not implemented so use
-      // workaround...
-      kernel = XformTPP(
-          in_rows_p,
-          in_cols_p / BS,
-          ldi / BS,
-          ldo,
-          ((dtype == LIBXSMM_DATATYPE_BF16 && BS == 4) ||
-           (dtype == LIBXSMM_DATATYPE_BF8 && BS == 8))
-              ? LIBXSMM_DATATYPE_F64
-              : LIBXSMM_DATATYPE_F32,
-          unary_type);
-    }
-
-    if ((xtype == XformTPP::XFORM_N2V_TPP ||
-         xtype == XformTPP::XFORM_XPOSE_TPP) &&
-        in_rows_p != in_rows) {
-      cpy = CpyTPP<T>(in_rows, in_cols, ldi, in_cols);
-      zero = SetZeroTPP<T>(in_rows_p - in_rows, in_cols);
-      zero_offset = in_rows * in_cols;
-    } else if (xtype == XformTPP::XFORM_XPOSE_N2V_TPP && in_cols_p != in_cols) {
-      cpy = CpyTPP<T>(in_rows, in_cols, ldi, in_cols_p);
-      zero = SetZeroTPP<T>(in_rows, in_cols_p - in_cols, in_cols_p);
-      zero_offset = in_cols;
-    } else if (xtype == XformTPP::XFORM_XPOSE_V2V_TPP && in_cols_p != in_cols) {
-      cpy = CpyTPP<T>(in_rows / BS, in_cols * BS, ldi * BS, in_cols_p * BS);
-      zero = SetZeroTPP<T>(
-          in_rows / BS, (in_cols_p - in_cols) * BS, in_cols_p * BS);
-      zero_offset = in_cols * BS;
-    }
-    if (std::is_same<T, bfloat16>::value)
-      cvt = ConvertTPP<float, bfloat16>(in_rows, in_cols);
-  }
-  void operator()(T* in, T* out) {
-    if (in != out) {
-      if (in_rows_p != in_rows || in_cols_p != in_cols) {
-        T tmp[in_rows_p * in_cols_p];
-        cpy(in, tmp);
-        zero(tmp + zero_offset);
-        kernel((void*)tmp, (void*)out);
-      } else {
-        kernel((void*)in, (void*)out);
-      }
-    }
-  }
-  void ref(T* in, T* out) {
-    const int BS = xsmm_get_vnni_block_size(dtype);
-    if (xtype == XformTPP::XFORM_XPOSE_TPP) {
-      for (int i = 0; i < out_rows; i++) {
-        for (int j = 0; j < out_cols; j++) {
-          out[i * ldo + j] = in[j * ldi + i];
-        }
-      }
-    } else if (xtype == XformTPP::XFORM_N2V_TPP) {
-      for (int i = 0; i < out_rows / BS; i++) {
-        for (int j = 0; j < out_cols; j++) {
-          for (int k = 0; k < BS; k++) {
-            if (i * BS + k < in_rows) {
-              out[i * ldo * BS + j * BS + k] = in[i * ldi * BS + k * ldi + j];
-            } else {
-              out[i * ldo * BS + j * BS + k] = 0;
-            }
-          }
-        }
-      }
-    } else if (xtype == XformTPP::XFORM_XPOSE_N2V_TPP) {
-      for (int i = 0; i < out_rows / BS; i++) {
-        for (int j = 0; j < out_cols; j++) {
-          for (int k = 0; k < BS; k++) {
-            if (i * BS + k < in_cols) {
-              out[i * ldo * BS + j * BS + k] = in[j * ldi + i * BS + k];
-            } else {
-              out[i * ldo * BS + j * BS + k] = 0;
-            }
-          }
-        }
-      }
-    } else if (xtype == XformTPP::XFORM_XPOSE_V2V_TPP) {
-      for (int j = 0; j < out_rows / BS; j++) {
-        for (int i = 0; i < in_rows / BS; i++) {
-          for (int k = 0; k < BS; k++) { // RBS
-            for (int l = 0; l < BS; l++) { // CBS
-              if (j * BS + l < in_cols && i * BS + k < out_cols) {
-                out[j * ldo * BS + i * BS * BS + k * BS + l] =
-                    in[i * ldi * BS + j * BS * BS + l * BS + k];
-              } else {
-                out[j * ldo * BS + i * BS * BS + k * BS + l] = 0;
-              }
-            }
-          }
-        }
-      }
-    } else {
-      TPP_ASSERT(false, "Should not come here\n");
-    }
-  }
-
-  void operator()(float* in, bfloat16* out) {
-    bfloat16 tmp2[in_rows * in_cols];
-    cvt(in, tmp2);
-    if (in_rows_p != in_rows || in_cols_p != in_cols) {
-      T tmp[in_rows_p * in_cols_p];
-      cpy(tmp2, tmp);
-      zero(tmp + zero_offset);
-      kernel((void*)tmp, (void*)out);
-    } else {
-      kernel((void*)tmp2, (void*)out);
-    }
-  }
-  void ref(float* in, bfloat16* out) {
-    auto BS = xsmm_get_vnni_block_size(dtype);
-    if (xtype == XformTPP::XFORM_XPOSE_TPP) {
-      for (int i = 0; i < out_rows; i++) {
-        for (int j = 0; j < out_cols; j++) {
-          out[i * ldo + j] = in[j * ldi + i];
-        }
-      }
-    } else if (xtype == XformTPP::XFORM_N2V_TPP) {
-      for (int i = 0; i < out_rows / BS; i++) {
-        for (int j = 0; j < out_cols; j++) {
-          for (int k = 0; k < BS; k++) {
-            if (i * BS + k < in_rows) {
-              out[i * ldo * BS + j * BS + k] = in[i * ldi * BS + k * ldi + j];
-            } else {
-              out[i * ldo * BS + j * BS + k] = 0;
-            }
-          }
-        }
-      }
-    } else if (xtype == XformTPP::XFORM_XPOSE_N2V_TPP) {
-      for (int i = 0; i < out_rows / BS; i++) {
-        for (int j = 0; j < out_cols; j++) {
-          for (int k = 0; k < BS; k++) {
-            if (i * BS + k < in_cols) {
-              out[i * ldo * BS + j * BS + k] = in[j * ldi + i * BS + k];
-            } else {
-              out[i * ldo * BS + j * BS + k] = 0;
-            }
-          }
-        }
-      }
-    } else if (xtype == XformTPP::XFORM_XPOSE_V2V_TPP) {
-      for (int j = 0; j < out_rows / BS; j++) {
-        for (int i = 0; i < out_cols / BS; i++) {
-          for (int k = 0; k < BS; k++) { // RBS
-            for (int l = 0; l < BS; l++) { // CBS
-              if (j * BS + l < in_cols) {
-                out[j * ldo * BS + i * BS * BS + k * BS + l] =
-                    in[i * ldi * BS + j * BS * BS + l * BS + k];
-              } else {
-                out[j * ldo * BS + i * BS * BS + k * BS + l] = 0;
-              }
-            }
-          }
-        }
-      }
-    } else {
-      TPP_ASSERT(false, "Should not come here\n");
-    }
-  }
-  void operator()(int count, long str_in, long str_out, T* in, T* out) {
-    for (int i = 0; i < count; i++) {
-      this->operator()(&in[i * str_in], &out[i * str_out]);
-    }
-  }
-  void ref(int count, long str_in, long str_out, T* in, T* out) {
-    for (int i = 0; i < count; i++) {
-      this->ref(&in[i * str_in], &out[i * str_out]);
-    }
-  }
-  void operator()(
-      int count,
-      long str_in,
-      long str_out,
-      float* in,
-      bfloat16* out) {
-    for (int i = 0; i < count; i++) {
-      this->operator()(&in[i * str_in], &out[i * str_out]);
-    }
-  }
-  void ref(int count, long str_in, long str_out, float* in, bfloat16* out) {
-    for (int i = 0; i < count; i++) {
-      this->ref(&in[i * str_in], &out[i * str_out]);
-    }
-  }
-
- private:
-  libxsmm_blasint in_rows = 0;
-  libxsmm_blasint in_cols = 0;
-  libxsmm_blasint out_rows = 0;
-  libxsmm_blasint out_cols = 0;
-  libxsmm_blasint ldi;
-  libxsmm_blasint ldo;
-  int in_rows_p = 0;
-  int in_cols_p = 0;
-  XformTPP::XFORM_TYPE xtype;
-  libxsmm_datatype dtype;
-  int zero_offset = 0;
-  XformTPP kernel;
-  ConvertTPP<float, bfloat16> cvt;
-  CpyTPP<T> cpy;
-  SetZeroTPP<T> zero;
-};
-
-template <typename Tin, typename Tout>
-class BrgemmTPP {
- public:
-  BrgemmTPP() {}
-  BrgemmTPP(
-      long M,
-      long N,
-      long K,
-      long str_a,
-      long str_b,
-      float beta = 1.0,
-      int a_trans = 0,
-      int unroll_hint = 0)
-      : BrgemmTPP(
-            M,
-            N,
-            K,
-            str_a,
-            str_b,
-            (a_trans == 0 ? K : M),
-            N,
-            N,
-            beta,
-            a_trans,
-            unroll_hint) {}
-  BrgemmTPP(
-      long M,
-      long N,
-      long K,
-      long str_a,
-      long str_b,
-      long lda,
-      long ldb,
-      long ldc,
-      float beta,
-      int a_trans,
-      int unroll_hint,
-      int b_vnni = 1)
-      : M(M),
-        N(N),
-        K(K),
-        str_a(str_a),
-        str_b(str_b),
-        lda(lda),
-        ldb(ldb),
-        ldc(ldc),
-        beta(beta),
-        a_trans(a_trans),
-        unroll_hint(unroll_hint),
-        b_vnni(b_vnni),
-        k_gemm_with_tc(this, 0),
-        k_cfg(this, 1),
-        k_rls(this, 2),
-        k_gemm_no_tc(this, 3) {}
-  void config() {
-    k_cfg(NULL);
-  }
-  void release() {
-    k_rls(NULL);
-  }
-  void operator()(
-      Tin* A,
-      Tin* B,
-      Tout* C,
-      unsigned long long count,
-      bool no_tile_cfg = false) {
-    libxsmm_gemm_param gemm_param;
-    memset(&gemm_param, 0, sizeof(libxsmm_gemm_param));
-    gemm_param.op.tertiary = &count;
-    gemm_param.c.primary = (void*)C;
-    gemm_param.a.primary = (void*)B;
-    gemm_param.b.primary = (void*)A;
-    if (!no_tile_cfg) {
-      k_gemm_with_tc(&gemm_param);
-    } else {
-      k_gemm_no_tc(&gemm_param);
-    }
-  }
-  void ref(
-      Tin* A,
-      Tin* B,
-      Tout* C,
-      unsigned long long count,
-      bool no_tile_cfg = false) {
-    auto dtype = XsmmDtype<Tin>();
-    for (uint64_t c = 0; c < count; c++) {
-      auto A_ = &A[c * str_a];
-      auto B_ = &B[c * str_b];
-      if (std::is_same<Tin, float>::value || b_vnni == 0) {
-        for (int i = 0; i < M; i++) {
-          for (int j = 0; j < N; j++) {
-            if (beta == 0.0 && c == 0)
-              C[i * N + j] = 0.0;
-            for (int k = 0; k < K; k++) {
-              if (a_trans == 1) {
-                C[i * ldc + j] += A_[k * lda + i] * B_[k * ldb + j];
-              } else {
-                C[i * ldc + j] += A_[i * lda + k] * B_[k * ldb + j];
-              }
-            }
-          }
-        }
-      } else {
-        const int BS = xsmm_get_vnni_block_size(dtype);
-        for (int i = 0; i < M; i++) {
-          for (int j = 0; j < N; j++) {
-            float sum =
-                ((beta == 0.0 && c == 0) ? 0.0f : (float)C[i * ldc + j]);
-            for (int k = 0; k < K / BS; k++) {
-              for (int b = 0; b < BS; b++) {
-                if (a_trans == 1) {
-                  sum += (float)A_[k * lda * BS + i * BS + b] *
-                      (float)B_[k * ldb * BS + j * BS + b];
-                } else {
-                  sum += (float)A_[i * lda + k * BS + b] *
-                      (float)B_[k * ldb * BS + j * BS + b];
-                }
-              }
-            }
-            C[i * ldc + j] = (Tout)sum;
-          }
-        }
-      }
-    }
-  }
-
-  long flops() {
-    return 2L * M * N * K;
-  }
-
-  class BrgemmKernel : public BaseTPP {
-   public:
-    BrgemmKernel() {}
-    BrgemmKernel(BrgemmTPP* p, int config) : p(p), config(config) {
-      auto dt_in = XsmmDtype<Tin>();
-      auto dt_out = XsmmDtype<Tout>();
-      long type = -1;
-      if (dt_in == LIBXSMM_DATATYPE_F32) {
-        TPP_ASSERT(dt_out == LIBXSMM_DATATYPE_F32, "BRGEMM Assert\n");
-        type = 0;
-      } else if (dt_out == LIBXSMM_DATATYPE_F32) {
-        type = 1;
-      } else if (dt_in == LIBXSMM_DATATYPE_F16) {
-        TPP_ASSERT(dt_out == LIBXSMM_DATATYPE_F16, "BRGEMM Assert\n");
-        type = 2;
-      } else {
-        type = 3;
-      }
-      // if (type != 0)
-      //   TPP_ASSERT(
-      //       p->a_trans == 0, "A Transpose supported only for FP32 BRGEMM\n");
-      brgemm_type = type;
-      kernel.gemm = (libxsmm_gemmfunction)get_kernel();
-      initialized = true;
-    }
-    void operator()(libxsmm_gemm_param* gemm_param) {
-      if (!initialized)
-        return;
-      kernel.gemm(gemm_param);
-    }
-
-   protected:
-    std::string hash_str() override {
-      char hash[200];
-      snprintf(
-          hash,
-          200,
-          "brgemm_m%ld_n%ld_k%ld_a%ld_b%ld_t%ld_beta%d_at%d_uh%d_ld_a%ld_b%ld_c%ld_cfg%d_bv%d",
-          p->M,
-          p->N,
-          p->K,
-          p->str_a,
-          p->str_b,
-          brgemm_type,
-          (int)p->beta,
-          p->a_trans,
-          p->unroll_hint,
-          (long)p->lda,
-          (long)p->ldb,
-          (long)p->ldc,
-          config,
-          p->b_vnni);
-      return std::string(hash);
-    }
-    void* build_kernel() override {
-      // float alpha = 1.0;
-      libxsmm_gemm_shape l_shape;
-      libxsmm_gemm_batch_reduce_config l_brconfig;
-      libxsmm_bitfield l_flags = LIBXSMM_GEMM_FLAGS('N', 'N');
-      libxsmm_bitfield l_prefetch_flags = 0;
-      libxsmm_xmmfunction l_test_jit = {NULL};
-
-      if (p->a_trans == 1)
-        l_flags |= LIBXSMM_GEMM_FLAG_TRANS_B;
-      if (brgemm_type != 0) {
-        if (p->b_vnni) l_flags |= LIBXSMM_GEMM_FLAG_VNNI_A;
-        if (p->a_trans == 1) {
-          l_flags |= LIBXSMM_GEMM_FLAG_VNNI_B;
-        }
-      }
-      if (p->beta == 0)
-        l_flags |= LIBXSMM_GEMM_FLAG_BETA_0;
-
-      // config = 0 - normal
-      // config = 1 - no tile release
-      // config = 2 - no tile config
-      // config = 3 - brgemm with no tile config or release
-      if (config == 1) {
-        l_flags |= LIBXSMM_GEMM_FLAG_NO_RESET_TILECONFIG;
-      } else if (config == 2) {
-        l_flags |= LIBXSMM_GEMM_FLAG_NO_SETUP_TILECONFIG;
-      } else if (config == 3) {
-        l_flags |=
-            (LIBXSMM_GEMM_FLAG_NO_SETUP_TILECONFIG |
-             LIBXSMM_GEMM_FLAG_NO_RESET_TILECONFIG);
-      }
-
-      /* setting update GEMM struct */
-      l_shape.m = p->N;
-      l_shape.n = p->M;
-      l_shape.k = p->K;
-      l_shape.lda = p->ldb;
-      l_shape.ldb = p->lda;
-      l_shape.ldc = p->ldc;
-      l_shape.a_in_type = XsmmDtype<Tin>();
-      l_shape.b_in_type = XsmmDtype<Tin>();
-      l_shape.comp_type = LIBXSMM_DATATYPE_F32;
-      // TODO(jgong5): we should not always assume u8*i8 for int8 gemm
-      if (std::is_same<Tin, int8_t>()) {
-        l_flags |= LIBXSMM_GEMM_FLAG_B_UNSIGNED;
-        l_shape.comp_type = LIBXSMM_DATATYPE_I32;
-      }
-      l_shape.out_type = XsmmDtype<Tout>();
-
-      l_brconfig.br_type = LIBXSMM_GEMM_BATCH_REDUCE_STRIDE;
-      l_brconfig.br_stride_a_hint = p->str_b * sizeof(Tin);
-      l_brconfig.br_stride_b_hint = p->str_a * sizeof(Tin);
-      l_brconfig.br_unroll_hint = p->unroll_hint;
-
-      l_test_jit.gemm = libxsmm_dispatch_brgemm_v2(
-          l_shape, l_flags, l_prefetch_flags, l_brconfig);
-
-      return (void*)l_test_jit.gemm;
-    }
-
-   private:
-    BrgemmTPP* p;
-    int config;
-    libxsmm_xmmfunction kernel;
-    long brgemm_type = -1;
-  };
-
- private:
-  long M, N, K, str_a, str_b;
-  libxsmm_blasint lda;
-  libxsmm_blasint ldb;
-  libxsmm_blasint ldc;
-  float beta;
-  int a_trans;
-  long brgemm_type = -1;
-  int unroll_hint;
-  int b_vnni;
-  BrgemmKernel k_gemm_with_tc;
-  BrgemmKernel k_cfg;
-  BrgemmKernel k_rls;
-  BrgemmKernel k_gemm_no_tc;
-};
-
-template <typename Tin, typename Tout = Tin>
-class GeluFwdTPP {
- public:
-  GeluFwdTPP() {}
-  GeluFwdTPP(int N) : GeluFwdTPP(1, N) {}
-  GeluFwdTPP(int M, int N) : GeluFwdTPP(M, N, N, N) {}
-  GeluFwdTPP(int M, int N, int ldi, int ldo)
-      : M(M),
-        N(N),
-        ldi(ldi),
-        ldo(ldo),
-        kernel(
-            M,
-            N,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_GELU) {}
-  void operator()(Tin* in, Tout* out) {
-    kernel((void*)in, (void*)out);
-  }
-  void ref(Tin* in, Tout* out) {
-#ifdef __AVX512F__
-    for (int j = 0; j < M; j++) {
-      int i;
-      for (i = 0; i < ALIGNDOWN(N, 16); i += 16) {
-        auto vin = _mm512_loadu_ps_auto(&in[j * ldi + i]);
-        // auto vout = LIBXSMM_INTRINSICS_MM512_TANH_PS_GELU_FWD(vin);
-        auto vout = LIBXSMM_INTRINSICS_MM512_GELU_FWD_PS_MINIMAX3(vin);
-        _mm512_storeu_ps_auto(&out[j * ldo + i], vout);
-      }
-      if (i < N) {
-        int rem = N - i;
-        __mmask16 mask = (1 << rem) - 1;
-        auto vin = _mm512_maskz_loadu_ps_auto(mask, &in[j * ldi + i]);
-        // auto vout = LIBXSMM_INTRINSICS_MM512_TANH_PS_GELU_FWD(vin);
-        auto vout = LIBXSMM_INTRINSICS_MM512_GELU_FWD_PS_MINIMAX3(vin);
-        _mm512_mask_storeu_ps_auto(&out[j * ldo + i], mask, vout);
-      }
-    }
-#else
-    for (int j = 0; j < M; j++) {
-      for (int i = 0; i < N; i++) {
-        float x = in[j * ldi + i];
-        out[j * ldo + i] = (erff(x / sqrtf(2.0)) + 1.0) * 0.5 * x;
-      }
-    }
-#endif
-  }
-
- private:
-  int M = 0;
-  int N = 0;
-  int ldi = 0;
-  int ldo = 0;
-  UnaryTPP kernel;
-};
-
-template <typename T1, typename T2 = T1, typename T3 = T1>
-class GeluBwdTPP : public BaseTPP {
- public:
-  GeluBwdTPP() {}
-  GeluBwdTPP(int N) : N(N) {
-    kernel = (libxsmm_matrix_eqn_function)get_kernel();
-    initialized = true;
-  }
-  void operator()(T1* gout, T2* in, T3* gin) {
-    if (!initialized)
-      return;
-    libxsmm_matrix_eqn_param eqn_param;
-    libxsmm_matrix_arg arg_array[2];
-    arg_array[0].primary = (void*)gout;
-    arg_array[1].primary = (void*)in;
-    eqn_param.inputs = arg_array;
-    eqn_param.output.primary = (void*)gin;
-
-    kernel(&eqn_param);
-  }
-  void ref(T1* gout, T2* in, T3* gin) {
-#ifdef __AVX512F__
-    int i;
-    for (i = 0; i < ALIGNDOWN(N, 16); i += 16) {
-      auto vgout = _mm512_loadu_ps_auto(&gout[i]);
-      auto vin_gelu = _mm512_loadu_ps_auto(&in[i]);
-      auto vgin_gelu = LIBXSMM_INTRINSICS_MM512_TANH_PS_GELU_BWD(vin_gelu);
-      // auto vgin_gelu =
-      // LIBXSMM_INTRINSICS_MM512_GELU_BWD_PS_MINIMAX3(vin_gelu);
-      auto vout = _mm512_mul_ps(vgin_gelu, vgout);
-      _mm512_storeu_ps_auto(&gin[i], vout);
-    }
-    if (i < N) {
-      int rem = N - i;
-      __mmask16 mask = (1 << rem) - 1;
-      auto vgout = _mm512_maskz_loadu_ps_auto(mask, &gout[i]);
-      auto vin_gelu = _mm512_maskz_loadu_ps_auto(mask, &in[i]);
-      auto vgin_gelu = LIBXSMM_INTRINSICS_MM512_TANH_PS_GELU_BWD(vin_gelu);
-      // auto vgin_gelu =
-      // LIBXSMM_INTRINSICS_MM512_GELU_BWD_PS_MINIMAX3(vin_gelu);
-      auto vout = _mm512_mul_ps(vgin_gelu, vgout);
-      _mm512_mask_storeu_ps_auto(&gin[i], mask, vout);
-    }
-#else
-    constexpr float PI = 3.14159265358979323846;
-    for (int i = 0; i < N; i++) {
-      float x = in[i];
-      gin[i] = (float)gout[i] *
-          (0.5 + 0.5 * erff(x / sqrtf(2.0)) +
-           x / (sqrtf(2.0 * PI)) * expf(-0.5 * x * x));
-    }
-#endif
-  }
-
- protected:
-  std::string hash_str() override {
-    char hash[200];
-    snprintf(
-        hash,
-        200,
-        "gelu_bwd_eqn_t%d_%d_%d_i%d",
-        XsmmDtype<T1>(),
-        XsmmDtype<T2>(),
-        XsmmDtype<T3>(),
-        N);
-    return std::string(hash);
-  }
-  void* build_kernel() override {
-    auto dt1 = XsmmDtype<T1>();
-    auto dt2 = XsmmDtype<T2>();
-    auto dt3 = XsmmDtype<T3>();
-    libxsmm_blasint ld = N;
-    libxsmm_blasint my_eqn0 = libxsmm_matrix_eqn_create();
-    meqn_push_binary_op(my_eqn0, LIBXSMM_MELTW_TYPE_BINARY_MUL);
-    meqn_push_arg(my_eqn0, N, 1, N, 0, 0, dt1);
-    meqn_push_unary_op(my_eqn0, LIBXSMM_MELTW_TYPE_UNARY_GELU_INV);
-    meqn_push_arg(my_eqn0, N, 1, N, 1, 0, dt2);
-    debug_print_eqn_tree(my_eqn0);
-    return (void*)meqn_dispatch(N, 1, &ld, dt3, my_eqn0);
-  }
-
- private:
-  int N = 0;
-  libxsmm_matrix_eqn_function kernel = NULL;
-};
-
-template <typename Tin, typename Tout = Tin>
-class SigmoidFwdTPP {
- public:
-  SigmoidFwdTPP() {}
-  SigmoidFwdTPP(int rows, int cols) : SigmoidFwdTPP(rows, cols, cols, cols) {}
-  SigmoidFwdTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_SIGMOID) {}
-  void operator()(Tin* in, Tout* out) {
-    kernel((void*)in, (void*)out);
-  }
-  void ref(Tin* in, Tout* out) {
-    for (int i = 0; i < rows; i++) {
-      for (int j = 0; j < cols; j++) {
-        out[i * ldo + j] = 1. / (1. + exp(-in[i * ldi + j]));
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tout = Tin>
-class SigmoidBwdTPP {
- public:
-  SigmoidBwdTPP() {}
-  SigmoidBwdTPP(int rows, int cols) : SigmoidBwdTPP(rows, cols, cols, cols) {}
-  SigmoidBwdTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        ksub(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_0,
-            LIBXSMM_MELTW_TYPE_BINARY_SUB),
-        kmul(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_NONE,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL) {}
-  void operator()(Tin* in_grad_act, Tin* in_act, Tout* out) {
-    Tin val = 1.0;
-    ksub(&val, (void*)in_act, (void*)out);
-    kmul((void*)out, (void*)in_act, (void*)out);
-    kmul((void*)out, (void*)in_grad_act, (void*)out);
-  }
-  void ref(Tin* in_grad_act, Tin* in_act, Tout* out) {
-    for (int i = 0; i < rows; i++) {
-      for (int j = 0; j < cols; j++) {
-        int outIndex = i * ldo + j;
-        int inIndex = i * ldi + j;
-        out[outIndex] =
-            in_grad_act[inIndex] * (1.0 - in_act[inIndex]) * in_act[inIndex];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  BinaryTPP ksub;
-  BinaryTPP kmul;
-};
-
-template <typename Tin, typename Tout = Tin>
-class ReLUFwdTPP {
- public:
-  ReLUFwdTPP() {}
-  ReLUFwdTPP(int N, bool bm) : ReLUFwdTPP(1, N, bm) {}
-  ReLUFwdTPP(int rows, int cols, bool bm)
-      : ReLUFwdTPP(rows, cols, cols, cols, bm) {}
-  ReLUFwdTPP(int rows, int cols, int ldi, int ldo, bool bm)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            bm ? LIBXSMM_MELTW_FLAG_UNARY_BITMASK_2BYTEMULT
-               : LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_RELU) {}
-  void operator()(Tin* in, Tout* out, short* mask = NULL) {
-    kernel((void*)in, (void*)out, (void*)mask);
-  }
-  void ref(Tin* in, Tout* out, short* mask = NULL) {
-    kernel((void*)in, (void*)out, (void*)mask);
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tout = Tin>
-class ReLUBwdTPP {
- public:
-  ReLUBwdTPP() {}
-  ReLUBwdTPP(int N, bool bm) : ReLUBwdTPP(1, N, bm) {}
-  ReLUBwdTPP(int rows, int cols, bool bm)
-      : ReLUBwdTPP(rows, cols, cols, cols, bm) {}
-  ReLUBwdTPP(int rows, int cols, int ldi, int ldo, bool bm)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        bm(bm),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            bm ? LIBXSMM_MELTW_FLAG_UNARY_BITMASK_2BYTEMULT
-               : LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_RELU_INV) {}
-  void operator()(Tin* in, Tout* out, Tin* in2 = NULL, short* mask = NULL) {
-    kernel(in, bm ? (void*)mask : (void*)in2, NULL, out, NULL);
-  }
-  void ref(Tin* in, Tout* out, Tin* in2 = NULL, short* mask = NULL) {
-    kernel(in, bm ? (void*)mask : (void*)in2, NULL, out, NULL);
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  bool bm;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tout = Tin>
-class ELUFwdTPP {
- public:
-  ELUFwdTPP() {}
-  ELUFwdTPP(int N, float alpha) : ELUFwdTPP(1, N, alpha) {}
-  ELUFwdTPP(int rows, int cols, float alpha)
-      : ELUFwdTPP(rows, cols, cols, cols, alpha) {}
-  ELUFwdTPP(int rows, int cols, int ldi, int ldo, float alpha)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        alpha(alpha),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_ELU) {}
-  void operator()(Tin* in, Tout* out) {
-    kernel(in, NULL, NULL, &alpha, NULL, NULL, out, NULL);
-  }
-  void ref(Tin* in, Tout* out) {
-    Tin a = alpha;
-    for (int i = 0; i < rows; i++)
-      for (int j = 0; j < cols; j++)
-        out[i * ldo + j] = in[i * ldi + j] > 0 ? in[i * ldi + j]
-                                               : a * (exp(in[i * ldi + j]) - 1);
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  float alpha;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tout = Tin>
-class ELUBwdTPP {
- public:
-  ELUBwdTPP() {}
-  ELUBwdTPP(int N, float alpha) : ELUBwdTPP(1, N, alpha) {}
-  ELUBwdTPP(int rows, int cols, float alpha)
-      : ELUBwdTPP(rows, cols, cols, cols, alpha) {}
-  ELUBwdTPP(int rows, int cols, int ldi, int ldo, float alpha)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        alpha(alpha),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_ELU_INV) {}
-  void operator()(Tin* in, Tin* in2, Tout* out) {
-    kernel(in, in2, NULL, &alpha, NULL, NULL, out, NULL);
-  }
-  void ref(Tin* in, Tin* in2, Tout* out) {
-    Tin a = alpha;
-    for (int i = 0; i < rows; i++)
-      for (int j = 0; j < cols; j++)
-        out[i * ldo + j] = in2[i * ldi + j] > 0
-            ? in[i * ldi + j]
-            : in[i * ldi + j] * in2[i * ldi + j] + a * in[i * ldi + j];
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  float alpha;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tout = Tin>
-class LeakyReLUFwdTPP {
- public:
-  LeakyReLUFwdTPP() {}
-  LeakyReLUFwdTPP(int N, float alpha) : LeakyReLUFwdTPP(1, N, alpha) {}
-  LeakyReLUFwdTPP(int rows, int cols, float alpha)
-      : LeakyReLUFwdTPP(rows, cols, cols, cols, alpha) {}
-  LeakyReLUFwdTPP(int rows, int cols, int ldi, int ldo, float alpha)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        alpha(alpha),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_BITMASK_2BYTEMULT,
-            LIBXSMM_MELTW_TYPE_UNARY_LEAKY_RELU) {}
-  void operator()(Tin* in, Tout* out, short* mask = NULL) {
-    kernel(in, NULL, NULL, &alpha, NULL, NULL, out, mask);
-  }
-  void ref(Tin* in, Tout* out, short* mask = NULL) {
-    float a = alpha;
-    // std::cout << " op: " << out << " inp: "<< in << std::endl;
-    for (int i = 0; i < rows; i++)
-      for (int j = 0; j < cols; j++) {
-        out[i * ldo + j] = in[i * ldi + j] > 0 ? (Tout)in[i * ldi + j]
-                                               : (Tout)(a * (in[i * ldi + j]));
-      }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  float alpha;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tout = Tin>
-class LeakyReLUBwdTPP {
- public:
-  LeakyReLUBwdTPP() {}
-  LeakyReLUBwdTPP(int N, float alpha) : LeakyReLUBwdTPP(1, N, alpha) {}
-  LeakyReLUBwdTPP(int rows, int cols, float alpha)
-      : LeakyReLUBwdTPP(rows, cols, cols, cols, alpha) {}
-  LeakyReLUBwdTPP(int rows, int cols, int ldi, int ldo, float alpha)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        alpha(alpha),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_BITMASK_2BYTEMULT,
-            LIBXSMM_MELTW_TYPE_UNARY_LEAKY_RELU_INV) {}
-  void operator()(Tin* in, Tout* out, Tin* in2 = NULL, short* mask = NULL) {
-    kernel(in, mask, NULL, &alpha, NULL, NULL, out, NULL);
-  }
-  void ref(Tin* in, Tout* out, Tin* in2, short* mask = NULL) {
-    float a = alpha;
-    for (int i = 0; i < rows; i++)
-      for (int j = 0; j < cols; j++) {
-        float grad_out = in[i * ldi + j];
-        out[i * ldo + j] =
-            in2[i * ldi + j] > 0 ? (Tout)grad_out : (Tout)(a * grad_out);
-      }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  float alpha;
-  UnaryTPP kernel;
-};
-
-template <typename T>
-class SiLUFwdTPP {
- public:
-  SiLUFwdTPP() {}
-  SiLUFwdTPP(int N) : SiLUFwdTPP(1, N) {}
-  SiLUFwdTPP(int rows, int cols) : SiLUFwdTPP(rows, cols, cols, cols) {}
-  SiLUFwdTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        sigmoid(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<T>(),
-            XsmmDtype<T>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_SIGMOID),
-        mul(rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<T>(),
-            XsmmDtype<T>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_NONE,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL) {}
-  void operator()(T* in, T* out, T* sigout) {
-    sigmoid((void*)in, (void*)sigout);
-    mul((void*)in, (void*)sigout, (void*)out);
-  }
-  void ref(T* in, T* out, T* sigout) {
-    for (int i = 0; i < rows; i++) {
-      for (int j = 0; j < cols; j++) {
-        sigout[i * ldo + j] = 1. / (1. + exp(-in[i * ldi + j]));
-        out[i * ldo + j] = in[i * ldi + j] * sigout[i * ldo + j];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  UnaryTPP sigmoid;
-  BinaryTPP mul;
-};
-
-template <typename Tin, typename Tout = Tin>
-class SiLUBwdTPP : public BaseTPP {
- public:
-  SiLUBwdTPP() {}
-  SiLUBwdTPP(int N) : SiLUBwdTPP(1, N) {}
-  SiLUBwdTPP(int rows, int cols) : SiLUBwdTPP(rows, cols, cols, cols) {}
-  SiLUBwdTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows), cols(cols), ldi(ldi), ldo(ldo) {
-    kernel = (libxsmm_matrix_eqn_function)get_kernel();
-    initialized = true;
-  }
-  void operator()(Tin* in, Tin* in2, Tin* in3, Tout* out) {
-    if (!initialized)
-      return;
-    libxsmm_matrix_eqn_param eqn_param;
-    libxsmm_matrix_arg arg_array[5];
-    float one = 1.;
-    arg_array[0].primary = (void*)in;
-    arg_array[1].primary = (void*)in2;
-    arg_array[2].primary = (void*)in3;
-    arg_array[3].primary = (void*)&one;
-    arg_array[4].primary = (void*)in2;
-    eqn_param.inputs = arg_array;
-    eqn_param.output.primary = (void*)out;
-
-    kernel(&eqn_param);
-  }
-  void ref(Tin* in, Tin* in2, Tin* in3, Tout* out) {
-    for (int i = 0; i < rows; i++) {
-      for (int j = 0; j < cols; j++) {
-        float grad_out = in[i * ldi + j];
-        float si = in2[i * ldi + j];
-        float fout = in3[i * ldi + j];
-
-        out[i] = grad_out * (si + fout * (1 - si));
-      }
-    }
-  }
-
- protected:
-  std::string hash_str() override {
-    char hash[200];
-    snprintf(hash, 200, "silu_bwd_eqn_%d_%d", rows, cols);
-    return std::string(hash);
-  }
-  void* build_kernel() override {
-    libxsmm_blasint my_eqn0 = libxsmm_matrix_eqn_create();
-    meqn_push_binary_op(my_eqn0, LIBXSMM_MELTW_TYPE_BINARY_MUL);
-    meqn_push_arg(my_eqn0, cols, rows, ldo, 0, 0, LIBXSMM_DATATYPE_F32);
-    meqn_push_binary_op(my_eqn0, LIBXSMM_MELTW_TYPE_BINARY_ADD);
-    meqn_push_arg(my_eqn0, cols, rows, ldo, 1, 0, LIBXSMM_DATATYPE_F32);
-    meqn_push_binary_op(my_eqn0, LIBXSMM_MELTW_TYPE_BINARY_MUL);
-    meqn_push_arg(my_eqn0, cols, rows, ldo, 2, 0, LIBXSMM_DATATYPE_F32);
-    meqn_push_binary_op(
-        my_eqn0,
-        LIBXSMM_MELTW_TYPE_BINARY_SUB,
-        LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_0);
-    meqn_push_arg(my_eqn0, cols, rows, ldo, 3, 0, LIBXSMM_DATATYPE_F32);
-    meqn_push_arg(my_eqn0, cols, rows, ldo, 4, 0, LIBXSMM_DATATYPE_F32);
-
-    auto func0 = meqn_dispatch(cols, rows, &ldo, XsmmDtype<Tout>(), my_eqn0);
-    return (void*)func0;
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  libxsmm_matrix_eqn_function kernel = NULL;
-};
-
-template <typename Tin, typename Tout = Tin>
-class DropOutFwdTPP {
- public:
-  DropOutFwdTPP() {}
-  DropOutFwdTPP(int N, float p) : DropOutFwdTPP(1, N, p) {}
-  DropOutFwdTPP(int rows, int cols, float p)
-      : DropOutFwdTPP(rows, cols, cols, cols, p) {}
-  DropOutFwdTPP(int rows, int cols, int ldi, int ldo, float p)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        p(p),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_BITMASK_2BYTEMULT,
-            LIBXSMM_MELTW_TYPE_UNARY_DROPOUT) {}
-  void operator()(Tin* in, void* rng_state, Tout* out, short* mask) {
-    kernel(in, NULL, NULL, &p, rng_state, NULL, out, mask);
-  }
-  void ref(Tin* in, void* rng_state, Tout* out, short* mask) {
-    kernel(in, NULL, NULL, &p, rng_state, NULL, out, mask);
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  float p;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tout = Tin>
-class DropOutBwdTPP {
- public:
-  DropOutBwdTPP() {}
-  DropOutBwdTPP(int N, float p) : DropOutBwdTPP(1, N, p) {}
-  DropOutBwdTPP(int rows, int cols, float p)
-      : DropOutBwdTPP(rows, cols, cols, cols, p) {}
-  DropOutBwdTPP(int rows, int cols, int ldi, int ldo, float p)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        p(p),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_BITMASK_2BYTEMULT,
-            LIBXSMM_MELTW_TYPE_UNARY_DROPOUT_INV) {}
-  void operator()(Tin* in, Tout* out, short* mask) {
-    kernel(in, mask, NULL, &p, NULL, NULL, out, NULL);
-  }
-  void ref(Tin* in, Tout* out, short* mask) {
-    kernel(in, mask, NULL, &p, NULL, NULL, out, NULL);
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  float p;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tout>
-class SoftMaxFwdTPP {
- public:
-  SoftMaxFwdTPP() {}
-  SoftMaxFwdTPP(int S1, int S2, int S3)
-      : S1(S1), S2(S2), S3(S3), eqn0(S1, S2, S3), eqn1(S1, S2, S3) {}
-  void operator()(Tin* in, Tout* out) {
-    LIBXSMM_ALIGNED(float tmp[S1 * S3], 64);
-    for (int s2 = 0; s2 < S2; s2++) {
-      eqn0(&in[s2 * S3], tmp);
-      eqn1(tmp, &out[s2 * S3]);
-    }
-  }
-  void ref(Tin* pinp, Tout* pout) {
-    int s1, s2, s3;
-    LIBXSMM_VLA_DECL(3, Tin, inp, pinp, S2, S3);
-    LIBXSMM_VLA_DECL(3, Tout, out, pout, S2, S3);
-#if defined(__AVX512F__)
-    for (s2 = 0; s2 < S2; s2++) {
-      float tmp[S1][S3];
-      float max =
-          upconvert_to_float(LIBXSMM_VLA_ACCESS(3, inp, 0, s2, 0, S2, S3));
-      float sum = 0.0;
-      __m512 vmax = _mm512_set1_ps(max);
-      __m512 vsum = _mm512_setzero_ps();
-
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < ALIGNDOWN(S3, 16); s3 += 16) {
-          vmax = _mm512_max_ps(
-              _mm512_loadu_ps_auto(
-                  &LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3)),
-              vmax);
-        }
-        if (s3 < S3) {
-          int rem = S3 - s3;
-          __mmask16 mask = (1 << rem) - 1;
-          vmax = _mm512_mask_max_ps(
-              vmax,
-              mask,
-              _mm512_maskz_loadu_ps_auto(
-                  mask, &LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3)),
-              vmax);
-        }
-      }
-      max = _mm512_reduce_max_ps(vmax);
-      vmax = _mm512_set1_ps(max);
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < ALIGNDOWN(S3, 16); s3 += 16) {
-          __m512 vz = LIBXSMM_INTRINSICS_MM512_EXP_PS_3DTS(_mm512_sub_ps(
-              // __m512 vz = LIBXSMM_INTRINSICS_MM512_EXP_PS (_mm512_sub_ps(
-              _mm512_loadu_ps_auto(
-                  &LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3)),
-              vmax));
-          _mm512_storeu_ps(&tmp[s1][s3], vz);
-          vsum = _mm512_add_ps(vsum, vz);
-        }
-        if (s3 < S3) {
-          int rem = S3 - s3;
-          __mmask16 mask = (1 << rem) - 1;
-          __m512 vz = LIBXSMM_INTRINSICS_MM512_EXP_PS_3DTS(_mm512_sub_ps(
-              // __m512 vz = LIBXSMM_INTRINSICS_MM512_EXP_PS (_mm512_sub_ps(
-              _mm512_maskz_loadu_ps_auto(
-                  mask, &LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3)),
-              vmax));
-          _mm512_mask_storeu_ps(&tmp[s1][s3], mask, vz);
-          vsum = _mm512_mask_add_ps(vsum, mask, vsum, vz);
-        }
-      }
-      sum = _mm512_reduce_add_ps(vsum);
-      sum = 1.0 / sum;
-      vsum = _mm512_set1_ps(sum);
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < ALIGNDOWN(S3, 16); s3 += 16) {
-          _mm512_storeu_ps_auto(
-              &LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3),
-              _mm512_mul_ps(vsum, _mm512_loadu_ps(&tmp[s1][s3])));
-        }
-        if (s3 < S3) {
-          int rem = S3 - s3;
-          __mmask16 mask = (1 << rem) - 1;
-          _mm512_mask_storeu_ps_auto(
-              &LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3),
-              mask,
-              _mm512_mul_ps(vsum, _mm512_maskz_loadu_ps(mask, &tmp[s1][s3])));
-        }
-      }
-    }
-#else
-    for (s2 = 0; s2 < S2; s2++) {
-      float tmp[S1][S3];
-      float max =
-          upconvert_to_float(LIBXSMM_VLA_ACCESS(3, inp, 0, s2, 0, S2, S3));
-      float sum = 0.0;
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          float cur = upconvert_to_float(
-              LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3));
-          if (max < cur)
-            max = cur;
-        }
-      }
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          float cur = upconvert_to_float(
-              LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3));
-          float z = expf(cur - max);
-          tmp[s1][s3] = z;
-          sum += z;
-        }
-      }
-      sum = 1.0 / sum;
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          float cur = tmp[s1][s3] * sum;
-          // libxsmm_rne_convert_fp32_bf16( &cur, &LIBXSMM_VLA_ACCESS(3, out,
-          // s1, s2, s3, S2, S3), 1 );
-          LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3) = cur;
-        }
-      }
-    }
-#endif
-  }
-  class Eqn0 : BaseTPP {
-   public:
-    Eqn0() {}
-    Eqn0(int S1, int S2, int S3) : S1(S1), S2(S2), S3(S3) {
-      kernel = (libxsmm_matrix_eqn_function)get_kernel();
-      initialized = true;
-    }
-    void operator()(Tin* in, float* out) {
-      if (!initialized)
-        return;
-      libxsmm_matrix_eqn_param eqn_param;
-      libxsmm_matrix_arg arg_array[1];
-      arg_array[0].primary = (void*)in;
-      eqn_param.inputs = arg_array;
-      eqn_param.output.primary = (void*)out;
-
-      kernel(&eqn_param);
-    }
-
-   protected:
-    std::string hash_str() override {
-      char hash[200];
-      snprintf(
-          hash,
-          200,
-          "softmax_fwd_eqn0_ti%d_to%d_S1%d_S2%d_S3%d",
-          XsmmDtype<Tin>(),
-          LIBXSMM_DATATYPE_F32,
-          S1,
-          S2,
-          S3);
-      return std::string(hash);
-    }
-    void* build_kernel() override {
-      auto dt_in = XsmmDtype<Tin>();
-      libxsmm_blasint tmp_ld = S3;
-      libxsmm_blasint ld = S2 * S3;
-      libxsmm_blasint my_eqn0 = libxsmm_matrix_eqn_create();
-      meqn_push_unary_op(my_eqn0, LIBXSMM_MELTW_TYPE_UNARY_EXP);
-      meqn_push_binary_op(
-          my_eqn0,
-          LIBXSMM_MELTW_TYPE_BINARY_SUB,
-          LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-      meqn_push_arg(my_eqn0, S3, S1, ld, 0, 0, dt_in);
-      meqn_push_unary_op(
-          my_eqn0,
-          LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_MAX,
-          LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS);
-      meqn_push_unary_op(
-          my_eqn0,
-          LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_MAX,
-          LIBXSMM_MELTW_FLAG_UNARY_REDUCE_COLS);
-      meqn_push_arg(my_eqn0, S3, S1, ld, 0, 0, dt_in);
-      debug_print_eqn_tree(my_eqn0); // printf
-      return (void*)meqn_dispatch(
-          S3, S1, &tmp_ld, LIBXSMM_DATATYPE_F32, my_eqn0);
-    }
-
-   private:
-    int S1, S2, S3;
-    libxsmm_matrix_eqn_function kernel = NULL;
-  };
-
-  class Eqn1 : BaseTPP {
-   public:
-    Eqn1() {}
-    Eqn1(int S1, int S2, int S3) : S1(S1), S2(S2), S3(S3) {
-      kernel = (libxsmm_matrix_eqn_function)get_kernel();
-      initialized = true;
-    }
-    void operator()(float* in, Tout* out) {
-      if (!initialized)
-        return;
-      libxsmm_matrix_eqn_param eqn_param;
-      libxsmm_matrix_arg arg_array[1];
-      arg_array[0].primary = (void*)in;
-      eqn_param.inputs = arg_array;
-      eqn_param.output.primary = (void*)out;
-
-      kernel(&eqn_param);
-    }
-
-   protected:
-    std::string hash_str() override {
-      char hash[200];
-      snprintf(
-          hash,
-          200,
-          "softmax_fwd_eqn1_ti%d_to%d_S1%d_S2%d_S3%d",
-          LIBXSMM_DATATYPE_F32,
-          XsmmDtype<Tout>(),
-          S1,
-          S2,
-          S3);
-      return std::string(hash);
-    }
-    void* build_kernel() override {
-      auto dt_out = XsmmDtype<Tout>();
-      libxsmm_blasint tmp_ld = S3;
-      libxsmm_blasint ld = S2 * S3;
-      libxsmm_blasint my_eqn1 = libxsmm_matrix_eqn_create();
-      meqn_push_binary_op(
-          my_eqn1,
-          LIBXSMM_MELTW_TYPE_BINARY_MUL,
-          LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-      meqn_push_arg(my_eqn1, S3, S1, tmp_ld, 0, 0, LIBXSMM_DATATYPE_F32);
-      meqn_push_unary_op(my_eqn1, LIBXSMM_MELTW_TYPE_UNARY_RECIPROCAL);
-      meqn_push_unary_op(
-          my_eqn1,
-          LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD,
-          LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS);
-      meqn_push_unary_op(
-          my_eqn1,
-          LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD,
-          LIBXSMM_MELTW_FLAG_UNARY_REDUCE_COLS);
-      meqn_push_arg(my_eqn1, S3, S1, tmp_ld, 0, 0, LIBXSMM_DATATYPE_F32);
-      /*debug_print_eqn_tree( my_eqn1 );*/
-      return (void*)meqn_dispatch(S3, S1, &ld, dt_out, my_eqn1);
-    }
-
-   private:
-    int S1, S2, S3;
-    libxsmm_matrix_eqn_function kernel = NULL;
-  };
-
- private:
-  int S1, S2, S3;
-  Eqn0 eqn0;
-  Eqn1 eqn1;
-};
-
-template <typename T1, typename T2, typename T3>
-class SoftMaxBwdTPP {
- public:
-  SoftMaxBwdTPP() {}
-  SoftMaxBwdTPP(int S1, int S2, int S3)
-      : S1(S1), S2(S2), S3(S3), eqn0(S1, S2, S3, 0), eqn1(S1, S2, S3, 1) {}
-  void operator()(T1* gin, T2* gout, T3* out) {
-    LIBXSMM_ALIGNED(float tmp[S1 * S3], 64);
-    for (int s2 = 0; s2 < S2; s2++) {
-      libxsmm_matrix_eqn_param eqn_param;
-      libxsmm_matrix_arg arg_array[2];
-      arg_array[0].primary = (void*)&gout[s2 * S3];
-      arg_array[1].primary = (void*)&out[s2 * S3];
-      eqn_param.inputs = arg_array;
-      eqn_param.output.primary = (void*)tmp;
-      eqn0(&eqn_param);
-
-      arg_array[0].primary = (void*)tmp;
-      eqn_param.output.primary = (void*)&gin[s2 * S3];
-      eqn1(&eqn_param);
-    }
-  }
-  void ref(T1* pgradinp, T2* pgradout, T3* pout) {
-    int s1, s2, s3;
-    LIBXSMM_VLA_DECL(3, T1, ginp, pgradinp, S2, S3);
-    LIBXSMM_VLA_DECL(3, T2, gout, pgradout, S2, S3);
-    LIBXSMM_VLA_DECL(3, T3, out, pout, S2, S3);
-#if defined(__AVX512F__)
-    for (s2 = 0; s2 < S2; s2++) {
-      float sum = 0.0;
-      __m512 vsum = _mm512_setzero_ps();
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < ALIGNDOWN(S3, 16); s3 += 16) {
-          __m512 vgo =
-              _mm512_loadu_ps(&LIBXSMM_VLA_ACCESS(3, gout, s1, s2, s3, S2, S3));
-          __m512 vo = _mm512_loadu_ps_auto(
-              &LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3));
-          vsum = _mm512_fmadd_ps(vgo, vo, vsum);
-        }
-        if (s3 < S3) {
-          int rem = S3 - s3;
-          __mmask16 mask = (1 << rem) - 1;
-          __m512 vgo = _mm512_maskz_loadu_ps(
-              mask, &LIBXSMM_VLA_ACCESS(3, gout, s1, s2, s3, S2, S3));
-          __m512 vo = _mm512_maskz_loadu_ps_auto(
-              mask, &LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3));
-          vsum = _mm512_fmadd_ps(vgo, vo, vsum);
-        }
-      }
-      sum = _mm512_reduce_add_ps(vsum);
-      vsum = _mm512_set1_ps(sum);
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < ALIGNDOWN(S3, 16); s3 += 16) {
-          __m512 tmp = _mm512_sub_ps(
-              _mm512_loadu_ps(&LIBXSMM_VLA_ACCESS(3, gout, s1, s2, s3, S2, S3)),
-              vsum);
-          _mm512_storeu_ps(
-              &LIBXSMM_VLA_ACCESS(3, ginp, s1, s2, s3, S2, S3),
-              _mm512_mul_ps(
-                  _mm512_loadu_ps_auto(
-                      &LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3)),
-                  tmp));
-        }
-        if (s3 < S3) {
-          int rem = S3 - s3;
-          __mmask16 mask = (1 << rem) - 1;
-          __m512 tmp = _mm512_sub_ps(
-              _mm512_maskz_loadu_ps(
-                  mask, &LIBXSMM_VLA_ACCESS(3, gout, s1, s2, s3, S2, S3)),
-              vsum);
-          _mm512_mask_storeu_ps(
-              &LIBXSMM_VLA_ACCESS(3, ginp, s1, s2, s3, S2, S3),
-              mask,
-              _mm512_mul_ps(
-                  _mm512_maskz_loadu_ps_auto(
-                      mask, &LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3)),
-                  tmp));
-        }
-      }
-    }
-#else
-    for (s2 = 0; s2 < S2; s2++) {
-      float sum = 0.0;
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          sum += LIBXSMM_VLA_ACCESS(3, gout, s1, s2, s3, S2, S3) *
-              upconvert_to_float(
-                     LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3));
-        }
-      }
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          LIBXSMM_VLA_ACCESS(3, ginp, s1, s2, s3, S2, S3) =
-              upconvert_to_float(
-                  LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3)) *
-              (LIBXSMM_VLA_ACCESS(3, gout, s1, s2, s3, S2, S3) - sum);
-        }
-      }
-    }
-#endif
-  }
-
-  class Eqn : BaseTPP {
-   public:
-    Eqn() {}
-    Eqn(int S1, int S2, int S3, int eqn_no)
-        : S1(S1), S2(S2), S3(S3), eqn_no(eqn_no) {
-      kernel = (libxsmm_matrix_eqn_function)get_kernel();
-      initialized = true;
-    }
-    void operator()(libxsmm_matrix_eqn_param* eqn_param) {
-      if (!initialized)
-        return;
-      kernel(eqn_param);
-    }
-
-   protected:
-    std::string hash_str() override {
-      char hash[200];
-      snprintf(
-          hash,
-          200,
-          "softmax_bwd_eqn%d_t1%d_t2%d_t3%d_S1%d_S2%d_S3%d",
-          eqn_no,
-          XsmmDtype<T2>(),
-          XsmmDtype<T3>(),
-          LIBXSMM_DATATYPE_F32,
-          S1,
-          S2,
-          S3);
-      return std::string(hash);
-    }
-    void* build_kernel() override {
-      auto dt_1 = XsmmDtype<T1>();
-      auto dt_2 = XsmmDtype<T2>();
-      auto dt_3 = XsmmDtype<T3>();
-      libxsmm_blasint tmp_ld = S3;
-      libxsmm_blasint ld = S2 * S3;
-      libxsmm_matrix_eqn_function func;
-      if (eqn_no == 0) {
-        libxsmm_blasint my_eqn2 = libxsmm_matrix_eqn_create();
-        meqn_push_binary_op(my_eqn2, LIBXSMM_MELTW_TYPE_BINARY_MUL);
-        meqn_push_arg(my_eqn2, S3, S1, ld, 0, 0, dt_2);
-        meqn_push_arg(my_eqn2, S3, S1, ld, 1, 0, dt_3);
-        debug_print_eqn_tree(my_eqn2); // printf
-        func = meqn_dispatch(S3, S1, &tmp_ld, LIBXSMM_DATATYPE_F32, my_eqn2);
-      } else if (eqn_no == 1) {
-        libxsmm_blasint my_eqn3 = libxsmm_matrix_eqn_create();
-#if 1
-        meqn_push_ternary_op(
-            my_eqn3,
-            LIBXSMM_MELTW_TYPE_TERNARY_NMULADD,
-            LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_0 |
-                LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-        meqn_push_unary_op(
-            my_eqn3,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS);
-        meqn_push_unary_op(
-            my_eqn3,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_COLS);
-        meqn_push_arg(my_eqn3, S3, S1, tmp_ld, 0, 0, LIBXSMM_DATATYPE_F32);
-        meqn_push_arg(my_eqn3, S3, S1, tmp_ld, 0, 0, LIBXSMM_DATATYPE_F32);
-        meqn_push_arg(my_eqn3, S3, S1, ld, 1, 0, dt_3);
-#else
-        meqn_push_binary_op(my_eqn3, LIBXSMM_MELTW_TYPE_BINARY_SUB);
-        meqn_push_arg(my_eqn3, S3, S1, tmp_ld, 0, 0, LIBXSMM_DATATYPE_F32);
-        meqn_push_binary_op(
-            my_eqn3,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-        meqn_push_arg(my_eqn3, S3, S1, ld, 1, 0, dt_3);
-        meqn_push_unary_op(
-            my_eqn3,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS);
-        meqn_push_unary_op(
-            my_eqn3,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_COLS);
-        meqn_push_arg(my_eqn3, S3, S1, tmp_ld, 0, 0, LIBXSMM_DATATYPE_F32);
-#endif
-        debug_print_eqn_tree(my_eqn3);
-        func = meqn_dispatch(S3, S1, &ld, dt_1, my_eqn3);
-      } else {
-        TPP_ASSERT(false, "Should not come here\n");
-      }
-      return (void*)func;
-    }
-
-   private:
-    int S1, S2, S3, eqn_no;
-    libxsmm_matrix_eqn_function kernel = NULL;
-  };
-
- private:
-  int S1, S2, S3;
-  Eqn eqn0, eqn1;
-};
-
-template <typename Tin, typename Tout>
-class VarSoftMaxFwdTPP {
- public:
-  VarSoftMaxFwdTPP() {}
-  VarSoftMaxFwdTPP(int S2, int S3)
-      : S2(S2),
-        S3(S3),
-        kmax(
-            1,
-            S3,
-            S3,
-            S3,
-            XsmmDtype<Tin>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_MAX),
-        ksub(
-            1,
-            S3,
-            S3,
-            S3,
-            XsmmDtype<Tin>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1,
-            LIBXSMM_MELTW_TYPE_BINARY_SUB),
-        kexp(
-            1,
-            S3,
-            S3,
-            S3,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_EXP),
-        ksum(
-            1,
-            S3,
-            S3,
-            S3,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD),
-        kmul(
-            1,
-            S3,
-            S3,
-            S3,
-            LIBXSMM_DATATYPE_F32,
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL) {}
-  void operator()(int S1, Tin* in, Tout* out) {
-    LIBXSMM_ALIGNED(float tmp[S1 * S3], 64);
-    for (int s2 = 0; s2 < S2; s2++) {
-      Tin max = in[s2 * S3];
-      float sum = 0.0f;
-      for (int s1 = 0; s1 < S1; s1++) {
-        float rmax = 0;
-        kmax(&in[s1 * S2 * S3 + s2 * S3], &rmax);
-        if (max < rmax)
-          max = rmax;
-      }
-      for (int s1 = 0; s1 < S1; s1++) {
-        LIBXSMM_ALIGNED(float tmp2[S3], 64);
-        ksub(&in[s1 * S2 * S3 + s2 * S3], &max, tmp2);
-        kexp(tmp2, &tmp[s1 * S3]);
-        float lsum;
-        ksum(&tmp[s1 * S3], &lsum);
-        sum += lsum;
-      }
-      sum = 1.0 / sum;
-      for (int s1 = 0; s1 < S1; s1++) {
-        kmul(&tmp[s1 * S3], &sum, &out[s1 * S2 * S3 + s2 * S3]);
-      }
-    }
-  }
-  void ref(int S1, Tin* pinp, Tout* pout) {
-    int s1, s2, s3;
-    LIBXSMM_VLA_DECL(3, Tin, inp, pinp, S2, S3);
-    LIBXSMM_VLA_DECL(3, Tout, out, pout, S2, S3);
-#if defined(__AVX512F__)
-    for (s2 = 0; s2 < S2; s2++) {
-      float tmp[S1][S3];
-      float max =
-          upconvert_to_float(LIBXSMM_VLA_ACCESS(3, inp, 0, s2, 0, S2, S3));
-      float sum = 0.0;
-      __m512 vmax = _mm512_set1_ps(max);
-      __m512 vsum = _mm512_setzero_ps();
-
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < ALIGNDOWN(S3, 16); s3 += 16) {
-          vmax = _mm512_max_ps(
-              _mm512_loadu_ps_auto(
-                  &LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3)),
-              vmax);
-        }
-        if (s3 < S3) {
-          int rem = S3 - s3;
-          __mmask16 mask = (1 << rem) - 1;
-          vmax = _mm512_mask_max_ps(
-              vmax,
-              mask,
-              _mm512_maskz_loadu_ps_auto(
-                  mask, &LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3)),
-              vmax);
-        }
-      }
-      max = _mm512_reduce_max_ps(vmax);
-      vmax = _mm512_set1_ps(max);
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < ALIGNDOWN(S3, 16); s3 += 16) {
-          __m512 vz = LIBXSMM_INTRINSICS_MM512_EXP_PS_3DTS(_mm512_sub_ps(
-              _mm512_loadu_ps_auto(
-                  &LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3)),
-              vmax));
-          _mm512_storeu_ps(&tmp[s1][s3], vz);
-          vsum = _mm512_add_ps(vsum, vz);
-        }
-        if (s3 < S3) {
-          int rem = S3 - s3;
-          __mmask16 mask = (1 << rem) - 1;
-          __m512 vz = LIBXSMM_INTRINSICS_MM512_EXP_PS_3DTS(_mm512_sub_ps(
-              _mm512_maskz_loadu_ps_auto(
-                  mask, &LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3)),
-              vmax));
-          _mm512_mask_storeu_ps(&tmp[s1][s3], mask, vz);
-          vsum = _mm512_mask_add_ps(vsum, mask, vsum, vz);
-        }
-      }
-      sum = _mm512_reduce_add_ps(vsum);
-      sum = 1.0 / sum;
-      vsum = _mm512_set1_ps(sum);
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < ALIGNDOWN(S3, 16); s3 += 16) {
-          _mm512_storeu_ps_auto(
-              &LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3),
-              _mm512_mul_ps(vsum, _mm512_loadu_ps(&tmp[s1][s3])));
-        }
-        if (s3 < S3) {
-          int rem = S3 - s3;
-          __mmask16 mask = (1 << rem) - 1;
-          _mm512_mask_storeu_ps_auto(
-              &LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3),
-              mask,
-              _mm512_mul_ps(vsum, _mm512_maskz_loadu_ps(mask, &tmp[s1][s3])));
-        }
-      }
-    }
-#else
-    // #warning "Not using AVX512 path for VarSoftMax"
-    for (s2 = 0; s2 < S2; s2++) {
-      float tmp[S1][S3];
-      float max =
-          upconvert_to_float(LIBXSMM_VLA_ACCESS(3, inp, 0, s2, 0, S2, S3));
-      float sum = 0.0;
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          float cur = upconvert_to_float(
-              LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3));
-          if (max < cur)
-            max = cur;
-        }
-      }
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          float cur = upconvert_to_float(
-              LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3));
-          float z = expf(cur - max);
-          tmp[s1][s3] = z;
-          sum += z;
-        }
-      }
-      sum = 1.0 / sum;
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          float cur = tmp[s1][s3] * sum;
-          // libxsmm_rne_convert_fp32_bf16( &cur, &LIBXSMM_VLA_ACCESS(3, out,
-          // s1, s2, s3, S2, S3), 1 );
-          LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3) = cur;
-        }
-      }
-    }
-#endif
-  }
-
- private:
-  int S2, S3;
-  UnaryTPP kmax;
-  BinaryTPP ksub;
-  UnaryTPP kexp;
-  UnaryTPP ksum;
-  BinaryTPP kmul;
-};
-
-template <typename T1, typename T2, typename T3>
-class VarSoftMaxBwdTPP {
- public:
-  VarSoftMaxBwdTPP() {}
-  VarSoftMaxBwdTPP(int S2, int S3) : S2(S2), S3(S3), eqn0(S3, 0), eqn1(S3, 1) {}
-  void operator()(int S1, T1* gin, T2* gout, T3* out) {
-    long S23 = S2 * S3;
-    for (int s2 = 0; s2 < S2; s2++) {
-      float tmp = 0.0f;
-      libxsmm_matrix_eqn_param eqn_param;
-      libxsmm_matrix_arg arg_array[3];
-      arg_array[2].primary = (void*)&tmp;
-      eqn_param.inputs = arg_array;
-      eqn_param.output.primary = (void*)&tmp;
-      for (int s1 = 0; s1 < S1; s1++) {
-        long ind = s1 * S23 + s2 * S3;
-        arg_array[0].primary = (void*)&gout[ind];
-        arg_array[1].primary = (void*)&out[ind];
-        eqn0(&eqn_param);
-      }
-      for (int s1 = 0; s1 < S1; s1++) {
-        long ind = s1 * S23 + s2 * S3;
-        arg_array[0].primary = (void*)&gout[ind];
-        arg_array[1].primary = (void*)&out[ind];
-        eqn_param.output.primary = (void*)&gin[ind];
-        eqn1(&eqn_param);
-      }
-    }
-  }
-  void ref(int S1, T1* pgradinp, T2* pgradout, T3* pout) {
-    int s1, s2, s3;
-    LIBXSMM_VLA_DECL(3, T1, ginp, pgradinp, S2, S3);
-    LIBXSMM_VLA_DECL(3, T2, gout, pgradout, S2, S3);
-    LIBXSMM_VLA_DECL(3, T3, out, pout, S2, S3);
-#if defined(__AVX512F__)
-    for (s2 = 0; s2 < S2; s2++) {
-      float sum = 0.0;
-      __m512 vsum = _mm512_setzero_ps();
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < ALIGNDOWN(S3, 16); s3 += 16) {
-          __m512 vgo =
-              _mm512_loadu_ps(&LIBXSMM_VLA_ACCESS(3, gout, s1, s2, s3, S2, S3));
-          __m512 vo = _mm512_loadu_ps_auto(
-              &LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3));
-          vsum = _mm512_fmadd_ps(vgo, vo, vsum);
-        }
-        if (s3 < S3) {
-          int rem = S3 - s3;
-          __mmask16 mask = (1 << rem) - 1;
-          __m512 vgo = _mm512_maskz_loadu_ps(
-              mask, &LIBXSMM_VLA_ACCESS(3, gout, s1, s2, s3, S2, S3));
-          __m512 vo = _mm512_maskz_loadu_ps_auto(
-              mask, &LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3));
-          vsum = _mm512_fmadd_ps(vgo, vo, vsum);
-        }
-      }
-      sum = _mm512_reduce_add_ps(vsum);
-      vsum = _mm512_set1_ps(sum);
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < ALIGNDOWN(S3, 16); s3 += 16) {
-          __m512 tmp = _mm512_sub_ps(
-              _mm512_loadu_ps(&LIBXSMM_VLA_ACCESS(3, gout, s1, s2, s3, S2, S3)),
-              vsum);
-          _mm512_storeu_ps(
-              &LIBXSMM_VLA_ACCESS(3, ginp, s1, s2, s3, S2, S3),
-              _mm512_mul_ps(
-                  _mm512_loadu_ps_auto(
-                      &LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3)),
-                  tmp));
-        }
-        if (s3 < S3) {
-          int rem = S3 - s3;
-          __mmask16 mask = (1 << rem) - 1;
-          __m512 tmp = _mm512_sub_ps(
-              _mm512_maskz_loadu_ps(
-                  mask, &LIBXSMM_VLA_ACCESS(3, gout, s1, s2, s3, S2, S3)),
-              vsum);
-          _mm512_mask_storeu_ps(
-              &LIBXSMM_VLA_ACCESS(3, ginp, s1, s2, s3, S2, S3),
-              mask,
-              _mm512_mul_ps(
-                  _mm512_maskz_loadu_ps_auto(
-                      mask, &LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3)),
-                  tmp));
-        }
-      }
-    }
-#else
-    for (s2 = 0; s2 < S2; s2++) {
-      float sum = 0.0;
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          sum += LIBXSMM_VLA_ACCESS(3, gout, s1, s2, s3, S2, S3) *
-              upconvert_to_float(
-                     LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3));
-        }
-      }
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          LIBXSMM_VLA_ACCESS(3, ginp, s1, s2, s3, S2, S3) =
-              upconvert_to_float(
-                  LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3)) *
-              (LIBXSMM_VLA_ACCESS(3, gout, s1, s2, s3, S2, S3) - sum);
-        }
-      }
-    }
-#endif
-  }
-  class Eqn : BaseTPP {
-   public:
-    Eqn() {}
-    Eqn(int S3, int eqn_no) : S3(S3), eqn_no(eqn_no) {
-      kernel = (libxsmm_matrix_eqn_function)get_kernel();
-      initialized = true;
-    }
-    void operator()(libxsmm_matrix_eqn_param* eqn_param) {
-      if (!initialized)
-        return;
-      kernel(eqn_param);
-    }
-
-   protected:
-    std::string hash_str() override {
-      char hash[200];
-      snprintf(
-          hash,
-          200,
-          "varsoftmax_bwd_eqn%d_t1%d_t2%d_t3%d_S3%d",
-          eqn_no,
-          XsmmDtype<T1>(),
-          XsmmDtype<T2>(),
-          XsmmDtype<T3>(),
-          S3);
-      return std::string(hash);
-    }
-    void* build_kernel() override {
-      auto dt_1 = XsmmDtype<T1>();
-      auto dt_2 = XsmmDtype<T2>();
-      auto dt_3 = XsmmDtype<T3>();
-      libxsmm_blasint tmp_ld = S3;
-      libxsmm_blasint ld = S3;
-      libxsmm_matrix_eqn_function func;
-      if (eqn_no == 0) {
-        libxsmm_blasint my_eqn2 = libxsmm_matrix_eqn_create();
-        meqn_push_binary_op(my_eqn2, LIBXSMM_MELTW_TYPE_BINARY_ADD);
-        meqn_push_arg(my_eqn2, 1, 1, 1, 2, 0, LIBXSMM_DATATYPE_F32);
-        meqn_push_unary_op(
-            my_eqn2,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS);
-        meqn_push_binary_op(my_eqn2, LIBXSMM_MELTW_TYPE_BINARY_MUL);
-        meqn_push_arg(my_eqn2, S3, 1, ld, 0, 0, dt_2);
-        meqn_push_arg(my_eqn2, S3, 1, ld, 1, 0, dt_3);
-        debug_print_eqn_tree(my_eqn2); // printf
-        func = meqn_dispatch(S3, 1, &tmp_ld, LIBXSMM_DATATYPE_F32, my_eqn2);
-      } else if (eqn_no == 1) {
-        libxsmm_blasint my_eqn3 = libxsmm_matrix_eqn_create();
-        meqn_push_binary_op(my_eqn3, LIBXSMM_MELTW_TYPE_BINARY_MUL);
-        meqn_push_arg(my_eqn3, S3, 1, ld, 1, 0, dt_3);
-        meqn_push_binary_op(
-            my_eqn3,
-            LIBXSMM_MELTW_TYPE_BINARY_SUB,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-        meqn_push_arg(my_eqn3, S3, 1, ld, 0, 0, dt_2);
-        meqn_push_arg(my_eqn3, 1, 1, 1, 2, 0, LIBXSMM_DATATYPE_F32);
-        debug_print_eqn_tree(my_eqn3);
-        func = meqn_dispatch(S3, 1, &ld, dt_1, my_eqn3);
-      } else {
-        TPP_ASSERT(false, "Should not come here\n");
-      }
-      return (void*)func;
-    }
-
-   private:
-    int S3, eqn_no;
-    libxsmm_matrix_eqn_function kernel = NULL;
-  };
-
- private:
-  int S2, S3;
-  Eqn eqn0, eqn1;
-};
-
-template <typename T, typename LT = T>
-class LayerNormFwdTPP {
- public:
-  LayerNormFwdTPP() {}
-  LayerNormFwdTPP(int S1, int S2, int S3, float eps)
-      : S1(S1),
-        S2(S2),
-        S3(S3),
-        eps(eps),
-        reduce_cols_kernel(
-            S1,
-            S3,
-            S2 * S3,
-            S3,
-            XsmmDtype<T>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_COLS,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_X2_OP_ADD),
-        reduce_rows_kernel(
-            1,
-            S3,
-            S3,
-            1,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD),
-        eqn(S1, S2, S3) {}
-  void operator()(
-      T* inp,
-      LT* gamma,
-      LT* beta,
-      float* mean,
-      float* var,
-      T* out) {
-    LIBXSMM_ALIGNED(float tmp[2 * S3], 64);
-    const float c = 1.0 / ((float)S1 * S3);
-    float m, v, s, b;
-    libxsmm_matrix_eqn_param eqn_param;
-    libxsmm_matrix_arg arg_array[5];
-    eqn_param.inputs = arg_array;
-    arg_array[1].primary = &s;
-    arg_array[2].primary = &b;
-    arg_array[3].primary = (void*)gamma;
-    arg_array[4].primary = (void*)beta;
-    for (int s2 = 0; s2 < S2; s2++) {
-      reduce_cols_kernel((void*)&inp[s2 * S3], (void*)tmp);
-      reduce_rows_kernel((void*)tmp, (void*)&m);
-      reduce_rows_kernel((void*)&tmp[S3], (void*)&v);
-      m = m * c;
-      v = v * c;
-      v = LIBXSMM_MAX(v - m * m, 0.0f);
-      v = 1.0f / ((float)sqrt(v + eps));
-      if (mean)
-        mean[s2] = m;
-      if (var)
-        var[s2] = v;
-      s = v;
-      b = -1.0 * v * m;
-      arg_array[0].primary = (void*)&inp[s2 * S3];
-      eqn_param.output.primary = (void*)&out[s2 * S3];
-      eqn(&eqn_param);
-    }
-  }
-  void ref(T* pinp, LT* pgamma, LT* pbeta, float* mean, float* var, T* pout) {
-    int s1, s2, s3;
-    LIBXSMM_VLA_DECL(3, T, inp, pinp, S2, S3);
-    LIBXSMM_VLA_DECL(3, T, out, pout, S2, S3);
-    LIBXSMM_VLA_DECL(2, LT, gamma, pgamma, S3);
-    LIBXSMM_VLA_DECL(2, LT, beta, pbeta, S3);
-    for (s2 = 0; s2 < S2; s2++) {
-      float m = 0;
-      float v = 0;
-      float c = 1.0 / (S1 * S3);
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          m += LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3);
-          v += LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3) *
-              LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3);
-        }
-      }
-      m = m * c;
-      v = v * c;
-      v = LIBXSMM_MAX(v - m * m, 0.0f);
-      v = 1.0f / ((float)sqrt(v + eps));
-      mean[s2] = m;
-      var[s2] = v;
-      float s = v;
-      float b = -1.0 * v * m;
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3) =
-              (LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3) * s + b) *
-                  LIBXSMM_VLA_ACCESS(2, gamma, s1, s3, S3) +
-              LIBXSMM_VLA_ACCESS(2, beta, s1, s3, S3);
-        }
-      }
-    }
-  }
-  class Eqn : BaseTPP {
-   public:
-    Eqn() {}
-    Eqn(int S1, int S2, int S3) : S1(S1), S2(S2), S3(S3) {
-      kernel = (libxsmm_matrix_eqn_function)get_kernel();
-      initialized = true;
-    }
-    void operator()(libxsmm_matrix_eqn_param* eqn_param) {
-      if (!initialized)
-        return;
-      kernel(eqn_param);
-    }
-
-   protected:
-    std::string hash_str() override {
-      char hash[200];
-      snprintf(
-          hash,
-          200,
-          "layernorm_fwd_eqn_t1%d_t2%d_S1%d_S2%d_S3%d",
-          XsmmDtype<T>(),
-          XsmmDtype<LT>(),
-          S1,
-          S2,
-          S3);
-      return std::string(hash);
-    }
-    void* build_kernel() override {
-      auto in_dt = XsmmDtype<T>();
-      auto bg_dt = XsmmDtype<LT>();
-      auto out_dt = XsmmDtype<T>();
-      libxsmm_blasint tmp_ld = 1;
-      libxsmm_blasint tmp_ld2 = S3;
-      libxsmm_blasint ld = S2 * S3;
-      libxsmm_blasint my_eqn0 = libxsmm_matrix_eqn_create();
-      meqn_push_ternary_op(
-          my_eqn0,
-          LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-          LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-      meqn_push_ternary_op(
-          my_eqn0,
-          LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-          LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_1 |
-              LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_2 |
-              LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-      meqn_push_arg(my_eqn0, S3, S1, ld, 0, 0, in_dt);
-      meqn_push_arg(my_eqn0, 1, 1, tmp_ld, 1, 0, LIBXSMM_DATATYPE_F32);
-      meqn_push_arg(my_eqn0, 1, 1, tmp_ld, 2, 0, LIBXSMM_DATATYPE_F32);
-      meqn_push_arg(my_eqn0, S3, S1, tmp_ld2, 3, 0, bg_dt);
-      meqn_push_arg(my_eqn0, S3, S1, tmp_ld2, 4, 0, bg_dt);
-      debug_print_eqn_tree(my_eqn0); // printf
-      return (void*)meqn_dispatch(S3, S1, &ld, out_dt, my_eqn0);
-    }
-
-   private:
-    int S1, S2, S3;
-    libxsmm_matrix_eqn_function kernel = NULL;
-  };
-
- private:
-  int S1, S2, S3;
-  float eps;
-  UnaryTPP reduce_cols_kernel;
-  UnaryTPP reduce_rows_kernel;
-  Eqn eqn;
-};
-
-template <typename T, typename LT = T>
-class LayerNormBwdTPP {
- public:
-  LayerNormBwdTPP() {}
-  LayerNormBwdTPP(int S1, int S2, int S3)
-      : S1(S1),
-        S2(S2),
-        S3(S3),
-        dgamma_func(S1, S2, S3, 1),
-        dbeta_func(S1, S2, S3, 2),
-        db_func(S1, S2, S3, 3),
-        ds_func(S1, S2, S3, 4),
-        din_func(S1, S2, S3, 5) {}
-  void operator()(
-      T* dout,
-      T* inp,
-      float* mean,
-      float* var,
-      LT* gamma,
-      T* din,
-      float* dgamma,
-      float* dbeta) {
-    float a, b, c, db, ds;
-    const float scale = 1.0f / ((float)S1 * S3);
-    libxsmm_matrix_eqn_param eqn_param;
-    libxsmm_matrix_arg arg_array[8];
-    eqn_param.inputs = arg_array;
-
-    arg_array[1].primary = &a;
-    arg_array[2].primary = &b;
-    arg_array[4].primary = (void*)dgamma;
-    arg_array[5].primary = (void*)dbeta;
-    arg_array[6].primary = (void*)gamma;
-    arg_array[7].primary = &c;
-
-    for (int s2 = 0; s2 < S2; s2++) {
-      a = var[s2];
-      b = -a * mean[s2];
-      arg_array[0].primary = (void*)&inp[s2 * S3];
-      arg_array[3].primary = (void*)&dout[s2 * S3];
-
-      eqn_param.output.primary = &ds;
-      ds_func(&eqn_param);
-
-      eqn_param.output.primary = &db;
-      db_func(&eqn_param);
-
-      eqn_param.output.primary = (void*)dgamma;
-      dgamma_func(&eqn_param);
-
-      eqn_param.output.primary = (void*)dbeta;
-      dbeta_func(&eqn_param);
-
-      b = (db * mean[s2] - ds) * a * a * a * scale;
-      c = -b * mean[s2] - db * a * scale;
-
-      eqn_param.output.primary = (void*)&din[s2 * S3];
-      din_func(&eqn_param);
-    }
-  }
-  void ref(
-      T* pdout,
-      T* pinp,
-      float* mean,
-      float* var,
-      LT* pgamma,
-      T* pdin,
-      float* pdgamma,
-      float* pdbeta) {
-    int s1, s2, s3;
-    LIBXSMM_VLA_DECL(3, T, din, pdin, S2, S3);
-    LIBXSMM_VLA_DECL(3, T, inp, pinp, S2, S3);
-    LIBXSMM_VLA_DECL(3, T, dout, pdout, S2, S3);
-    LIBXSMM_VLA_DECL(2, LT, gamma, pgamma, S3);
-    LIBXSMM_VLA_DECL(2, float, dgamma, pdgamma, S3);
-    LIBXSMM_VLA_DECL(2, float, dbeta, pdbeta, S3);
-    for (s2 = 0; s2 < S2; s2++) {
-      float a = var[s2], c;
-      float b = -a * mean[s2];
-      float ds = 0.0f;
-      float db = 0.0f;
-      float scale = 1.0f / (S1 * S3);
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          LIBXSMM_VLA_ACCESS(2, dgamma, s1, s3, S3) +=
-              (a * LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3) + b) *
-              LIBXSMM_VLA_ACCESS(3, dout, s1, s2, s3, S2, S3);
-          LIBXSMM_VLA_ACCESS(2, dbeta, s1, s3, S3) +=
-              LIBXSMM_VLA_ACCESS(3, dout, s1, s2, s3, S2, S3);
-          ds += LIBXSMM_VLA_ACCESS(3, dout, s1, s2, s3, S2, S3) *
-              LIBXSMM_VLA_ACCESS(2, gamma, s1, s3, S3) *
-              LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3);
-          db += LIBXSMM_VLA_ACCESS(3, dout, s1, s2, s3, S2, S3) *
-              LIBXSMM_VLA_ACCESS(2, gamma, s1, s3, S3);
-        }
-      }
-      b = (db * mean[s2] - ds) * a * a * a * scale;
-      c = -b * mean[s2] - db * a * scale;
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          LIBXSMM_VLA_ACCESS(3, din, s1, s2, s3, S2, S3) =
-              LIBXSMM_VLA_ACCESS(3, dout, s1, s2, s3, S2, S3) * a *
-                  LIBXSMM_VLA_ACCESS(2, gamma, s1, s3, S3) +
-              b * LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3) + c;
-        }
-      }
-    }
-  }
-
-  class Eqn : BaseTPP {
-   public:
-    Eqn() {}
-    Eqn(int S1, int S2, int S3, int eqn_no)
-        : S1(S1), S2(S2), S3(S3), eqn_no(eqn_no) {
-      kernel = (libxsmm_matrix_eqn_function)get_kernel();
-      initialized = true;
-    }
-    void operator()(libxsmm_matrix_eqn_param* eqn_param) {
-      if (!initialized)
-        return;
-      kernel(eqn_param);
-    }
-
-   protected:
-    std::string hash_str() override {
-      char hash[200];
-      snprintf(
-          hash,
-          200,
-          "layernorm_bwd_eqn%d_t1%d_t2%d_S1%d_S2%d_S3%d",
-          eqn_no,
-          XsmmDtype<T>(),
-          XsmmDtype<LT>(),
-          S1,
-          S2,
-          S3);
-      return std::string(hash);
-    }
-    void* build_kernel() override {
-      auto in_dt = XsmmDtype<T>();
-      auto bg_dt = XsmmDtype<LT>();
-      // auto out_dt = XsmmDtype<T>();
-      libxsmm_blasint tmp_ld = S3;
-      libxsmm_blasint tmp_ld2 = 1;
-      libxsmm_blasint ld = S2 * S3;
-      libxsmm_matrix_eqn_function func = NULL;
-      if (eqn_no == 1) {
-        /* dgamma function  */
-        libxsmm_blasint my_eqn1 = libxsmm_matrix_eqn_create();
-        meqn_push_ternary_op(
-            my_eqn1,
-            LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-            LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-        meqn_push_ternary_op(
-            my_eqn1,
-            LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-            LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_1 |
-                LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_2 |
-                LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-        meqn_push_arg(my_eqn1, S3, S1, ld, 0, 0, in_dt);
-        meqn_push_arg(my_eqn1, 1, 1, 1, 1, 0, LIBXSMM_DATATYPE_F32);
-        meqn_push_arg(my_eqn1, 1, 1, 1, 2, 0, LIBXSMM_DATATYPE_F32);
-        meqn_push_arg(my_eqn1, S3, S1, ld, 3, 0, in_dt);
-        meqn_push_arg(my_eqn1, S3, S1, tmp_ld, 4, 0, LIBXSMM_DATATYPE_F32);
-        /*debug_print_eqn_tree( my_eqn1 );*/
-        func = meqn_dispatch(S3, S1, &tmp_ld, LIBXSMM_DATATYPE_F32, my_eqn1);
-      } else if (eqn_no == 2) {
-        /* dbeta function  */
-        libxsmm_blasint my_eqn2 = libxsmm_matrix_eqn_create();
-        meqn_push_binary_op(my_eqn2, LIBXSMM_MELTW_TYPE_BINARY_ADD);
-        meqn_push_arg(my_eqn2, S3, S1, ld, 3, 0, in_dt);
-        meqn_push_arg(my_eqn2, S3, S1, tmp_ld, 5, 0, LIBXSMM_DATATYPE_F32);
-        /*debug_print_eqn_tree( my_eqn1 );*/
-        func = meqn_dispatch(S3, S1, &tmp_ld, LIBXSMM_DATATYPE_F32, my_eqn2);
-      } else if (eqn_no == 3) {
-        /* db equation */
-        libxsmm_blasint my_eqn3 = libxsmm_matrix_eqn_create();
-        meqn_push_binary_op(
-            my_eqn3, LIBXSMM_MELTW_TYPE_BINARY_MUL_AND_REDUCE_TO_SCALAR_OP_ADD);
-        meqn_push_arg(my_eqn3, S3, S1, ld, 3, 0, in_dt);
-        meqn_push_arg(my_eqn3, S3, S1, tmp_ld, 6, 0, bg_dt);
-        func = meqn_dispatch(1, 1, &tmp_ld2, LIBXSMM_DATATYPE_F32, my_eqn3);
-      } else if (eqn_no == 4) {
-        /* ds equation */
-        libxsmm_blasint my_eqn4 = libxsmm_matrix_eqn_create();
-        meqn_push_binary_op(
-            my_eqn4, LIBXSMM_MELTW_TYPE_BINARY_MUL_AND_REDUCE_TO_SCALAR_OP_ADD);
-        meqn_push_binary_op(my_eqn4, LIBXSMM_MELTW_TYPE_BINARY_MUL);
-        meqn_push_arg(my_eqn4, S3, S1, ld, 3, 0, in_dt);
-        meqn_push_arg(my_eqn4, S3, S1, tmp_ld, 6, 0, bg_dt);
-        meqn_push_arg(my_eqn4, S3, S1, ld, 0, 0, in_dt);
-        func = meqn_dispatch(1, 1, &tmp_ld2, LIBXSMM_DATATYPE_F32, my_eqn4);
-      } else if (eqn_no == 5) {
-        /* din equation */
-        libxsmm_blasint my_eqn5 = libxsmm_matrix_eqn_create();
-        meqn_push_ternary_op(
-            my_eqn5,
-            LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-            LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-        meqn_push_binary_op(
-            my_eqn5,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-        meqn_push_arg(my_eqn5, S3, S1, tmp_ld, 6, 0, bg_dt);
-        meqn_push_arg(my_eqn5, 1, 1, 1, 1, 0, LIBXSMM_DATATYPE_F32);
-        meqn_push_arg(my_eqn5, S3, S1, ld, 3, 0, in_dt);
-        meqn_push_ternary_op(
-            my_eqn5,
-            LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-            LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_1 |
-                LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_2 |
-                LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-        meqn_push_arg(my_eqn5, S3, S1, ld, 0, 0, in_dt);
-        meqn_push_arg(my_eqn5, 1, 1, 1, 2, 0, LIBXSMM_DATATYPE_F32);
-        meqn_push_arg(my_eqn5, 1, 1, 1, 7, 0, LIBXSMM_DATATYPE_F32);
-        func = meqn_dispatch(S3, S1, &ld, in_dt, my_eqn5);
-      } else {
-        TPP_ASSERT(false, "LayerNormBwdTPP: invalid eqn. number %d\n", eqn_no);
-      }
-      return (void*)func;
-    }
-
-   private:
-    int S1, S2, S3, eqn_no;
-    libxsmm_matrix_eqn_function kernel = NULL;
-  };
-
- private:
-  int S1, S2, S3;
-  Eqn dgamma_func, dbeta_func, db_func, ds_func, din_func;
-};
-
-template <typename T>
-class GroupNormFwdTPP {
- public:
-  GroupNormFwdTPP() {}
-  GroupNormFwdTPP(int S1, int S2, int S3, float eps)
-      : S1(S1),
-        S2(S2),
-        S3(S3),
-        eps(eps),
-        reduce_cols_kernel(
-            S1,
-            S3,
-            S2 * S3,
-            S3,
-            XsmmDtype<T>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_COLS,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_X2_OP_ADD),
-        reduce_rows_kernel(
-            1,
-            S3,
-            S3,
-            1,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD),
-        eqn(S1, S2, S3) {}
-  void operator()(T* inp, T* gamma, T* beta, float* mean, float* var, T* out) {
-    LIBXSMM_ALIGNED(float tmp[2 * S3], 64);
-    const float c = 1.0 / ((float)S1 * S3);
-    float m, v, s, b;
-    libxsmm_matrix_eqn_param eqn_param;
-    libxsmm_matrix_arg arg_array[5];
-    eqn_param.inputs = arg_array;
-    arg_array[1].primary = &s;
-    arg_array[2].primary = &b;
-    arg_array[3].primary = (void*)gamma;
-    arg_array[4].primary = (void*)beta;
-    for (int s2 = 0; s2 < S2; s2++) {
-      reduce_cols_kernel((void*)&inp[s2 * S3], (void*)tmp);
-      reduce_rows_kernel((void*)tmp, (void*)&m);
-      reduce_rows_kernel((void*)&tmp[S3], (void*)&v);
-      m = m * c;
-      v = v * c;
-      v = LIBXSMM_MAX(v - m * m, 0.0f);
-      v = 1.0f / ((float)sqrt(v + eps));
-      mean[s2] = m;
-      var[s2] = v;
-      s = v;
-      b = -1.0 * v * m;
-      arg_array[0].primary = (void*)&inp[s2 * S3];
-      eqn_param.output.primary = (void*)&out[s2 * S3];
-      eqn(&eqn_param);
-    }
-  }
-  void ref(T* pinp, T* gamma, T* beta, float* mean, float* var, T* pout) {
-    int s1, s2, s3;
-    LIBXSMM_VLA_DECL(3, T, inp, pinp, S2, S3);
-    LIBXSMM_VLA_DECL(3, T, out, pout, S2, S3);
-    for (s2 = 0; s2 < S2; s2++) {
-      float m = 0;
-      float v = 0;
-      float c = 1.0 / (S1 * S3);
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          m += LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3);
-          v += LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3) *
-              LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3);
-        }
-      }
-      m = m * c;
-      v = v * c;
-      v = LIBXSMM_MAX(v - m * m, 0.0f);
-      v = 1.0f / ((float)sqrt(v + eps));
-      mean[s2] = m;
-      var[s2] = v;
-      float s = v;
-      float b = -1.0 * v * m;
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          LIBXSMM_VLA_ACCESS(3, out, s1, s2, s3, S2, S3) =
-              (LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3) * s + b) *
-                  gamma[s1] +
-              beta[s1];
-        }
-      }
-    }
-  }
-  class Eqn : BaseTPP {
-   public:
-    Eqn() {}
-    Eqn(int S1, int S2, int S3) : S1(S1), S2(S2), S3(S3) {
-      kernel = (libxsmm_matrix_eqn_function)get_kernel();
-      initialized = true;
-    }
-    void operator()(libxsmm_matrix_eqn_param* eqn_param) {
-      if (!initialized)
-        return;
-      kernel(eqn_param);
-    }
-
-   protected:
-    std::string hash_str() override {
-      char hash[200];
-      snprintf(
-          hash,
-          200,
-          "group_norm_fwd_eqn_t%d_S1%d_S2%d_S3%d",
-          XsmmDtype<T>(),
-          S1,
-          S2,
-          S3);
-      return std::string(hash);
-    }
-    void* build_kernel() override {
-      auto in_dt = XsmmDtype<T>();
-      auto out_dt = XsmmDtype<T>();
-      libxsmm_blasint tmp_ld = 1;
-      libxsmm_blasint tmp_ld2 = S3;
-      libxsmm_blasint ld = S2 * S3;
-      libxsmm_blasint my_eqn0 = libxsmm_matrix_eqn_create();
-      meqn_push_ternary_op(
-          my_eqn0,
-          LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-          LIBXSMM_MELTW_FLAG_TERNARY_BCAST_ROW_IN_1 |
-              LIBXSMM_MELTW_FLAG_TERNARY_BCAST_ROW_IN_2 |
-              LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-      meqn_push_ternary_op(
-          my_eqn0,
-          LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-          LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_1 |
-              LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_2 |
-              LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-      meqn_push_arg(my_eqn0, S3, S1, ld, 0, 0, in_dt);
-      meqn_push_arg(my_eqn0, 1, 1, tmp_ld, 1, 0, LIBXSMM_DATATYPE_F32);
-      meqn_push_arg(my_eqn0, 1, 1, tmp_ld, 2, 0, LIBXSMM_DATATYPE_F32);
-      meqn_push_arg(my_eqn0, 1, S1, 1, 3, 0, in_dt);
-      meqn_push_arg(my_eqn0, 1, S1, 1, 4, 0, in_dt);
-      debug_print_eqn_tree(my_eqn0); // printf
-      return (void*)meqn_dispatch(S3, S1, &ld, out_dt, my_eqn0);
-    }
-
-   private:
-    int S1, S2, S3;
-    libxsmm_matrix_eqn_function kernel = NULL;
-  };
-
- private:
-  int S1, S2, S3;
-  float eps;
-  UnaryTPP reduce_cols_kernel;
-  UnaryTPP reduce_rows_kernel;
-  Eqn eqn;
-};
-
-template <typename T>
-class GroupNormBwdTPP {
- public:
-  GroupNormBwdTPP() {}
-  GroupNormBwdTPP(int S1, int S2, int S3)
-      : S1(S1),
-        S2(S2),
-        S3(S3),
-        dgamma_func(S1, S2, S3, 1),
-        dbeta_func(S1, S2, S3, 2),
-        db_func(S1, S2, S3, 3),
-        ds_func(S1, S2, S3, 4),
-        din_func(S1, S2, S3, 5) {}
-  void operator()(
-      T* dout,
-      T* inp,
-      float* mean,
-      float* var,
-      T* gamma,
-      T* din,
-      float* dgamma,
-      float* dbeta) {
-    float a, b, c, db, ds;
-    const float scale = 1.0f / ((float)S1 * S3);
-    libxsmm_matrix_eqn_param eqn_param;
-    libxsmm_matrix_arg arg_array[8];
-    eqn_param.inputs = arg_array;
-
-    arg_array[1].primary = &a;
-    arg_array[2].primary = &b;
-    arg_array[4].primary = (void*)dgamma;
-    arg_array[5].primary = (void*)dbeta;
-    arg_array[6].primary = (void*)gamma;
-    arg_array[7].primary = &c;
-
-    for (int s2 = 0; s2 < S2; s2++) {
-      a = var[s2];
-      b = -a * mean[s2];
-      arg_array[0].primary = (void*)&inp[s2 * S3];
-      arg_array[3].primary = (void*)&dout[s2 * S3];
-
-      eqn_param.output.primary = &ds;
-      ds_func(&eqn_param);
-
-      eqn_param.output.primary = &db;
-      db_func(&eqn_param);
-
-      eqn_param.output.primary = (void*)dgamma;
-      dgamma_func(&eqn_param);
-
-      eqn_param.output.primary = (void*)dbeta;
-      dbeta_func(&eqn_param);
-
-      b = (db * mean[s2] - ds) * a * a * a * scale;
-      c = -b * mean[s2] - db * a * scale;
-
-      eqn_param.output.primary = (void*)&din[s2 * S3];
-      din_func(&eqn_param);
-    }
-  }
-  void ref(
-      T* pdout,
-      T* pinp,
-      float* mean,
-      float* var,
-      T* gamma,
-      T* pdin,
-      float* dgamma,
-      float* dbeta) {
-    int s1, s2, s3;
-    LIBXSMM_VLA_DECL(3, T, din, pdin, S2, S3);
-    LIBXSMM_VLA_DECL(3, T, inp, pinp, S2, S3);
-    LIBXSMM_VLA_DECL(3, T, dout, pdout, S2, S3);
-    for (s2 = 0; s2 < S2; s2++) {
-      float a = var[s2], c;
-      float b = -a * mean[s2];
-      float ds = 0.0f;
-      float db = 0.0f;
-      float scale = 1.0f / (S1 * S3);
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          dgamma[s1] +=
-              (a * LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3) + b) *
-              LIBXSMM_VLA_ACCESS(3, dout, s1, s2, s3, S2, S3);
-          dbeta[s1] += LIBXSMM_VLA_ACCESS(3, dout, s1, s2, s3, S2, S3);
-          ds += LIBXSMM_VLA_ACCESS(3, dout, s1, s2, s3, S2, S3) * gamma[s1] *
-              LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3);
-          db += LIBXSMM_VLA_ACCESS(3, dout, s1, s2, s3, S2, S3) * gamma[s1];
-        }
-      }
-      b = (db * mean[s2] - ds) * a * a * a * scale;
-      c = -b * mean[s2] - db * a * scale;
-      for (s1 = 0; s1 < S1; s1++) {
-        for (s3 = 0; s3 < S3; s3++) {
-          LIBXSMM_VLA_ACCESS(3, din, s1, s2, s3, S2, S3) =
-              LIBXSMM_VLA_ACCESS(3, dout, s1, s2, s3, S2, S3) * a * gamma[s1] +
-              b * LIBXSMM_VLA_ACCESS(3, inp, s1, s2, s3, S2, S3) + c;
-        }
-      }
-    }
-  }
-
-  class Eqn : BaseTPP {
-   public:
-    Eqn() {}
-    Eqn(int S1, int S2, int S3, int eqn_no)
-        : S1(S1), S2(S2), S3(S3), eqn_no(eqn_no) {
-      kernel = (libxsmm_matrix_eqn_function)get_kernel();
-      initialized = true;
-    }
-    void operator()(libxsmm_matrix_eqn_param* eqn_param) {
-      if (!initialized)
-        return;
-      kernel(eqn_param);
-    }
-
-   protected:
-    std::string hash_str() override {
-      char hash[200];
-      snprintf(
-          hash,
-          200,
-          "group_norm_bwd_eqn%d_t%d_S1%d_S2%d_S3%d",
-          eqn_no,
-          XsmmDtype<T>(),
-          S1,
-          S2,
-          S3);
-      return std::string(hash);
-    }
-    void* build_kernel() override {
-      auto in_dt = XsmmDtype<T>();
-      // auto out_dt = XsmmDtype<T>();
-      libxsmm_blasint tmp_ld = S3;
-      libxsmm_blasint tmp_ld2 = 1;
-      libxsmm_blasint ld = S2 * S3;
-      libxsmm_matrix_eqn_function func = NULL;
-      if (eqn_no == 1) {
-        /* dgamma function  */
-        libxsmm_blasint my_eqn1 = libxsmm_matrix_eqn_create();
-        meqn_push_binary_op(my_eqn1, LIBXSMM_MELTW_TYPE_BINARY_ADD);
-        meqn_push_unary_op(
-            my_eqn1,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS);
-        meqn_push_binary_op(my_eqn1, LIBXSMM_MELTW_TYPE_BINARY_MUL);
-        meqn_push_ternary_op(
-            my_eqn1,
-            LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-            LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_1 |
-                LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_2 |
-                LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-        meqn_push_arg(my_eqn1, S3, S1, ld, 0, 0, in_dt);
-        meqn_push_arg(my_eqn1, 1, 1, 1, 1, 0, LIBXSMM_DATATYPE_F32);
-        meqn_push_arg(my_eqn1, 1, 1, 1, 2, 0, LIBXSMM_DATATYPE_F32);
-        meqn_push_arg(my_eqn1, S3, S1, ld, 3, 0, in_dt);
-        meqn_push_arg(my_eqn1, S3, S1, tmp_ld, 4, 0, LIBXSMM_DATATYPE_F32);
-        debug_print_eqn_tree(my_eqn1);
-        func = meqn_dispatch(S3, S1, &tmp_ld, LIBXSMM_DATATYPE_F32, my_eqn1);
-      } else if (eqn_no == 2) {
-        /* dbeta function  */
-        libxsmm_blasint my_eqn2 = libxsmm_matrix_eqn_create();
-        meqn_push_binary_op(my_eqn2, LIBXSMM_MELTW_TYPE_BINARY_ADD);
-        meqn_push_unary_op(
-            my_eqn2,
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_X_OP_ADD,
-            LIBXSMM_MELTW_FLAG_UNARY_REDUCE_ROWS);
-        meqn_push_arg(my_eqn2, S3, S1, ld, 3, 0, in_dt);
-        meqn_push_arg(my_eqn2, S3, S1, tmp_ld, 5, 0, LIBXSMM_DATATYPE_F32);
-        debug_print_eqn_tree(my_eqn2);
-        func = meqn_dispatch(S3, S1, &tmp_ld, LIBXSMM_DATATYPE_F32, my_eqn2);
-      } else if (eqn_no == 3) {
-        /* db equation */
-        libxsmm_blasint my_eqn3 = libxsmm_matrix_eqn_create();
-        meqn_push_binary_op(
-            my_eqn3,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL_AND_REDUCE_TO_SCALAR_OP_ADD,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_ROW_IN_1);
-        meqn_push_arg(my_eqn3, S3, S1, ld, 3, 0, in_dt);
-        meqn_push_arg(my_eqn3, 1, S1, 1, 6, 0, in_dt);
-        func = meqn_dispatch(1, 1, &tmp_ld2, LIBXSMM_DATATYPE_F32, my_eqn3);
-      } else if (eqn_no == 4) {
-        /* ds equation */
-        libxsmm_blasint my_eqn4 = libxsmm_matrix_eqn_create();
-        meqn_push_binary_op(
-            my_eqn4, LIBXSMM_MELTW_TYPE_BINARY_MUL_AND_REDUCE_TO_SCALAR_OP_ADD);
-        meqn_push_binary_op(
-            my_eqn4,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_ROW_IN_1);
-        meqn_push_arg(my_eqn4, S3, S1, ld, 3, 0, in_dt);
-        meqn_push_arg(my_eqn4, 1, S1, 1, 6, 0, in_dt);
-        meqn_push_arg(my_eqn4, S3, S1, ld, 0, 0, in_dt);
-        func = meqn_dispatch(1, 1, &tmp_ld2, LIBXSMM_DATATYPE_F32, my_eqn4);
-      } else if (eqn_no == 5) {
-        /* din equation */
-        libxsmm_blasint my_eqn5 = libxsmm_matrix_eqn_create();
-        meqn_push_ternary_op(
-            my_eqn5,
-            LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-            LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-        meqn_push_binary_op(
-            my_eqn5,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_ROW_IN_0 |
-                LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-        meqn_push_arg(my_eqn5, 1, S1, 1, 6, 0, in_dt);
-        meqn_push_arg(my_eqn5, 1, 1, 1, 1, 0, LIBXSMM_DATATYPE_F32);
-        meqn_push_arg(my_eqn5, S3, S1, ld, 3, 0, in_dt);
-        meqn_push_ternary_op(
-            my_eqn5,
-            LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-            LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_1 |
-                LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_2 |
-                LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-        meqn_push_arg(my_eqn5, S3, S1, ld, 0, 0, in_dt);
-        meqn_push_arg(my_eqn5, 1, 1, 1, 2, 0, LIBXSMM_DATATYPE_F32);
-        meqn_push_arg(my_eqn5, 1, 1, 1, 7, 0, LIBXSMM_DATATYPE_F32);
-        func = meqn_dispatch(S3, S1, &ld, in_dt, my_eqn5);
-      } else {
-        TPP_ASSERT(false, "GroupNormBwdTPP: invalid eqn. number %d\n", eqn_no);
-      }
-      return (void*)func;
-    }
-
-   private:
-    int S1, S2, S3, eqn_no;
-    libxsmm_matrix_eqn_function kernel = NULL;
-  };
-
- private:
-  int S1, S2, S3;
-  Eqn dgamma_func, dbeta_func, db_func, ds_func, din_func;
-};
-
-// class SplitSGDTPP : public BaseTPP {
-//  public:
-//   SplitSGDTPP() {}
-//   SplitSGDTPP(int N) : N(N) {
-//     kernel = (libxsmm_matrix_eqn_function)get_kernel();
-//     initialized = true;
-//   }
-//   void operator()(bfloat16* hi, bfloat16* lo, bfloat16* grad, float lr) {
-//     if (!initialized)
-//       return;
-//     libxsmm_matrix_eqn_param eqn_param;
-//     libxsmm_matrix_arg arg_array[4];
-//     arg_array[0].primary = (void*)lo;
-//     arg_array[1].primary = (void*)hi;
-//     arg_array[2].primary = (void*)&lr;
-//     arg_array[3].primary = (void*)grad;
-//     eqn_param.inputs = arg_array;
-//     eqn_param.output.primary = (void*)lo;
-//     auto offset = (long long)((char*)hi - (char*)lo);
-//     eqn_param.output.secondary = (void*)offset;
-
-//     kernel(&eqn_param);
-//   }
-//   void ref(bfloat16* hi, bfloat16* lo, bfloat16* grad, float lr) {
-// #ifndef __AVX512F__
-//     auto dwt = (libxsmm_bfloat16*)grad;
-//     auto out_hi = (libxsmm_bfloat16*)hi;
-//     auto out_lo = (libxsmm_bfloat16*)lo;
-//     for (int i = 0; i < N; i++) {
-//       union libxsmm_bfloat16_f32 bf16_hp;
-//       union libxsmm_bfloat16_f32 bf16_wt;
-//       bf16_wt.i[0] = 0;
-//       bf16_wt.i[1] = dwt[i];
-//       bf16_hp.i[0] = out_lo[i];
-//       bf16_hp.i[1] = out_hi[i];
-//       bf16_hp.f = bf16_wt.f * lr + bf16_hp.f;
-//       out_lo[i] = bf16_hp.i[0];
-//       out_hi[i] = bf16_hp.i[1];
-//     }
-// #else
-//     long sz = N;
-//     auto vlr = _mm512_set1_ps(lr);
-//     long i;
-//     for (i = 0; i < ALIGNDOWN(sz, 16); i += 16) {
-//       auto grad_i = _mm512_loadu_ps_auto(&grad[i]);
-//       auto data_i = _mm512_split_loadu_ps(&hi[i], &lo[i]);
-//       data_i = _mm512_add_ps(data_i, _mm512_mul_ps(grad_i, vlr));
-//       _mm512_split_storeu_ps(&hi[i], &lo[i], data_i);
-//     }
-//     if (i < sz) {
-//       int rem = sz - i;
-//       __mmask16 mask = (1 << rem) - 1;
-//       auto grad_i = _mm512_maskz_loadu_ps_auto(mask, &grad[i]);
-//       auto data_i = _mm512_maskz_split_loadu_ps(mask, &hi[i], &lo[i]);
-//       data_i = _mm512_add_ps(data_i, _mm512_mul_ps(grad_i, vlr));
-//       _mm512_mask_split_storeu_ps(&hi[i], &lo[i], mask, data_i);
-//     }
-// #endif
-//   }
-
-//  protected:
-//   std::string hash_str() override {
-//     char hash[200];
-//     snprintf(hash, 200, "split_sgd_eqn_i%d", N);
-//     return std::string(hash);
-//   }
-//   void* build_kernel() override {
-//     libxsmm_blasint ld = N;
-//     libxsmm_blasint my_eqn0 = libxsmm_matrix_eqn_create();
-//     meqn_push_unary_op(my_eqn0, LIBXSMM_MELTW_TYPE_UNARY_UNPACK_TO_BLOCKS);
-//     meqn_push_ternary_op(
-//         my_eqn0,
-//         LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-//         LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_1 |
-//             LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-//     /* This is the "gradient" weights   */
-//     meqn_push_arg(my_eqn0, N, 1, ld, 3, 0, LIBXSMM_DATATYPE_BF16);
-//     /* This is the scalar learning rate */
-//     meqn_push_arg(my_eqn0, 1, 1, 1, 2, 0, LIBXSMM_DATATYPE_F32);
-//     meqn_push_binary_op(my_eqn0, LIBXSMM_MELTW_TYPE_BINARY_PACK);
-//     /* This is the tensor with lo bits  */
-//     meqn_push_arg(my_eqn0, N, 1, ld, 0, 0, LIBXSMM_DATATYPE_I16);
-//     /* This is the tensor with hi bits  */
-//     meqn_push_arg(my_eqn0, N, 1, ld, 1, 0, LIBXSMM_DATATYPE_I16);
-//     debug_print_eqn_tree(my_eqn0);
-//     auto func0 = meqn_dispatch(N, 1, &ld, LIBXSMM_DATATYPE_I16, my_eqn0);
-//     return (void*)func0;
-//   }
-
-//  private:
-//   int N = 0;
-//   libxsmm_matrix_eqn_function kernel = NULL;
-// };
-
-template <typename Tin, typename Tout, typename Tind>
-class EmbBagFwdTPP {
- public:
-  EmbBagFwdTPP() {}
-  EmbBagFwdTPP(int E)
-      : E(E),
-        kernel(
-            0,
-            E,
-            E,
-            E,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            (libxsmm_meltw_unary_flags)(
-                sizeof(Tind) == 8 ? LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_8BYTES
-                                  : LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_4BYTES),
-            LIBXSMM_MELTW_TYPE_UNARY_REDUCE_COLS_IDX_OP_ADD) {}
-  void operator()(Tout* output, Tin* weight, Tind* input, int N) {
-    unsigned long long _N = N;
-    kernel((void*)weight, (void*)input, (void*)&_N, (void*)output, NULL);
-  }
-  void ref(Tout* output, Tin* weight, Tind* input, int N) {
-    for (long v = 0; v < E; v++)
-      output[v] = 0;
-    for (long s = 0; s < N; s++) {
-      auto ind = input[s];
-      for (long v = 0; v < E; v++)
-        output[v] += weight[ind * E + v];
-    }
-  }
-
- private:
-  int E;
-  UnaryTPP kernel;
-};
-
-template <typename Tin, typename Tout>
-class EmbBagBwdTPP {
- public:
-  EmbBagBwdTPP() {}
-  EmbBagBwdTPP(int E)
-      : E(E),
-        kernel(
-            0,
-            E,
-            E,
-            E,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_MELTW_FLAG_UNARY_NONE,
-            LIBXSMM_MELTW_TYPE_UNARY_REPLICATE_COL_VAR) {}
-  void operator()(Tin* in, Tout* out, uint64_t N) {
-    kernel((void*)in, NULL, NULL, (void*)&N, NULL, NULL, (void*)out, NULL);
-  }
-  void ref(Tin* in, Tout* out, uint64_t N) {
-    for (uint64_t i = 0; i < N; i++) {
-      for (int v = 0; v < E; v++) {
-        out[i * E + v] = in[v];
-      }
-    }
-  }
-
- private:
-  int E;
-  UnaryTPP kernel;
-};
-
-template <typename T>
-class FusedAdamWTPP {
- public:
-  FusedAdamWTPP() {}
-  FusedAdamWTPP(int N, float beta1, float beta2, float weight_decay, float eps)
-      : N(N),
-        beta1(beta1),
-        beta2(beta2),
-        weight_decay(weight_decay),
-        eps(eps),
-        eqn0(this, 0),
-        eqn1(this, 1),
-        eqn2(this, 2) {}
-  void operator()(
-      T* data,
-      T* grad,
-      T* exp_avg,
-      T* exp_avg_sq,
-      float step_size,
-      float lr) {
-    float beta1_1 = 1.0f - beta1;
-    float beta2_1 = 1.0f - beta2;
-    float lrwd_1 = 1.0f - lr * weight_decay;
-    libxsmm_matrix_eqn_param eqn_param;
-    libxsmm_matrix_arg arg_array[6];
-    arg_array[0].primary = (void*)grad;
-    arg_array[1].primary = (void*)&beta1_1;
-    arg_array[2].primary = (void*)exp_avg;
-    arg_array[3].primary = (void*)&beta1;
-    eqn_param.inputs = arg_array;
-    eqn_param.output.primary = (void*)exp_avg;
-    eqn0(&eqn_param);
-
-    // arg_array[0].primary = (void*)grad;
-    arg_array[1].primary = (void*)&beta2_1;
-    arg_array[2].primary = (void*)exp_avg_sq;
-    arg_array[3].primary = (void*)&beta2;
-    eqn_param.output.primary = (void*)exp_avg_sq;
-    eqn1(&eqn_param);
-
-    arg_array[0].primary = (void*)exp_avg_sq;
-    arg_array[1].primary = (void*)&eps;
-    arg_array[2].primary = (void*)exp_avg;
-    arg_array[3].primary = (void*)&step_size;
-    arg_array[4].primary = (void*)data;
-    arg_array[5].primary = (void*)&lrwd_1;
-    eqn_param.output.primary = (void*)data;
-    eqn2(&eqn_param);
-  }
-
-  void ref(
-      T* data,
-      T* grad,
-      T* exp_avg,
-      T* exp_avg_sq,
-      float step_size,
-      float lr) {
-    long sz = N;
-    float beta1_1 = 1.0f - beta1;
-    float beta2_1 = 1.0f - beta2;
-#ifndef __AVX512F__
-    for (long i = 0; i < sz; i++) {
-      auto avg_i = exp_avg[i];
-      auto avg_sq_i = exp_avg_sq[i];
-      auto grad_i = grad[i];
-      auto data_i = data[i];
-      avg_i = avg_i * beta1 + grad_i * beta1_1;
-      avg_sq_i = avg_sq_i * beta2 + grad_i * grad_i * beta2_1;
-      auto denom = sqrtf(avg_sq_i) + eps;
-      data_i = data_i - step_size * (avg_i / denom);
-      if (weight_decay > 0.0)
-        data_i = data_i - data_i * lr * weight_decay;
-      exp_avg[i] = avg_i;
-      exp_avg_sq[i] = avg_sq_i;
-      data[i] = data_i;
-    }
-#else
-    auto vbeta1 = _mm512_set1_ps(beta1);
-    auto vbeta1_1 = _mm512_set1_ps(beta1_1);
-    auto vbeta2 = _mm512_set1_ps(beta2);
-    auto vbeta2_1 = _mm512_set1_ps(beta2_1);
-    auto veps = _mm512_set1_ps(eps);
-    auto vstep_size = _mm512_set1_ps(step_size);
-    auto vweight_decay = _mm512_set1_ps(lr * weight_decay);
-    long i;
-    for (i = 0; i < ALIGNDOWN(sz, 16); i += 16) {
-      auto avg_i = _mm512_loadu_ps(&exp_avg[i]);
-      auto avg_sq_i = _mm512_loadu_ps(&exp_avg_sq[i]);
-      auto grad_i = _mm512_loadu_ps(&grad[i]);
-      auto data_i = _mm512_loadu_ps(&data[i]);
-      avg_i = _mm512_add_ps(
-          _mm512_mul_ps(avg_i, vbeta1), _mm512_mul_ps(grad_i, vbeta1_1));
-      avg_sq_i = _mm512_add_ps(
-          _mm512_mul_ps(avg_sq_i, vbeta2),
-          _mm512_mul_ps(_mm512_mul_ps(grad_i, grad_i), vbeta2_1));
-      auto denom = _mm512_add_ps(_mm512_sqrt_ps(avg_sq_i), veps);
-      data_i = _mm512_sub_ps(
-          data_i, _mm512_mul_ps(vstep_size, _mm512_div_ps(avg_i, denom)));
-      // if (weight_decay > 0.0)
-      data_i = _mm512_sub_ps(data_i, _mm512_mul_ps(data_i, vweight_decay));
-      _mm512_storeu_ps(&exp_avg[i], avg_i);
-      _mm512_storeu_ps(&exp_avg_sq[i], avg_sq_i);
-      _mm512_storeu_ps(&data[i], data_i);
-    }
-    if (i < sz) {
-      int rem = sz - i;
-      __mmask16 mask = (1 << rem) - 1;
-      auto avg_i = _mm512_maskz_loadu_ps(mask, &exp_avg[i]);
-      auto avg_sq_i = _mm512_maskz_loadu_ps(mask, &exp_avg_sq[i]);
-      auto grad_i = _mm512_maskz_loadu_ps(mask, &grad[i]);
-      auto data_i = _mm512_maskz_loadu_ps(mask, &data[i]);
-      avg_i = _mm512_add_ps(
-          _mm512_mul_ps(avg_i, vbeta1), _mm512_mul_ps(grad_i, vbeta1_1));
-      avg_sq_i = _mm512_add_ps(
-          _mm512_mul_ps(avg_sq_i, vbeta2),
-          _mm512_mul_ps(_mm512_mul_ps(grad_i, grad_i), vbeta2_1));
-      auto denom = _mm512_add_ps(_mm512_sqrt_ps(avg_sq_i), veps);
-      data_i = _mm512_sub_ps(
-          data_i, _mm512_mul_ps(vstep_size, _mm512_div_ps(avg_i, denom)));
-      // if (weight_decay > 0.0)
-      data_i = _mm512_sub_ps(data_i, _mm512_mul_ps(data_i, vweight_decay));
-      _mm512_mask_storeu_ps(&exp_avg[i], mask, avg_i);
-      _mm512_mask_storeu_ps(&exp_avg_sq[i], mask, avg_sq_i);
-      _mm512_mask_storeu_ps(&data[i], mask, data_i);
-    }
-#endif
-  }
-  class Eqn : BaseTPP {
-   public:
-    Eqn() {}
-    Eqn(FusedAdamWTPP* p, int eqn_no) : p(p), eqn_no(eqn_no) {
-      kernel = (libxsmm_matrix_eqn_function)get_kernel();
-      initialized = true;
-    }
-    void operator()(libxsmm_matrix_eqn_param* eqn_param) {
-      if (!initialized)
-        return;
-      kernel(eqn_param);
-    }
-
-   protected:
-    std::string hash_str() override {
-      char hash[200];
-      snprintf(
-          hash,
-          200,
-          "fused_adamw_eqn%d_t%d_n%d_wd%d",
-          eqn_no,
-          XsmmDtype<T>(),
-          p->N,
-          (p->weight_decay == 0.0 ? 0 : 1));
-      return std::string(hash);
-    }
-    void* build_kernel() override {
-      auto in_dt = XsmmDtype<T>();
-      libxsmm_blasint ld = p->N;
-      auto N = p->N;
-      int use_wd = p->weight_decay == 0.0 ? 0 : 1;
-      libxsmm_matrix_eqn_function func;
-      if (eqn_no == 0) {
-        // Equation for exp_avg
-        auto my_eqn0 = libxsmm_matrix_eqn_create();
-        meqn_push_ternary_op(
-            my_eqn0,
-            LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-            LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_1 |
-                LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-        meqn_push_arg(my_eqn0, N, 1, ld, 2, 0, in_dt); // avg_i
-        meqn_push_arg(my_eqn0, 1, 1, 1, 3, 0, LIBXSMM_DATATYPE_F32); // beta1
-        meqn_push_binary_op(
-            my_eqn0,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-        meqn_push_arg(my_eqn0, N, 1, ld, 0, 0, in_dt); // grad_i
-        meqn_push_arg(my_eqn0, 1, 1, 1, 1, 0, LIBXSMM_DATATYPE_F32); // beta1_1
-        debug_print_eqn_tree(my_eqn0);
-        func = meqn_dispatch(N, 1, &ld, in_dt, my_eqn0);
-      } else if (eqn_no == 1) {
-        // Equation for exp_avg_sq
-        auto my_eqn1 = libxsmm_matrix_eqn_create();
-        meqn_push_ternary_op(
-            my_eqn1,
-            LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-            LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_1 |
-                LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-        meqn_push_arg(my_eqn1, N, 1, ld, 2, 0, in_dt); // avg_sq_i
-        meqn_push_arg(my_eqn1, 1, 1, 1, 3, 0, LIBXSMM_DATATYPE_F32); // beta2
-        meqn_push_binary_op(
-            my_eqn1,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-        meqn_push_unary_op(my_eqn1, LIBXSMM_MELTW_TYPE_UNARY_X2);
-        meqn_push_arg(my_eqn1, N, 1, ld, 0, 0, in_dt); // grad_i
-        meqn_push_arg(my_eqn1, 1, 1, 1, 1, 0, LIBXSMM_DATATYPE_F32); // beta2_1
-        debug_print_eqn_tree(my_eqn1);
-        func = meqn_dispatch(N, 1, &ld, in_dt, my_eqn1);
-      } else if (eqn_no == 2) {
-        // Equation for data_i (with decay)
-        auto my_eqn2 = libxsmm_matrix_eqn_create();
-        if (use_wd == 1) {
-          meqn_push_binary_op(
-              my_eqn2,
-              LIBXSMM_MELTW_TYPE_BINARY_MUL,
-              LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-        }
-        meqn_push_binary_op(my_eqn2, LIBXSMM_MELTW_TYPE_BINARY_SUB);
-        meqn_push_arg(my_eqn2, N, 1, ld, 4, 0, in_dt); // data_i
-        meqn_push_binary_op(
-            my_eqn2,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-        meqn_push_binary_op(my_eqn2, LIBXSMM_MELTW_TYPE_BINARY_DIV);
-        meqn_push_arg(my_eqn2, N, 1, ld, 2, 0, in_dt); // avg_i
-        meqn_push_binary_op(
-            my_eqn2,
-            LIBXSMM_MELTW_TYPE_BINARY_ADD,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-        meqn_push_unary_op(my_eqn2, LIBXSMM_MELTW_TYPE_UNARY_SQRT);
-        meqn_push_arg(my_eqn2, N, 1, ld, 0, 0, in_dt); // avg_sq_i
-        meqn_push_arg(my_eqn2, 1, 1, 1, 1, 0, LIBXSMM_DATATYPE_F32); // eps
-        meqn_push_arg(
-            my_eqn2, 1, 1, 1, 3, 0, LIBXSMM_DATATYPE_F32); // step_size
-        if (use_wd == 1) {
-          // this scalar is (1-lr*weight_decay)
-          meqn_push_arg(my_eqn2, 1, 1, 1, 5, 0, LIBXSMM_DATATYPE_F32);
-        }
-        debug_print_eqn_tree(my_eqn2);
-        func = meqn_dispatch(N, 1, &ld, in_dt, my_eqn2);
-      } else {
-        TPP_ASSERT(false, "Should not come here\n");
-      }
-      return (void*)func;
-    }
-
-   private:
-    FusedAdamWTPP* p;
-    int eqn_no;
-    libxsmm_matrix_eqn_function kernel = NULL;
-  };
-
- private:
-  int N = 0;
-  float beta1, beta2, weight_decay, eps;
-  Eqn eqn0, eqn1, eqn2;
-  friend class Eqn;
-};
-
-// class FusedSplitAdamWTPP {
-//  public:
-//   typedef bfloat16 T;
-//   FusedSplitAdamWTPP() {}
-//   FusedSplitAdamWTPP(
-//       int N,
-//       float beta1,
-//       float beta2,
-//       float weight_decay,
-//       float eps)
-//       : N(N),
-//         beta1(beta1),
-//         beta2(beta2),
-//         weight_decay(weight_decay),
-//         eps(eps),
-//         eqn0(this, 0),
-//         eqn1(this, 1),
-//         eqn2(this, 2) {}
-//   void operator()(
-//       T* hi,
-//       T* lo,
-//       T* grad,
-//       T* exp_avg,
-//       T* exp_avg_sq,
-//       float step_size,
-//       float lr) {
-//     float beta1_1 = 1.0f - beta1;
-//     float beta2_1 = 1.0f - beta2;
-//     float lrwd_1 = 1.0f - lr * weight_decay;
-//     libxsmm_matrix_eqn_param eqn_param;
-//     libxsmm_matrix_arg arg_array[7];
-//     arg_array[0].primary = (void*)grad;
-//     arg_array[1].primary = (void*)&beta1_1;
-//     arg_array[2].primary = (void*)exp_avg;
-//     arg_array[3].primary = (void*)&beta1;
-//     eqn_param.inputs = arg_array;
-//     eqn_param.output.primary = (void*)exp_avg;
-//     eqn0(&eqn_param);
-
-//     // arg_array[0].primary = (void*)grad;
-//     arg_array[1].primary = (void*)&beta2_1;
-//     arg_array[2].primary = (void*)exp_avg_sq;
-//     arg_array[3].primary = (void*)&beta2;
-//     eqn_param.output.primary = (void*)exp_avg_sq;
-//     eqn1(&eqn_param);
-
-//     arg_array[0].primary = (void*)exp_avg_sq;
-//     arg_array[1].primary = (void*)&eps;
-//     arg_array[2].primary = (void*)exp_avg;
-//     arg_array[3].primary = (void*)&step_size;
-//     arg_array[4].primary = (void*)lo;
-//     arg_array[5].primary = (void*)hi;
-//     arg_array[6].primary = (void*)&lrwd_1;
-//     eqn_param.output.primary = (void*)lo;
-//     auto offset = (long long)((char*)hi - (char*)lo);
-//     eqn_param.output.secondary = (void*)offset;
-//     eqn2(&eqn_param);
-//   }
-
-//   void ref(
-//       T* hi,
-//       T* lo,
-//       T* grad,
-//       T* exp_avg,
-//       T* exp_avg_sq,
-//       float step_size,
-//       float lr) {
-//     long sz = N;
-//     float beta1_1 = 1.0f - beta1;
-//     float beta2_1 = 1.0f - beta2;
-// #ifndef __AVX512F__
-//     for (long i = 0; i < sz; i++) {
-//       union libxsmm_bfloat16_f32 data_hp;
-//       float avg_i = exp_avg[i];
-//       float avg_sq_i = exp_avg_sq[i];
-//       float grad_i = grad[i];
-//       data_hp.i[0] = lo[i];
-//       data_hp.i[1] = hi[i];
-//       float data_i = data_hp.f;
-
-//       avg_i = avg_i * beta1 + grad_i * beta1_1;
-//       avg_sq_i = avg_sq_i * beta2 + grad_i * grad_i * beta2_1;
-//       auto denom = sqrtf(avg_sq_i) + eps;
-//       data_i = data_i - step_size * (avg_i / denom);
-//       if (weight_decay > 0.0)
-//         data_i = data_i - data_i * lr * weight_decay;
-//       exp_avg[i] = avg_i;
-//       exp_avg_sq[i] = avg_sq_i;
-//       data_hp.f = data_i;
-//       lo[i] = data_hp.i[0];
-//       hi[i] = data_hp.i[1];
-//     }
-// #else
-//     auto vbeta1 = _mm512_set1_ps(beta1);
-//     auto vbeta1_1 = _mm512_set1_ps(beta1_1);
-//     auto vbeta2 = _mm512_set1_ps(beta2);
-//     auto vbeta2_1 = _mm512_set1_ps(beta2_1);
-//     auto veps = _mm512_set1_ps(eps);
-//     auto vstep_size = _mm512_set1_ps(step_size);
-//     auto vweight_decay = _mm512_set1_ps(lr * weight_decay);
-//     long i;
-//     for (i = 0; i < ALIGNDOWN(sz, 16); i += 16) {
-//       auto avg_i = _mm512_loadu_ps(&exp_avg[i]);
-//       auto avg_sq_i = _mm512_loadu_ps(&exp_avg_sq[i]);
-//       auto grad_i = _mm512_loadu_ps(&grad[i]);
-//       auto data_i = _mm512_split_loadu_ps(&hi[i], &lo[i]);
-//       avg_i = _mm512_add_ps(
-//           _mm512_mul_ps(avg_i, vbeta1), _mm512_mul_ps(grad_i, vbeta1_1));
-//       avg_sq_i = _mm512_add_ps(
-//           _mm512_mul_ps(avg_sq_i, vbeta2),
-//           _mm512_mul_ps(_mm512_mul_ps(grad_i, grad_i), vbeta2_1));
-//       auto denom = _mm512_add_ps(_mm512_sqrt_ps(avg_sq_i), veps);
-//       data_i = _mm512_sub_ps(
-//           data_i, _mm512_mul_ps(vstep_size, _mm512_div_ps(avg_i, denom)));
-//       if (weight_decay > 0.0)
-//         data_i = _mm512_sub_ps(data_i, _mm512_mul_ps(data_i, vweight_decay));
-//       _mm512_storeu_ps(&exp_avg[i], avg_i);
-//       _mm512_storeu_ps(&exp_avg_sq[i], avg_sq_i);
-//       _mm512_split_storeu_ps(&hi[i], &lo[i], data_i);
-//     }
-//     if (i < sz) {
-//       int rem = sz - i;
-//       __mmask16 mask = (1 << rem) - 1;
-//       auto avg_i = _mm512_maskz_loadu_ps(mask, &exp_avg[i]);
-//       auto avg_sq_i = _mm512_maskz_loadu_ps(mask, &exp_avg_sq[i]);
-//       auto grad_i = _mm512_maskz_loadu_ps(mask, &grad[i]);
-//       auto data_i = _mm512_maskz_split_loadu_ps(mask, &hi[i], &lo[i]);
-//       avg_i = _mm512_add_ps(
-//           _mm512_mul_ps(avg_i, vbeta1), _mm512_mul_ps(grad_i, vbeta1_1));
-//       avg_sq_i = _mm512_add_ps(
-//           _mm512_mul_ps(avg_sq_i, vbeta2),
-//           _mm512_mul_ps(_mm512_mul_ps(grad_i, grad_i), vbeta2_1));
-//       auto denom = _mm512_add_ps(_mm512_sqrt_ps(avg_sq_i), veps);
-//       data_i = _mm512_sub_ps(
-//           data_i, _mm512_mul_ps(vstep_size, _mm512_div_ps(avg_i, denom)));
-//       if (weight_decay > 0.0)
-//         data_i = _mm512_sub_ps(data_i, _mm512_mul_ps(data_i, vweight_decay));
-//       _mm512_mask_storeu_ps(&exp_avg[i], mask, avg_i);
-//       _mm512_mask_storeu_ps(&exp_avg_sq[i], mask, avg_sq_i);
-//       _mm512_mask_split_storeu_ps(&hi[i], &lo[i], mask, data_i);
-//     }
-// #endif
-//   }
-//   class Eqn : BaseTPP {
-//    public:
-//     Eqn() {}
-//     Eqn(FusedSplitAdamWTPP* p, int eqn_no) : p(p), eqn_no(eqn_no) {
-//       kernel = (libxsmm_matrix_eqn_function)get_kernel();
-//       initialized = true;
-//     }
-//     void operator()(libxsmm_matrix_eqn_param* eqn_param) {
-//       if (!initialized)
-//         return;
-//       kernel(eqn_param);
-//     }
-
-//    protected:
-//     std::string hash_str() override {
-//       char hash[200];
-//       snprintf(
-//           hash,
-//           200,
-//           "fused_split_adamw_eqn%d_t%d_n%d_wd%d",
-//           eqn_no,
-//           XsmmDtype<T>(),
-//           p->N,
-//           (p->weight_decay == 0.0 ? 0 : 1));
-//       return std::string(hash);
-//     }
-//     void* build_kernel() override {
-//       auto in_dt = XsmmDtype<T>();
-//       libxsmm_blasint ld = p->N;
-//       auto N = p->N;
-//       int use_wd = p->weight_decay == 0.0 ? 0 : 1;
-//       libxsmm_matrix_eqn_function func;
-//       if (eqn_no == 0) {
-//         // Equation for exp_avg
-//         auto my_eqn0 = libxsmm_matrix_eqn_create();
-//         meqn_push_ternary_op(
-//             my_eqn0,
-//             LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-//             LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_1 |
-//                 LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-//         meqn_push_arg(my_eqn0, N, 1, ld, 2, 0, in_dt); // avg_i
-//         meqn_push_arg(my_eqn0, 1, 1, 1, 3, 0, LIBXSMM_DATATYPE_F32); // beta1
-//         meqn_push_binary_op(
-//             my_eqn0,
-//             LIBXSMM_MELTW_TYPE_BINARY_MUL,
-//             LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-//         meqn_push_arg(my_eqn0, N, 1, ld, 0, 0, in_dt); // grad_i
-//         meqn_push_arg(my_eqn0, 1, 1, 1, 1, 0, LIBXSMM_DATATYPE_F32); // beta1_1
-//         debug_print_eqn_tree(my_eqn0);
-//         func = meqn_dispatch(N, 1, &ld, in_dt, my_eqn0);
-//       } else if (eqn_no == 1) {
-//         // Equation for exp_avg_sq
-//         auto my_eqn1 = libxsmm_matrix_eqn_create();
-//         meqn_push_ternary_op(
-//             my_eqn1,
-//             LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-//             LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_1 |
-//                 LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-//         meqn_push_arg(my_eqn1, N, 1, ld, 2, 0, in_dt); // avg_sq_i
-//         meqn_push_arg(my_eqn1, 1, 1, 1, 3, 0, LIBXSMM_DATATYPE_F32); // beta2
-//         meqn_push_binary_op(
-//             my_eqn1,
-//             LIBXSMM_MELTW_TYPE_BINARY_MUL,
-//             LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-//         meqn_push_unary_op(my_eqn1, LIBXSMM_MELTW_TYPE_UNARY_X2);
-//         meqn_push_arg(my_eqn1, N, 1, ld, 0, 0, in_dt); // grad_i
-//         meqn_push_arg(my_eqn1, 1, 1, 1, 1, 0, LIBXSMM_DATATYPE_F32); // beta2_1
-//         debug_print_eqn_tree(my_eqn1);
-//         func = meqn_dispatch(N, 1, &ld, in_dt, my_eqn1);
-//       } else if (eqn_no == 2) {
-//         // Equation for data_i (with decay)
-//         auto my_eqn2 = libxsmm_matrix_eqn_create();
-//         meqn_push_unary_op(my_eqn2, LIBXSMM_MELTW_TYPE_UNARY_UNPACK_TO_BLOCKS);
-//         if (use_wd == 1) {
-//           meqn_push_binary_op(
-//               my_eqn2,
-//               LIBXSMM_MELTW_TYPE_BINARY_MUL,
-//               LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-//         }
-//         meqn_push_binary_op(my_eqn2, LIBXSMM_MELTW_TYPE_BINARY_SUB);
-//         meqn_push_binary_op(my_eqn2, LIBXSMM_MELTW_TYPE_BINARY_PACK);
-//         meqn_push_arg(
-//             my_eqn2, N, 1, ld, 4, 0, LIBXSMM_DATATYPE_I16); // data_i lo
-//         meqn_push_arg(
-//             my_eqn2, N, 1, ld, 5, 0, LIBXSMM_DATATYPE_I16); // data_i hi
-
-//         meqn_push_binary_op(
-//             my_eqn2,
-//             LIBXSMM_MELTW_TYPE_BINARY_MUL,
-//             LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-//         meqn_push_binary_op(my_eqn2, LIBXSMM_MELTW_TYPE_BINARY_DIV);
-//         meqn_push_arg(my_eqn2, N, 1, ld, 2, 0, in_dt); // avg_i
-//         meqn_push_binary_op(
-//             my_eqn2,
-//             LIBXSMM_MELTW_TYPE_BINARY_ADD,
-//             LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-//         meqn_push_unary_op(my_eqn2, LIBXSMM_MELTW_TYPE_UNARY_SQRT);
-//         meqn_push_arg(my_eqn2, N, 1, ld, 0, 0, in_dt); // avg_sq_i
-//         meqn_push_arg(my_eqn2, 1, 1, 1, 1, 0, LIBXSMM_DATATYPE_F32); // eps
-//         meqn_push_arg(
-//             my_eqn2, 1, 1, 1, 3, 0, LIBXSMM_DATATYPE_F32); // step_size
-//         if (use_wd == 1) {
-//           // this scalar is (1-lr*weight_decay)
-//           meqn_push_arg(my_eqn2, 1, 1, 1, 6, 0, LIBXSMM_DATATYPE_F32);
-//         }
-//         debug_print_eqn_tree(my_eqn2);
-//         func = meqn_dispatch(N, 1, &ld, LIBXSMM_DATATYPE_I16, my_eqn2);
-//       } else {
-//         TPP_ASSERT(false, "Should not come here\n");
-//       }
-//       return (void*)func;
-//     }
-
-//    private:
-//     FusedSplitAdamWTPP* p;
-//     int eqn_no;
-//     libxsmm_matrix_eqn_function kernel = NULL;
-//   };
-
-//  private:
-//   int N = 0;
-//   float beta1, beta2, weight_decay, eps;
-//   Eqn eqn0, eqn1, eqn2;
-//   friend class Eqn;
-// };
-
-template <typename T>
-class FusedAdamStepTPP {
- public:
-  FusedAdamStepTPP() {}
-  FusedAdamStepTPP(
-      int N,
-      float beta1,
-      float beta2,
-      float eps,
-      bool use_weight_decay,
-      bool use_bias_correction)
-      : N(N),
-        beta1(beta1),
-        beta2(beta2),
-        eps(eps),
-        use_weight_decay(use_weight_decay),
-        use_bias_correction(use_bias_correction),
-        eqn0(this, 0),
-        eqn1(this, 1),
-        eqn2(this, 2) {}
-  void operator()(
-      T* data,
-      T* grad,
-      T* exp_avg,
-      T* exp_avg_sq,
-      T* adam_step,
-      float weight_decay = 0.0,
-      float exp_avg_scale = 1.0,
-      float exp_avg_sq_scale = 1.0) {
-    float beta1_1 = 1.0f - beta1;
-    float beta2_1 = 1.0f - beta2;
-    libxsmm_matrix_eqn_param eqn_param;
-    libxsmm_matrix_arg arg_array[7];
-    arg_array[0].primary = (void*)grad;
-    arg_array[1].primary = (void*)&beta1_1;
-    arg_array[2].primary = (void*)exp_avg;
-    arg_array[3].primary = (void*)&beta1;
-    eqn_param.inputs = arg_array;
-    eqn_param.output.primary = (void*)exp_avg;
-    eqn0(&eqn_param);
-
-    // arg_array[0].primary = (void*)grad;
-    arg_array[1].primary = (void*)&beta2_1;
-    arg_array[2].primary = (void*)exp_avg_sq;
-    arg_array[3].primary = (void*)&beta2;
-    eqn_param.output.primary = (void*)exp_avg_sq;
-    eqn1(&eqn_param);
-
-    arg_array[0].primary = (void*)exp_avg_sq;
-    arg_array[1].primary = (void*)&eps;
-    arg_array[2].primary = (void*)exp_avg;
-    arg_array[3].primary = (void*)data;
-    arg_array[4].primary = (void*)&weight_decay;
-    arg_array[5].primary = (void*)&exp_avg_scale;
-    arg_array[6].primary = (void*)&exp_avg_sq_scale;
-    eqn_param.output.primary = (void*)adam_step;
-    eqn2(&eqn_param);
-  }
-
-  void ref(
-      T* data,
-      T* grad,
-      T* exp_avg,
-      T* exp_avg_sq,
-      T* adam_step,
-      float weight_decay = 0.0,
-      float exp_avg_scale = 1.0,
-      float exp_avg_sq_scale = 1.0) {
-    long sz = N;
-    float beta1_1 = 1.0f - beta1;
-    float beta2_1 = 1.0f - beta2;
-#ifndef __AVX512F__
-    for (long i = 0; i < sz; i++) {
-      float avg_i = exp_avg[i];
-      float avg_sq_i = exp_avg_sq[i];
-      float grad_i = grad[i];
-      avg_i = avg_i * beta1 + grad_i * beta1_1;
-      avg_sq_i = avg_sq_i * beta2 + grad_i * grad_i * beta2_1;
-      exp_avg[i] = avg_i;
-      exp_avg_sq[i] = avg_sq_i;
-      if (use_bias_correction) {
-        avg_i = avg_i * exp_avg_scale;
-        avg_sq_i = avg_sq_i * exp_avg_sq_scale;
-      }
-      float denom = sqrtf(avg_sq_i) + eps;
-      float adam_step_i = avg_i / denom;
-      if (use_weight_decay) {
-        float data_i = data[i];
-        adam_step_i += data_i * weight_decay;
-      }
-      adam_step[i] = adam_step_i;
-    }
-#else
-    auto vbeta1 = _mm512_set1_ps(beta1);
-    auto vbeta1_1 = _mm512_set1_ps(beta1_1);
-    auto vbeta2 = _mm512_set1_ps(beta2);
-    auto vbeta2_1 = _mm512_set1_ps(beta2_1);
-    auto veps = _mm512_set1_ps(eps);
-    // auto vstep_size = _mm512_set1_ps(step_size);
-    auto vweight_decay = _mm512_set1_ps(weight_decay);
-    auto vexp_avg_scale = _mm512_set1_ps(exp_avg_scale);
-    auto vexp_avg_sq_scale = _mm512_set1_ps(exp_avg_sq_scale);
-    long i;
-    for (i = 0; i < ALIGNDOWN(sz, 16); i += 16) {
-      auto avg_i = _mm512_loadu_ps_auto(&exp_avg[i]);
-      auto avg_sq_i = _mm512_loadu_ps_auto(&exp_avg_sq[i]);
-      auto grad_i = _mm512_loadu_ps_auto(&grad[i]);
-      avg_i = _mm512_add_ps(
-          _mm512_mul_ps(avg_i, vbeta1), _mm512_mul_ps(grad_i, vbeta1_1));
-      avg_sq_i = _mm512_add_ps(
-          _mm512_mul_ps(avg_sq_i, vbeta2),
-          _mm512_mul_ps(_mm512_mul_ps(grad_i, grad_i), vbeta2_1));
-      _mm512_storeu_ps_auto(&exp_avg[i], avg_i);
-      _mm512_storeu_ps_auto(&exp_avg_sq[i], avg_sq_i);
-      if (use_bias_correction) {
-        avg_i = _mm512_mul_ps(avg_i, vexp_avg_scale);
-        avg_sq_i = _mm512_mul_ps(avg_sq_i, vexp_avg_sq_scale);
-      }
-      auto denom = _mm512_add_ps(_mm512_sqrt_ps(avg_sq_i), veps);
-      auto adam_step_i = _mm512_div_ps(avg_i, denom);
-      if (use_weight_decay) {
-        auto data_i = _mm512_loadu_ps_auto(&data[i]);
-        adam_step_i =
-            _mm512_add_ps(adam_step_i, _mm512_mul_ps(data_i, vweight_decay));
-      }
-      _mm512_storeu_ps_auto(&adam_step[i], adam_step_i);
-    }
-    if (i < sz) {
-      int rem = sz - i;
-      __mmask16 mask = (1 << rem) - 1;
-      auto avg_i = _mm512_maskz_loadu_ps_auto(mask, &exp_avg[i]);
-      auto avg_sq_i = _mm512_maskz_loadu_ps_auto(mask, &exp_avg_sq[i]);
-      auto grad_i = _mm512_maskz_loadu_ps_auto(mask, &grad[i]);
-      avg_i = _mm512_add_ps(
-          _mm512_mul_ps(avg_i, vbeta1), _mm512_mul_ps(grad_i, vbeta1_1));
-      avg_sq_i = _mm512_add_ps(
-          _mm512_mul_ps(avg_sq_i, vbeta2),
-          _mm512_mul_ps(_mm512_mul_ps(grad_i, grad_i), vbeta2_1));
-      _mm512_mask_storeu_ps_auto(&exp_avg[i], mask, avg_i);
-      _mm512_mask_storeu_ps_auto(&exp_avg_sq[i], mask, avg_sq_i);
-      if (use_bias_correction) {
-        avg_i = _mm512_mul_ps(avg_i, vexp_avg_scale);
-        avg_sq_i = _mm512_mul_ps(avg_sq_i, vexp_avg_sq_scale);
-      }
-      auto denom = _mm512_add_ps(_mm512_sqrt_ps(avg_sq_i), veps);
-      auto adam_step_i = _mm512_div_ps(avg_i, denom);
-      if (use_weight_decay) {
-        auto data_i = _mm512_maskz_loadu_ps_auto(mask, &data[i]);
-        adam_step_i =
-            _mm512_add_ps(adam_step_i, _mm512_mul_ps(data_i, vweight_decay));
-      }
-      _mm512_mask_storeu_ps_auto(&adam_step[i], mask, adam_step_i);
-    }
-#endif
-  }
-  class Eqn : BaseTPP {
-   public:
-    Eqn() {}
-    Eqn(FusedAdamStepTPP* p, int eqn_no) : p(p), eqn_no(eqn_no) {
-      kernel = (libxsmm_matrix_eqn_function)get_kernel();
-      initialized = true;
-    }
-    void operator()(libxsmm_matrix_eqn_param* eqn_param) {
-      if (!initialized)
-        return;
-      kernel(eqn_param);
-    }
-
-   protected:
-    std::string hash_str() override {
-      char hash[200];
-      snprintf(
-          hash,
-          200,
-          "fused_adam_step_eqn%d_t%d_n%d_wd%d",
-          eqn_no,
-          XsmmDtype<T>(),
-          p->N,
-          p->use_weight_decay);
-      return std::string(hash);
-    }
-    void* build_kernel() override {
-      auto in_dt = XsmmDtype<T>();
-      libxsmm_blasint ld = p->N;
-      auto N = p->N;
-      int use_wd = p->use_weight_decay;
-      int use_bc = p->use_bias_correction;
-      libxsmm_matrix_eqn_function func;
-      if (eqn_no == 0) {
-        // Equation for exp_avg
-        auto my_eqn0 = libxsmm_matrix_eqn_create();
-        meqn_push_ternary_op(
-            my_eqn0,
-            LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-            LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_1 |
-                LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-        meqn_push_arg(my_eqn0, N, 1, ld, 2, 0, in_dt); // avg_i
-        meqn_push_arg(my_eqn0, 1, 1, 1, 3, 0, LIBXSMM_DATATYPE_F32); // beta1
-        meqn_push_binary_op(
-            my_eqn0,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-        meqn_push_arg(my_eqn0, N, 1, ld, 0, 0, in_dt); // grad_i
-        meqn_push_arg(my_eqn0, 1, 1, 1, 1, 0, LIBXSMM_DATATYPE_F32); // beta1_1
-        debug_print_eqn_tree(my_eqn0);
-        func = meqn_dispatch(N, 1, &ld, in_dt, my_eqn0);
-      } else if (eqn_no == 1) {
-        // Equation for exp_avg_sq
-        auto my_eqn1 = libxsmm_matrix_eqn_create();
-        meqn_push_ternary_op(
-            my_eqn1,
-            LIBXSMM_MELTW_TYPE_TERNARY_MULADD,
-            LIBXSMM_MELTW_FLAG_TERNARY_BCAST_SCALAR_IN_1 |
-                LIBXSMM_MELTW_FLAG_TERNARY_REUSE_IN_2_AS_OUT);
-        meqn_push_arg(my_eqn1, N, 1, ld, 2, 0, in_dt); // avg_sq_i
-        meqn_push_arg(my_eqn1, 1, 1, 1, 3, 0, LIBXSMM_DATATYPE_F32); // beta2
-        meqn_push_binary_op(
-            my_eqn1,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-        meqn_push_unary_op(my_eqn1, LIBXSMM_MELTW_TYPE_UNARY_X2);
-        meqn_push_arg(my_eqn1, N, 1, ld, 0, 0, in_dt); // grad_i
-        meqn_push_arg(my_eqn1, 1, 1, 1, 1, 0, LIBXSMM_DATATYPE_F32); // beta2_1
-        debug_print_eqn_tree(my_eqn1);
-        func = meqn_dispatch(N, 1, &ld, in_dt, my_eqn1);
-      } else if (eqn_no == 2) {
-        // Equation for adam_step_i (with decay)
-        auto my_eqn2 = libxsmm_matrix_eqn_create();
-        if (use_wd == 1) {
-          meqn_push_binary_op(my_eqn2, LIBXSMM_MELTW_TYPE_BINARY_ADD);
-          meqn_push_binary_op(
-              my_eqn2,
-              LIBXSMM_MELTW_TYPE_BINARY_MUL,
-              LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-          meqn_push_arg(my_eqn2, N, 1, ld, 3, 0, in_dt); // data_i
-          // weight_decay
-          meqn_push_arg(my_eqn2, 1, 1, 1, 4, 0, LIBXSMM_DATATYPE_F32);
-        }
-        meqn_push_binary_op(my_eqn2, LIBXSMM_MELTW_TYPE_BINARY_DIV);
-        if (use_bc) {
-          meqn_push_binary_op(
-              my_eqn2,
-              LIBXSMM_MELTW_TYPE_BINARY_MUL,
-              LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_0);
-          meqn_push_arg(
-              my_eqn2, 1, 1, 1, 5, 0, LIBXSMM_DATATYPE_F32); // avg_i_scale
-        }
-        meqn_push_arg(my_eqn2, N, 1, ld, 2, 0, in_dt); // avg_i
-        meqn_push_binary_op(
-            my_eqn2,
-            LIBXSMM_MELTW_TYPE_BINARY_ADD,
-            LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_1);
-        meqn_push_unary_op(my_eqn2, LIBXSMM_MELTW_TYPE_UNARY_SQRT);
-        if (use_bc) {
-          meqn_push_binary_op(
-              my_eqn2,
-              LIBXSMM_MELTW_TYPE_BINARY_MUL,
-              LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_0);
-          meqn_push_arg(
-              my_eqn2, 1, 1, 1, 6, 0, LIBXSMM_DATATYPE_F32); // avg_sq_i_scale
-        }
-        meqn_push_arg(my_eqn2, N, 1, ld, 0, 0, in_dt); // avg_sq_i
-        meqn_push_arg(my_eqn2, 1, 1, 1, 1, 0, LIBXSMM_DATATYPE_F32); // eps
-        debug_print_eqn_tree(my_eqn2);
-        func = meqn_dispatch(N, 1, &ld, in_dt, my_eqn2);
-      } else {
-        TPP_ASSERT(false, "Should not come here\n");
-      }
-      return (void*)func;
-    }
-
-   private:
-    FusedAdamStepTPP* p;
-    int eqn_no;
-    libxsmm_matrix_eqn_function kernel = NULL;
-  };
-
- private:
-  int N = 0;
-  float beta1, beta2, eps;
-  bool use_weight_decay, use_bias_correction;
-  Eqn eqn0, eqn1, eqn2;
-  friend class Eqn;
-};
-
-}; // namespace tpp
-
-#endif // _TPP_XSMM_FUNCTORS_H_
\ No newline at end of file
diff --git a/csrc/cpu/tpp/woq/utils.h b/csrc/cpu/tpp/woq/utils.h
deleted file mode 100644
index b28a58aa..00000000
--- a/csrc/cpu/tpp/woq/utils.h
+++ /dev/null
@@ -1,157 +0,0 @@
-/******************************************************************************
- * Copyright (c) 2022 Intel Corporation - All rights reserved.                *
- *                                                                            *
- * For information on the license, see the LICENSE file.                      *
- * Further information: https://github.com/libxsmm/tpp-pytorch-extension/     *
- * SPDX-License-Identifier: BSD-3-Clause                                      *
- ******************************************************************************/
-/* Author: Dhiraj Kalamkar (Intel Corp.)
- ******************************************************************************/
-
-#ifndef _TPP_UTILS_H_
-#define _TPP_UTILS_H_
-
-#include <cxxabi.h>
-#include <iostream>
-#include <typeinfo>
-#include <vector>
-#ifdef _OPENMP
-#include <omp.h>
-#else
-#define omp_get_max_threads() 1
-#define omp_get_num_threads() 1
-#define omp_get_thread_num() 0
-#endif
-
-#define MAX_THREADS 640
-#define ALIGNDOWN(N, A) ((N) & ~((A)-1))
-#define TPP_ASSERT(cond, x...) \
-  do {                         \
-    if (!(cond)) {             \
-      printf(x);               \
-      fflush(stdout);          \
-      exit(1);                 \
-    }                          \
-  } while (0)
-
-#define TLA_ASSERT(cond, x...) \
-  do {                         \
-    if (!(cond)) {             \
-      printf(x);               \
-      printf("\n");            \
-      fflush(stdout);          \
-      exit(1);                 \
-    }                          \
-  } while (0)
-
-#define DECL_VLA_PTR(type, name, dims, ptr) type(*name) dims = (type(*) dims)ptr
-#define DECL_VLA_PTR_PT(type, name, dims, t) \
-  type(*name) dims = (type(*) dims)(pt_get_data_ptr<type>(t))
-
-extern double ifreq; // defined in init.cpp
-
-#if 0
-// Defined in xsmm.cpp
-extern thread_local unsigned int* rng_state;
-extern thread_local struct drand48_data drng_state; // For non AVX512 version
-unsigned int* get_rng_state();
-#endif
-
-template <typename T>
-inline std::string get_class_name() {
-  auto cname = abi::__cxa_demangle(typeid(T).name(), 0, 0, NULL);
-  std::string name(cname);
-  free(cname);
-  return name;
-}
-
-#ifdef __x86_64__
-static __inline__ unsigned long long rdtsc(void) {
-  unsigned hi, lo;
-  __asm__ __volatile__("rdtsc" : "=a"(lo), "=d"(hi));
-  return ((unsigned long long)lo) | (((unsigned long long)hi) << 32);
-}
-#elif defined(__aarch64__)
-static __inline__ unsigned long long rdtsc(void) {
-  unsigned long long val;
-
-  /*
-   * According to ARM DDI 0487F.c, from Armv8.0 to Armv8.5 inclusive, the
-   * system counter is at least 56 bits wide; from Armv8.6, the counter
-   * must be 64 bits wide.  So the system counter could be less than 64
-   * bits wide and it is attributed with the flag 'cap_user_time_short'
-   * is true.
-   */
-  asm volatile("mrs %0, cntvct_el0" : "=r"(val));
-
-  return val;
-}
-#else
-#error "Unsupported architecture for rdtsc"
-#endif
-inline double getFreq() {
-  long long int s = rdtsc();
-  // sleep(1);
-  long long int e = rdtsc();
-  return (e - s) * 1.0;
-}
-
-inline double getTime() {
-  return rdtsc() * ifreq;
-}
-
-inline int env2int(const char* env_name, int def_val = 0) {
-  int val = def_val;
-  auto env = getenv(env_name);
-  if (env)
-    val = atoi(env);
-  // printf("Using %s = %d\n", env_name, val);
-  return val;
-}
-
-inline int guess_mpi_rank() {
-  const char* env_names[] = {
-      "RANK", "PMI_RANK", "OMPI_COMM_WORLD_RANK", "MV2_COMM_WORLD_RANK"};
-  static int guessed_rank = -1;
-  if (guessed_rank >= 0)
-    return guessed_rank;
-  for (int i = 0; i < 4; i++) {
-    if (getenv(env_names[i]) != NULL) {
-      int r = atoi(getenv(env_names[i]));
-      if (r >= 0) {
-        printf("My guessed rank = %d\n", r);
-        guessed_rank = r;
-        return guessed_rank;
-      }
-    }
-  }
-  guessed_rank = 0;
-  return guessed_rank;
-}
-
-// A class for forced loop unrolling at compile time
-// These macro utils and the small gemm intrinsics kernels are implemented
-// based on the initial code by pujiang.he@intel.com.
-template <int i>
-struct compile_time_for {
-  template <typename Lambda, typename... Args>
-  inline static void op(const Lambda& function, Args... args) {
-    compile_time_for<i - 1>::op(function, std::forward<Args>(args)...);
-    function(std::integral_constant<int, i - 1>{}, std::forward<Args>(args)...);
-  }
-};
-template <>
-struct compile_time_for<1> {
-  template <typename Lambda, typename... Args>
-  inline static void op(const Lambda& function, Args... args) {
-    function(std::integral_constant<int, 0>{}, std::forward<Args>(args)...);
-  }
-};
-template <>
-struct compile_time_for<0> {
-  // 0 loops, do nothing
-  template <typename Lambda, typename... Args>
-  inline static void op(const Lambda& function, Args... args) {}
-};
-
-#endif //_TPP_UTILS_H_
diff --git a/csrc/cpu/tpp/woq/vec.h b/csrc/cpu/tpp/woq/vec.h
deleted file mode 100644
index da8f5dbe..00000000
--- a/csrc/cpu/tpp/woq/vec.h
+++ /dev/null
@@ -1,164 +0,0 @@
-#ifndef _TLA_VEC_H_
-#define _TLA_VEC_H_
-
-#include <immintrin.h>
-
-template <typename VT>
-struct VecOps {
-  using ST = float;
-  static constexpr int VLEN = 4;
-  static VT loadu(const void* p) {
-    TLA_ASSERT(false, "should not reach here");
-  }
-  static void storeu(void *p, VT v) {
-    TLA_ASSERT(false, "should not reach here");
-  }
-  static VT set1(ST v) {
-    TLA_ASSERT(false, "should not reach here");
-  }
-  static VT setzero() {
-    TLA_ASSERT(false, "should not reach here");
-  }
-  static VT fmadd(VT a, VT b, VT c) {
-    TLA_ASSERT(false, "should not reach here");
-  }
-  static VT set_0_to_15() {
-    TLA_ASSERT(false, "should not reach here");
-  }
-  static VT mul() {
-    TLA_ASSERT(false, "should not reach here");
-  }
-};
-
-template <typename ST>
-struct VecType {
-  using type = void*;
-};
-
-#ifdef __AVX512F__
-template <>
-struct VecOps<__m512> {
-  using ST = float;
-  static constexpr int VLEN = sizeof(__m512) / sizeof(ST);
-  static inline __m512 loadu(const void* p) { return _mm512_loadu_ps(p); }
-  static inline void storeu(void *p, __m512 v) { _mm512_storeu_ps(p, v); }
-  static inline __m512 set1(ST v) { return _mm512_set1_ps(v); }
-  static inline __m512 setzero() { return _mm512_setzero_ps(); }
-  static inline __m512 fmadd(__m512 a, __m512 b, __m512 c) { return _mm512_fmadd_ps(a, b, c); }
-  static inline __m512 mul(__m512 a, __m512 b) { return _mm512_mul_ps(a, b); }
-  static inline __m512 set_0_to_15() {
-    return _mm512_set_ps(15.0f, 14.0f, 13.0f, 12.0f, 11.0f, 10.0f, 9.0f, 8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f);
-  }
-};
-
-template <>
-struct VecType<float> {
-  using type = __m512;
-};
-
-#endif
-
-#ifdef __AVX512FP16__
-template <>
-struct VecOps<__m512h> {
-  using ST = _Float16;
-  static constexpr int VLEN = sizeof(__m512h) / sizeof(ST);
-  static inline __m512h loadu(const void* p) { return _mm512_loadu_ph(p); }
-  static inline void storeu(void *p, __m512h v) { _mm512_storeu_ph(p, v); }
-  static inline __m512h set1(ST v) { return _mm512_set1_ph(v); }
-  static inline __m512h setzero() { return _mm512_setzero_ph(); }
-  static inline __m512h fmadd(__m512h a, __m512h b, __m512h c) { return _mm512_fmadd_ph(a, b, c); }
-  static inline __m512h mul(__m512h a, __m512h b) { return _mm512_mul_ph(a, b); }
-  static inline __m512h set_0_to_15() {
-    return _mm512_set_ph(
-      15.0f, 14.0f, 13.0f, 12.0f, 11.0f, 10.0f, 9.0f, 8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f,
-      15.0f, 14.0f, 13.0f, 12.0f, 11.0f, 10.0f, 9.0f, 8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f
-    );
-  }
-};
-
-template <>
-struct VecType<tpp::half> {
-  using type = __m512h;
-};
-
-template <>
-struct VecType<_Float16> {
-  using type = __m512h;
-};
-
-#endif
-
-#ifdef __AVX512F__
-// load 32 bfloat16 values as 1 tuple of 2 float32 __m512 registers: (low, high)
-inline std::tuple<__m512, __m512> _vec_load_bfloat16_as_two_floats(const tpp::bfloat16* addr) {
-  auto cvt_bf16_to_fp32 = [](__m256i src) -> __m512 {
-    auto y = _mm512_cvtepu16_epi32(src);
-    return _mm512_castsi512_ps(_mm512_bslli_epi128(y, 2));
-  };
-
-  __m512i v = _mm512_loadu_si512(addr);
-  // convert lower 16 bfloat16 values to float32
-  auto v0 = cvt_bf16_to_fp32(_mm512_castsi512_si256(v));
-  // convert higher 16 bfloat16 values to float32
-  auto v1 = cvt_bf16_to_fp32(_mm512_extracti64x4_epi64(v, 1));
-  return std::make_tuple(v0, v1);
-};
-
-// store 32 bfloat16 values from 2 float32 __m512 registers: v0: low, v1: high
-inline void _vec_store_two_floats_as_bfloat16(tpp::bfloat16* addr, __m512 v0, __m512 v1) {
-#ifdef __AVX512BF16__
-  // convert lower 16 float32 values to bfloat16
-  auto v0_bf16 = reinterpret_cast<__m256i>(_mm512_cvtneps_pbh(v0));
-  // convert higher 16 float32 values to bfloat16
-  auto v1_bf16 = reinterpret_cast<__m256i>(_mm512_cvtneps_pbh(v1));
-  // combine the lower 16 and higher 16 bfloat16 values
-  auto v = _mm512_castsi256_si512(v0_bf16);
-  v = _mm512_inserti64x4(v, v1_bf16, 1);
-  _mm512_storeu_si512(addr, v);
-#else
-  // TODO(jgong5): emuclate AVX512BF16 downcast
-  TLA_ASSERT(false, "not implemented");
-#endif
-};
-#endif // __AVX512F__
-
-// TODO(jgong5): support prefetch hint?
-template <long N, typename T>
-struct VecArray {
-  using vec_type = typename VecType<T>::type;
-  using vec_ops = VecOps<vec_type>;
-  static constexpr long n = N;
-  static constexpr long num_vec = N / vec_ops::VLEN;
-  using type = typename std::array<typename VecType<T>::type, num_vec>;
-
-  static inline type load1d(T* p) {
-    type result;
-    compile_time_for<num_vec>::op(
-      [&](auto i) {
-        result[i] = vec_ops::loadu(p + i*vec_ops::VLEN);
-      }
-    );
-    return result;
-  }
-};
-
-template <long N>
-struct VecArray<N, tpp::bfloat16> {
-  using vec_type = typename VecType<float>::type;
-  using vec_ops = VecOps<vec_type>;
-  static constexpr long num_vec = N / vec_ops::VLEN;
-  using type = typename std::array<typename VecType<float>::type, num_vec>;
-
-  static inline type load1d(tpp::bfloat16* p) {
-    type result;
-    compile_time_for<num_vec/2>::op(
-      [&](auto i) {
-        std::tie(result[i*2], result[i*2+1]) = _vec_load_bfloat16_as_two_floats(p + 32 * i);
-      }
-    );
-    return result;
-  }
-};
-
-#endif
\ No newline at end of file
diff --git a/csrc/cpu/tpp/woq/vla.h b/csrc/cpu/tpp/woq/vla.h
deleted file mode 100644
index f35d8c06..00000000
--- a/csrc/cpu/tpp/woq/vla.h
+++ /dev/null
@@ -1,151 +0,0 @@
-/******************************************************************************
- * Copyright (c) 2022 Intel Corporation - All rights reserved.                *
- *                                                                            *
- * For information on the license, see the LICENSE file.                      *
- * Further information: https://github.com/libxsmm/tpp-pytorch-extension/     *
- * SPDX-License-Identifier: BSD-3-Clause                                      *
- ******************************************************************************/
-/* Author: Dhiraj Kalamkar (Intel Corp.)
- ******************************************************************************/
-
-#ifndef _TPP_VLA_H_
-#define _TPP_VLA_H_
-
-#include <cstdint>
-
-template <typename T, typename index_t = int64_t>
-class VLAAccessorBase {
- public:
-  typedef T* PtrType;
-
-  VLAAccessorBase(PtrType data_, const index_t* strides_)
-      : data_(data_), strides_(strides_) {}
-
- protected:
-  PtrType data_;
-  const index_t* strides_;
-};
-
-template <typename T, std::size_t N, typename index_t = int64_t>
-class VLAAccessor : public VLAAccessorBase<T, index_t> {
- public:
-  typedef T* PtrType;
-
-  VLAAccessor(PtrType data_, const index_t* strides_)
-      : VLAAccessorBase<T, index_t>(data_, strides_) {}
-
-  VLAAccessor<T, N - 1, index_t> operator[](index_t i) {
-    return VLAAccessor<T, N - 1, index_t>(
-        this->data_ + this->strides_[0] * i, this->strides_ + 1);
-  }
-
-  const VLAAccessor<T, N - 1, index_t> operator[](index_t i) const {
-    return VLAAccessor<T, N - 1, index_t>(
-        this->data_ + this->strides_[0] * i, this->strides_ + 1);
-  }
-};
-
-template <typename T, typename index_t>
-class VLAAccessor<T, 1, index_t> : public VLAAccessorBase<T, index_t> {
- public:
-  typedef T* PtrType;
-
-  VLAAccessor(PtrType data_, const index_t* strides_)
-      : VLAAccessorBase<T, index_t>(data_, strides_) {}
-  T* operator[](index_t i) {
-    return this->data_ + i * this->strides_[0];
-  }
-  const T* operator[](index_t i) const {
-    return this->data_ + i * this->strides_[0];
-  }
-};
-
-template <typename T, typename index_t>
-class VLAAccessor<T, 0, index_t> : public VLAAccessorBase<T, index_t> {
- public:
-  typedef T* PtrType;
-
-  VLAAccessor(PtrType data_, const index_t* strides_)
-      : VLAAccessorBase<T, index_t>(data_, strides_) {}
-  T& operator[](index_t i) {
-    return this->data_[i];
-  }
-  const T& operator[](index_t i) const {
-    return this->data_[i];
-  }
-  operator T*() {
-    return this->data_;
-  }
-  operator const T*() const {
-    return this->data_;
-  }
-};
-
-template <typename T, std::size_t N, typename index_t = int64_t>
-class VLAPtr {
- public:
-  VLAPtr(T* data_, const index_t (&sizes)[N]) : data_(data_) {
-    strides[N - 1] = sizes[N - 1];
-    for (long i = N - 2; i >= 0; i--)
-      strides[i] = strides[i + 1] * sizes[i];
-  }
-  VLAAccessor<T, N - 1, index_t> operator[](index_t i) {
-    return VLAAccessor<T, N - 1, index_t>(data_ + i * strides[0], strides + 1);
-  }
-  operator bool() {
-    return data_ != nullptr;
-  }
-
- protected:
-  index_t strides[N];
-  T* data_;
-};
-
-template <typename T>
-class VLAPtr<T, 1, int64_t> {
- public:
-  typedef int64_t index_t;
-  VLAPtr(T* data_, const index_t (&sizes)[1]) : data_(data_) {
-    strides[0] = sizes[0];
-  }
-  T* operator[](index_t i) {
-    return data_ + i * strides[0];
-  }
-  operator bool() {
-    return data_ != nullptr;
-  }
-
- protected:
-  index_t strides[1];
-  T* data_;
-};
-
-typedef int64_t index_t;
-
-template <typename T, std::size_t N>
-VLAPtr<T, N, index_t> GetVLAPtr(T* data_, const index_t (&list)[N]) {
-  return VLAPtr<T, N, index_t>(data_, list);
-}
-
-template <typename T>
-inline T* pt_get_data_ptr(at::Tensor t) {
-  if (!t.is_contiguous()) {
-    std::cout << "Warning: Tensor t " << t.sizes() << " is not contiguous"
-              << std::endl;
-  }
-  return t.data_ptr<T>();
-}
-
-template <typename T, std::size_t N> //, typename index_t = int64_t>
-VLAPtr<T, N, index_t> GetVLAPtr(at::Tensor t, const index_t (&sizes)[N]) {
-  if (!t.defined()) {
-    return  VLAPtr<T, N, index_t>(nullptr, sizes);
-  }
-  return VLAPtr<T, N, index_t>(pt_get_data_ptr<T>(t), sizes);
-}
-template <typename T>
-T* GetVLAPtr(at::Tensor t) {
-  return pt_get_data_ptr<T>(t);
-}
-
-#endif // _TPP_VLA_H_
\ No newline at end of file
diff --git a/csrc/cpu/tpp/xsmm_functors.h b/csrc/cpu/tpp/xsmm_functors.h
index 846bfbb7..6a2bbc8e 100644
--- a/csrc/cpu/tpp/xsmm_functors.h
+++ b/csrc/cpu/tpp/xsmm_functors.h
@@ -7,11 +7,11 @@
 
 #include <libxsmm.h>
 #include <libxsmm_intrinsics_x86.h>
-// #ifdef TORCH_API_INCLUDE_EXTENSION_H
-// #include <torch/extension.h>
-// #else
-//  include <pytorch_extension_wrapper.h>
-// #endif
+//#ifdef TORCH_API_INCLUDE_EXTENSION_H
+//#include <torch/extension.h>
+//#else
+// include <pytorch_extension_wrapper.h>
+//#endif
 #include <string>
 #include <unordered_map>
 
@@ -164,37 +164,6 @@ inline void debug_print_eqn_tree(libxsmm_blasint eqn_no) {
   }
 }
 
-inline int xsmm_get_vnni_block_size(libxsmm_datatype dtype) {
-  int bs = libxsmm_cpuid_dot_pack_factor(dtype);
-  if (bs <= 0) {
-    throw std::invalid_argument("Unsupported datatype");
-  }
-  return bs;
-}
-inline libxsmm_datatype convert_dtype_pt2xsmm(at::ScalarType dtype) {
-  static const std::map<at::ScalarType, libxsmm_datatype> pt2xsmmDtypes = {
-      {at::kDouble, LIBXSMM_DATATYPE_F64},
-      {at::kFloat, LIBXSMM_DATATYPE_F32},
-      {at::kHalf, LIBXSMM_DATATYPE_F16},
-      {at::kBFloat16, LIBXSMM_DATATYPE_BF16},
-      {at::kByte, LIBXSMM_DATATYPE_I8},
-      {at::kChar, LIBXSMM_DATATYPE_I8},
-      {at::kShort, LIBXSMM_DATATYPE_I16},
-      {at::kInt, LIBXSMM_DATATYPE_I32},
-      {at::kLong, LIBXSMM_DATATYPE_I64}};
-
-  return pt2xsmmDtypes.at(dtype);
-}
-inline int get_vnni_block_size(at::ScalarType dtype) {
-  auto xsmm_dtype = convert_dtype_pt2xsmm(dtype);
-  return xsmm_get_vnni_block_size(xsmm_dtype);
-}
-
-template <typename T>
-inline int get_vnni_block_size() {
-  auto xsmm_dtype = XsmmDtype<T>();
-  return xsmm_get_vnni_block_size(xsmm_dtype);
-}
 inline int meqn_push_arg(
     const libxsmm_blasint idx,
     const libxsmm_blasint m,
@@ -934,46 +903,6 @@ class ReduceAddRowTPP {
   AddTPP<Tout, Tout> add;
 };
 
-template <typename Tin, typename Tout = Tin>
-class MulTPP {
- public:
-  MulTPP() {}
-  MulTPP(int N) : MulTPP(1, N) {}
-  MulTPP(int rows, int cols) : MulTPP(rows, cols, cols, cols) {}
-  MulTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
-        kernel(
-            rows,
-            cols,
-            ldi,
-            ldo,
-            XsmmDtype<Tin>(),
-            XsmmDtype<Tout>(),
-            LIBXSMM_DATATYPE_F32,
-            LIBXSMM_MELTW_FLAG_BINARY_NONE,
-            LIBXSMM_MELTW_TYPE_BINARY_MUL) {}
-  void operator()(Tin* in0, Tin* in1, Tout* out) {
-    kernel((void*)in0, (void*)in1, (void*)out);
-  }
-  void ref(Tin* in0, Tin* in1, Tout* out) {
-    for (int r = 0; r < rows; r++) {
-      for (int c = 0; c < cols; c++) {
-        out[r * ldo + c] = (float)in0[r * ldi + c] * (float)in1[r * ldi + c];
-      }
-    }
-  }
-
- private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
-  BinaryTPP kernel;
-};
-
 template <typename T>
 class BCastMulTPP {
  public:
@@ -1187,43 +1116,31 @@ template <typename Tin, typename Tout>
 class ScaleAddTPP {
  public:
   ScaleAddTPP() {}
-  ScaleAddTPP(int N) : ScaleAddTPP(1, N) {}
-  ScaleAddTPP(int rows, int cols) : ScaleAddTPP(rows, cols, cols, cols) {}
-  ScaleAddTPP(int rows, int cols, int ldi, int ldo)
-      : rows(rows),
-        cols(cols),
-        ldi(ldi),
-        ldo(ldo),
+  ScaleAddTPP(int N)
+      : N(N),
         kernel(
-            rows,
-            cols,
             1,
-            ldi,
-            ldo,
-            XsmmDtype<float>(),
+            N,
+            N,
+            N,
             XsmmDtype<Tin>(),
             XsmmDtype<Tout>(),
             LIBXSMM_DATATYPE_F32,
             LIBXSMM_MELTW_FLAG_BINARY_BCAST_SCALAR_IN_0,
             LIBXSMM_MELTW_TYPE_BINARY_MULADD) {}
   void operator()(Tin* in, Tout* out, float scale) {
-    float alpha = scale;
+    Tin alpha = scale;
     kernel((void*)&alpha, (void*)in, (void*)out);
   }
   void ref(Tin* in, Tout* out, float scale) {
-    float alpha = scale;
-    for (int i = 0; i < rows; i++) {
-      for (int j = 0; j < cols; j++) {
-        out[i * ldo + j] += (float)in[i * ldi + j] * (float)alpha;
-      }
+    Tin alpha = scale;
+    for (int i = 0; i < N; i++) {
+      out[i] += (float)in[i] * (float)alpha;
     }
   }
 
  private:
-  int rows = 0;
-  int cols = 0;
-  int ldi;
-  int ldo;
+  int N = 0;
   BinaryTPP kernel;
 };
 
@@ -1828,18 +1745,13 @@ template <typename Tin, typename Tout = Tin>
 class GeluFwdTPP {
  public:
   GeluFwdTPP() {}
-  GeluFwdTPP(int N) : GeluFwdTPP(1, N) {}
-  GeluFwdTPP(int M, int N) : GeluFwdTPP(M, N, N, N) {}
-  GeluFwdTPP(int M, int N, int ldi, int ldo)
-      : M(M),
-        N(N),
-        ldi(ldi),
-        ldo(ldo),
+  GeluFwdTPP(int N)
+      : N(N),
         kernel(
-            M,
+            1,
+            N,
+            N,
             N,
-            ldi,
-            ldo,
             XsmmDtype<Tin>(),
             XsmmDtype<Tout>(),
             LIBXSMM_DATATYPE_F32,
@@ -1850,38 +1762,31 @@ class GeluFwdTPP {
   }
   void ref(Tin* in, Tout* out) {
 #ifdef __AVX512F__
-    for (int j = 0; j < M; j++) {
-      int i;
-      for (i = 0; i < ALIGNDOWN(N, 16); i += 16) {
-        auto vin = _mm512_loadu_ps_auto(&in[j * ldi + i]);
-        // auto vout = LIBXSMM_INTRINSICS_MM512_TANH_PS_GELU_FWD(vin);
-        auto vout = LIBXSMM_INTRINSICS_MM512_GELU_FWD_PS_MINIMAX3(vin);
-        _mm512_storeu_ps_auto(&out[j * ldo + i], vout);
-      }
-      if (i < N) {
-        int rem = N - i;
-        __mmask16 mask = (1 << rem) - 1;
-        auto vin = _mm512_maskz_loadu_ps_auto(mask, &in[j * ldi + i]);
-        // auto vout = LIBXSMM_INTRINSICS_MM512_TANH_PS_GELU_FWD(vin);
-        auto vout = LIBXSMM_INTRINSICS_MM512_GELU_FWD_PS_MINIMAX3(vin);
-        _mm512_mask_storeu_ps_auto(&out[j * ldo + i], mask, vout);
-      }
+    int i;
+    for (i = 0; i < ALIGNDOWN(N, 16); i += 16) {
+      auto vin = _mm512_loadu_ps_auto(&in[i]);
+      // auto vout = LIBXSMM_INTRINSICS_MM512_TANH_PS_GELU_FWD(vin);
+      auto vout = LIBXSMM_INTRINSICS_MM512_GELU_FWD_PS_MINIMAX3(vin);
+      _mm512_storeu_ps_auto(&out[i], vout);
+    }
+    if (i < N) {
+      int rem = N - i;
+      __mmask16 mask = (1 << rem) - 1;
+      auto vin = _mm512_maskz_loadu_ps_auto(mask, &in[i]);
+      // auto vout = LIBXSMM_INTRINSICS_MM512_TANH_PS_GELU_FWD(vin);
+      auto vout = LIBXSMM_INTRINSICS_MM512_GELU_FWD_PS_MINIMAX3(vin);
+      _mm512_mask_storeu_ps_auto(&out[i], mask, vout);
     }
 #else
-    for (int j = 0; j < M; j++) {
-      for (int i = 0; i < N; i++) {
-        float x = in[j * ldi + i];
-        out[j * ldo + i] = (erff(x / sqrtf(2.0)) + 1.0) * 0.5 * x;
-      }
+    for (int i = 0; i < N; i++) {
+      float x = in[i];
+      out[i] = (erff(x / sqrtf(2.0)) + 1.0) * 0.5 * x;
     }
 #endif
   }
 
  private:
-  int M = 0;
   int N = 0;
-  int ldi = 0;
-  int ldo = 0;
   UnaryTPP kernel;
 };
 
@@ -2251,28 +2156,20 @@ class SiLUFwdTPP {
             cols,
             ldi,
             ldo,
-            ldo,
-            XsmmDtype<T>(),
             XsmmDtype<T>(),
             XsmmDtype<T>(),
             LIBXSMM_DATATYPE_F32,
             LIBXSMM_MELTW_FLAG_BINARY_NONE,
             LIBXSMM_MELTW_TYPE_BINARY_MUL) {}
-  void operator()(T* in, T* out, T* sigout = nullptr) {
-    T tmp[rows * ldo];
-    if (sigout == nullptr)
-      sigout = tmp;
+  void operator()(T* in, T* out, T* sigout) {
     sigmoid((void*)in, (void*)sigout);
     mul((void*)in, (void*)sigout, (void*)out);
   }
-  void ref(T* in, T* out, T* sigout = nullptr) {
-    T tmp[rows * ldo];
-    if (sigout == nullptr)
-      sigout = tmp;
+  void ref(T* in, T* out, T* sigout) {
     for (int i = 0; i < rows; i++) {
       for (int j = 0; j < cols; j++) {
         sigout[i * ldo + j] = 1. / (1. + exp(-in[i * ldi + j]));
-        out[i * ldo + j] = in[i * ldi + j] * sigout[i * ldo + j];
+        out[i * ldo + j] = in[i * ldo + j] * sigout[i * ldo + j];
       }
     }
   }
@@ -3043,7 +2940,7 @@ class VarSoftMaxFwdTPP {
       }
     }
 #else
-    // #warning "Not using AVX512 path for VarSoftMax"
+    //#warning "Not using AVX512 path for VarSoftMax"
     for (s2 = 0; s2 < S2; s2++) {
       float tmp[S1][S3];
       float max =
@@ -4081,8 +3978,8 @@ class SplitSGDTPP : public BaseTPP {
     auto out_hi = (libxsmm_bfloat16*)hi;
     auto out_lo = (libxsmm_bfloat16*)lo;
     for (int i = 0; i < N; i++) {
-      union libxsmm_bfloat16_f32 bf16_hp;
-      union libxsmm_bfloat16_f32 bf16_wt;
+      union libxsmm_bfloat16_hp bf16_hp;
+      union libxsmm_bfloat16_hp bf16_wt;
       bf16_wt.i[0] = 0;
       bf16_wt.i[1] = dwt[i];
       bf16_hp.i[0] = out_lo[i];
@@ -4160,9 +4057,7 @@ class EmbBagFwdTPP {
             XsmmDtype<Tin>(),
             XsmmDtype<Tout>(),
             LIBXSMM_DATATYPE_F32,
-            (libxsmm_meltw_unary_flags)(sizeof(Tind) == 8
-                                            ? LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_8BYTES
-                                            : LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_4BYTES),
+            (libxsmm_meltw_unary_flags)(sizeof(Tind) == 8 ? LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_8BYTES : LIBXSMM_MELTW_FLAG_UNARY_IDX_SIZE_4BYTES),
             LIBXSMM_MELTW_TYPE_UNARY_REDUCE_COLS_IDX_OP_ADD) {}
   void operator()(Tout* output, Tin* weight, Tind* input, int N) {
     unsigned long long _N = N;
@@ -4534,7 +4429,7 @@ class FusedSplitAdamWTPP {
     float beta2_1 = 1.0f - beta2;
 #ifndef __AVX512F__
     for (long i = 0; i < sz; i++) {
-      union libxsmm_bfloat16_f32 data_hp;
+      union libxsmm_bfloat16_hp data_hp;
       float avg_i = exp_avg[i];
       float avg_sq_i = exp_avg_sq[i];
       float grad_i = grad[i];
diff --git a/csrc/cpu/vec/vec256/vec256_int8.h b/csrc/cpu/vec/vec256/vec256_int8.h
index 560e7d71..22f94f19 100644
--- a/csrc/cpu/vec/vec256/vec256_int8.h
+++ b/csrc/cpu/vec/vec256/vec256_int8.h
@@ -60,6 +60,25 @@ static inline __attribute__((always_inline)) int8_t _dot_s8s8_scale_s32s8(
   return (int8_t)c;
 }
 
+static inline __attribute__((always_inline)) void scale_fp32_and_fma(
+    float* out,
+    const int8_t* in,
+    float scale,
+    int64_t len) {
+  int64_t i;
+  __m256 scale_vec256 = _mm256_set1_ps(scale);
+  for (i = 0; i < len - 7; i += 8) {
+    auto i8 = _mm_loadl_epi64(reinterpret_cast<const __m128i*>(in + i));
+    auto i32 = _mm256_cvtepi8_epi32(i8);
+    auto f32 = _mm256_cvtepi32_ps(i32);
+    auto fma_out = _mm256_fmadd_ps(scale_vec256, f32, _mm256_loadu_ps(out + i));
+    _mm256_storeu_ps(out + i, fma_out);
+  }
+  for (; i < len; i++) {
+    out[i] = std::fma(scale, float(in[i]), out[i]);
+  }
+}
+
 } // namespace kernel
 } // namespace cpu
 } // namespace torch_ipex
diff --git a/csrc/cpu/vec/vec512/perf_kernel/add_softmax.h b/csrc/cpu/vec/vec512/perf_kernel/add_softmax.h
index dcc064f9..656015bb 100644
--- a/csrc/cpu/vec/vec512/perf_kernel/add_softmax.h
+++ b/csrc/cpu/vec/vec512/perf_kernel/add_softmax.h
@@ -221,31 +221,6 @@ inline void _dil_normalization_kernel(
   }
 }
 
-template <typename scalar_t>
-inline void _dil_add_kernel(
-    const scalar_t* src,
-    float* dst,
-    const int& size) {
-  __m512 vec_a = {};
-  __m512 vec_out = {};
-
-  int j = 0;
-  for (; j <= size - 16; j += 16) {
-    vec_a = _loadu(src + j);
-    vec_out = _loadu(dst + j);
-    vec_out = _mm512_add_ps(vec_a, vec_out);
-    _storeu(dst + j, vec_out);
-  }
-
-  if (j < size) {
-    __mmask16 mask = (1 << (size - j)) - 1;
-    vec_a = _maskz_loadu(src + j, mask);
-    vec_out = _maskz_loadu(dst + j, mask);
-    vec_out = _mm512_add_ps(vec_out, vec_a);
-    _mask_storeu(dst + j, vec_out, mask);
-  }
-}
-
 inline void _dil_add_reduce_max_fusion_kernel(
     float* a,
     const float* b,
@@ -279,32 +254,6 @@ inline void _dil_add_reduce_max_fusion_kernel(
   max = _mm512_reduce_max_ps(vec_ps_min);
 }
 
-inline void _dil_reduce_max_fusion_kernel(
-    const float* a,
-    const int& size,
-    float* out,
-    float& max) {
-  auto vec_ps_min = _mm512_set1_ps(std::numeric_limits<float>::lowest());
-  auto vec_out = vec_ps_min;
-
-  int i = 0;
-  for (; i <= size - 16; i += 16) {
-    vec_out = _loadu(a + i);
-    vec_ps_min = _mm512_max_ps(vec_ps_min, vec_out);
-    _mm512_storeu_ps(out + i, vec_out);
-  }
-
-  if (i < size) {
-    __mmask16 mask = (1 << (size - i)) - 1;
-    vec_out = _maskz_loadu(a + i, mask);
-    vec_ps_min = _mm512_mask_max_ps(vec_ps_min, mask, vec_out, vec_ps_min);
-    _mm512_mask_storeu_ps(out + i, mask, vec_out);
-  }
-
-  // NOTE: _mm512_reduce_max_ps is sequence instruction
-  max = _mm512_reduce_max_ps(vec_ps_min);
-}
-
 inline void _dil_mul_reduce_max_fusion_kernel(
     const float* a,
     const float& scale,
diff --git a/csrc/cpu/vec/vec512/vec512_bfloat16.h b/csrc/cpu/vec/vec512/vec512_bfloat16.h
index c8092342..eeea99cf 100644
--- a/csrc/cpu/vec/vec512/vec512_bfloat16.h
+++ b/csrc/cpu/vec/vec512/vec512_bfloat16.h
@@ -312,40 +312,6 @@ inline __attribute__((always_inline)) void add_ker(
   }
 }
 
-template <>
-inline __attribute__((always_inline)) void add_ker(
-    at::BFloat16* inout,
-    const float* in,
-    int64_t len) {
-  int64_t i = 0;
-#pragma unroll(2)
-  for (i = 0; i < len - 31; i += 32) {
-    auto in1 = _mm512_loadu_ps(in + i);
-    auto in2 = _mm512_loadu_ps(in + i + 16);
-    auto inout1 = cvt_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(inout + i)));
-    auto inout2 = cvt_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(inout + i + 16)));
-    inout1 = _mm512_add_ps(inout1, in1);
-    inout2 = _mm512_add_ps(inout2, in2);
-    _mm256_storeu_si256((__m256i*)(inout + i), cvt_fp32_to_bf16(inout1));
-    _mm256_storeu_si256((__m256i*)(inout + i + 16), cvt_fp32_to_bf16(inout2));
-  }
-
-  if (i < len - 15) {
-    auto in1 = _mm512_loadu_ps(in + i);
-    auto inout1 = cvt_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(inout + i)));
-    inout1 = _mm512_add_ps(inout1, in1);
-    _mm256_storeu_si256((__m256i*)(inout + i), cvt_fp32_to_bf16(inout1));
-    i += 16;
-  }
-
-  if (i < len) {
-    auto mask = (1 << (len - i)) - 1;
-    auto in1 = _mm512_maskz_loadu_ps(mask, in + i);
-    auto inout1 = cvt_bf16_to_fp32(_mm256_maskz_loadu_epi16(mask, inout + i));
-    inout1 = _mm512_add_ps(inout1, in1);
-    _mm256_mask_storeu_epi16(inout + i, mask, cvt_fp32_to_bf16(inout1));
-  }
-}
 template <>
 inline __attribute__((always_inline)) void move_ker(
     at::BFloat16* out,
diff --git a/csrc/cpu/vec/vec512/vec512_int8.h b/csrc/cpu/vec/vec512/vec512_int8.h
index 75bce29f..d2c0d252 100644
--- a/csrc/cpu/vec/vec512/vec512_int8.h
+++ b/csrc/cpu/vec/vec512/vec512_int8.h
@@ -32,7 +32,7 @@ inline __attribute__((always_inline)) void zero_ker(int8_t* out, int64_t len) {
   }
 
   if (i < len) {
-    auto mask = ((1 << (len - i)) - 1);
+    __mmask64 mask = ((size_t(1) << (len - i)) - 1);
     _mm512_mask_storeu_epi8(out + i, mask, zero_512);
   }
 }
@@ -88,7 +88,7 @@ inline __attribute__((always_inline)) void move_ker(
   }
 
   if (i < len) {
-    auto mask = ((1 << (len - i)) - 1);
+    __mmask64 mask = ((size_t(1) << (len - i)) - 1);
     auto in0 = _mm512_maskz_loadu_epi8(mask, in + i);
     _mm512_mask_storeu_epi8(out + i, mask, in0);
   }
@@ -107,7 +107,7 @@ inline __attribute__((always_inline)) void move_ker(
   }
 
   if (i < len) {
-    auto mask = ((1 << (len - i)) - 1);
+    __mmask64 mask = ((size_t(1) << (len - i)) - 1);
     auto in0 = _mm512_maskz_loadu_epi8(mask, in + i);
     _mm512_mask_storeu_epi8(out + i, mask, in0);
   }
@@ -126,7 +126,7 @@ inline __attribute__((always_inline)) void move_ker(
   }
 
   if (i < len) {
-    auto mask = ((1 << (len - i)) - 1);
+    __mmask64 mask = (size_t(1) << (len - i)) - 1;
     auto in0 = _mm512_maskz_loadu_epi8(mask, in + i);
     _mm512_mask_storeu_epi8(out + i, mask, in0);
   }
@@ -899,6 +899,25 @@ static inline void scale_int32_and_store_int8_maskz_16(
   _mm_mask_storeu_epi8((void*)out, mask, out_i8);
 }
 
+static inline __attribute__((always_inline)) void scale_fp32_and_fma(
+    float* out,
+    const int8_t* in,
+    float scale,
+    int64_t len) {
+  int64_t i;
+  __m512 scale_vec512 = _mm512_set1_ps(scale);
+  for (i = 0; i < len - 15; i += 16) {
+    auto i8 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(in + i));
+    auto i32 = _mm512_cvtepi8_epi32(i8);
+    auto f32 = _mm512_cvtepi32_ps(i32);
+    auto fma_out = _mm512_fmadd_ps(scale_vec512, f32, _mm512_loadu_ps(out + i));
+    _mm512_storeu_ps(out + i, fma_out);
+  }
+  for (; i < len; i++) {
+    out[i] = std::fma(scale, float(in[i]), out[i]);
+  }
+}
+
 } // namespace kernel
 } // namespace cpu
 } // namespace torch_ipex
diff --git a/docker/Dockerfile.compile b/docker/Dockerfile.compile
new file mode 100644
index 00000000..beb173fe
--- /dev/null
+++ b/docker/Dockerfile.compile
@@ -0,0 +1,57 @@
+# NOTE: To build this you will need a docker version >= 19.03 and DOCKER_BUILDKIT=1
+#
+#       If you do not use buildkit you are not going to have a good time
+#
+#       For reference:
+#           https://docs.docker.com/develop/develop-images/build_enhancements/
+
+ARG BASE_IMAGE=ubuntu:20.04
+FROM ${BASE_IMAGE} AS dev-base
+RUN --mount=type=cache,id=apt-dev,target=/var/cache/apt \
+    apt-get update && \
+    DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y \
+    ca-certificates \
+    git \
+    curl \
+    vim \
+    build-essential \
+    ccache \
+    libgoogle-perftools-dev \
+    numactl \
+    cmake \
+    libjpeg-dev \
+    pybind11-dev \
+    libpng-dev \
+    pybind11-dev \
+    && rm -rf /var/lib/apt/lists/*
+RUN /usr/sbin/update-ccache-symlinks
+RUN mkdir /opt/ccache && ccache --set-config=cache_dir=/opt/ccache
+ENV PATH /opt/conda/bin:$PATH
+
+FROM dev-base as conda
+ARG PYTHON_VERSION=3.8
+RUN curl -fsSL -v -o ~/miniconda.sh -O  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh  && \
+    chmod +x ~/miniconda.sh && \
+    ~/miniconda.sh -b -p /opt/conda && \
+    rm ~/miniconda.sh && \
+    /opt/conda/bin/conda install -y python=${PYTHON_VERSION} conda-build pyyaml numpy ipython mkl mkl-include ninja cython typing pybind11 Pillow && \
+    /opt/conda/bin/conda clean -ya
+
+FROM dev-base AS build
+COPY --from=conda /opt/conda /opt/conda
+RUN --mount=type=cache,target=/opt/ccache \
+    curl -fsSL -v -o compile_bundle.sh -O https://raw.githubusercontent.com/intel/intel-extension-for-pytorch/master/scripts/compile_bundle.sh && \
+    bash compile_bundle.sh && \
+    python -m pip install --no-cache-dir intel-extension-for-pytorch/dist/*.whl && \
+    rm -rf intel-extension-for-pytorch llvm-project compile_bundle.sh
+
+FROM dev-base as dev
+COPY --from=build /opt/conda /opt/conda
+ARG OMP_NUM_THREADS=1
+ENV OMP_NUM_THREADS ${OMP_NUM_THREADS}
+ARG KMP_BLOCKTIME=1
+ENV KMP_BLOCKTIME ${KMP_BLOCKTIME}
+ARG KMP_HW_SUBSET=1T
+ENV KMP_HW_SUBSET ${KMP_HW_SUBSET}
+ENV LD_PRELOAD "/opt/conda/lib/libiomp5.so /usr/lib/x86_64-linux-gnu/libtcmalloc.so"
+ENV LD_LIBRARY_PATH "/opt/conda/lib/python3.8/site-packages/lib/"
diff --git a/docker/Dockerfile.prebuilt b/docker/Dockerfile.prebuilt
new file mode 100644
index 00000000..96cc7a84
--- /dev/null
+++ b/docker/Dockerfile.prebuilt
@@ -0,0 +1,40 @@
+# NOTE: To build this you will need a docker version >= 19.03 and DOCKER_BUILDKIT=1
+#
+#       If you do not use buildkit you are not going to have a good time
+#
+#       For reference:
+#           https://docs.docker.com/develop/develop-images/build_enhancements/
+
+ARG UBUNTU_VERSION=20.04
+
+FROM ubuntu:${UBUNTU_VERSION} as base
+
+# See http://bugs.python.org/issue19846
+ENV LANG C.UTF-8
+ARG PYTHON=python3
+
+RUN apt-get update -y && \
+    apt-get upgrade -y && \
+    apt-get install -y --no-install-recommends --fix-missing \
+    ${PYTHON} \
+    ${PYTHON}-pip
+
+RUN ${PYTHON} -m pip --no-cache-dir install --upgrade \
+    pip \
+    setuptools \
+    psutil
+
+# Some TF tools expect a "python" binary
+RUN ln -s $(which ${PYTHON}) /usr/local/bin/python
+
+ARG IPEX_VERSION=2.0.100
+ARG PYTORCH_VERSION=2.0.1
+ARG TORCHAUDIO_VERSION=2.0.2
+ARG TORCHVISION_VERSION=0.15.2
+ARG TORCH_CPU_URL=https://download.pytorch.org/whl/cpu/torch_stable.html
+
+RUN \
+    python -m pip install --no-cache-dir \
+    torch==${PYTORCH_VERSION}+cpu torchvision==${TORCHVISION_VERSION}+cpu torchaudio==${TORCHAUDIO_VERSION}+cpu -f ${TORCH_CPU_URL} && \
+    python -m pip install --no-cache-dir \
+    intel_extension_for_pytorch==${IPEX_VERSION}
diff --git a/docker/README.md b/docker/README.md
new file mode 100644
index 00000000..8e8e16c9
--- /dev/null
+++ b/docker/README.md
@@ -0,0 +1,23 @@
+# Use docker
+
+* Notes
+
+  If you use linux kernerl under version 5.4 in host, upgrade it.
+
+* How to build an image
+
+  Run the following commands to build a `pip` based container with the latest stable version prebuilt wheel files:
+
+  ```console
+  $ cd $DOCKERFILE_DIR
+  $ DOCKER_BUILDKIT=1 docker build -f Dockerfile.prebuilt -t intel-extension-for-pytorch:prebuilt .
+  $ docker run --rm intel-extension-for-pytorch:prebuilt python -c "import torch; import intel_extension_for_pytorch as ipex; print('torch:', torch.__version__,' ipex:',ipex.__version__)"
+  ```
+
+  Run the following commands to build a `conda` based container with Intel® Extension for PyTorch\* compiled from source:
+
+  ```console
+  $ cd $DOCKERFILE_DIR
+  $ DOCKER_BUILDKIT=1 docker build -f Dockerfile.compile -t intel-extension-for-pytorch:compile .
+  $ docker run --rm intel-extension-for-pytorch:compile python -c "import torch; import intel_extension_for_pytorch as ipex; print('torch:', torch.__version__,' ipex:',ipex.__version__)"
+  ```
diff --git a/docs/design_doc/cpu/isa_dyndisp.md b/docs/design_doc/cpu/isa_dyndisp.md
new file mode 100644
index 00000000..74e6cc60
--- /dev/null
+++ b/docs/design_doc/cpu/isa_dyndisp.md
@@ -0,0 +1,476 @@
+# Intel® Extension for PyTorch\* CPU ISA Dynamic Dispatch Design Doc
+
+This document explains the dynamic kernel dispatch mechanism for Intel® Extension for PyTorch\* (IPEX) based on CPU ISA. It is an extension to the similar mechanism in PyTorch.
+
+## Overview
+
+IPEX dyndisp is forked from **PyTorch:** `ATen/native/DispatchStub.h` and `ATen/native/DispatchStub.cpp`. IPEX adds additional CPU ISA level support, such as `AVX512_VNNI`, `AVX512_BF16` and `AMX`.
+
+PyTorch & IPEX CPU ISA support statement:
+ | | DEFAULT | AVX2 | AVX2_VNNI | AVX512 | AVX512_VNNI | AVX512_BF16 | AMX |
+ | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
+ | PyTorch | ✔ | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ |
+ | IPEX-1.11 | ✘ | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ |
+ | IPEX-1.12 | ✘ | ✔ | ✘ | ✔ | ✔ | ✔ | ✔ |
+ | IPEX-1.13 | ✘ | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ |
+
+\* Current IPEX DEFAULT level implemented as same as AVX2 level.
+
+### CPU ISA build compiler requirement
+ | ISA Level | GCC requirement |
+ | ---- | ---- |
+ | AVX2 | Any |
+ | AVX512 | GCC 9.2+ |
+ | AVX512_VNNI | GCC 9.2+ |
+ | AVX512_BF16 | GCC 10.3+ |
+ | AVX2_VNNI | GCC 11.2+ |
+ | AMX | GCC 11.2+ |
+
+\* Check with `cmake/Modules/FindAVX.cmake` for detailed compiler checks.
+
+## Dynamic Dispatch Design
+
+Dynamic dispatch copies the kernel implementation source files to multiple folders for each ISA level. It then builds each file using its ISA specific parameters. Each generated object file will contain its function body (**Kernel Implementation**).
+
+Kernel Implementation uses an anonymous namespace so that different CPU versions won't conflict.
+
+**Kernel Stub** is a "virtual function" with polymorphic kernel implementations pertaining to ISA levels.
+
+At the runtime, **Dispatch Stub implementation** will check CPUIDs and OS status to determins which ISA level pointer best matches the function body.
+
+### Code Folder Struct
+>#### **Kernel implementation:** `csrc/cpu/aten/kernels/xyzKrnl.cpp`
+>#### **Kernel Stub:** `csrc/cpu/aten/xyz.cpp` and `csrc/cpu/aten/xyz.h`
+>#### **Dispatch Stub implementation:** `csrc/cpu/dyndisp/DispatchStub.cpp` and `csrc/cpu/dyndisp/DispatchStub.h`
+
+### CodeGen Process
+IPEX build system will generate code for each ISA level with specifiy complier parameters. The CodeGen script is located at `cmake/cpu/IsaCodegen.cmake`.
+
+The CodeGen will copy each cpp files from **Kernel implementation**, and then add ISA level as new file suffix.
+
+> **Sample:**
+>
+> ----
+>
+> **Origin file:**
+>
+> `csrc/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp`
+>
+> **Generate files:**
+>
+> DEFAULT: `build/Release/csrc/isa_codegen/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp.DEFAULT.cpp -O3 -D__AVX__ -DCPU_CAPABILITY_AVX2 -mavx2 -mfma -mno-avx256-split-unaligned-load -mno-avx256-split-unaligned-store -DCPU_CAPABILITY=DEFAULT -DCPU_CAPABILITY_DEFAULT`
+>
+> AVX2: `build/Release/csrc/isa_codegen/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp.AVX2.cpp -O3 -D__AVX__ -mavx2 -mfma -mno-avx256-split-unaligned-load -mno-avx256-split-unaligned-store -DCPU_CAPABILITY=AVX2 -DCPU_CAPABILITY_AVX2`
+>
+> AVX512: `build/Release/csrc/isa_codegen/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp.AVX512.cpp -O3 -D__AVX512F__ -mavx512f -mavx512bw -mavx512vl -mavx512dq -mfma -DCPU_CAPABILITY=AVX512 -DCPU_CAPABILITY_AVX512`
+>
+> AVX512_VNNI: `build/Release/csrc/isa_codegen/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp.AVX512_VNNI.cpp -O3 -D__AVX512F__ -DCPU_CAPABILITY_AVX512 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vnni -mfma -DCPU_CAPABILITY=AVX512_VNNI -DCPU_CAPABILITY_AVX512_VNNI`
+>
+> AVX512_BF16: `build/Release/csrc/isa_codegen/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp.AVX512_BF16.cpp -O3 -D__AVX512F__ -DCPU_CAPABILITY_AVX512 -DCPU_CAPABILITY_AVX512_VNNI -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vnni -mavx512bf16 -mfma -DCPU_CAPABILITY=AVX512_BF16 -DCPU_CAPABILITY_AVX512_BF16`
+>
+> AMX: `build/Release/csrc/isa_codegen/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp.AMX.cpp -O3  -D__AVX512F__ -DCPU_CAPABILITY_AVX512 -DCPU_CAPABILITY_AVX512_VNNI -DCPU_CAPABILITY_AVX512_BF16 -mavx512f -mavx512bw -mavx512vl -mavx512dq -mavx512vnni -mavx512bf16 -mfma -mamx-tile -mamx-int8 -mamx-bf16 -DCPU_CAPABILITY=AMX -DCPU_CAPABILITY_AMX`
+
+---
+
+>**Note:**
+>1. DEFAULT level kernels is not fully implemented in IPEX. In order to align to PyTorch, we build default use AVX2 parameters in stead of that. So, IPEX minimal required executing machine support AVX2.
+>2. `-D__AVX__` and `-D__AVX512F__` is defined for depends library [sleef](https://sleef.org/) .
+>3. `-DCPU_CAPABILITY_AVX512` and `-DCPU_CAPABILITY_AVX2` are must to be defined for **PyTorch:** `aten/src/ATen/cpu/vec`, it determins vec register width.
+>4. `-DCPU_CAPABILITY=[ISA_NAME]` is must to be defined for **PyTorch:** `aten/src/ATen/cpu/vec`, it is used as inline namespace name.
+>5. Higher ISA level is compatible to lower ISA levels, so it needs to contains level ISA feature definitions. Such as AVX512_BF16 need contains `-DCPU_CAPABILITY_AVX512` `-DCPU_CAPABILITY_AVX512_VNNI`. But AVX512 don't contains AVX2 definitions, due to there are different vec register width.
+
+## Add Custom Kernel
+
+If you want to add a new custom kernel, and the kernel uses CPU ISA instructions, refer to these tips:
+
+1. Add CPU ISA related kernel implementation to the folder:  `csrc/cpu/aten/kernels/NewKernelKrnl.cpp`
+2. Add kernel stub to the folder: `csrc/cpu/aten/NewKernel.cpp`
+3. Include header file: `csrc/cpu/dyndisp/DispatchStub.h`, and reference to the comment in the header file.
+```c++
+// Implements instruction set specific function dispatch.
+//
+// Kernels that may make use of specialized instruction sets (e.g. AVX2) are
+// compiled multiple times with different compiler flags (e.g. -mavx2). A
+// DispatchStub contains a table of function pointers for a kernel. At runtime,
+// the fastest available kernel is chosen based on the features reported by
+// cpuinfo.
+//
+// Example:
+//
+// In csrc/cpu/aten/MyKernel.h:
+//   using fn_type = void(*)(const Tensor& x);
+//   DECLARE_DISPATCH(fn_type, stub);
+//
+// In csrc/cpu/aten/MyKernel.cpp
+//   DEFINE_DISPATCH(stub);
+//
+// In csrc/cpu/aten/kernels/MyKernel.cpp:
+//   namespace {
+//     // use anonymous namespace so that different cpu versions won't conflict
+//     void kernel(const Tensor& x) { ... }
+//   }
+//   REGISTER_DISPATCH(stub, &kernel);
+//
+// To call:
+//   stub(kCPU, tensor);
+```
+4. Write the kernel follow the guide. It contains: declare function type, register stub, call stub, etc.
+
+>**Note:**
+>
+>1. Some kernels only call **oneDNN** or **iDeep** implementation, or other backend implementation, which is not needed to add kernel implementations. (Refer: `BatchNorm.cpp`)
+>2. Vec related header file must be included in kernel implementation files, but can not be included in kernel stub. Kernel stub is common code for all ISA level, and can't pass ISA related compiler parameters.
+>3. For more intrinsics, check the [Intel® Intrinsics Guide](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html).
+
+### ISA intrinics specific kernel example:
+
+This is a FP32 convert to BF16 function example, and it is implemented for `AVX512_BF16`, `AVX512` and `DEFAULT` ISA levels.
+
+```c++
+//csrc/cpu/aten/CvtFp32ToBf16.h
+
+#pragma once
+
+#include <dyndisp/DispatchStub.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+void cvt_fp32_to_bf16(at::BFloat16* dst, const float* src, int len);
+
+namespace {
+
+void cvt_fp32_to_bf16_kernel_impl(at::BFloat16* dst, const float* src, int len);
+
+}
+
+using cvt_fp32_to_bf16_kernel_fn = void (*)(at::BFloat16*, const float*, int);
+DECLARE_DISPATCH(cvt_fp32_to_bf16_kernel_fn, cvt_fp32_to_bf16_kernel_stub);
+} // namespace cpu
+} // namespace torch_ipex
+
+```
+```c++
+//csrc/cpu/aten/CvtFp32ToBf16.cpp
+
+#include "CvtFp32ToBf16.h"
+
+namespace torch_ipex {
+namespace cpu {
+
+DEFINE_DISPATCH(cvt_fp32_to_bf16_kernel_stub);
+
+void cvt_fp32_to_bf16(at::BFloat16* dst, const float* src, int len) {
+  return cvt_fp32_to_bf16_kernel_stub(kCPU, dst, src, len);
+}
+
+} // namespace cpu
+} // namespace torch_ipex
+
+```
+Macro `CPU_CAPABILITY_AVX512` and `CPU_CAPABILITY_AVX512_BF16` are defined by compiler check, it is means that current compiler havs capability to generate defined ISA level code.
+
+Because of `AVX512_BF16` is higher level than `AVX512`, and it compatible to `AVX512`. `CPU_CAPABILITY_AVX512_BF16` can be contained in `CPU_CAPABILITY_AVX512` region.
+```c++
+//csrc/cpu/aten/kernels/CvtFp32ToBf16Krnl.cpp
+
+#include <ATen/cpu/vec/vec.h>
+#include "csrc/aten/cpu/CvtFp32ToBf16.h"
+
+namespace torch_ipex {
+namespace cpu {
+
+namespace {
+
+#if defined(CPU_CAPABILITY_AVX512)
+#include <ATen/cpu/vec/vec512/vec512.h>
+#else
+#include <ATen/cpu/vec/vec256/vec256.h>
+#endif
+using namespace at::vec;
+
+#if defined(CPU_CAPABILITY_AVX512)
+#include <immintrin.h>
+
+inline __m256i _cvt_fp32_to_bf16(const __m512 src) {
+#if (defined CPU_CAPABILITY_AVX512_BF16) // AVX512_BF16 ISA implementation.
+  return reinterpret_cast<__m256i>(_mm512_cvtneps_pbh(src));
+#else  // AVX512 ISA implementation.
+  __m512i value = _mm512_castps_si512(src);
+  __m512i nan = _mm512_set1_epi32(0xffff);
+  auto mask_value = _mm512_cmp_ps_mask(src, src, _CMP_ORD_Q);
+  __m512i ones = _mm512_set1_epi32(0x1);
+  __m512i vec_bias = _mm512_set1_epi32(0x7fff);
+  // uint32_t lsb = (input >> 16) & 1;
+  auto t_value = _mm512_and_si512(_mm512_srli_epi32(value, 16), ones);
+  // uint32_t rounding_bias = 0x7fff + lsb;
+  t_value = _mm512_add_epi32(t_value, vec_bias);
+  // input += rounding_bias;
+  t_value = _mm512_add_epi32(t_value, value);
+  // input = input >> 16;
+  t_value = _mm512_srli_epi32(t_value, 16);
+  // Check NaN before converting back to bf16
+  t_value = _mm512_mask_blend_epi32(mask_value, nan, t_value);
+  return _mm512_cvtusepi32_epi16(t_value);
+#endif
+}
+
+void cvt_fp32_to_bf16_kernel_impl(
+    at::BFloat16* dst,
+    const float* src,
+    int len) {
+  int i = 0;
+  for (; i < len - 15; i += 16) {
+    auto f32 = _mm512_loadu_ps(src + i);
+    _mm256_storeu_si256((__m256i*)(dst + i), _cvt_fp32_to_bf16(f32));
+  }
+  if (i < len) {
+    auto mask = (1 << (len - i)) - 1;
+    auto f32 = _mm512_maskz_loadu_ps(mask, src + i);
+    _mm256_mask_storeu_epi16(dst + i, mask, _cvt_fp32_to_bf16(f32));
+  }
+}
+
+#else // DEFAULT ISA implementation.
+
+void cvt_fp32_to_bf16_kernel_impl(
+    at::BFloat16* dst,
+    const float* src,
+    int len) {
+  for (int j = 0; j < len; j++) {
+    *(dst + j) = *(src + j);
+  }
+}
+
+#endif
+
+} // anonymous namespace
+
+REGISTER_DISPATCH(cvt_fp32_to_bf16_kernel_stub, &cvt_fp32_to_bf16_kernel_impl);
+
+} // namespace cpu
+} // namespace torch_ipex
+
+```
+
+### Vec specific kernel example:
+This example shows how to get the data type size and its Vec size. In different ISA, Vec has a different register width and a different Vec size.
+
+```c++
+//csrc/cpu/aten/GetVecLength.h
+#pragma once
+
+#include <dyndisp/DispatchStub.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+std::tuple<int, int> get_cpp_typesize_and_vecsize(at::ScalarType dtype);
+
+namespace {
+
+std::tuple<int, int> get_cpp_typesize_and_vecsize_kernel_impl(
+    at::ScalarType dtype);
+}
+
+using get_cpp_typesize_and_vecsize_kernel_fn =
+    std::tuple<int, int> (*)(at::ScalarType);
+DECLARE_DISPATCH(
+    get_cpp_typesize_and_vecsize_kernel_fn,
+    get_cpp_typesize_and_vecsize_kernel_stub);
+
+} // namespace cpu
+} // namespace torch_ipex
+
+```
+
+```c++
+//csrc/cpu/aten/GetVecLength.cpp
+
+#include "GetVecLength.h"
+
+namespace torch_ipex {
+namespace cpu {
+
+DEFINE_DISPATCH(get_cpp_typesize_and_vecsize_kernel_stub);
+
+// get cpp typesize and vectorsize by at::ScalarType
+std::tuple<int, int> get_cpp_typesize_and_vecsize(at::ScalarType dtype) {
+  return get_cpp_typesize_and_vecsize_kernel_stub(kCPU, dtype);
+}
+
+} // namespace cpu
+} // namespace torch_ipex
+
+```
+
+```c++
+//csrc/cpu/aten/kernels/GetVecLengthKrnl.cpp
+
+#include <ATen/cpu/vec/vec.h>
+#include "csrc/cpu/aten/GetVecLength.h"
+
+namespace torch_ipex {
+namespace cpu {
+
+namespace {
+
+std::tuple<int, int> get_cpp_typesize_and_vecsize_kernel_impl(
+    at::ScalarType dtype) {
+  switch (dtype) {
+    case at::ScalarType::Double:
+      return std::make_tuple(
+          sizeof(double), at::vec::Vectorized<double>::size());
+    case at::ScalarType::Float:
+      return std::make_tuple(sizeof(float), at::vec::Vectorized<float>::size());
+    case at::ScalarType::ComplexDouble:
+      return std::make_tuple(
+          sizeof(c10::complex<double>),
+          at::vec::Vectorized<c10::complex<double>>::size());
+    case at::ScalarType::ComplexFloat:
+      return std::make_tuple(
+          sizeof(c10::complex<float>),
+          at::vec::Vectorized<c10::complex<float>>::size());
+    case at::ScalarType::BFloat16:
+      return std::make_tuple(
+          sizeof(decltype(
+              c10::impl::ScalarTypeToCPPType<at::ScalarType::BFloat16>::t)),
+          at::vec::Vectorized<decltype(c10::impl::ScalarTypeToCPPType<
+                                       at::ScalarType::BFloat16>::t)>::size());
+    case at::ScalarType::Half:
+      return std::make_tuple(
+          sizeof(decltype(
+              c10::impl::ScalarTypeToCPPType<at::ScalarType::Half>::t)),
+          at::vec::Vectorized<decltype(c10::impl::ScalarTypeToCPPType<
+                                       at::ScalarType::Half>::t)>::size());
+    default:
+      TORCH_CHECK(
+          false,
+          "Currently only floating and complex ScalarType are supported.");
+  }
+}
+
+} // anonymous namespace
+
+REGISTER_DISPATCH(
+    get_cpp_typesize_and_vecsize_kernel_stub,
+    &get_cpp_typesize_and_vecsize_kernel_impl);
+
+} // namespace cpu
+} // namespace torch_ipex
+
+```
+## Private Debug APIs
+
+Here are three ISA-related private APIs that can help debugging::
+1. Query current ISA level.
+2. Query max CPU supported ISA level.
+3. Query max binary supported ISA level.
+>**Note:**
+>
+>1. Max CPU supported ISA level only depends on CPU features.
+>2. Max binary supported ISA level only depends on built complier version.
+>3. Current ISA level, it is the smaller of `max CPU ISA level` and `max binary ISA level`.
+
+### Example:
+```bash
+python
+Python 3.9.7 (default, Sep 16 2021, 13:09:58)
+[GCC 7.5.0] :: Anaconda, Inc. on linux
+Type "help", "copyright", "credits" or "license" for more information.
+>>> import intel_extension_for_pytorch._C as core
+>>> core._get_current_isa_level()
+'AMX'
+>>> core._get_highest_cpu_support_isa_level()
+'AMX'
+>>> core._get_highest_binary_support_isa_level()
+'AMX'
+>>> quit()
+```
+
+## Select ISA level manually.
+
+By default, IPEX dispatches to the kernels with the maximum ISA level supported by the underlying CPU hardware. This ISA level can be overridden by the environment variable `ATEN_CPU_CAPABILITY` (same environment variable as PyTorch). The available values are {`avx2`, `avx512`, `avx512_vnni`, `avx512_bf16`, `amx`}. The effective ISA level would be the minimal level between `ATEN_CPU_CAPABILITY` and the maximum level supported by the hardware.
+### Example:
+```bash
+$ python -c 'import intel_extension_for_pytorch._C as core;print(core._get_current_isa_level())'
+AMX
+$ ATEN_CPU_CAPABILITY=avx2 python -c 'import intel_extension_for_pytorch._C as core;print(core._get_current_isa_level())'
+AVX2
+```
+>**Note:**
+>
+>`core._get_current_isa_level()` is an IPEX internal function used for checking the current effective ISA level. It is used for debugging purpose only and subject to change.
+
+## CPU feature check
+
+An addtional CPU feature check tool in the subfolder: `tests/cpu/isa`
+
+```bash
+$ cmake .
+-- The C compiler identification is GNU 11.2.1
+-- The CXX compiler identification is GNU 11.2.1
+-- Detecting C compiler ABI info
+-- Detecting C compiler ABI info - done
+-- Check for working C compiler: /opt/rh/gcc-toolset-11/root/usr/bin/cc - skipped
+-- Detecting C compile features
+-- Detecting C compile features - done
+-- Detecting CXX compiler ABI info
+-- Detecting CXX compiler ABI info - done
+-- Check for working CXX compiler: /opt/rh/gcc-toolset-11/root/usr/bin/c++ - skipped
+-- Detecting CXX compile features
+-- Detecting CXX compile features - done
+-- Configuring done
+-- Generating done
+-- Build files have been written to: tests/cpu/isa
+$ make
+[ 33%] Building CXX object CMakeFiles/cpu_features.dir/intel_extension_for_pytorch/csrc/cpu/isa/cpu_feature.cpp.o
+[ 66%] Building CXX object CMakeFiles/cpu_features.dir/intel_extension_for_pytorch/csrc/cpu/isa/cpu_feature_main.cpp.o
+[100%] Linking CXX executable cpu_features
+[100%] Built target cpu_features
+$ ./cpu_features
+XCR0: 00000000000602e7
+os --> avx: true
+os --> avx2: true
+os --> avx512: true
+os --> amx: true
+mmx:                    true
+sse:                    true
+sse2:                   true
+sse3:                   true
+ssse3:                  true
+sse4_1:                 true
+sse4_2:                 true
+aes_ni:                 true
+sha:                    true
+xsave:                  true
+fma:                    true
+f16c:                   true
+avx:                    true
+avx2:                   true
+avx_vnni:                       true
+avx512_f:                       true
+avx512_cd:                      true
+avx512_pf:                      false
+avx512_er:                      false
+avx512_vl:                      true
+avx512_bw:                      true
+avx512_dq:                      true
+avx512_ifma:                    true
+avx512_vbmi:                    true
+avx512_vpopcntdq:                       true
+avx512_4fmaps:                  false
+avx512_4vnniw:                  false
+avx512_vbmi2:                   true
+avx512_vpclmul:                 true
+avx512_vnni:                    true
+avx512_bitalg:                  true
+avx512_fp16:                    true
+avx512_bf16:                    true
+avx512_vp2intersect:                    true
+amx_bf16:                       true
+amx_tile:                       true
+amx_int8:                       true
+prefetchw:                      true
+prefetchwt1:                    false
+```
diff --git a/docs/index.rst b/docs/index.rst
index c2168ee7..51f532a4 100644
--- a/docs/index.rst
+++ b/docs/index.rst
@@ -1,220 +1,43 @@
 .. meta::
    :description: This website introduces Intel® Extension for PyTorch*
-   :keywords: Intel optimization, PyTorch, Intel® Extension for PyTorch*, LLM
+   :keywords: Intel optimization, PyTorch, Intel® Extension for PyTorch*, GPU, discrete GPU, Intel discrete GPU
 
-============================================================================
-Intel® Extension for PyTorch* Large Language Model (LLM) Feature Get Started
-============================================================================
+Welcome to Intel® Extension for PyTorch* Documentation
+######################################################
 
-Intel® Extension for PyTorch* extends optimizations to large language models (LLM). Optimizations are at development and experimental phase at this moment. You are welcomed to have a try with these optimizations on 4th Gen Intel® Xeon® Scalable processors.
+Intel® Extension for PyTorch* extends PyTorch* with up-to-date features optimizations for an extra performance boost on Intel hardware. Optimizations take advantage of AVX-512 Vector Neural Network Instructions (AVX512 VNNI) and Intel® Advanced Matrix Extensions (Intel® AMX) on Intel CPUs as well as Intel X\ :sup:`e`\  Matrix Extensions (XMX) AI engines on Intel discrete GPUs. Moreover, through PyTorch* `xpu` device, Intel® Extension for PyTorch* provides easy GPU acceleration for Intel discrete GPUs with PyTorch*.
 
-System Requirements
-===================
+Intel® Extension for PyTorch* provides optimizations for both eager mode and graph mode, however, compared to eager mode, graph mode in PyTorch* normally yields better performance from optimization techniques, such as operation fusion. Intel® Extension for PyTorch* amplifies them with more comprehensive graph optimizations. Therefore we recommend you to take advantage of Intel® Extension for PyTorch* with `TorchScript <https://pytorch.org/docs/stable/jit.html>`_ whenever your workload supports it. You could choose to run with `torch.jit.trace()` function or `torch.jit.script()` function, but based on our evaluation, `torch.jit.trace()` supports more workloads so we recommend you to use `torch.jit.trace()` as your first choice.
 
-.. list-table::
-   :widths: auto
-   :header-rows: 0
-   :stub-columns: 1
+The extension can be loaded as a Python module for Python programs or linked as a C++ library for C++ programs. In Python scripts users can enable it dynamically by importing `intel_extension_for_pytorch`.
 
-   * - Hardware
-     - 4th Gen Intel® Xeon® Scalable processors
-   * - OS
-     - CentOS/RHEL 8
-   * - Linux Kernel
-     - Intel® 4th Gen Xeon® Platinum: 5.15.0; Intel® 4th Gen Xeon® Max: 5.19.0
-   * - Python
-     - 3.9, conda is required.
-   * - Compiler
-     - Preset in the compilation script below, if compile from source
+Intel® Extension for PyTorch* is structured as shown in the following figure:
 
-Installation
-============
+.. figure:: ../images/intel_extension_for_pytorch_structure.png
+  :width: 800
+  :align: center
+  :alt: Architecture of Intel® Extension for PyTorch*
 
-Prebuilt wheel file are available for Python 3.9. Alternatively, a script is provided to compile from source.
+|
 
-Install From Prebuilt Wheel Files
----------------------------------
+Optimizations for both eager mode and graph mode contribute to extra performance accelerations with the extension. In eager mode, the PyTorch frontend is extended with custom Python modules (such as fusion modules), optimal optimizers, and INT8 quantization APIs. Further performance boost is available by converting the eager-mode model into graph mode via extended graph fusion passes. In the graph mode, the fusions reduce operator/kernel invocation overheads, and thus increase performance. On CPU, Intel® Extension for PyTorch* dispatches the operators into their underlying kernels automatically based on ISA that it detects and leverages vectorization and matrix acceleration units available on Intel hardware. Intel® Extension for PyTorch* runtime extension brings better efficiency with finer-grained thread runtime control and weight sharing. On GPU, optimized operators and kernels are implemented and registered through PyTorch dispatching mechanism. These operators and kernels are accelerated from native vectorization feature and matrix calculation feature of Intel GPU hardware. Intel® Extension for PyTorch* for GPU utilizes the `DPC++ <https://github.com/intel/llvm#oneapi-dpc-compiler>`_ compiler that supports the latest `SYCL* <https://registry.khronos.org/SYCL/specs/sycl-2020/html/sycl-2020.html>`_ standard and also a number of extensions to the SYCL* standard, which can be found in the `sycl/doc/extensions <https://github.com/intel/llvm/tree/sycl/sycl/doc/extensions>`_ directory.
 
-.. code:: shell
+.. note:: GPU features are not included in CPU only packages.
 
-  python -m pip install torch==2.1.0.dev20230711+cpu torchvision==0.16.0.dev20230711+cpu torchaudio==2.1.0.dev20230711+cpu --index-url https://download.pytorch.org/whl/nightly/cpu
-  python -m pip install https://intel-extension-for-pytorch.s3.amazonaws.com/ipex_dev/cpu/intel_extension_for_pytorch-2.1.0.dev0%2Bcpu.llm-cp39-cp39-linux_x86_64.whl
-  conda install -y libstdcxx-ng=12 -c conda-forge
+Intel® Extension for PyTorch* has been released as an open–source project at `Github <https://github.com/intel/intel-extension-for-pytorch>`_. Source code is available at `xpu-master branch <https://github.com/intel/intel-extension-for-pytorch/tree/xpu-master>`_. Check `the tutorial <https://intel.github.io/intel-extension-for-pytorch/xpu/latest/>`_ for detailed information. Due to different development schedule, optimizations for CPU only might have a newer code base. Source code is available at `master branch <https://github.com/intel/intel-extension-for-pytorch/tree/master>`_. Check `the CPU tutorial <https://intel.github.io/intel-extension-for-pytorch/cpu/latest/>`_ for detailed information on the CPU side.
 
-Compile From Source
--------------------
+.. toctree::
+   :hidden:
+   :maxdepth: 1
 
-.. code:: shell
-
-  wget https://github.com/intel/intel-extension-for-pytorch/raw/v2.1.0.dev%2Bcpu.llm/scripts/compile_bundle.sh
-  bash compile_bundle.sh
-
-Launch Examples
-===============
-
-Supported Models
-----------------
-
-The following 3 models are supported. When running the example scripts, it is needed to replace the place holder *<MODEL_ID>* in example launch commands with:
-
-.. list-table::
-   :widths: auto
-   :header-rows: 0
-   :stub-columns: 1
-
-   * - GPT-J
-     - "EleutherAI/gpt-j-6b"
-   * - GPT-Neox
-     - "EleutherAI/gpt-neox-20b"
-   * - Llama 2
-     - Model directory path output from the `transformers conversion tool <https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py>`_.* Verified `meta-llama/Llama-2-7b-chat <https://huggingface.co/meta-llama/Llama-2-7b-chat>`_ and `meta-llama/Llama-2-13b-chat <https://huggingface.co/meta-llama/Llama-2-13b-chat>`_.
-
-\* Llama 2 model conversion steps:
-
-  1. Follow `instructions <https://github.com/facebookresearch/llama#access-on-hugging-face>`_ to download model files for conversion.
-  2. Decompress the downloaded model file.
-  3. Follow `instructions <https://github.com/facebookresearch/llama-recipes#model-conversion-to-hugging-face>`_ to convert the model.
-  4. Launch example scripts with the place holder *<MODEL_ID>* substituted by the *\-\-output_dir* argument value of the conversion script.
-
-Install Dependencies
---------------------
-
-.. code:: shell
-
-  conda install -y gperftools -c conda-forge
-  conda install -y intel-openmp
-  python -m pip install transformers==4.28.1 cpuid accelerate datasets sentencepiece protobuf==3.20.3
-
-  # [Optional] install neural-compressor for GPT-J INT8 only
-  python -m pip install neural-compressor==2.2
-
-  # [Optional] The following is only for DeepSpeed case
-  git clone https://github.com/delock/DeepSpeedSYCLSupport
-  cd DeepSpeedSYCLSupport
-  git checkout gma/run-opt-branch
-  python -m pip install -r requirements/requirements.txt
-  python setup.py install
-  cd ../
-  git clone https://github.com/oneapi-src/oneCCL.git
-  cd oneCCL
-  mkdir build
-  cd build
-  cmake ..
-  make -j install
-  source _install/env/setvars.sh
-  cd ../..
-
-.. note::
-
-  If an error complaining *ninja* is not found when compiling deepspeed, please use conda and pip command to uninstall all ninja packages, and reinstall it with pip.
-
-Run Examples
-------------
-
-The following 5 python scripts are provided in Github repo `example directory <https://github.com/intel/intel-extension-for-pytorch/tree/v2.1.0.dev%2Bcpu.llm/examples/cpu/inference/python/llm/>`_ to launch inference workloads with supported models.
-
-- run_generation.py
-- run_generation_with_deepspeed.py
-- run_gpt-j_int8.py
-- run_gpt-neox_int8.py
-- run_llama_int8.py
-
-Preparations
-^^^^^^^^^^^^
-
-A separate *prompt.json* file is required to run performance benchmarks. You can use the command below to download a sample file. For simple testing, an argument *\-\-prompt* is provided by the scripts to take a text for processing.
-
-To get these Python scripts, you can either get the entire Github repository down with git command, or use the following wget commands to get individual scripts.
-
-.. code:: shell
-
-  # Get the example scripts with git command
-  git clone https://github.com/intel/intel-extension-for-pytorch.git
-  cd intel-extension-for-pytorch
-  git checkout v2.1.0.dev+cpu.llm
-  cd examples/cpu/inference/python/llm
-
-  # Alternatively, get individual example scripts
-  wget https://github.com/intel/intel-extension-for-pytorch/raw/v2.1.0.dev%2Bcpu.llm/examples/cpu/inference/python/llm/run_generation.py
-  wget https://github.com/intel/intel-extension-for-pytorch/raw/v2.1.0.dev%2Bcpu.llm/examples/cpu/inference/python/llm/run_generation_with_deepspeed.py
-  wget https://github.com/intel/intel-extension-for-pytorch/raw/v2.1.0.dev%2Bcpu.llm/examples/cpu/inference/python/llm/run_gpt-j_int8.py
-  wget https://github.com/intel/intel-extension-for-pytorch/raw/v2.1.0.dev%2Bcpu.llm/examples/cpu/inference/python/llm/run_gpt-neox_int8.py
-  wget https://github.com/intel/intel-extension-for-pytorch/raw/v2.1.0.dev%2Bcpu.llm/examples/cpu/inference/python/llm/run_llama_int8.py
-
-  # Get the sample prompt.json
-  # Make sure the downloaded prompt.json file is under the same directory as that of the python scripts mentioned above.
-  wget https://intel-extension-for-pytorch.s3.amazonaws.com/miscellaneous/llm/prompt.json
-
-The following environment variables are required to achieve a good performance on 4th Gen Intel® Xeon® Scalable processors.
-
-.. code:: shell
-
-  export LD_PRELOAD=${CONDA_PREFIX}/lib/libstdc++.so.6
-
-  # Setup environment variables for performance on Xeon
-  export KMP_BLOCKTIME=INF
-  export KMP_TPAUSE=0
-  export KMP_SETTINGS=1
-  export KMP_AFFINITY=granularity=fine,compact,1,0
-  export KMP_FORJOIN_BARRIER_PATTERN=dist,dist
-  export KMP_PLAIN_BARRIER_PATTERN=dist,dist
-  export KMP_REDUCTION_BARRIER_PATTERN=dist,dist
-  export LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libiomp5.so # Intel OpenMP
-
-  # Tcmalloc is a recommended malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support.
-  export LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libtcmalloc.so
-
-Single Instance Performance
-^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-.. code:: shell
-
-  # Get prompt file to the path of scripts
-  mv PATH/TO/prompt.json WORK_DIR
-
-  # bfloat16 benchmark
-  OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run_generation.py --benchmark -m <MODEL_ID> --dtype bfloat16 --ipex --jit
-
-  # int8 benchmark
-  ## (1) Do quantization to get the quantized model
-  mkdir saved_results
-
-  ## GPT-J quantization
-  python run_gpt-j_int8.py --ipex-smooth-quant --lambada --output-dir "saved_results" --jit --int8-bf16-mixed -m <GPTJ MODEL_ID>
-  ## Llama 2 quantization
-  python run_llama_int8.py --ipex-smooth-quant --lambada --output-dir "saved_results" --jit --int8-bf16-mixed -m <LLAMA MODEL_ID>
-  ## GPT-NEOX quantization
-  python run_gpt-neox_int8.py --ipex-weight-only-quantization --lambada --output-dir "saved_results" --jit --int8 -m <GPT-NEOX MODEL_ID>
-
-  ## (2) Run int8 performance test (note that GPT-NEOX uses --int8 instead of --int8-bf16-mixed)
-  OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <cpu list> python run_<MODEL>_int8.py -m <MODEL_ID> --quantized-model-path "./saved_results/best_model.pt" --benchmark --jit --int8-bf16-mixed
-
-Single Instance Accuracy
-^^^^^^^^^^^^^^^^^^^^^^^^
-
-.. code:: shell
-
-  # bfloat16
-  OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run_generation.py --accuracy-only -m <MODEL_ID> --dtype bfloat16 --ipex --jit --lambada
-
-  # Quantization as a performance part
-  # (1) Do quantization to get the quantized model as mentioned above
-  # (2) Run int8 accuracy test (note that GPT-NEOX uses --int8 instead of --int8-bf16-mixed)
-  OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <cpu list> python run_<MODEL>_int8.py -m <MODEL ID> --quantized-model-path "./saved_results/best_model.pt" --accuracy-only --jit --int8-bf16-mixed --lambada
-
-Distributed Performance with DeepSpeed (autoTP)
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-.. code:: shell
-
-  export DS_SHM_ALLREDUCE=1
-  unset KMP_AFFINITY
-
-  # Get prompt file to the path of scripts
-  mv PATH/TO/prompt.json WORK_DIR
-
-  # Run GPTJ/LLAMA with bfloat16  DeepSpeed
-  deepspeed --bind_cores_to_rank run_generation_with_deepspeed.py --benchmark -m <MODEL_ID> --dtype bfloat16 --ipex --jit
-
-  # Run GPT-NeoX with ipex weight only quantization
-  deepspeed --bind_cores_to_rank run_generation_with_deepspeed.py --benchmark -m EleutherAI/gpt-neox-20b --dtype float32 --ipex --jit --ipex-weight-only-quantization
+   tutorials/getting_started
+   tutorials/features
+   tutorials/releases
+   tutorials/installation
+   tutorials/examples
+   tutorials/api_doc
+   tutorials/performance_tuning
+   tutorials/performance
+   tutorials/blogs_publications
+   tutorials/contribution
+   tutorials/license
diff --git a/docs/tutorials/api_doc.rst b/docs/tutorials/api_doc.rst
new file mode 100644
index 00000000..707bc194
--- /dev/null
+++ b/docs/tutorials/api_doc.rst
@@ -0,0 +1,47 @@
+API Documentation
+#################
+
+General
+*******
+
+.. currentmodule:: intel_extension_for_pytorch
+.. autofunction:: optimize
+.. autoclass:: verbose
+
+Fast Bert (Experimental)
+************************
+
+.. currentmodule:: intel_extension_for_pytorch
+.. autofunction:: fast_bert
+
+Graph Optimization
+******************
+
+.. currentmodule:: intel_extension_for_pytorch
+.. autofunction:: enable_onednn_fusion
+
+Quantization
+************
+
+.. automodule:: intel_extension_for_pytorch.quantization
+.. autofunction:: prepare
+.. autofunction:: convert
+
+Experimental API, introduction is avaiable at `feature page <./features/int8_recipe_tuning_api.md>`_.
+
+.. autofunction:: autotune
+
+CPU Runtime
+***********
+
+.. automodule:: intel_extension_for_pytorch.cpu.runtime
+.. autofunction:: is_runtime_ext_enabled
+.. autoclass:: CPUPool
+.. autoclass:: pin
+.. autoclass:: MultiStreamModuleHint
+.. autoclass:: MultiStreamModule
+.. autoclass:: Task
+.. autofunction:: get_core_list_of_node_id
+
+.. .. automodule:: intel_extension_for_pytorch.quantization
+..    :members:
diff --git a/docs/tutorials/blogs_publications.md b/docs/tutorials/blogs_publications.md
new file mode 100644
index 00000000..1d900ec1
--- /dev/null
+++ b/docs/tutorials/blogs_publications.md
@@ -0,0 +1,37 @@
+Blogs & Publications
+====================
+
+* [Intel® Deep Learning Boost (Intel® DL Boost) - Improve Inference Performance of Hugging Face BERT Base Model in Google Cloud Platform (GCP) Technology Guide, Apr 2023](https://networkbuilders.intel.com/solutionslibrary/intel-deep-learning-boost-intel-dl-boost-improve-inference-performance-of-hugging-face-bert-base-model-in-google-cloud-platform-gcp-technology-guide)
+* [Get Started with Intel® Extension for PyTorch\* on GPU | Intel Software, Mar 2023](https://www.youtube.com/watch?v=Id-rE2Q7xZ0&t=1s)
+* [Accelerate PyTorch\* INT8 Inference with New “X86” Quantization Backend on X86 CPUs, Mar 2023](https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-pytorch-int8-inf-with-new-x86-backend.html)
+* [Accelerating PyTorch Transformers with Intel Sapphire Rapids, Part 1, Jan 2023](https://huggingface.co/blog/intel-sapphire-rapids)
+* [Intel® Deep Learning Boost - Improve Inference Performance of BERT Base Model from Hugging Face for Network Security Technology Guide, Jan 2023](https://networkbuilders.intel.com/solutionslibrary/intel-deep-learning-boost-improve-inference-performance-of-bert-base-model-from-hugging-face-for-network-security-technology-guide)
+* [Scaling inference on CPUs with TorchServe, PyTorch Conference, Dec 2022](https://www.youtube.com/watch?v=066_Jd6cwZg)
+* [What is New in Intel Extension for PyTorch, PyTorch Conference, Dec 2022](https://www.youtube.com/watch?v=SE56wFXdvP4&t=1s)
+* [Accelerating PyG on Intel CPUs, Dec 2022](https://www.pyg.org/ns-newsarticle-accelerating-pyg-on-intel-cpus)
+* [Accelerating PyTorch Deep Learning Models on Intel XPUs, Dec, 2022](https://www.oneapi.io/event-sessions/accelerating-pytorch-deep-learning-models-on-intel-xpus-2-ai-hpc-2022/)
+* [Introducing the Intel® Extension for PyTorch\* for GPUs, Dec 2022](https://www.intel.com/content/www/us/en/developer/articles/technical/introducing-intel-extension-for-pytorch-for-gpus.html)
+* [PyTorch Stable Diffusion Using Hugging Face and Intel Arc, Nov 2022](https://towardsdatascience.com/pytorch-stable-diffusion-using-hugging-face-and-intel-arc-77010e9eead6)
+* [PyTorch 1.13: New Potential for AI Developers to Enhance Model Performance and Accuracy, Nov 2022](https://www.intel.com/content/www/us/en/developer/articles/technical/pytorch-1-13-new-potential-for-ai-developers.html)
+* [Easy Quantization in PyTorch Using Fine-Grained FX, Sep 2022](https://medium.com/intel-analytics-software/easy-quantization-in-pytorch-using-fine-grained-fx-80be2c4bc2d6)
+* [Empowering PyTorch on Intel® Xeon® Scalable processors with Bfloat16, Aug 2022](https://pytorch.org/blog/empowering-pytorch-on-intel-xeon-scalable-processors-with-bfloat16/)
+* [Accelerating PyTorch Vision Models with Channels Last on CPU, Aug 2022](https://pytorch.org/blog/accelerating-pytorch-vision-models-with-channels-last-on-cpu/)
+* [One-Click Enabling of Intel Neural Compressor Features in PyTorch Scripts, Aug 2022](https://medium.com/intel-analytics-software/one-click-enable-intel-neural-compressor-features-in-pytorch-scripts-5d4e31f5a22b)
+* [Increase PyTorch Inference Throughput by 4x, Jul 2022](https://www.intel.com/content/www/us/en/developer/articles/technical/increase-pytorch-inference-throughput-by-4x.html)
+* [PyTorch Inference Acceleration with Intel® Neural Compressor, Jun 2022](https://medium.com/pytorch/pytorch-inference-acceleration-with-intel-neural-compressor-842ef4210d7d)
+* [Accelerating PyTorch with Intel® Extension for PyTorch, May 2022](https://medium.com/pytorch/accelerating-pytorch-with-intel-extension-for-pytorch-3aef51ea3722)
+* [Grokking PyTorch Intel CPU performance from first principles (parts 1), Apr 2022](https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html)
+* [Grokking PyTorch Intel CPU performance from first principles (parts 2), Apr 2022](https://pytorch.org/tutorials/intermediate/torchserve_with_ipex_2.html)
+* [Grokking PyTorch Intel CPU performance from first principles, Apr 2022](https://medium.com/pytorch/grokking-pytorch-intel-cpu-performance-from-first-principles-7e39694412db)
+* [KT Optimizes Performance for Personalized Text-to-Speech, Nov 2021](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/KT-Optimizes-Performance-for-Personalized-Text-to-Speech/post/1337757)
+* [Accelerating PyTorch distributed fine-tuning with Intel technologies, Nov 2021](https://huggingface.co/blog/accelerating-pytorch)
+* [Scaling up BERT-like model Inference on modern CPU - parts 1, Apr 2021](https://huggingface.co/blog/bert-cpu-scaling-part-1)
+* [Scaling up BERT-like model Inference on modern CPU - parts 2, Nov 2021](https://huggingface.co/blog/bert-cpu-scaling-part-2)
+* [NAVER: Low-Latency Machine-Learning Inference](https://www.intel.com/content/www/us/en/customer-spotlight/stories/naver-ocr-customer-story.html)
+* [Intel® Extensions for PyTorch, Feb 2021](https://pytorch.org/tutorials/recipes/recipes/intel_extension_for_pytorch.html)
+* [Optimizing DLRM by using PyTorch with oneCCL Backend, Feb 2021](https://pytorch.medium.com/optimizing-dlrm-by-using-pytorch-with-oneccl-backend-9f85b8ef6929)
+* [Accelerate PyTorch with IPEX and oneDNN using Intel BF16 Technology, Feb 2021](https://medium.com/pytorch/accelerate-pytorch-with-ipex-and-onednn-using-intel-bf16-technology-dca5b8e6b58f)
+  *Note*: APIs mentioned in it are deprecated.
+* [Intel and Facebook Accelerate PyTorch Performance with 3rd Gen Intel® Xeon® Processors and Intel® Deep Learning Boost’s new BFloat16 capability, Jun 2020](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-and-Facebook-Accelerate-PyTorch-Performance-with-3rd-Gen/post/1335659)
+* [Intel and Facebook\* collaborate to boost PyTorch\* CPU performance, Apr 2019](https://www.intel.com/content/www/us/en/developer/articles/case-study/intel-and-facebook-collaborate-to-boost-pytorch-cpu-performance.html)
+* [Intel and Facebook\* Collaborate to Boost Caffe\*2 Performance on Intel CPU’s, Apr 2017](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-and-facebook-collaborate-to-boost-caffe2-performance-on-intel-cpu-s.html)
diff --git a/docs/tutorials/contribution.md b/docs/tutorials/contribution.md
new file mode 100644
index 00000000..b52f6f9d
--- /dev/null
+++ b/docs/tutorials/contribution.md
@@ -0,0 +1,200 @@
+Contribution
+============
+
+## Contributing to Intel® Extension for PyTorch\*
+
+Thank you for your interest in contributing to Intel® Extension for PyTorch\*. Before you begin writing code, it is important that you share your intention to contribute with the team, based on the type of contribution:
+
+1. You want to propose a new feature and implement it.
+    - Post about your intended feature in a [GitHub issue](https://github.com/intel/intel-extension-for-pytorch/issues), and we shall discuss the design and implementation. Once we agree that the plan looks good, go ahead and implement it.
+2. You want to implement a feature or bug-fix for an outstanding issue.
+    - Search for your issue in the [GitHub issue list](https://github.com/intel/intel-extension-for-pytorch/issues).
+    - Pick an issue and comment that you'd like to work on the feature or bug-fix.
+    - If you need more context on a particular issue, ask and we shall provide.
+
+Once you implement and test your feature or bug-fix, submit a Pull Request to https://github.com/intel/intel-extension-for-pytorch.
+
+## Developing Intel® Extension for PyTorch\*
+
+A full set of instructions on installing Intel® Extension for PyTorch\* from source is in the [Installation document](installation.md#install-via-source-compilation).
+
+To develop on your machine, here are some tips:
+
+1. Uninstall all existing Intel® Extension for PyTorch\* installs. You may need to run `pip uninstall intel_extension_for_pytorch` multiple times. You'll know `intel_extension_for_pytorch` is fully uninstalled when you see `WARNING: Skipping intel_extension_for_pytorch as it is not installed`. (You should only have to `pip uninstall` a few times, but you can always `uninstall` with `timeout` or in a loop if you're feeling lazy.)
+
+   ```bash
+   yes | pip uninstall intel_extension_for_pytorch
+   ```
+
+2. Clone a copy of Intel® Extension for PyTorch\* from source:
+
+   ```bash
+   git clone https://github.com/intel/intel-extension-for-pytorch.git
+   cd intel-extension-for-pytorch
+   ```
+
+   If you already have Intel® Extension for PyTorch\* from source, update it:
+
+   ```bash
+   git pull --rebase
+   git submodule sync --recursive
+   git submodule update --init --recursive --jobs 0
+   ```
+
+3. Install Intel® Extension for PyTorch\* in `develop` mode:
+
+   Replace:
+
+   ```bash
+   python setup.py install
+   ```
+
+   with:
+
+   ```bash
+   python setup.py develop
+   ```
+
+   This mode will symlink the Python files from the current local source tree into the Python install. After than, if you modify a Python file, you do not need to reinstall PyTorch again. This is especially useful if you are only changing Python files.
+
+   For example:
+   - Install local Intel® Extension for PyTorch\* in `develop` mode
+   - modify your Python file `intel_extension_for_pytorch/__init__.py` (for example)
+   - test functionality
+
+You do not need to repeatedly install after modifying Python files (`.py`). However, you would need to reinstall if you modify a Python interface (`.pyi`, `.pyi.in`) or non-Python files (`.cpp`, `.cc`, `.cu`, `.h`, etc.).
+
+If you want to reinstall, make sure that you uninstall Intel® Extension for PyTorch\* first by running `pip uninstall intel_extension_for_pytorch` until you see `WARNING: Skipping intel_extension_for_pytorch as it is not installed`; next run `python setup.py clean`. After that, you can install in `develop` mode again.
+
+### Tips and Debugging
+
+* Cmake must be installed before installing Intel® Extension for PyTorch\*. If youre developing on MacOS or Linux, We recommend installing Cmake with [Homebrew](https://brew.sh/) with `brew install cmake`.
+* Our `setup.py` requires Python >= 3.6
+* If you run into errors when running `python setup.py develop`, here are some debugging steps:
+  1. Run `printf '#include <stdio.h>\nint main() { printf("Hello World");}'|clang -x c -; ./a.out` to make sure your CMake works and can compile this simple Hello World program without errors.
+  2. Remove your `build` directory. The `setup.py` script compiles binaries into the `build` folder and caches many details along the way. This saves time the next time you build. If you're running into issues, you can always `rm -rf build` from the toplevel `pytorch` directory and start over.
+  3. If you have made edits to the Intel® Extension for PyTorch\* repo, commit any change you'd like to keep and clean the repo with the following commands (note that clean _really_ removes all untracked files and changes.):
+     ```bash
+     git submodule deinit -f .
+     git clean -xdf
+     python setup.py clean
+     git submodule update --init --recursive --jobs 0 # very important to sync the submodules
+     python setup.py develop                          # then try running the command again
+     ```
+  4. The main step within `python setup.py develop` is running `make` from the `build` directory. If you want to experiment with some environment variables, you can pass them into the command:
+     ```bash
+     ENV_KEY1=ENV_VAL1[, ENV_KEY2=ENV_VAL2]* python setup.py develop
+     ```
+
+## Unit testing
+
+### Python Unit Testing
+
+All PyTorch test suites are located in the `test` folder and start with `test_`. Run individual test suites using the command `python test/cpu/FILENAME.py`, where `FILENAME` represents the file containing the test suite you wish to run.
+
+For example, to run all the TorchScript JIT tests (located at `test/cpu/test_jit.py`), you would run:
+
+```bash
+python test/cpu/test_jit.py
+```
+
+You can narrow down what you're testing even further by specifying the name of an individual test with `TESTCLASSNAME.TESTNAME`. Here, `TESTNAME` is the name of the test you want to run, and `TESTCLASSNAME` is the name of the class in which it is defined.
+
+Let's say you want to run `test_Sequential`, which is defined as part of the `TestJit` class in `test/cpu/test_jit.py`. Your command would be:
+
+```bash
+python test/test_jit.py TestJit.test_Sequential
+```
+
+The `expecttest` and `hypothesis` libraries must be installed to run the tests. `mypy` is an optional dependency, and `pytest` may help run tests more selectively. All these packages can be installed with `conda` or `pip`.
+
+### Better local unit tests with `pytest`
+
+We don't officially support `pytest`, but it works well with our `unittest` tests and offers a number of useful features for local developing. Install it via `pip install pytest`.
+
+If you want to run only tests that contain a specific substring, you can use the `-k` flag:
+
+```bash
+pytest test/cpu/test_nn.py -k Loss -v
+```
+
+The above is an example of testing a change to all Loss functions: this command runs tests such as `TestNN.test_BCELoss` and `TestNN.test_MSELoss` and can be useful to save keystrokes.
+
+### Local linting
+
+You can run the same linting steps that are used in CI locally via `make`:
+
+```bash
+# Lint all files
+make lint -j 6  # run lint (using 6 parallel jobs)
+
+# Lint only the files you have changed
+make quicklint -j 6
+```
+
+These jobs may require extra dependencies that aren't dependencies of Intel® Extension for PyTorch\* itself, so you can install them via this command, which you should only have to run once:
+
+```bash
+make setup_lint
+```
+
+To run a specific linting step, use one of these targets or see the Makefile for a complete list of options.
+
+```bash
+# Check for tabs, trailing newlines, etc.
+make quick_checks
+
+make flake8
+
+make mypy
+
+make cmakelint
+
+make clang-tidy
+```
+
+To run a lint only on changes, add the `CHANGED_ONLY` option:
+
+```bash
+make <name of lint> CHANGED_ONLY=--changed-only
+```
+
+### C++ Unit Testing
+
+Intel® Extension for PyTorch\* offers tests located in the `test/cpp` folder. These tests are written in C++ and use the Google Test testing framework. After compiling Intel® Extension for PyTorch\* from source, the test runner binaries will be written to the `build/bin` folder. The command to run one of these tests is `./build/bin/FILENAME --gtest_filter=TESTSUITE.TESTNAME`, where `TESTNAME` is the name of the test you'd like to run and `TESTSUITE` is the suite that test is defined in.
+
+For example, if you wanted to run the test `MayContainAlias`, which is part of the test suite `ContainerAliasingTest` in the file `test/cpp/jit/test_alias_analysis.cpp`, the command would be:
+
+```bash
+./build/bin/test_jit --gtest_filter=ContainerAliasingTest.MayContainAlias
+```
+
+## Writing documentation
+
+So you want to write some documentation for your code contribution and don't know where to start?
+
+Intel® Extension for PyTorch\* uses [Google style](http://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html) for formatting docstrings. Length of line inside docstrings block must be limited to 80 characters to fit into Jupyter documentation popups.
+
+### Building documentation
+
+To build the documentation:
+
+1. Build and install Intel® Extension for PyTorch\* (as discussed above)
+
+2. Install the prerequisites:
+
+   ```bash
+   cd docs
+   pip install -r requirements.txt
+   ```
+
+3. Generate the documentation HTML files. The generated files will be in `docs/_build/html`.
+
+   ```bash
+   make clean
+   make html
+   ```
+
+#### Tips
+
+The `.rst` source files live in [docs/tutorials](https://github.com/intel/intel-extension-for-pytorch/tree/master/docs/tutorials). Some of the `.rst` files pull in docstrings from Intel® Extension for PyTorch\* Python code (for example, via the `autofunction` or `autoclass` directives). To shorten doc build times, it is helpful to remove the files you are not working on, only keeping the base `index.rst` file and the files you are editing. The Sphinx build will produce missing file warnings but will still complete.
diff --git a/docs/tutorials/examples.md b/docs/tutorials/examples.md
new file mode 100644
index 00000000..0f49b1ab
--- /dev/null
+++ b/docs/tutorials/examples.md
@@ -0,0 +1,315 @@
+Examples
+========
+
+**_NOTE:_** Check individual feature page for examples of feature usage. All features are listed in the [feature page](./features.rst).
+
+**_NOTE:_** Feature examples and examples below are available at Github source tree, under `examples` directory.
+
+## Training
+
+### Single-instance Training
+
+#### Code Changes Highlight
+
+There is only a line of code change required to use Intel® Extension for PyTorch\* on training, as shown:
+1. `ipex.optimize` function applies optimizations against the model object, as well as an optimizer object.
+
+```
+...
+import torch
+import intel_extension_for_pytorch as ipex
+...
+model = Model()
+criterion = ...
+optimizer = ...
+model.train()
+# For Float32
+model, optimizer = ipex.optimize(model, optimizer=optimizer)
+# For BFloat16
+model, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.bfloat16)
+...
+optimizer.zero_grad()
+output = model(data)
+...
+```
+
+#### Complete - Float32
+
+[//]: # (marker_train_single_fp32_complete)
+[//]: # (marker_train_single_fp32_complete)
+
+#### Complete - BFloat16
+
+[//]: # (marker_train_single_bf16_complete)
+[//]: # (marker_train_single_bf16_complete)
+
+### Distributed Training
+
+Distributed training with PyTorch DDP is accelerated by oneAPI Collective Communications Library Bindings for Pytorch\* (oneCCL Bindings for Pytorch\*). The extension supports FP32 and BF16 data types. More detailed information and examples are available at its [Github repo](https://github.com/intel/torch-ccl).
+
+**Note:** When performing distributed training with BF16 data type, use oneCCL Bindings for Pytorch\*. Due to a PyTorch limitation, distributed training with BF16 data type with Intel® Extension for PyTorch\* is not supported.
+
+[//]: # (marker_train_ddp_complete)
+```
+import os
+import torch
+import torch.distributed as dist
+import torchvision
+import oneccl_bindings_for_pytorch as torch_ccl
+import intel_extension_for_pytorch as ipex
+
+LR = 0.001
+DOWNLOAD = True
+DATA = 'datasets/cifar10/'
+
+transform = torchvision.transforms.Compose([
+    torchvision.transforms.Resize((224, 224)),
+    torchvision.transforms.ToTensor(),
+    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
+])
+train_dataset = torchvision.datasets.CIFAR10(
+        root=DATA,
+        train=True,
+        transform=transform,
+        download=DOWNLOAD,
+)
+train_loader = torch.utils.data.DataLoader(
+        dataset=train_dataset,
+        batch_size=128
+)
+
+os.environ['MASTER_ADDR'] = '127.0.0.1'
+os.environ['MASTER_PORT'] = '29500'
+os.environ['RANK'] = os.environ.get('PMI_RANK', 0)
+os.environ['WORLD_SIZE'] = os.environ.get('PMI_SIZE', 1)
+dist.init_process_group(
+backend='ccl',
+init_method='env://'
+)
+
+model = torchvision.models.resnet50()
+criterion = torch.nn.CrossEntropyLoss()
+optimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)
+model.train()
+model, optimizer = ipex.optimize(model, optimizer=optimizer)
+
+model = torch.nn.parallel.DistributedDataParallel(model)
+
+for batch_idx, (data, target) in enumerate(train_loader):
+    optimizer.zero_grad()
+    output = model(data)
+    loss = criterion(output, target)
+    loss.backward()
+    optimizer.step()
+    print('batch_id: {}'.format(batch_idx))
+torch.save({
+     'model_state_dict': model.state_dict(),
+     'optimizer_state_dict': optimizer.state_dict(),
+     }, 'checkpoint.pth')
+```
+[//]: # (marker_train_ddp_complete)
+
+## Inference
+
+The `optimize` function of Intel® Extension for PyTorch\* applies optimizations to the model, bringing additional performance boosts. For both computer vision workloads and NLP workloads, we recommend applying the `optimize` function against the model object.
+
+### Float32
+
+#### Imperative Mode
+
+##### Resnet50
+
+[//]: # (marker_inf_rn50_imp_fp32)
+[//]: # (marker_inf_rn50_imp_fp32)
+
+##### BERT
+
+[//]: # (marker_inf_bert_imp_fp32)
+[//]: # (marker_inf_bert_imp_fp32)
+
+#### TorchScript Mode
+
+We recommend you take advantage of Intel® Extension for PyTorch\* with [TorchScript](https://pytorch.org/docs/stable/jit.html) for further optimizations.
+
+##### Resnet50
+
+[//]: # (marker_inf_rn50_ts_fp32)
+[//]: # (marker_inf_rn50_ts_fp32)
+
+##### BERT
+
+[//]: # (marker_inf_bert_ts_fp32)
+[//]: # (marker_inf_bert_ts_fp32)
+
+#### TorchDynamo Mode (Experimental, _NEW feature from 2.0.0_)
+
+##### Resnet50
+
+[//]: # (marker_inf_rn50_dynamo_fp32)
+[//]: # (marker_inf_rn50_dynamo_fp32)
+
+##### BERT
+
+[//]: # (marker_inf_bert_dynamo_fp32)
+[//]: # (marker_inf_bert_dynamo_fp32)
+
+### BFloat16
+
+Similar to running with FP32, the `optimize` function also works for BFloat16 data type. The only difference is setting `dtype` parameter to `torch.bfloat16`.
+We recommend using Auto Mixed Precision (AMP) with BFloat16 data type.
+
+#### Imperative Mode
+
+##### Resnet50
+
+[//]: # (marker_inf_rn50_imp_bf16)
+[//]: # (marker_inf_rn50_imp_bf16)
+
+##### BERT
+
+[//]: # (marker_inf_bert_imp_bf16)
+[//]: # (marker_inf_bert_imp_bf16)
+
+#### TorchScript Mode
+
+We recommend you take advantage of Intel® Extension for PyTorch\* with [TorchScript](https://pytorch.org/docs/stable/jit.html) for further optimizations.
+
+##### Resnet50
+
+[//]: # (marker_inf_rn50_ts_bf16)
+[//]: # (marker_inf_rn50_ts_bf16)
+
+##### BERT
+
+[//]: # (marker_inf_bert_ts_bf16)
+[//]: # (marker_inf_bert_ts_bf16)
+
+### Fast Bert (*Experimental*)
+
+[//]: # (marker_inf_bert_fast_bf16)
+[//]: # (marker_inf_bert_fast_bf16)
+
+### INT8
+
+Starting from Intel® Extension for PyTorch\* 1.12.0, quantization feature supports both static and dynamic modes.
+
+#### Calibration
+
+##### Static Quantization
+
+Please follow the steps below to perform static calibration:
+
+1. Import `intel_extension_for_pytorch` as `ipex`.
+2. Import `prepare` and `convert` from `intel_extension_for_pytorch.quantization`.
+3. Instantiate a config object from `torch.ao.quantization.QConfig` to save configuration data during calibration.
+4. Prepare model for calibration.
+5. Perform calibration against dataset.
+6. Invoke `ipex.quantization.convert` function to apply the calibration configure object to the fp32 model object to get an INT8 model.
+7. Save the INT8 model into a `pt` file.
+
+
+[//]: # (marker_int8_static)
+[//]: # (marker_int8_static)
+
+##### Dynamic Quantization
+
+Please follow the steps below to perform static calibration:
+
+1. Import `intel_extension_for_pytorch` as `ipex`.
+2. Import `prepare` and `convert` from `intel_extension_for_pytorch.quantization`.
+3. Instantiate a config object from `torch.ao.quantization.QConfig` to save configuration data during calibration.
+4. Prepare model for quantization.
+5. Convert the model.
+6. Run inference to perform dynamic quantization.
+7. Save the INT8 model into a `pt` file.
+
+[//]: # (marker_int8_dynamic)
+[//]: # (marker_int8_dynamic)
+
+#### Deployment
+
+For deployment, the INT8 model is loaded from the local file and can be used directly on the inference.
+
+Follow the steps below:
+
+1. Import `intel_extension_for_pytorch` as `ipex`.
+2. Load the INT8 model from the saved file.
+3. Run inference.
+
+[//]: # (marker_int8_deploy)
+[//]: # (marker_int8_deploy)
+
+oneDNN provides [oneDNN Graph Compiler](https://github.com/oneapi-src/oneDNN/tree/dev-graph-preview4/doc#onednn-graph-compiler) as a prototype feature that could boost performance for selective topologies. No code change is required. Install <a class="reference external" href="installation.md#installation_onednn_graph_compiler">a binary</a> with this feature enabled. We verified this feature with `Bert-large`, `bert-base-cased`, `roberta-base`, `xlm-roberta-base`, `google-electra-base-generator` and `google-electra-base-discriminator`.
+
+## C++
+
+To work with libtorch, C++ library of PyTorch, Intel® Extension for PyTorch\* provides its C++ dynamic library as well. The C++ library is supposed to handle inference workload only, such as service deployment. For regular development, use the Python interface. Unlike using libtorch, no specific code changes are required. Compilation follows the recommended methodology with CMake. Detailed instructions can be found in [PyTorch tutorial](https://pytorch.org/tutorials/advanced/cpp_export.html#depending-on-libtorch-and-building-the-application).
+
+During compilation, Intel optimizations will be activated automatically once C++ dynamic library of Intel® Extension for PyTorch\* is linked.
+
+The example code below works for all data types.
+
+**example-app.cpp**
+
+[//]: # (marker_cppsdk_sample)
+[//]: # (marker_cppsdk_sample)
+
+**CMakeLists.txt**
+
+[//]: # (marker_cppsdk_cmake)
+[//]: # (marker_cppsdk_cmake)
+
+**Command for compilation**
+
+```bash
+$ cd examples/cpu/inference/cpp
+$ mkdir build
+$ cd build
+$ cmake -DCMAKE_PREFIX_PATH=<LIBPYTORCH_PATH> ..
+$ make
+```
+
+If *Found IPEX* is shown as with a dynamic library path, the extension had been linked into the binary. This can be verified with Linux command *ldd*.
+
+```bash
+$ cmake -DCMAKE_PREFIX_PATH=/workspace/libtorch ..
+-- The C compiler identification is GNU 11.2.1
+-- The CXX compiler identification is GNU 11.2.1
+-- Detecting C compiler ABI info
+-- Detecting C compiler ABI info - done
+-- Check for working C compiler: /usr/bin/cc - skipped
+-- Detecting C compile features
+-- Detecting C compile features - done
+-- Detecting CXX compiler ABI info
+-- Detecting CXX compiler ABI info - done
+-- Check for working CXX compiler: /usr/bin/c++ - skipped
+-- Detecting CXX compile features
+-- Detecting CXX compile features - done
+CMake Warning at /workspace/libtorch/share/cmake/Torch/TorchConfig.cmake:22 (message):
+  static library kineto_LIBRARY-NOTFOUND not found.
+Call Stack (most recent call first):
+  /workspace/libtorch/share/cmake/Torch/TorchConfig.cmake:127 (append_torchlib_if_found)
+  /workspace/libtorch/share/cmake/IPEX/IPEXConfig.cmake:84 (FIND_PACKAGE)
+  CMakeLists.txt:4 (find_package)
+
+
+-- Found Torch: /workspace/libtorch/lib/libtorch.so
+-- Found IPEX: /workspace/libtorch/lib/libintel-ext-pt-cpu.so
+-- Configuring done
+-- Generating done
+-- Build files have been written to: examples/cpu/inference/cpp/build
+
+$ ldd example-app
+        ...
+        libtorch.so => /workspace/libtorch/lib/libtorch.so (0x00007f3cf98e0000)
+        libc10.so => /workspace/libtorch/lib/libc10.so (0x00007f3cf985a000)
+        libintel-ext-pt-cpu.so => /workspace/libtorch/lib/libintel-ext-pt-cpu.so (0x00007f3cf70fc000)
+        libtorch_cpu.so => /workspace/libtorch/lib/libtorch_cpu.so (0x00007f3ce16ac000)
+        ...
+        libdnnl_graph.so.0 => /workspace/libtorch/lib/libdnnl_graph.so.0 (0x00007f3cde954000)
+        ...
+```
+
+## Model Zoo
+
+Use cases that had already been optimized by Intel engineers are available at [Model Zoo for Intel® Architecture](https://github.com/IntelAI/models/tree/pytorch-r2.0.100-models). A bunch of PyTorch use cases for benchmarking are also available on the [GitHub page](https://github.com/IntelAI/models/tree/pytorch-r2.0.100-models/benchmarks#pytorch-use-cases). You can get performance benefits out-of-box by simply running scipts in the Model Zoo.
diff --git a/docs/tutorials/features.rst b/docs/tutorials/features.rst
new file mode 100644
index 00000000..062750ea
--- /dev/null
+++ b/docs/tutorials/features.rst
@@ -0,0 +1,212 @@
+Features
+========
+
+Ease-of-use Python API
+----------------------
+
+With only two or three clauses added to your original code, Intel® Extension for PyTorch\* provides simple frontend Python APIs and utilities to get performance optimizations such as graph optimization and operator optimization.
+
+Check the `API Documentation`_ for details of API functions. `Examples <examples.md>`_ are also available.
+
+.. note::
+
+  The package name used when you import Intel® Extension for PyTorch\* changed
+  from ``intel_pytorch_extension`` (for versions 1.2.0 through 1.9.0) to
+  ``intel_extension_for_pytorch`` (for versions 1.10.0 and later). Use the
+  correct package name depending on the version you are using.
+
+Here are detailed discussions of specific feature topics, summarized in the rest
+of this document:
+
+torch.compile (Experimental, *NEW feature from 2.0.0*)
+------------------------------------------------------
+
+PyTorch* 2.0 introduces a new feature, `torch.compile`, to speed up PyTorch* code. It makes PyTorch code run faster by JIT-compiling PyTorch code into optimized kernels, all while requiring minimal code changes. Intel® Extension for PyTorch\* enables a backend, `ipex`, in the `torch.compile` to optimize generation of the graph model.
+
+Usage is as simple as importing Intel® Extension for PyTorch\* and setting `backend` parameter of the `torch.compile` to `ipex`. While optimizations with `torch.compile` applies to backend, invocation of `ipex.optimize` function is highly recommended as well to apply optimizations in frontend.
+
+.. code-block:: python
+
+   import torch
+   import intel_extension_for_pytorch as ipex
+   ...
+   model = ipex.optimize(model)
+   model = torch.compile(model, backend='ipex')
+
+ISA Dynamic Dispatching
+-----------------------
+
+Intel® Extension for PyTorch\* features dynamic dispatching functionality to automatically adapt execution binaries to the most advanced instruction set available on your machine.
+
+For more detailed information, check `ISA Dynamic Dispatching <features/isa_dynamic_dispatch.md>`_.
+
+.. toctree::
+   :hidden:
+   :maxdepth: 1
+
+   features/isa_dynamic_dispatch
+
+Auto Channels Last
+------------------
+
+Comparing to the default NCHW memory format, using channels_last (NHWC) memory format could further accelerate convolutional neural networks. In Intel® Extension for PyTorch*, NHWC memory format has been enabled for most key CPU operators. More detailed information is available at `Channels Last <features/nhwc.md>`_.
+
+Intel® Extension for PyTorch* automatically converts a model to channels last memory format when users optimize the model with `ipex.optimize(model)`. With this feature users won't need to manually apply `model=model.to(memory_format=torch.channels_last)` any more. More detailed information is available at `Auto Channels Last <features/auto_channels_last.md>`_.
+
+.. toctree::
+   :hidden:
+   :maxdepth: 1
+
+   features/nhwc
+   features/auto_channels_last
+
+Auto Mixed Precision (AMP)
+--------------------------
+
+Low precision data type BFloat16 has been natively supported on 3rd Generation Xeon® Scalable Processors (aka Cooper Lake) with AVX512 instruction set. It will also be supported on the next generation of Intel® Xeon® Scalable Processors with Intel® Advanced Matrix Extensions (Intel® AMX) instruction set providing further boosted performance. The support of Auto Mixed Precision (AMP) with BFloat16 for CPU and BFloat16 optimization of operators has been enabled in Intel® Extension for PyTorch\*, and partially upstreamed to PyTorch master branch. These optimizations will be landed in PyTorch master through PRs that are being submitted and reviewed.
+
+For more detailed information, check `Auto Mixed Precision (AMP) <features/amp.md>`_.
+
+Bfloat16 computation can be conducted on platforms with AVX512 instruction set. On platforms with `AVX512 BFloat16 instruction <https://www.intel.com/content/www/us/en/developer/articles/technical/intel-deep-learning-boost-new-instruction-bfloat16.html>`_, there will be an additional performance boost.
+
+.. toctree::
+   :hidden:
+   :maxdepth: 1
+
+   features/amp
+
+Graph Optimization
+------------------
+
+To further optimize TorchScript performance, Intel® Extension for PyTorch\* supports transparent fusion of frequently used operator patterns such as Conv2D+ReLU and Linear+ReLU.
+For more detailed information, check `Graph Optimization <features/graph_optimization.md>`_.
+
+Compared to eager mode, graph mode in PyTorch normally yields better performance from optimization methodologies such as operator fusion. Intel® Extension for PyTorch* provides further optimizations in graph mode. We recommend you take advantage of Intel® Extension for PyTorch* with `TorchScript <https://pytorch.org/docs/stable/jit.html>`_. You may wish to run with the `torch.jit.trace()` function first, since it generally works better with Intel® Extension for PyTorch* than using the `torch.jit.script()` function. More detailed information can be found at the `pytorch.org website <https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html#tracing-modules>`_.
+
+.. toctree::
+   :hidden:
+   :maxdepth: 1
+
+   features/graph_optimization
+
+Operator Optimization
+---------------------
+
+Intel® Extension for PyTorch* also optimizes operators and implements several customized operators for performance boosts. A few ATen operators are replaced by their optimized counterparts in Intel® Extension for PyTorch* via the ATen registration mechanism. Some customized operators are implemented for several popular topologies. For instance, ROIAlign and NMS are defined in Mask R-CNN. To improve performance of these topologies, Intel® Extension for PyTorch* also optimized these customized operators.
+
+.. currentmodule:: intel_extension_for_pytorch.nn
+.. autoclass:: FrozenBatchNorm2d
+
+.. currentmodule:: intel_extension_for_pytorch.nn.functional
+.. autofunction:: interaction
+
+.. currentmodule:: intel_extension_for_pytorch.nn.modules
+.. autoclass:: MergedEmbeddingBag
+.. autoclass:: MergedEmbeddingBagWithSGD
+
+**Auto kernel selection** is a feature that enables users to tune for better performance with GEMM operations. It is provided as parameter –auto_kernel_selection, with boolean value, of the ipex.optimize() function. By default, the GEMM kernel is computed with oneMKL primitives. However, under certain circumstances oneDNN primitives run faster. Users are able to set –auto_kernel_selection to True to run GEMM kernels with oneDNN primitives.” -> "We aim to provide good default performance by leveraging the best of math libraries and enabled weights_prepack, and it has been verified with broad set of models. If you would like to try other alternatives, you can use auto_kernel_selection toggle in ipex.optimize to switch, and you can disable weights_preack in ipex.optimize if you are concerning the memory footprint more than performance gain. However in majority cases, keeping default is what we recommend.
+
+Optimizer Optimization
+----------------------
+
+Optimizers are one of key parts of the training workloads. Intel® Extension for PyTorch* brings two types of optimizations to optimizers:
+
+1.	Operator fusion for the computation in the optimizers.
+2.	SplitSGD for BF16 training, which reduces the memory footprint of the master weights by half.
+
+
+For more detailed information, check `Optimizer Fusion <features/optimizer_fusion.md>`_ and `Split SGD <features/split_sgd.html>`_ 
+
+.. toctree::
+   :hidden:
+   :maxdepth: 1
+
+   features/optimizer_fusion
+   features/split_sgd
+
+Runtime Extension
+-----------------
+
+Intel® Extension for PyTorch* Runtime Extension provides PyTorch frontend APIs for users to get finer-grained control of the thread runtime and provides:
+
+- Multi-stream inference via the Python frontend module MultiStreamModule.
+- Spawn asynchronous tasks from both Python and C++ frontend.
+- Program core bindings for OpenMP threads from both Python and C++ frontend.
+
+.. note:: Intel® Extension for PyTorch* Runtime extension is still in the experimental stage. The API is subject to change. More detailed descriptions are available in the `API Documentation <api_doc.html>`_.
+
+For more detailed information, check `Runtime Extension <features/runtime_extension.md>`_.
+
+.. toctree::
+   :hidden:
+   :maxdepth: 1
+
+   features/runtime_extension
+
+INT8 Quantization
+-----------------
+
+Intel® Extension for PyTorch* provides built-in quantization recipes to deliver good statistical accuracy for most popular DL workloads including CNN, NLP and recommendation models.
+
+Users are always recommended to try quantization with the built-in quantization recipe first with Intel® Extension for PyTorch* quantization APIs. For even higher accuracy demandings, users can try with separate `recipe tuning APIs <features/int8_recipe_tuning_api.md>`_. The APIs are powered by Intel® Neural Compressor to take advantage of its tuning feature.
+
+Check more detailed information for `INT8 Quantization <features/int8_overview.md>`_ and `INT8 recipe tuning API guide (Experimental, *NEW feature in 1.13.0*) <features/int8_recipe_tuning_api.md>`_.
+
+.. toctree::
+   :hidden:
+   :maxdepth: 1
+
+   features/int8_overview
+   features/int8_recipe_tuning_api
+
+Codeless Optimization (Experimental, *NEW feature from 1.13.0*)
+---------------------------------------------------------------
+
+This feature enables users to get performance benefits from Intel® Extension for PyTorch* without changing Python scripts. It hopefully eases the usage and has been verified working well with broad scope of models, though in few cases there could be small overhead comparing to applying optimizations with Intel® Extension for PyTorch* APIs.
+
+For more detailed information, check `Codeless Optimization <features/codeless_optimization.md>`_.
+
+.. toctree::
+   :hidden:
+   :maxdepth: 1
+
+   features/codeless_optimization.md
+
+Graph Capture (Experimental, *NEW feature from 1.13.0*)
+-------------------------------------------------------
+
+Since graph mode is key for deployment performance, this feature automatically captures graphs based on set of technologies that PyTorch supports, such as TorchScript and TorchDynamo. Users won't need to learn and try different PyTorch APIs to capture graphs, instead, they can turn on a new boolean flag `--graph_mode` (default off) in `ipex.optimize` to get the best of graph optimization.
+
+For more detailed information, check `Graph Capture <features/graph_capture.md>`_.
+
+.. toctree::
+   :hidden:
+   :maxdepth: 1
+
+   features/graph_capture
+
+HyperTune (Experimental, *NEW feature from 1.13.0*)
+---------------------------------------------------
+
+HyperTune is an experimental feature to perform hyperparameter/execution configuration searching. The searching is used in various areas such as optimization of hyperparameters of deep learning models. The searching is extremely useful in real situations when the number of hyperparameters, including configuration of script execution, and their search spaces are huge that manually tuning these hyperparameters/configuration is impractical and time consuming. Hypertune automates this process of execution configuration searching for the `launcher <performance_tuning/launch_script.md>`_ and Intel® Extension for PyTorch*.
+
+For more detailed information, check `HyperTune <features/hypertune.md>`_.
+
+.. toctree::
+   :hidden:
+   :maxdepth: 1
+
+   features/hypertune
+
+Fast BERT Optimization (Experimental, *NEW feature from 2.0.0*)
+---------------------------------------------------------------
+
+Intel proposed a technique to speed up BERT workloads. Implementation is integrated into Intel® Extension for PyTorch\*. An API `ipex.fast_bert` is provided for a simple usage.
+
+For more detailed information, check `Fast BERT <features/fast_bert.md>`_.
+
+.. toctree::
+   :hidden:
+   :maxdepth: 1
+
+   features/fast_bert
diff --git a/docs/tutorials/features/amp.md b/docs/tutorials/features/amp.md
new file mode 100644
index 00000000..87981658
--- /dev/null
+++ b/docs/tutorials/features/amp.md
@@ -0,0 +1,102 @@
+Auto Mixed Precision (AMP)
+==========================
+
+## Introduction
+
+`torch.cpu.amp` provides convenience for auto data type conversion at runtime. Deep learning workloads can benefit from lower-precision floating point data types such as `torch.float16` or `torch.bfloat16`, because of its lighter calculation workload and smaller memory usage. Accuracy is sacrificed when using lower-precision floating point data types so there's a trade-off between accuracy and performance. Thus, some operations should use the slower but more accurate`torch.float32`, while others can be converted to use the faster but less accurate `torch.float16` data type. The Auto Mixed Precision (AMP) feature automates the tuning of data type conversions over all operators.
+
+`torch.cpu.amp` only supports `torch.bfloat16`. It is the default lower precision floating point data type when `torch.cpu.amp` is enabled. `torch.cpu.amp` primarily benefits when running on Intel CPU with BFloat16 instruction set support.
+
+## Use Case
+
+The following simple network should show a speedup with mixed precision.
+
+```
+class SimpleNet(torch.nn.Module):
+    def __init__(self):
+        super(SimpleNet, self).__init__()
+        self.conv = torch.nn.Conv2d(64, 128, (3, 3), stride=(2, 2), padding=(1, 1), bias=False)
+
+    def forward(self, x):
+        return self.conv(x)
+```
+
+### Default Precision
+
+Without `torch.cpu.amp`, the network executes all operators with default precision (`torch.float32`).
+```
+model = SimpleNet()
+x = torch.rand(64, 64, 224, 224)
+y = model(x)
+```
+
+### Inference with Imperative Path
+
+`torch.cpu.amp.autocast` is designed to be a context manager that allow scopes of your script to run with mixed precision. In these scopes, operations run in a data type chosen by the `autocast` class to improve performance while maintaining accuracy. See the operations category section for details on what precision the `autocast` class chooses for each operator, and under what circumstances.
+
+```
+model = SimpleNet().eval()
+x = torch.rand(64, 64, 224, 224)
+with torch.cpu.amp.autocast():
+    y = model(x)
+```
+
+### Inference with TorchScript Path
+
+`torch.cpu.amp.autocast` can be used with `torch.jit.trace` to apply graph optimization. Due to PyTorch limitation, only `torch.jit.trace` is supported.
+
+```
+model = SimpleNet().eval()
+x = torch.rand(64, 64, 224, 224)
+with torch.cpu.amp.autocast():
+    model = torch.jit.trace(model, x)
+    model = torch.jit.freeze(model)
+    y = model(x)
+```
+
+### Training Support
+
+`torch.cpu.amp.autocast` can be used in training to improve performance.
+
+```
+model = SimpleNet()
+optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
+for images, label in train_loader():
+    with torch.cpu.amp.autocast():
+        loss = criterion(model(images), label)
+    loss.backward()
+    optimizer.step()
+```
+
+## Autocast Op Reference
+
+### Op Eligibility
+
+Ops that run in `float64` or non-floating-point dtypes are not eligible for mixed precision, and will run in these types whether or not autocast is enabled.
+
+Only out-of-place ops and Tensor methods are eligible for mixed precision. In-place variants and calls that explicitly supply an `out=...` Tensor
+are allowed in autocast-enabled regions, but won't go through autocasting. For example, in an autocast-enabled region `a.addmm(b, c)` can autocast, but `a.addmm_(b, c)` and `a.addmm(b, c, out=d)` cannot. For best performance and stability, use out-of-place ops in autocast-enabled regions.
+
+### Op-Specific Behavior
+
+The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a `torch.nn.Module`, as a function, or as a `torch.Tensor` method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.
+
+Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they're downstream from autocasted ops.
+
+If an op is unlisted, we assume it's numerically stable in `bfloat16`. If you believe that an unlisted op is numerically unstable in `bfloat16`, file a [GitHub issue](https://github.com/intel/intel-extension-for-pytorch/issues).
+
+#### Ops that can autocast to `bfloat16`
+
+`conv1d`, `conv2d`, `conv3d`, `conv_transpose1d`, `conv_transpose2d`, `conv_transpose3d`, `bmm`, `mm`, `baddbmm`, `addmm`, `addbmm`, `linear`, `matmul`, `conv_tbc`, `group_norm`, `_native_multi_head_attention`
+
+#### Ops that can autocast to `float32`
+
+`avg_pool3d`, `binary_cross_entropy`, `grid_sampler`, `polar`, `prod`, `quantile`, `nanquantile`, `stft`, `cdist`, `trace`, `view_as_complex`, `cholesky`, `cholesky_inverse`, `cholesky_solve`, `inverse`, `lu_solve`, `matrix_rank`, `orgqr`, `ormqr`, `pinverse`, `max_unpool2d`, `max_unpool3d`, `adaptive_avg_pool3d`, `reflection_pad1d`, `reflection_pad2d`, `replication_pad1d`, `replication_pad2d`, `replication_pad3d`, `mse_loss`, `cosine_embedding_loss`, `nll_loss`, `nll_loss2d`, `hinge_embedding_loss`, `poisson_nll_loss`, `smooth_l1_loss`, `cross_entropy_loss`, `l1_loss`, `huber_loss`, `margin_ranking_loss`, `soft_margin_loss`, `triplet_margin_loss`, `multi_margin_loss`, `ctc_loss`, `kl_div`, `multilabel_margin_loss`, `binary_cross_entropy_with_logits`, `fft_fft`, `fft_ifft`, `fft_fft2`, `fft_ifft2`, `fft_fftn`, `fft_ifftn`, `fft_rfft`, `fft_irfft`, `fft_rfft2`, `fft_irfft2`, `fft_rfftn`, `fft_irfftn`, `fft_hfft`, `fft_ihfft`, `linalg_cond`, `linalg_matrix_rank`, `linalg_solve`, `linalg_cholesky`, `linalg_svdvals`, `linalg_eigvals`, `linalg_eigvalsh`, `linalg_inv`, `linalg_householder_product`, `linalg_tensorinv`, `linalg_tensorsolve`, `fake_quantize_per_tensor_affine`, `eig`, `geqrf`, `lstsq`, `_lu_with_info`, `qr`, `svd`, `symeig`, `triangular_solve`, `fractional_max_pool2d`, `fractional_max_pool3d`, `adaptive_max_pool3d`, `multilabel_margin_loss_forward`, `linalg_qr`, `linalg_cholesky_ex`, `linalg_svd`, `linalg_eig`, `linalg_eigh`, `linalg_lstsq`, `linalg_inv_ex`
+
+#### Ops that promote to the widest input type
+
+These ops don't require a particular dtype for stability, but take multiple inputs and require that the inputs' dtypes match.  If all of the inputs are `bfloat16`, the op runs in `bfloat16`.  If any of the inputs is `float32`, autocast casts all inputs to `float32` and runs the op in `float32`.
+
+`cat`, `stack`, `index_copy`
+
+Some ops not listed here (e.g., binary ops like `add`) natively promote inputs without autocasting's intervention.  If inputs are a mixture of `bfloat16` and `float32`, these ops run in `float32` and produce `float32` output, regardless of whether autocast is enabled.
diff --git a/docs/tutorials/features/auto_channels_last.md b/docs/tutorials/features/auto_channels_last.md
new file mode 100644
index 00000000..5a563216
--- /dev/null
+++ b/docs/tutorials/features/auto_channels_last.md
@@ -0,0 +1,29 @@
+Auto Channels Last
+==================
+
+Channels last memory format is known to have performance advantage over channels first memory format. Refer to [Channels Last](./nhwc.md) for details.
+Intel® Extension for PyTorch\* automatically converts the model to channels last memory format by default when users optimize their model with `ipex.optimize(model)`.
+
+## Ease-of-use auto channels last API
+#### default
+```python
+model = ipex.optimize(model) # by default, model is channels last
+```
+
+#### enable
+```python
+ipex.enable_auto_channels_last()
+model = ipex.optimize(model) # enable, model is channels last
+```
+
+#### disable
+```python
+ipex.disable_auto_channels_last()
+model = ipex.optimize(model) # disable, model is channels first 
+```
+
+## Known issue 
+For broad models, channels last memory format brings performance boost over channels first memory format. However, for few use cases, this may bring performance regression. If performance regression is observed, we recommend to feed sample input data to `ipex.optimize(model, sample_input=...)`.
+```python
+model = ipex.optimize(model, sample_input=...)
+```
diff --git a/docs/tutorials/features/codeless_optimization.md b/docs/tutorials/features/codeless_optimization.md
new file mode 100644
index 00000000..d8b58954
--- /dev/null
+++ b/docs/tutorials/features/codeless_optimization.md
@@ -0,0 +1,106 @@
+Codeless Optimization (Experimental)
+====================================
+
+This feature aims to get inference performance benefits from Intel® Extension for PyTorch\* without changing code in your python scripts, which can raise Out-of-Box (OOB) experience to get started with Intel® Extension for PyTorch\* easily. Users who already known how to apply optimizations with Intel® Extension for PyTorch\* APIs are not targeted for this feature, due to the inevitable overhead and limitations we mentioned below.
+
+## Motivation
+
+A typical use case of inference as in [transformer](https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/trainer.py#L3187) can be simplified as the code snippet below:
+
+```
+import torch
+model = Model().eval()
+with torch.no_grad():
+    for input in dataloader():
+        model(**input)
+```
+
+To utilize optimizations of Intel® Extension for PyTorch\* for optimum performance, several lines code changes are required/recommended.
+
+```
+import torch
+impot intel_extension_for_pytorch as ipex # clause added
+model = Model().eval()
+model = ipex.optimization(model)          # clause added
+with torch.no_grad():
+  with torch.cpu.amp.autocast():          # clause added for running with BFloat16 (Optional)
+    input = ...                           # clause added for TorchScript (Optional, but recommended) 
+    model = torch.jit.trace(input)        # clause added for TorchScript (Optional, but recommended) 
+    model = torch.jit.freeze()            # clause added for TorchScript (Optional, but recommended) 
+    for input in dataloader():
+      model(**input)
+```
+
+With this feature, code changes above done manually are not required any more. Intel® Extension for PyTorch\* optimizations will be applied automatically during execution in a monkey patch way. 
+* Automatically import `intel_extension_for_pytorch` package: It applies Intel® Extension for PyTorch\* optimizations, such as: `torch.embedding_bag`, `torch.cpu.amp.autocast`. It also registers Intel® Extension for PyTorch\* JIT fusion pass and thus benefits the graph mode inference performance.
+* Automatically apply `ipex.optimize()` function. Only features enabled by default parameter values are supported, such as:
+    * Auto generate FX or Jit Graph.
+    * Auto Channel Last convert.
+    * Conv-Bn folding.
+    * Weight prepack.
+    * Replace dropout with identity.
+    * Optimize LSTM.
+* Automatically apply `torch.cpu.amp.autocast` with BFloat16 data type for inference.
+
+## Example Usage with HuggingFace
+Let's take the [QA case](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) in HuggingFace as an example.
+
+### The origin command with ipex launch
+Here is the command to run with [`ipexrun`](../performance_tuning/launch_script.md).
+```
+clear && ipexrun --memory-allocator default --ninstances 2 --ncores-per-instance 28 run_qa.py --model_name_or_path bert-base-uncased --dataset_name squad --do_eval --per_device_train_batch_size 12 --learning_rate 3e-5 --num_train_epochs 2 --max_seq_length 384 --doc_stride 128 --output_dir /tmp/debug_squad/
+```
+
+### Command to apply ipex optimization for FP32
+Added `--auto-ipex`
+```
+clear && ipexrun --memory-allocator default --ninstances 2 --ncores-per-instance 28 --auto-ipex run_qa.py --model_name_or_path bert-base-uncased --dataset_name squad --do_eval --per_device_train_batch_size 12 --learning_rate 3e-5 --num_train_epochs 2 --max_seq_length 384 --doc_stride 128 --output_dir /tmp/debug_squad/
+```
+
+### Command to apply ipex optimization for BF16
+Added `--auto-ipex --dtype bfloat16`
+```
+clear && ipexrun --memory-allocator default --ninstances 2 --ncores-per-instance 28 --auto-ipex --dtype bfloat16 run_qa.py --model_name_or_path bert-base-uncased --dataset_name squad --do_eval --per_device_train_batch_size 12 --learning_rate 3e-5 --num_train_epochs 2 --max_seq_length 384 --doc_stride 128 --output_dir /tmp/debug_squad/
+```
+
+## Use Case not supported
+### Module uses forward method explicitly instead of the `__call__` attr 
+```
+import torch
+class DummyModule(torch.nn.Module):
+    def __init__(self,):
+        super(DummyModule, self).__init__()
+        self.input1 = torch.randn(1, 3, 224, 224)
+        self.conv = torch.nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
+        self.bn = torch.nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
+
+    def forward(self, x):
+        return self.bn(self.conv(x))
+
+    def customized_forward(self, x):
+        return self.bn(self.conv(x))
+
+# Method1 will success
+DummyModule()(input)
+# Method2 will fail to apply ipex.optimize in the top-level model
+DummyModule().customized_forward(input)
+```
+If a model uses forward method explicitly instead of the `__call__` attr, we are unable to hook the execution of this model. As result, we are unable to auto apply the optimizations to this `DummyModule()`.
+
+### Already using `ipex.optimize`
+User already invokes `ipex.optimize` in script is not targeted for this feature. The behaviour as repeated invoking of `ipex.optimize` is not defined. The second invoking of `ipex.optimize` for the same module will fail with error message to avoid this behaviour.
+
+### Already using Jit Trace
+For Jit trace case (as below example code) is not planned to support at first stage:
+```
+import torch
+model = Model().eval()
+traced_model = torch.jit.trace(model, x).eval()
+traced_model = torch.jit.freeze(traced_model)
+with torch.no_grad():
+    for input in dataloader():
+        traced_model(input)
+```
+For 2 reasons:
+* The auto graph mode support has already been included in `ipex.optimize` with graph first API in 1.13.
+* Extra launch parameters and Monkey patches are needed to support above case. We will focus on the feasibility of first use case in TorchVision and HuggingFace workloads. 
diff --git a/docs/tutorials/features/fast_bert.md b/docs/tutorials/features/fast_bert.md
new file mode 100644
index 00000000..f84bbcc7
--- /dev/null
+++ b/docs/tutorials/features/fast_bert.md
@@ -0,0 +1,19 @@
+Fast BERT (Experimental)
+========================
+
+### Feature Description
+
+Intel proposed a technique to speed up BERT workloads. Implementation leverages the idea from [*Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads*](https://arxiv.org/pdf/2104.05755.pdf).
+
+The Implementation is integrated into Intel® Extension for PyTorch\*. BERT could benefit from this new technique, for both training and inference.
+
+### Prerequisite
+
+- Transformers 4.6.0 ~ 4.20.0
+
+### Usage Example
+
+An API `ipex.fast_bert` is provided for a simple usage. Usage of this API follows the pattern of `ipex.optimize` function. More detailed description of API is available at [Fast BERT API doc](../api_doc)
+
+[//]: # (marker_inf_bert_fast_bf16)
+[//]: # (marker_inf_bert_fast_bf16)
diff --git a/docs/tutorials/features/graph_capture.md b/docs/tutorials/features/graph_capture.md
new file mode 100644
index 00000000..7be2f275
--- /dev/null
+++ b/docs/tutorials/features/graph_capture.md
@@ -0,0 +1,11 @@
+Graph Capture (Experimental)
+============================
+
+### Feature Description
+
+This feature automatically applies a combination of TorchScript trace technique and TorchDynamo to try to generate a graph model, for providing a good user experience while keeping execution fast. Specifically, the process tries to generate a graph with TorchScript trace functionality first. In case of generation failure or incorrect results detected, it changes to TorchDynamo with TorchScript backend. Failure of the graph generation with TorchDynamo triggers a warning message. Meanwhile the generated graph model falls back to the original one. I.e. the inference workload runs in eager mode. Users can take advantage of this feature through a new knob `--graph_mode` of the `ipex.optimize()` function to automatically run into graph mode.
+
+### Usage Example
+
+[//]: # (marker_feature_graph_capture)
+[//]: # (marker_feature_graph_capture)
diff --git a/docs/tutorials/features/graph_optimization.md b/docs/tutorials/features/graph_optimization.md
new file mode 100644
index 00000000..94e4bcf1
--- /dev/null
+++ b/docs/tutorials/features/graph_optimization.md
@@ -0,0 +1,135 @@
+Graph Optimization
+==================
+
+Most Deep Learning models could be described as a DAG (directed acyclic graph). Optimizing a deep learning model from a graph perspective is straight forward. Compared to the operator optimization and algorithm optimization, the graph optimization is at a higher level. It covers not only the graph but also the runtime. From the operator perspective, the graph optimization contains the operator fusing and constant folding. From the runtime perspective, the graph optimization contains the operator scheduling, computation resources management, and memory management.
+
+The Intel® Extension for PyTorch\* focuses on operator related graph optimizations. The extension also provides some experimental features for the related runtime optimizations. Refer to the runtime extension for more details about runtime optimization.
+
+## Ease-of-use graph optimization API
+The graph optimizations of Intel® Extension for PyTorch\* are enabled by default. Users can disable it by calling:
+```
+ipex.enable_onednn_fusion(False)
+```
+
+### FP32 and BF16 models
+
+[//]: # (marker_feature_graph_optimization_fp32_bf16)
+[//]: # (marker_feature_graph_optimization_fp32_bf16)
+
+Compared to the original code, the model launcher needs to add a few lines of code and the extension will automatically accelerate the model. Regarding the RN50, the extension will automatically fuse the Conv + ReLU and Conv + Sum + ReLU as ConvReLU and ConvSumReLU. If you check the output of `graph_for`, you will observe the fused operators.
+
+### INT8 models
+
+[//]: # (marker_feature_graph_optimization_int8)
+[//]: # (marker_feature_graph_optimization_int8)
+
+## Methodology
+### Fusion
+#### FP32 and BF16 fusion patterns
+- Conv1D/Conv2D/Conv3D/Linear/ConvTranspose2D/ConvTranspose3D + Abs/Clamp/Elu/Exp/GELU/HardTanh/HardSwish/Log/Mish/Sigmoid/Pow/ReLU/Round/Sqrt/Square/Tanh/Leaky_ReLU/SiLU
+- Conv1D/Conv2D/Conv3D/Linear/ConvTranspose2D/ConvTranspose3D + Sigmoid + MUL
+- Conv1D/Conv2D/Conv3D/Linear + SUM
+- Conv1D/Conv2D/Conv3D + SUM + ReLU
+- Add + LayerNorm
+- Div + Add + Softmax
+- Linear + Linear + Linear
+- View + Transpose + Contiguous + View
+
+#### INT8 fusion patterns
+The `ipex.quantization.convert(model, conf, inputs)` API will convert an FP32 `torch.nn.Module` to a quantized JIT ScriptModule according to the given quantization recipes.
+
+For example, for a FP32 model of one single convolution, the graph before and after conversion will be:
+![image](../../../images/graph_optimization/int8_pattern.png)
+
+The oneDNN graph backend will select `dequantize` and `convolution` into one partition. During execution, this partition will execute a convolution with int8 as input and fp32 as output.
+
+Here listed all the currently supported int8 patterns in Intel® Extension for PyTorch\* using oneDNN graph backend:
+
+1. Conv/Linear/Matmul related fusion patterns
+   ```
+                                            |
+                                        [Quantize]*
+                   |                        |
+              Dequantize                Dequantize
+                   \                      /
+              Conv1D/Conv2D/Conv3D/Linear/MatMul
+                                |
+            [Abs/Elu/GELU/HardTanh/Leaky_ReLU/Sigmoid/
+       ReLU/Sqrt/Square/Tanh/[Dequantize+Add]*[0,1] ]*[0,3]
+                                |
+                            [Quantize]*
+                                |
+   ```
+
+   ```
+        |              |
+      Dequantize   Dequantize
+         \___      ___/
+             MatMul
+                \    /
+                Divide
+                   \   /
+                   [Add]*
+                     |
+   ```
+
+2. Non-Conv/Linear/Matmul related fusion patterns
+   ```
+              |
+          Dequantize
+              |
+          MaxPool2D
+              |
+           Quantize
+   ```
+3. INT8-BF16 mixed-precision fusion patterns
+   ```
+        |              |
+      Dequantize   Dequantize
+        |              |
+       To             To
+         \___      ___/
+             MatMul
+                \      /
+                [Divide]*
+                    \     /
+                     [Add]*
+                       |
+   ```
+
+   ```
+        |              |
+      Dequantize   Dequantize
+        |              |
+       To             To
+         \___      ___/
+             MatMul
+               |
+             [GeLU]*
+               |
+              To
+               |
+            Quantize
+               |
+   ```
+
+   ```
+        |              |
+      Dequantize   Dequantize
+        |              |
+        To            To     Dequantize
+         \___      ___/          |
+             MatMul              To
+                \_____        ___/
+                       [Add]*
+                         |
+   ```
+
+
+### Folding
+Stock PyTorch provids constant propagation and BatchNormalization folding. These optimizations are automatically applied to the jit model by invoking `torch.jit.freeze`. Take the Resnet50 as an example:
+
+[//]: # (marker_feature_graph_optimization_folding)
+[//]: # (marker_feature_graph_optimization_folding)
+
+If the model owner does not invoke the `torch.jit.freeze`, the `BatchNormalization` still exists on the graph. Otheriwse, the `BatchNormalization` will be folded on the graph to save the compuation and then improve the performance. Refer to the [Constant Folding Wikipedia page](https://en.wikipedia.org/wiki/Constant_folding) for more details.
diff --git a/docs/tutorials/features/hypertune.md b/docs/tutorials/features/hypertune.md
new file mode 100644
index 00000000..f6b66a45
--- /dev/null
+++ b/docs/tutorials/features/hypertune.md
@@ -0,0 +1,120 @@
+HyperTune (Experimental)
+========================
+
+![HyperTune](../../../images/hypertune/hypertune.png)
+
+HyperTune is an experimental feature to perform hyperparameter/execution configuration searching. The searching is used in various areas such as optimization of hyperparameters of deep learning models. The searching is extremely useful in real situations when the number of hyperparameters, including configuration of script execution, and their search spaces are huge that manually tuning these hyperparameters/configuration is impractical and time consuming. Hypertune automates this process of execution configuration searching for the [launcher](../performance_tuning/launch_script.md) and Intel® Extension for PyTorch\*.
+
+## Usage of Hypertune
+```
+python -m intel_extension_for_pytorch.cpu.hypertune --conf-file <your_conf_file> <your_python_script> [args]
+```
+
+There are two things to provide Hypertune (1) `<your_conf_file>` .yaml file to define the hyperparameters and their search spaces (2) `<your_python_script>` as an optimization function.
+
+### `your_conf_file`
+The .yaml file is used to define configuration of Hypertune. There are two main information needed: (1) hyperparameters to tune and their search spaces (2) tuning strategy. See comments below together with a sample .yaml file.
+
+```
+tuning:                                                        # optional.
+  strategy: grid                                               # optional. The tuning strategy. Default is grid. Must be one of {grid, random}.
+  max_trials: 100                                              # optional. Allowed number of trials. Default is 100. If given time, set max_trials to product of length of all search spaces to try all possible combinations of hyperparameters.
+
+output_dir: /path/to/saving/directory                          # optional. Directory to which the tuning history will be saved in record.csv file. Default is current working directory.
+
+hyperparams:                                                   # mandatory.
+  launcher:                                                    # optional.
+    hp: ['ncores_per_instance', 'ninstances']                  # mandatory. Mandatory if hyperparams.launcher is specified. Specify the launcher hyperparameters to tune.
+    ncores_per_instance: all_physical_cores                    # optional.  Search space of ncores_per_instance if chosen to tune. If not defined, default search space of ncore_per_instance is used.
+    ninstances:  [1]                                           # optional.  Search space of ninstances if chosen to tune. If not defined, default search space of ninstances is used.
+```
+
+### Hyperparameters
+#### Launcher Hyperparameters
+Currently hypertune tunes for the following launcher hyperparameters:
+
+| hyperparameter | default value | default search space | search space format |
+| :-- | :--: | :--: | :--: |
+| ```ncores_per_instance``` | -1 | `all_logical_cores` | `str or list of int. str must be one of {'all_logical_cores', 'all_physical_cores'}` |
+| ```ninstances``` | -1 | `all_logical_cores` | `str or list of int. str must be one of {'all_logical_cores', 'all_physical_cores'}` |
+| ```use_all_nodes``` | True | `[True, False] if num_nodes > 1 else [True]` | `list of bool` |
+| ```use_logical_cores``` | False | `[True, False] if is_hyperthreading_enabled else [False]` | `list of bool` |
+| ```disable_numactl``` | False | `[True, False]` | `list of bool` |
+| ```disable_iomp``` | False | `[True, False]` | `list of bool` |
+| ```malloc``` | tc | `['tc', 'je', 'pt']` | `list of str. str must be in {'tc', 'je', 'pt'}` |
+
+### Defining hyperparameters and their search spaces
+#### 1. Defining hyperparameters to tune:
+
+List the hyperparameters to tune in `hp`. For example, to tune all launcher hyperparameters:
+```
+hyperparams:
+  launcher:
+    hp: ['ncores_per_instance', 'ninstances', 'use_all_nodes', 'use_logical_cores', 'disable_numactl', 'disable_iomp', 'malloc']
+```
+
+For example, to tune only launcher `ncores_per_instance`:
+```
+hyperparams:
+  launcher:
+    hp: ['ncores_per_instance']
+```
+All other launcher hyperparameters (`ninstances`, `use_all_nodes`, `use_logical_core`, `disable_numactl`, `disable_iomp`, `malloc`) will not be tuned and instead will use the default value defined in the previous section.
+
+#### 2. Defining the search spaces of the hyperparameters:
+
+#### Default search space
+
+If you don't specify the search space of a hyperparamter, then the default search space defined in the previous section will be used for the hyperparameters defined in `hp`. For example,
+```
+hyperparams:
+  launcher:
+    hp: ['malloc']
+```
+`malloc` will be tuned using its default search space, `['tc', 'je', 'pt']`. All other launcher hyperparamters (`ncores_per_instance`, `ninstances`, `use_all_nodes`, `use_logical_cores`, `disable_numactl`, `disable_iomp`) will not be tuned and instead will use their default values.
+
+#### User defined search space
+
+Specify the search space of a hyperparameter. For example,
+```
+hyperparams:
+  launcher:
+    hp: ['ncores_per_instance', 'ninstances', 'malloc']
+    ninstances: [1]
+    ncore_per_instance: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
+```
+`ninstances` and `ncores_per_instance` will use user defined spaces `[1]` and `[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]` respectively. `malloc` will use its default search space, `['tc', 'je', 'pt']`.
+
+### `<your_python_script>`
+This is the script as an optimization function.
+- Step 1. Print the objective(s) you want to optimize. Make sure this is just an int or float to be minimized or maximized.
+- Step 2. Just before the objective(s), add print statement(s) of the `@hypertune {'name': str, 'higher_is_better': bool, 'target_val': int or float}`.
+```
+'name'                                     # mandatory. The name of your objective function.
+'higher_is_better'                         # optional. True if objective function is to be maximized, False if to be minimized. Default is False.
+'target_val'                               # optional. Target value of the objective function. Default is -float('inf')
+```
+
+Have a look at the [example script](https://github.com/intel/intel-extension-for-pytorch/tree/v2.0.100+cpu/intel_extension_for_pytorch/cpu/hypertune/example/resnet50.py).
+
+## Usage Examples
+
+**Tuning `ncores_per_instance` for minimum `latency`**
+
+Suppose we want to tune `ncores_per_instance` for a single instance to minimize latency for resnet50 on a machine with two Intel(R) Xeon(R) Platinum 8180M CPUs. Each socket has 28 physical cores and another 28 logical cores.
+
+Run the following command with [example.yaml](https://github.com/intel/intel-extension-for-pytorch/tree/v2.0.100+cpu/intel_extension_for_pytorch/cpu/hypertune/example/example.yaml) and [resnet50.py](https://github.com/intel/intel-extension-for-pytorch/tree/v2.0.100+cpu/intel_extension_for_pytorch/cpu/hypertune/example/resnet50.py):
+```
+python -m intel_extension_for_pytorch.cpu.hypertune --conf_file <hypertune_directory>/example/example.yaml <hypertune_directory>/example/resnet50.py
+```
+
+Once search completes, it will print to terminal the best tune result and best tune configuration found. Below is an output for this example:
+```
+Best configuration found is: {'ncores_per_instance': 15, 'ninstances': 1, 'use_all_nodes': True, 'use_logical_cores': False, 'disable_numactl': False, 'disable_iomp': False, 'malloc': 'tc'}
+latency: 12.339081764221191
+```
+15 `ncores_per_instance` gave the minimum latency.
+
+You will also find the tuning history in `<output_dir>/record.csv`. You can take [a sample csv file](https://github.com/intel/intel-extension-for-pytorch/tree/v2.0.100+cpu/intel_extension_for_pytorch/cpu/hypertune/example/record.csv) as a reference.
+
+Hypertune can also optimize multi-objective function. Add as many objectives as you would like to your script.
diff --git a/docs/tutorials/features/int8_overview.md b/docs/tutorials/features/int8_overview.md
new file mode 100644
index 00000000..27c45c51
--- /dev/null
+++ b/docs/tutorials/features/int8_overview.md
@@ -0,0 +1,146 @@
+Intel® Extension for PyTorch\* optimizations for quantization
+=============================================================
+
+The quantization functionality in Intel® Extension for PyTorch\* currently only supports post-training quantization. This tutorial introduces how the quantization works in the Intel® Extension for PyTorch\* side.
+
+We fully utilize Pytorch quantization components as much as possible, such as PyTorch [Observer method](https://pytorch.org/docs/1.11/quantization-support.html#torch-quantization-observer). To make a PyTorch user be able to easily use the quantization API, API for quantization in Intel® Extension for PyTorch\* is very similar to those in PyTorch. Intel® Extension for PyTorch\* quantization supports a default recipe to automatically decide which operators should be quantized or not. This brings a satisfying performance and accuracy tradeoff.
+
+## Static Quantization
+
+```python
+import intel_extension_for_pytorch as ipex
+from intel_extension_for_pytorch.quantization import prepare, convert
+```
+
+### Define qconfig
+
+Using the default qconfig(recommended):
+
+```python
+qconfig = ipex.quantization.default_static_qconfig
+# equal to
+# QConfig(activation=HistogramObserver.with_args(reduce_range=False),
+#         weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric)) 
+```
+
+or define your own qconfig as:
+
+```python
+from torch.ao.quantization import MinMaxObserver, PerChannelMinMaxObserver, QConfig
+qconfig = QConfig(activation=MinMaxObserver.with_args(qscheme=torch.per_tensor_affine, dtype=torch.quint8),
+                  weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric))
+```
+
+Note: we fully use PyTorch [observer methonds](https://pytorch.org/docs/stable/quantization-support.html#torch-quantization-observer), so you can use a different PyTorch obsever methond to define the [QConfig](https://pytorch.org/docs/1.11/generated/torch.quantization.qconfig.QConfig.html). For weight observer, we only support **torch.qint8** dtype now.
+
+**Suggestion**:
+
+1. For activation observer, if using **qscheme** as **torch.per_tensor_affine**, **torch.quint8** is preferred. If using **qscheme** as **torch.per_tensor_symmetric**, **torch.qint8** is preferred. For weight observer, setting **qscheme** to **torch.per_channel_symmetric** can get a better accuracy.
+2. If your CPU device doesn't support VNNI, seting the observer's **reduce_range** to **True** can get a better accuracy, such as skylake.
+
+### Prepare Model and Do Calibration
+
+```python
+# prepare model, do conv+bn folding, and init model quant_state.
+user_model = ...
+user_model.eval()
+example_inputs = ..
+prepared_model = prepare(user_model, qconfig, example_inputs=example_inputs, inplace=False)
+
+for x in calibration_data_set:
+    prepared_model(x)
+
+# Optional, if you want to tuning(performance or accuracy), you can save the qparams as json file which
+# including the quantization state, such as scales, zero points and inference dtype.
+# And then you can achange the json file's settings, loading the changed json file
+# to model which will override the model's original quantization's settings.  
+#  
+# prepared_model.save_qconf_summary(qconf_summary = "configure.json")
+# prepared_model.load_qconf_summary(qconf_summary = "configure.json")
+```
+
+### Convert to Static Quantized Model and Deploy
+
+```python
+# make sure the example_inputs's size is same as the real input's size 
+convert_model = convert(prepared_model)
+with torch.no_grad():
+    traced_model = torch.jit.trace(convert_model, example_input)
+    traced_model = torch.jit.freeze(traced_model)
+# for inference 
+y = traced_model(x)
+
+# or save the model to deploy
+
+# traced_model.save("quantized_model.pt")
+# quantized_model = torch.jit.load("quantized_model.pt")
+# quantized_model = torch.jit.freeze(quantized_model.eval())
+# ...
+```
+
+## Dynamic Quantization
+
+```python
+import intel_extension_for_pytorch as ipex
+from intel_extension_for_pytorch.quantization import prepare, convert
+```
+
+### Define QConfig
+
+Using the default qconfig(recommended):
+
+```python
+dynamic_qconfig = ipex.quantization.default_dynamic_qconfig
+# equal to 
+# QConfig(activation=PlaceholderObserver.with_args(dtype=torch.float, compute_dtype=torch.quint8),
+#         weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric))
+```
+
+or define your own qconfig as:
+
+```python
+from torch.ao.quantization import MinMaxObserver, PlaceholderObserver, QConfig
+dynamic_qconfig = QConfig(activation = PlaceholderObserver.with_args(dtype=torch.float, compute_dtype=torch.quint8),
+                          weight = MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))
+```
+
+Note: For weight observer, it only supports dtype **torch.qint8**, and the qscheme can only be **torch.per_tensor_symmetric** or **torch.per_channel_symmetric**. For activation observer, it only supports dtype **torch.float**, and the **compute_dtype** can be **torch.quint8** or **torch.qint8**.
+
+**Suggestion**:
+
+1. For weight observer, setting **qscheme** to **torch.per_channel_symmetric** can get a better accuracy.
+2. If your CPU device doesn't support VNNI, setting the observer's **reduce_range** to **True** can get a better accuracy, such as skylake.
+
+### Prepare Model
+
+```python
+prepared_model = prepare(user_model, dynamic_qconfig, example_inputs=example_inputs)
+```
+
+## Convert to Dynamic Quantized Model and Deploy
+
+```python
+# make sure the example_inputs's size is same as the real input's size
+convert_model = convert(prepared_model)
+# Optional: convert the model to traced model
+#with torch.no_grad():
+#    traced_model = torch.jit.trace(convert_model, example_input)
+#    traced_model = torch.jit.freeze(traced_model)
+
+# or save the model to deploy
+# traced_model.save("quantized_model.pt")
+# quantized_model = torch.jit.load("quantized_model.pt")
+# quantized_model = torch.jit.freeze(quantized_model.eval())
+# ...
+# for inference 
+y = convert_model(x)
+```
+
+Note: we only support the following ops to do dynamic quantization:
+
+- torch.nn.Linear
+- torch.nn.LSTM
+- torch.nn.GRU
+- torch.nn.LSTMCell
+- torch.nn.RNNCell
+- torch.nn.GRUCell
diff --git a/docs/tutorials/features/int8_recipe_tuning_api.md b/docs/tutorials/features/int8_recipe_tuning_api.md
new file mode 100644
index 00000000..d3ed54b6
--- /dev/null
+++ b/docs/tutorials/features/int8_recipe_tuning_api.md
@@ -0,0 +1,11 @@
+INT8 Recipe Tuning API (Experimental)
+=====================================
+
+This [new API](../api_doc.html#ipex.quantization.autotune) `ipex.quantization.autotune` supports INT8 recipe tuning by using Intel® Neural Compressor as the backend in Intel® Extension for PyTorch\*. In general, we provid default recipe in Intel® Extension for PyTorch\*, and we still recommend users to try out the default recipe first without bothering tuning. If the default recipe doesn't bring about desired accuracy, users can use this API to tune for a more advanced receipe.
+
+Users need to provide a prepared model and some parameters required for tuning. The API will return a tuned model with advanced recipe.
+
+### Usage Example
+
+[//]: # (marker_feature_int8_autotune)
+[//]: # (marker_feature_int8_autotune)
diff --git a/docs/tutorials/features/isa_dynamic_dispatch.md b/docs/tutorials/features/isa_dynamic_dispatch.md
new file mode 100644
index 00000000..5e6aacc8
--- /dev/null
+++ b/docs/tutorials/features/isa_dynamic_dispatch.md
@@ -0,0 +1,122 @@
+ISA Dynamic Dispatching
+=======================
+
+This document explains the dynamic kernel dispatch mechanism for Intel® Extension for PyTorch\* (Intel® Extension for PyTorch\*) based on CPU ISA. It is an extension to the similar mechanism in PyTorch.
+
+## Overview
+
+Forked from PyTorch, Intel® Extension for PyTorch\* adds additional CPU ISA level support, such as `AVX512_VNNI`, `AVX512_BF16` and `AMX`.
+
+PyTorch & Intel® Extension for PyTorch\* CPU ISA support statement:
+
+ | | DEFAULT | AVX2 | AVX2_VNNI | AVX512 | AVX512_VNNI | AVX512_BF16 | AMX |
+ | ---- | :----: | :----: | :----: | :----: | :----: | :----: | :----: |
+ | PyTorch | ✔ | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ |
+ | Intel® Extension for PyTorch\* 1.11 | ✘ | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ |
+ | Intel® Extension for PyTorch\* 1.12 | ✘ | ✔ | ✘ | ✔ | ✔ | ✔ | ✔ |
+
+\* `DEFAULT` in Intel® Extension for PyTorch\* 1.12 implies `AVX2`.
+
+### CPU ISA build compiler requirement
+
+ | ISA Level | GCC requirement |
+ | ---- | :----: |
+ | AVX2 | Any |
+ | AVX512 | GCC 9.2+ |
+ | AVX512_VNNI | GCC 9.2+ |
+ | AVX512_BF16 | GCC 10.3+ |
+ | AVX2_VNNI | GCC 11.2+ |
+ | AMX | GCC 11.2+ |
+
+\* Check with `cmake/Modules/FindAVX.cmake` for detailed compiler checks.
+
+## Select ISA Level
+
+By default, Intel® Extension for PyTorch\* dispatches to kernels with the maximum ISA level supported on the underlying CPU hardware. This ISA level can be overridden by an environment variable `ATEN_CPU_CAPABILITY` (same environment variable as PyTorch). Available values are {`avx2`, `avx512`, `avx512_vnni`, `avx512_bf16`, `amx`}. The effective ISA level would be the minimal level between `ATEN_CPU_CAPABILITY` and the maximum level supported by the hardware.
+
+### Example:
+
+```bash
+$ python -c 'import intel_extension_for_pytorch._C as core;print(core._get_current_isa_level())'
+AMX
+$ ATEN_CPU_CAPABILITY=avx2 python -c 'import intel_extension_for_pytorch._C as core;print(core._get_current_isa_level())'
+AVX2
+```
+>**Note:**
+>
+>`core._get_current_isa_level()` is an Intel® Extension for PyTorch\* internal function used for checking the current effective ISA level. It is used for debugging purpose only and subject to change.
+
+## CPU feature check
+
+An addtional CPU feature check tool in the subfolder: `tests/cpu/isa`
+
+```bash
+$ cmake .
+-- The C compiler identification is GNU 11.2.1
+-- The CXX compiler identification is GNU 11.2.1
+-- Detecting C compiler ABI info
+-- Detecting C compiler ABI info - done
+-- Check for working C compiler: /opt/rh/gcc-toolset-11/root/usr/bin/cc - skipped
+-- Detecting C compile features
+-- Detecting C compile features - done
+-- Detecting CXX compiler ABI info
+-- Detecting CXX compiler ABI info - done
+-- Check for working CXX compiler: /opt/rh/gcc-toolset-11/root/usr/bin/c++ - skipped
+-- Detecting CXX compile features
+-- Detecting CXX compile features - done
+-- Configuring done
+-- Generating done
+-- Build files have been written to: tests/cpu/isa
+
+$ make
+[ 33%] Building CXX object CMakeFiles/cpu_features.dir/intel_extension_for_pytorch/csrc/cpu/isa/cpu_feature.cpp.o
+[ 66%] Building CXX object CMakeFiles/cpu_features.dir/intel_extension_for_pytorch/csrc/cpu/isa/cpu_feature_main.cpp.o
+[100%] Linking CXX executable cpu_features
+[100%] Built target cpu_features
+
+$ ./cpu_features
+XCR0: 00000000000602e7
+os --> avx: true
+os --> avx2: true
+os --> avx512: true
+os --> amx: true
+mmx:                    true
+sse:                    true
+sse2:                   true
+sse3:                   true
+ssse3:                  true
+sse4_1:                 true
+sse4_2:                 true
+aes_ni:                 true
+sha:                    true
+xsave:                  true
+fma:                    true
+f16c:                   true
+avx:                    true
+avx2:                   true
+avx_vnni:                       true
+avx512_f:                       true
+avx512_cd:                      true
+avx512_pf:                      false
+avx512_er:                      false
+avx512_vl:                      true
+avx512_bw:                      true
+avx512_dq:                      true
+avx512_ifma:                    true
+avx512_vbmi:                    true
+avx512_vpopcntdq:               true
+avx512_4fmaps:                  false
+avx512_4vnniw:                  false
+avx512_vbmi2:                   true
+avx512_vpclmul:                 true
+avx512_vnni:                    true
+avx512_bitalg:                  true
+avx512_fp16:                    true
+avx512_bf16:                    true
+avx512_vp2intersect:            true
+amx_bf16:                       true
+amx_tile:                       true
+amx_int8:                       true
+prefetchw:                      true
+prefetchwt1:                    false
+```
diff --git a/docs/tutorials/features/nhwc.md b/docs/tutorials/features/nhwc.md
new file mode 100644
index 00000000..21a61167
--- /dev/null
+++ b/docs/tutorials/features/nhwc.md
@@ -0,0 +1,190 @@
+Channels Last
+=============
+
+## What is Channels Last
+
+**Note**: In PyTorch, **memory format** refers to data representation that describes how multidimensional arrays (nD) are stored in linear (1D) memory address space. **Memory format** has the same semantic meaning as **layout** in oneDNN. **Layout** in PyTorch has other semantic of describing **dense** or **sparse** with the attributes: 'torch.strided', 'torch.sparse_coo'.
+
+On CNN models, the canonical order of tensor dimensions is assigned with semantic meaning. For example the input tensor of 2D convolution is of NCHW by default on PyTorch - <batch_size, channels, height, width>. NHWC is an alternative way of describing the tensor dimensions - <batch_size, height, width, channels>.
+
+Look at the following image of illustrating NCHW and NHWC when N=1. Actually when N=1, NHWC has the same format with BMP file image.
+![fig-1-memory-layout](../../../images/channels_last/figure1_memory_layout.png)
+
+PyTorch refers to NCHW as `torch.contiguous_format` (the default memory format) and to NHWC as `torch.channels_last`, which is a new feature as of the 1.5 release.
+
+TensorFlow uses NHWC as the default memory format because NHWC has a performance advantage over NCHW. On CPU platforms, we propose to optimize Channels Last memory path for the following reasons:
+* **Performance** - NHWC performance is not as good as blocked memory format (nChw16c), but it is close, and much better performance than NCHW.
+* **User Experience** - Operator coverage of NHWC would be higher than blocked memory format (`to_mkldnn()` method), so user experience is better. To be specific, it is difficult to enable operators that manipulates `dim` on blocked format such as `sum(dim=?)`. You would need to convert tensor from blocked memory format back to NHWC using `to_dense()`, before feeding it into `sum()`. This is naturally supported on Channels Last memory format already.
+* **Upstream** - Will be easier since CPU doesn't hold secret ingredient and both inference and training will be covered.
+
+## Memory Format Is All That Matters
+
+On CNN models, memory format is almost the foundation of any upper level design. One important fact is that converting memory format could be very expensive. Thus, in case that multiple CNN operators are performed in sequence, e.g. `Conv2d -> ReLU -> Conv2d`, it's beneficial to transform them from different memory formats once, do computation and reorder them back.
+
+On PyTorch, you can use 3 types of memory formats on CNN models:
+
+### a. NCHW (default)
+
+```python
+## NB: internally blocked format will still be used.
+##   aka. we do 'reorder' for 'input', 'weight' and 'output',
+##   and believe me this is expensive, roughly 50% perf loss...
+input = torch.randn(1, 10, 32, 32)
+model = torch.nn.Conv2d(10, 20, 1, 1)
+output = model(input)
+```
+
+### b. NHWC (WIP for CPU)
+
+```python
+input = torch.randn(1, 10, 32, 32)
+model = torch.nn.Conv2d(10, 20, 1, 1)
+## NB: convert to Channels Last memory format.
+##   oneDNN supports NHWC for feature maps (input, output),
+##   but weight still needs to be of blocked format.
+##   Still we can save reorders for feature maps.
+input = input.to(memory_format=torch.channels_last)
+model = model.to(memory_format=torch.channels_last)
+output = model(input)
+```
+
+### c. Blocked (nChw16c)
+
+```python
+from torch.utils import mkldnn as mkldnn_utils
+input = torch.randn(1, 10, 32, 32)
+model = torch.nn.Conv2d(10, 20, 1, 1)
+## NB: convert to blocked memory format.
+##   Note that 'output' is in blocked memory format,
+##   in case the subsequent operator doesn't support blocked memory format
+##   you need to manually reorder it back to NCHW by output.to_dense()
+##   mkldnn_utils.to_mkldnn(model) is used to prepack the weight, this will save weight reorder time
+##   for inference. For training, it is not needed.
+input = input.to_mkldnn()
+model = mkldnn_utils.to_mkldnn(model)
+output = model(input)
+```
+
+Better to explain the concepts here with a diagram, the **dotted lines** indicate simple memory view, no hard copy.
+![fig-2(1)-pt-conv-layout-path-dispatch](../../../images/channels_last/figure2_dispatch.png)
+
+**Conclusion** is that NHWC path saves the reorders from feature maps compared with NCHW path, but still weight reorder is necessary since oneDNN requires weights to be in blocked memory format. From performance perspective, when `batch_size=N`, weight reorder is minimum compared to feature map reorder. But when `batch_size=1`, weight reorder is usually not negligible. So whether to enable weight prepacking on channels last memory format needs further discussion.
+
+## PyTorch Strided Layout
+
+Before moving on, I feel it is necessary to explain how PyTorch organizes tensors in memory - the **layout**. Here we only focus on **dense** tensors, skip 'coo' layout of **sparse** tensor.
+
+The question itself can be reinterpreted as for a tensor of size <N, C, H, W>, how does PyTorch access the element with index <n, c, h, w> from memory, the answer is **stride**:
+```
+tensor: <N, C, H, W>
+index: <n, c, h, w>
+strides: <CHW, HW, W, 1>
+offset(n,c,h,w) = stride_n * n + stride_c * c + stride_h * h + stride_w * w
+                = CHW * n + HW * c + W * h + 1 * w
+```
+
+One merit of introducing **stride** is that it can express noncontiguous tensors, e.g. a slice of big tensor. For example, the 'Xs' in the following image have a stride of <n1+n2, 1>.
+
+![fig-3-pytorch-strided-layout](../../../images/channels_last/figure3_strided_layout.png)
+
+Keep in mind that PyTorch Tensor does not have an attribute called 'memory_format' or something else. The memory format expression completely relies on **size** and **stride**. The design principle can be found at reference: [RFC: Memory format (aka layout aka NHWC) support](https://github.com/pytorch/pytorch/issues/19092). No matter what the tensor's memory format is, we need a logical canonical order for the dimensions - that is **NCHW** on PyTorch. Thus, **size** and **stride** are ALWAYS described in the order of **NCHW**. Let's now look at the Channels Last case of the previous question:
+```
+tensor: <N, C, H, W>
+index: <n, c, h, w>
+strides: <HWC, 1, WC, C>
+offset(n,c,h,w) = stride_n * n + stride_c * c + stride_h * h + stride_w * w
+                = HWC * n + 1 * c + WC * h + C * w
+```
+
+Actually, this pattern applies to ALL other memory formats as long as it is 4-dim, e.g. strides for CHWN would be <1, HWN, WN, N>.
+
+## PyTorch Channels Last Memory Format APIs
+
+### a. tensor creation
+```python
+x = torch.empty(N, C, H, W, memory_format=torch.channels_last)
+```
+
+### b. tensor conversion
+```python
+## .contiguous() transforms NHWC noncontiguous to NHWC contiguous.
+## .to() converts NCHW tensor to NHWC one, it is outplace.
+x = x.contiguous(memory_format=torch.channels_last)
+x = x.to(memory_format=torch.channels_last)
+
+## contiguous check
+x.is_contiguous(memory_format=torch.channels_last)
+```
+
+### c. model conversion
+```python
+## NB: tensor.to() is an outplace operation
+##   model.to() is inplace. It calls _apply() which is inplace.
+model = model.to(memory_format=torch.channels_last)
+input = input.to(memory_format=torch.channels_last)
+```
+
+### d. operator coverage
+
+Detailed operator coverage information has been listed at reference [Operators-with-Channels-Last-support](https://github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-support). In brief, ImageNet training topologies on GPU already have full support on Channels Last memory format, while CPU doesn't.
+
+Some spontaneous questions:
+* **How to tell whether this model or operator support Channels Last?** - This requires manual memory format check, aka. 'torch.channels_last' input and weight shall NOT generate 'torch.contiguous_format' output.
+* **What if the model comprises of operator not supported Channels Last?** - No errors messages will be shown, the NHWC tensor will be handled by the operator as a non-contiguous NCHW tensor, so result might not be correct depending on the algorithm of this operator.
+
+## Writing Channels Last Kernels
+
+### a. Status on CPU
+
+* **No support** - Requires to register Channels Last kernel for CPU path, e.g. Conv2d;
+* **Explicit support** - Already have Channels Last kernel for CPU path (in ATen native manner), need to compare oneDNN counterpart performance, e.g. BatchNorm;
+* **Implicit support** - Supported via meta structures like 'TensorIterator', need to compare oneDNN counterpart performance, e.g. ReLU.
+
+### b. Register Channels Last Kernel in ATen Native Manner
+
+The general guideline has been listed under reference [Writing-memory-format-aware-operators](https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators), not to repeat here. You may take one of my recent PR [optimize upsample performance linear mode on CPU](https://github.com/pytorch/pytorch/pull/34864) as an example, which also demonstrates NHWC performance advantage over NCHW because of the ease of vectorization.
+
+### c. Register oneDNN Kernel on Channels Last
+
+Registering a oneDNN kernel under Channels Last memory format on CPU is no different from [cuDNN](https://github.com/pytorch/pytorch/pull/23861): Only very few upper level changes are needed, such as accommodate 'contiguous()' to 'contiguous(suggested_memory_format)'. The automatic reorder of oneDNN weight shall have been hidden in ideep.
+
+## oneDNN NHWC APIs
+
+Compared to NCHW interfaces, 2 parts need to be addressed on NHWC interfaces:
+
+### a. Create NHWC Memory
+
+The logical size and stride description of oneDNN is always in NCHW, this is identical to PyTorch. Example code such as
+```cpp
+/* create md from memory::format_tag */
+auto src_md = memory::desc(
+        {N, C, H, W}, // logical dims, the order is defined by a primitive
+        memory::data_type::f32, // tensor's data type
+        memory::format_tag::nhwc // memory format, NHWC in this case
+);
+
+/* alternative: create md from strides */
+auto src_md = memory::desc(
+        {N, C, H, W}, // logical dims, the order is defined by a primitive
+        memory::data_type::f32, // tensor's data type
+        {stride_N, stride_C, stride_H, stride_W} // the strides
+);
+
+/* create memory */
+auto src_mem = memory(src_md, src_data_ptr, engine);
+```
+
+### b. Create Convolution Primitive
+
+* **NCHW** - create `memory::desc` with *any* card for 'input', 'output' and 'weight'; query proposed `memory::desc` from convolution primitive;
+* **NHWC** - create `memory::desc` with `format_tag::nhwc` for 'input' and 'output', use *any* for 'weight'; if we use `hwio` for 'weight' convolution primitive will be created with gemm rather jit avx512.
+
+## CPU Channels Last Targets
+
+* **User Experience** - No special user level code change, only 'input' and 'model' conversion is required;
+* **Scenarios** - cover both training and inference;
+* **Models** - ResNet50 and ResNext101, extended targets: torchvision models, detectron2;
+* **Performance Targets** - training >0.8x blocked; inference throughput > 0.8x blocked; inference latency? (need further discussion)
+* **Operator Coverage** - No less than GPU path;
+* **BFloat16** - This part shall align with BFloat16 integration (need further discussion);
+* **int8** - Need further discussion.
diff --git a/docs/tutorials/features/optimizer_fusion.md b/docs/tutorials/features/optimizer_fusion.md
new file mode 100644
index 00000000..6f8f722e
--- /dev/null
+++ b/docs/tutorials/features/optimizer_fusion.md
@@ -0,0 +1,36 @@
+Optimizer Fusion
+================
+
+## Introduction
+As with TorchScript, operation fusion reduces the number of operators that will be executed, and reduces overhead time. This methodology is also applied in ipex optimizer Optimization. We support Lamb/Adagrad/SGD fusion for both FP32/BF16(Split) at current stage.
+
+Let's use [adagrad update](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html?highlight=adagrad#torch.optim.Adagrad) as an example.
+
+```python
+    if weight_decay != 0:
+        grad = grad.add(param, alpha=weight_decay)
+    clr = lr / (1 + (step - 1) * lr_decay)
+    state_sum.addcmul_(grad, grad, value=1)
+    std = state_sum.sqrt().add_(eps)
+    param.addcdiv_(grad, std, value=-clr)
+```
+
+## Operation Fusion
+
+One problem of the native implementation above is that we need to access the whole storage of "grad", "parameters", and "state sum" several times. For example, we need to access the whole storage of "parameters" and "grad" at the first clause. For large topologies, it is possible that the "grad" and "parameters" cannot be stored on the onboard CPU cache. When we need to access the storage of "grad" again when executing the third clause, the processor must read data out from memory again instead of the more efficient onboard high speed CPU cache. This is a memory-bound bottle neck preventing good performance.
+
+Fusion is the methodology to solve this problem. Since the 5 clauses in the pseudo code are all element-wise operations. We can fuse them into a single one, like the pseudo code below.
+
+```python
+   adagrad_fused_step(param, grad, state_sum, ...(other args))
+```
+
+ In our fused operators, we can separate the storage of  "grad", "parameters" and "state sum" in several groups and ensure each group is small enough to be stored in the cache. The pseudo code below illustrates our execution process.
+
+```python
+  grad = (grad0, grad1, ..., grad_n)
+  param = (param, param, ..., param_n)
+  state_sum = (state_sum, state_sum, ..., state_sum_n)
+  for i in range(n):
+    adagrad_step(grad_i, param_i, state_sum_i, ...(other_args))
+```
diff --git a/docs/tutorials/features/runtime_extension.md b/docs/tutorials/features/runtime_extension.md
new file mode 100644
index 00000000..2a8f029f
--- /dev/null
+++ b/docs/tutorials/features/runtime_extension.md
@@ -0,0 +1,174 @@
+Runtime Extension
+=================
+
+Intel® Extension for PyTorch\* Runtime Extension provides a couple of PyTorch frontend APIs for users to get finer-grained control of the thread runtime. It provides:
+
+1. Multi-stream inference via the Python frontend module `ipex.cpu.runtime.MultiStreamModule`.
+2. Spawn asynchronous tasks via the Python frontend module `ipex.cpu.runtime.Task`.
+3. Program core bindings for OpenMP threads via the Python frontend `ipex.cpu.runtime.pin`.
+
+**note**: Intel® Extension for PyTorch\* Runtime extension is in the **experimental** stage. The API is subject to change. More detailed descriptions are available at [API Documentation page](../api_doc.rst).
+
+## Requirements
+
+Intel® Extension for PyTorch\* Runtime Extension relies on `intel omp` to bind threads to cores. If you want to use it in your application, start model script with an extra flag: `LD_PRELOAD=$LD_PRELOAD:$PATH/libiomp5.so python model_script.py`.
+
+## Use Cases
+
+### Example of MultiStream Module
+
+Runtime extension supports weight-sharing multi-stream inference for throughput mode on CPU. You need to convert the original model into multi-stream model and run the new multi-stream model as normal. The detailed description of parameters to create `MultiStreamModule` is available at [API Documentation page](../api_doc.rst).
+
+`MultiStreamModule` can improve performance for inference in throughput mode. We suggest creating `MultiStreamModule` with `num_streams` of "AUTO", which heuristically decides the number of streams. Usually, it provides a reasonable performance. However, it may not be optimal for some cases (refer to the section [Performance recipes](#performance-recipes) for details). Manual tuning for number of streams is needed.
+
+The `MultiStreamModule` creates number of streams based on input parameter `num_streams` and bind cores to stream based on input parameter `cpu_pool`. If the number of cores inside `cpu_pool` is divisible by `num_streams`, the cores will be allocated equally to each stream. If the number of cores inside `cpu_pool` is not divisible by `num_streams` with remainder N, one extra core will be allocated to the first N streams. We suggest to set the `num_streams` as divisor of core number inside `cpu_pool`.
+
+If the inputs' batchsize is larger than and divisible by ``num_streams``, the batchsize will be allocated equally to each stream. If batchsize is not divisible by ``num_streams`` with remainder N, one extra piece will be allocated to the first N streams. If the inputs' batchsize is less than ``num_streams``, only the first batchsize's streams are used with mini batch as one. We suggest to set inputs' batchsize larger than and divisible by ``num_streams``. When creating `MultiStreamModule`, if you leave num of streams as "AUTO", we suggest to set inputs' batchsize larger than and divisible by number of cores.
+
+Let's create some ExampleNets that will be used by further examples:
+```
+import torch
+import intel_extension_for_pytorch as ipex
+
+class ExampleNet1(torch.nn.Module):
+    def __init__(self):
+        super(ExampleNet1, self).__init__()
+        self.conv = torch.nn.Conv2d(64, 128, (3, 3), stride=(2, 2), padding=(1, 1), bias=False)
+
+    def forward(self, x):
+        x1 = self.conv(x)
+        y = torch.flatten(x1, start_dim=1)
+        return y
+
+class ExampleNet2(torch.nn.Module):
+    def __init__(self):
+        super(ExampleNet2, self).__init__()
+        self.conv = torch.nn.Conv2d(64, 128, (3, 3), stride=(2, 2), padding=(1, 1), bias=False)
+        self.conv2 = torch.nn.Conv2d(64, 128, (3, 3), stride=(2, 2), padding=(1, 1), bias=False)
+
+    def forward(self, x1, x2):
+        y1 = self.conv(x1)
+        y2 = self.conv2(x2)
+        y = torch.flatten(y1, start_dim=1)
+        return y1, y
+
+model1 = ExampleNet1()
+model1.eval()
+x = torch.rand(16, 64, 3, 3)
+
+with torch.no_grad():
+    traced_model1 = torch.jit.trace(model1, x)
+    traced_model1 = torch.jit.freeze(traced_model1)
+
+model2 = ExampleNet2()
+model2.eval()
+x2 = torch.rand(16, 64, 3, 3)
+
+with torch.no_grad():
+    traced_model2 = torch.jit.trace(model2, (x, x2))
+    traced_model2 = torch.jit.freeze(traced_model2)
+```
+
+#### Examples1: Basic Usage
+Here is the example of a model with single tensor input/output. We create a CPUPool with all the cores available on numa node 0. And creating a `MultiStreamModule` with stream number of 2 to do inference.
+```
+# Convert the model into multi_Stream_model
+cpu_pool = ipex.cpu.runtime.CPUPool(node_id=0)
+multi_Stream_model = ipex.cpu.runtime.MultiStreamModule(traced_model1, num_streams=2, cpu_pool=cpu_pool)
+
+with torch.no_grad():
+    y = multi_Stream_model(x)
+```
+
+#### Examples2: Usage with "AUTO" setting
+When creating a `MultiStreamModule`, we have default settings for `num_streams` ("AUTO") and `cpu_pool` (with all the cores available on numa node 0). For the `num_streams` of "AUTO", there are limitations to use with int8 datatype as we mentioned in below performance receipts section.
+```
+# Convert the model into multi_Stream_model
+multi_Stream_model = ipex.cpu.runtime.MultiStreamModule(traced_model1)
+
+with torch.no_grad():
+    y = multi_Stream_model(x)
+```
+
+#### Examples3: Usage for models with structure inputs/outputs
+For module such as ExampleNet2 with structure input/output tensors, user needs to create `MultiStreamModuleHint` as input hint and output hint. `MultiStreamModuleHint` tells `MultiStreamModule` how to auto split the input into streams and concat the output from each steam.
+```
+# Convert the model into multi_Stream_model
+cpu_pool = ipex.cpu.runtime.CPUPool(node_id=0)
+# Create the input hint object
+input_hint = ipex.cpu.runtime.MultiStreamModuleHint(0, 0)
+# Create the output hint object
+# When Python module has multi output tensors, it will be auto pack into a tuple, So we pass a tuple(0, 0) to create the output_hint
+output_hint = ipex.cpu.runtime.MultiStreamModuleHint((0, 0))
+multi_Stream_model = ipex.cpu.runtime.MultiStreamModule(traced_model2,
+                                                        num_streams=2,
+                                                        cpu_pool=cpu_pool,
+                                                        input_split_hint=input_hint,
+                                                        output_concat_hint=output_hint)
+
+with torch.no_grad():
+    y = multi_Stream_model(x, x2)
+```
+
+#### Performance recipes
+There are two motivations to use the `MultiStreamModule`:
+1. Better cache locality: With `MultiStreamModule`, the activations will be limited in the CPU cores allocated to this stream instead of the whole cpu_pool.
+2. Reduce the OMP sync overhead: if one CPU core allocated to one stream, the whole execution needs to do OMP sync once after all streams finish execution instead of sync per layer.
+
+Thus, `MultiStreamModule` may benefit performance for inference in throughput mode. However, the end-to-end performance is impacted by these issues:
+1. The kernels' efficiency, which are different under different OMP threads' number.
+2. The overhead of inputs' auto split and outputs' auto concat for each stream.
+3. The overhead of pthread (stream async execution) wakes up and threads' synchronization after stream execution.
+
+Here are some performance receipes that we recommend for better multi-stream performance.
+
+* When creating `MultiStreamModule` with `torch.nn.Module` as imperative path module, each stream inside `MultiStreamModule` suffers the GIL issue when doing inference together. This hurts end-to-end performance. We recommend creating `MultiStreamModule` with the `torch.jit.ScriptModule`.
+
+* For convolution network, `intel_extension_for_pytorch` has the quick path getting convolution primitive to mitigate overhead when `OMP_NUM_THREADS` is the same between the `torch.jit.trace` and model execution phases. To use this quick path for better performance, we recommend setting the `OMP_NUM_THREADS` environment before launching the model script. The recommended value of `OMP_NUM_THREADS` should equal the threads number used by each stream. For example, creating `MultiStreamModule` as stream number `s1` and CPUPool with core number `c1`, each stream will allocate threads number as `c1/s1`. We recommend setting `OMP_NUM_THREADS` as this value.
+
+* `Numactl` and the threads management in `MultiStreamModule` work at different levels. `MultiStreamModule` has the thread affinity setting for each stream, which works in the thread level. However, for the Python modules outside the stream, such as the dataloader, are out of view for `MultiStreamModule`. As the result, we recommend using `numactl -C core_ids -m node_id` for the process level core and memory resource management. For the core resource setting by `numactl`, set it the same or superset of the core resource to create `CPUPool`. Otherwise, the behavior is undefined in current implementation.
+
+#### Known issues
+* Intel® Extension for PyTorch\* runtime extension feature with Int8 data type does not support dynamic shape well. To avoid performance issues, we recommend setting the batchsize to do `jit.trace` with same mini batchsize used by each stream. For example, creating `MultiStreamModule` as stream number of `s1` and input global batchsize as `gb`, each stream will inference with mini-batchsize of `gb/s1`. We should use this mini-batchsize value to do `jit.trace`. To be aware of the `num_streams` value, we recommend creating `MultiStreamModule` with `num_streams` setting explicitly instead of "AUTO". Due to the same limitation, the behavior that each stream inference with different mini batchsize of int8 data type is undefined and not supported.
+
+### Example of asynchronous task
+
+Here is an example for using asynchronous tasks. With the support of a runtime API, you can run 2 modules simultaneously. Each module runs on the corresponding cpu pool.
+
+```
+cpu_pool1 = ipex.cpu.runtime.CPUPool([0, 1, 2, 3])
+cpu_pool2 = ipex.cpu.runtime.CPUPool([4, 5, 6, 7])
+
+task1 = ipex.cpu.runtime.Task(traced_model1, cpu_pool1)
+task2 = ipex.cpu.runtime.Task(traced_model1, cpu_pool2)
+
+y1_future = task1(x)
+y2_future = task2(x)
+
+y1 = y1_future.get()
+y2 = y2_future.get()
+```
+
+### Example of configuring core binding
+
+Runtime Extension provides API of `ipex.cpu.runtime.pin` to a CPU Pool for binding physical cores. We can use it without the async task feature. Here is the example to use `ipex.cpu.runtime.pin` in the `with` context.
+
+```
+cpu_pool = ipex.cpu.runtime.CPUPool(node_id=0)
+with ipex.cpu.runtime.pin(cpu_pool):
+    y_runtime = traced_model1(x)
+```
+
+## Detail Design
+
+### How the core binding is implemented
+
+The Runtime Extension relies on the `kmp_*` API inside `iomp` share library to fulfill the core binding. During the initialization of async threads, `kmp_*` API functions are invoked internally to start up an OpenMP group with specified number of worker threads. Each worker thread is then bound to the designated physical core(s) inside this OpenMP group. After initialization, when you submit a task, the OpenMP group will serve the requested task.
+
+### Design of Task
+
+Task is an abstraction of computation based on PyTorch module and is scheduled asynchronously. When a task is created with specific `nn.Module` or `jit module`, a sub-thread is initialized and bound to this task. During the initialization, an OpenMP worker group is created and bound to this sub-thread. After initialization, the sub-thread waits for input. When the main thread submits an input to this task, the sub-thread will wake up and execute the input. The main thread returns a `FutureTensor` and is not block until an explicit `FutureTensor.get()` is invoked to get the results executed in the sub-thread.
+
+### IOMP preload or load during the runtime
+
+Since Runtime Extension relies on the APIs from IOMP, we need to preload IOMP before executing the application. We want Intel® Extension for PyTorch\* built with Runtime API enabled. This means it should work fine without loading IOMP if the user didn't use the runtime API. Here we choose to `dlopen` IOMP library during runtime and we ensure the IOMP symbols are initialized once globally.
diff --git a/docs/tutorials/features/split_sgd.rst b/docs/tutorials/features/split_sgd.rst
new file mode 100644
index 00000000..e9576611
--- /dev/null
+++ b/docs/tutorials/features/split_sgd.rst
@@ -0,0 +1,91 @@
+Split SGD
+=========
+
+Both optimizations for inference workloads and training workloads are within Intel's optimization scope. Optimizations for train optimizer functions are an important perspective. The optimizations use a mechanism called **Split SGD** and take advantage of BFloat16 data type and operator fusion. Optimizer **adagrad**, **lamb** and **sgd** are supported.
+
+BFloat16
+--------
+
+The figure below shows definition of Float32 (top) and `BFloat16 <https://www.intel.com/content/www/us/en/developer/articles/technical/intel-deep-learning-boost-new-instruction-bfloat16.html>`_ (bottom) data types. Compared to Float32, BFloat16 is only half as long, and thus saves half the memory. It is supported natively at the instruction set level to boost deep learning workloads from the 3rd Generation of Intel® Xeon® Scalable Processors. It is compatible to Float32 since both have the same bit length for "sign" and "exponent" part. BFloat16 only has a 7-bit "mantissa" part while Float32 has 23 bits. BFloat16 has the same capacity to represent "digit ranges" with that of Float32, but has a shorter "precision" part.
+
+.. image:: https://user-images.githubusercontent.com/33838455/86600181-00f5c200-bfa0-11ea-93f0-95af3f0bff08.png
+  :width: 1200
+  :align: center
+  :alt: Data types
+
+An advantage of BFloat16 is that it saves memory and reduces computation workload, but the fewer mantissa bits brings negative effects as well. Let's use an "ADD" operation as an example to explain the disadvantage. To perform addition of 2 floating point numbers, we need to shift the mantissa part of the numbers left or right to align their exponent parts. Since BFloat16 has a shorter mantissa part, it is much easier than Float32 to lose its mantissa part after the shifting, and thus cause an accuracy loss issue.
+
+Let's use the following two decimal numbers **x** and **y** as an example. We first do the calculation in a high precision data type (10 valid numbers after decimal point).
+
+.. math::
+
+   x &= 0.1234500000*10^{10} \\
+   y &= 0.1234500000*10^{5} \\
+   x+y &= 0.1234500000*10^{10} + 0.1234500000*10^{5} \\
+       &= 0.1234500000*10^{10} + 0.0000012345*10^{10} \\
+	   & =0.1234512345*10^{10}
+
+This makes sense because after shifting **y** right by 5 digits, the fraction part is still there.
+
+Let's do the calculation using a low precision data type (5 valid numbers after decimal point):
+
+.. math::
+
+   x &= 0.12345*10^{10} \\
+   y &= 0.12345*10^{5} \\
+   x+y &= 0.12345*10^{10} + 0.12345*10^{5} \\
+       &= 0.12345*10^{10} + 0.00000*10^{10} \\
+       &= 0.12345*10^{10}
+
+Since the data type has only 5 digits for the fraction part, after shifting **y** by 5 digits, its fraction part is fully removed. This causes significant accuracy loss and, buy their nature, is a drawback of lower-precision data types.
+
+Stochastic Gradient Descent (SGD)
+---------------------------------
+
+Basically, training involves 3 steps:
+
+1. Forward propagation: Performance inference once and compare the results with ground truth to get loss number.
+2. Backward propagation: Utilize chain rule to calculate gradients of parameters based on the loss number.
+3. Parameter update: Update value of parameters by gradients along with calculated loss values.
+
+The training is actually a loop of these 3 steps in sequence until the loss number meets requirements or after a determined timeout duration. The Stochastic Gradient Descent (SGD) is most widely used at the 3rd step to update parameter values. To make it easy to understand, the 3rd step is described as the following formula:
+
+.. math::
+
+  W = W + α * gW
+
+Where :math:`W` denotes parameters to be updated. :math:`gW` denotes gradient received during backward propagation and :math:`α` denotes learning rate.
+
+Split SGD
+---------
+
+Since the addition applied in SGD is repeated, because of the low precision data loss mentioned earlier, if both the :math:`W` and :math:`gW` are stored in BFloat16 data type, we will most likely lose valid bits and make the training results inaccurate. Using FP32 master parameters is a common practice for avoiding the round-off errors at parameter update step.
+To keep FP32 master parameters, we have 3 design choices:
+1. Only save FP32 parameters: For this choice, we need introduce additional FP32->BF16 cast at each iter to get benefit from BF16 at forward and backward propagation steps.
+2. Save both FP32 and BF16 parameters: BF16 parameter is used at forward and backward propagation steps. Use FP32 master parameter at update steps. For this choice we introduce more memory footprint.
+3. "Split" choice: In order to get performance benefits with BFloat16 at forward and backward propagation steps, while avoiding increase the memory footprint, we propose the mechanism **"Split SGD"**.
+
+The idea is to "split" a 32-bit floating point number into 2 parts:
+
+1. Top half: First 16 bits can be viewed as exactly a BFloat16 number.
+2. Bottom half: Last 16 bits are still kept to avoid accuracy loss.
+
+FP32 parameters are split into "Top half" and "Bottom half". When performing forward and backward propagations, the Top halves are used to take advantage of Intel BFloat16 support. When performing parameter update with SGD, we concatenate the Top half and the Bottom half to recover the parameters back to FP32 and then perform regular SGD operations.
+
+It is a common practice to use FP32 for master parameters in order to avoid round-off errors with BF16 parameter update. **SplitSGD** is an optimization of storing FP32 master parameters with reduced memory footprint.
+
+.. image:: ../../../images/split_sgd/split_sgd.png
+  :width: 800
+  :align: center
+  :alt: Split SGD
+
+|
+
+The following pseudo code illustrates the process of Split SGD.
+
+.. code-block:: python
+
+   fp32_w = concat_fp32_from_bf16(bf16_w, trail)
+   fp32_gw = bf16_gw.float()
+   fp32_w += α* fp32_gw (sgd step without weight_dacay, momentum)
+   bf16_w, trail = split_bf16_from_fp32(fp32_w)
diff --git a/docs/tutorials/getting_started.md b/docs/tutorials/getting_started.md
new file mode 100644
index 00000000..09f7de64
--- /dev/null
+++ b/docs/tutorials/getting_started.md
@@ -0,0 +1,63 @@
+# Getting Started
+
+## Installation
+
+Prebuilt wheel files are released for multiple Python versions. You can install them simply with the following pip command.
+
+```bash
+python -m pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu
+python -m pip install intel_extension_for_pytorch
+```
+
+You can run a simple sanity test to double confirm if the correct version is installed, and if the software stack can get correct hardware information onboard your system.
+
+```bash
+python -c "import torch; import intel_extension_for_pytorch as ipex; print(torch.__version__); print(ipex.__version__);"
+```
+
+More detailed instructions can be found at [Installation Guide](./installation.md).
+
+
+## Coding
+
+Intel® Extension for PyTorch\* doesn't require complex code changes to get it working. Usage is as simple as several-line code change.
+
+In general, APIs invocation should follow orders below.
+
+1. `import intel_extension_for_pytorch as ipex`
+2. Invoke `optimize()` function to apply optimizations.
+3. Convert the imperative model to a graph model.
+    - For TorchScript, invoke `torch.jit.trace()` and `torch.jit.freeze()`.
+    - For TorchDynamo, invoke `torch.compile(model, backend="ipex")`. (*Experimental feature*, FP32 ONLY)
+
+**Note:** It is highly recommended to `import intel_extension_for_pytorch` right after `import torch`, prior to importing other packages.
+
+```python
+import torch
+############## import ipex ###############
+import intel_extension_for_pytorch as ipex
+##########################################
+
+model = Model()
+model.eval()
+data = ...
+
+############## TorchScript ###############
+model = ipex.optimize(model, dtype=torch.bfloat16)
+
+with torch.no_grad(), torch.cpu.amp.autocast():
+  model = torch.jit.trace(model, data)
+  model = torch.jit.freeze(model)
+  model(data)
+##########################################
+
+############## TorchDynamo ###############
+model = ipex.optimize(model)
+
+model = torch.compile(model, backend="ipex")
+with torch.no_grad():
+  model(data)
+##########################################
+```
+
+More examples, including training and usage of low precision data types are available at [Examples](./examples.md).
diff --git a/docs/tutorials/installation.md b/docs/tutorials/installation.md
new file mode 100644
index 00000000..33f84fbb
--- /dev/null
+++ b/docs/tutorials/installation.md
@@ -0,0 +1,179 @@
+Installation Guide
+==================
+
+## System Requirements
+
+|Category|Content|
+|--|--|
+|Compiler|Recommend using GCC 11|
+|Operating System|CentOS 7, RHEL 8, Rocky Linux 8.5, Ubuntu newer than 18.04|
+|Python|See prebuilt wheel files availability matrix below|
+
+* Intel® Extension for PyTorch\* is functional on systems with AVX2 instruction set support (such as Intel® Core™ Processor Family and Intel® Xeon® Processor formerly Broadwell). However, it is highly recommended to run on systems with AVX-512 and above instructions support for optimal performance (such as Intel® Xeon® Scalable Processors).
+
+## Install PyTorch
+
+Make sure PyTorch is installed so that the extension will work properly. For each PyTorch release, we have a corresponding release of the extension. Here are the PyTorch versions that we support and the mapping relationship:
+
+|PyTorch Version|Extension Version|
+|--|--|
+|[v2.0.\*](https://github.com/pytorch/pytorch/tree/v2.0.1 "v2.0.1")|[v2.0.\*](https://github.com/intel/intel-extension-for-pytorch/tree/v2.0.100+cpu)|
+|[v1.13.\*](https://github.com/pytorch/pytorch/tree/v1.13.0 "v1.13.0")|[v1.13.\*](https://github.com/intel/intel-extension-for-pytorch/tree/v1.13.100+cpu)|
+|[v1.12.\*](https://github.com/pytorch/pytorch/tree/v1.12.0 "v1.12.0")|[v1.12.\*](https://github.com/intel/intel-extension-for-pytorch/tree/v1.12.300)|
+|[v1.11.\*](https://github.com/pytorch/pytorch/tree/v1.11.0 "v1.11.0")|[v1.11.\*](https://github.com/intel/intel-extension-for-pytorch/tree/v1.11.200)|
+|[v1.10.\*](https://github.com/pytorch/pytorch/tree/v1.10.0 "v1.10.0")|[v1.10.\*](https://github.com/intel/intel-extension-for-pytorch/tree/v1.10.100)|
+|[v1.9.0](https://github.com/pytorch/pytorch/tree/v1.9.0 "v1.9.0")|[v1.9.0](https://github.com/intel/intel-extension-for-pytorch/tree/v1.9.0)|
+|[v1.8.0](https://github.com/pytorch/pytorch/tree/v1.8.0 "v1.8.0")|[v1.8.0](https://github.com/intel/intel-extension-for-pytorch/tree/v1.8.0)|
+|[v1.7.0](https://github.com/pytorch/pytorch/tree/v1.7.0 "v1.7.0")|[v1.2.0](https://github.com/intel/intel-extension-for-pytorch/tree/v1.2.0)|
+|[v1.5.0-rc3](https://github.com/pytorch/pytorch/tree/v1.5.0-rc3 "v1.5.0-rc3")|[v1.1.0](https://github.com/intel/intel-extension-for-pytorch/tree/v1.1.0)|
+|[v1.5.0-rc3](https://github.com/pytorch/pytorch/tree/v1.5.0-rc3 "v1.5.0-rc3")|[v1.0.2](https://github.com/intel/intel-extension-for-pytorch/tree/v1.0.2)|
+|[v1.5.0-rc3](https://github.com/pytorch/pytorch/tree/v1.5.0-rc3 "v1.5.0-rc3")|[v1.0.1](https://github.com/intel/intel-extension-for-pytorch/tree/v1.0.1)|
+|[v1.5.0-rc3](https://github.com/pytorch/pytorch/tree/v1.5.0-rc3 "v1.5.0-rc3")|[v1.0.0](https://github.com/intel/intel-extension-for-pytorch/tree/v1.0.0)|
+
+Please install CPU version of PyTorch through its official channel. For more details, refer to [pytorch.org](https://pytorch.org/get-started/locally/).
+
+---
+
+**Note:**
+
+For the extension version earlier than 1.8.0, a patch has to be manually applied to PyTorch source code. Check that version's installation guide.
+
+From 1.8.0, compiling PyTorch from source is not required. If you still want to compile PyTorch, follow these [installation instructions](https://github.com/pytorch/pytorch#installation). Make sure to check out the correct PyTorch version according to the table above.
+
+---
+
+## Install via wheel file
+
+Prebuilt wheel files availability matrix for Python versions
+
+| Extension Version | Python 3.6 | Python 3.7 | Python 3.8 | Python 3.9 | Python 3.10 | Python 3.11 |
+| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
+| 2.0.100 |  |  | ✔️ | ✔️ | ✔️ | ✔️ |
+| 2.0.0 |  |  | ✔️ | ✔️ | ✔️ | ✔️ |
+| 1.13.100 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.13.0 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.12.300 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.12.100 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.12.0 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.11.200 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.11.0 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.10.100 | ✔️ | ✔️ | ✔️ | ✔️ |  |  |
+| 1.10.0 | ✔️ | ✔️ | ✔️ | ✔️ |  |  |
+| 1.9.0 | ✔️ | ✔️ | ✔️ | ✔️ |  |  |
+| 1.8.0 |  | ✔️ |  |  |  |  |
+
+**Note:** Intel® Extension for PyTorch\* has PyTorch version requirement. Check the mapping table above.
+
+Starting from 1.11.0, you can use normal pip command to install the package with the latest version.
+
+```
+python -m pip install intel_extension_for_pytorch
+```
+
+Alternatively, you can also install the latest version with the following commands:
+
+```
+python -m pip install intel_extension_for_pytorch -f https://developer.intel.com/ipex-whl-stable-cpu
+```
+
+**Note:** For versions before 1.10.0, use package name `torch_ipex`, rather than `intel_extension_for_pytorch`.
+
+**Note:** To install a history package with a specific version, run with the following command:
+
+```
+python -m pip install <package_name>==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu
+```
+
+## Install via source compilation
+
+To ensure a smooth compilation, a script is provided in the Github repo. If you would like to compile the binaries from source, it is highly recommended to utilize this script.
+
+```bash
+$ wget https://raw.githubusercontent.com/intel/intel-extension-for-pytorch/master/scripts/compile_bundle.sh
+$ bash compile_bundle.sh
+```
+
+**Note:** Recommend to use the `compile_bundle.sh` script in a clean docker container.
+
+**Note:** Use the `compile_bundle.sh` script under a `conda` environment.
+
+**Note:** Depends on what applications are available on your OS, you probably need to install some Linux commands, like `git`, etc. Installation of these Linux commands are not included in this script.
+
+**Note:** The `compile_bundle.sh` script downloads source code of llvm-project and Intel® Extension for PyTorch\* into individual folders in its directory. You can consider to create a specific folder to use this script. Wheel files will be generated under `dist` folder of each source code directory. Besides, compilation progress is dumped into a log file `build.log` in source code directory. The log file is helpful to identify errors occurred during compilation. Should any failure happened, after addressing the issue, you can simply run the `compile_bundle.sh` script again with the same command.
+
+```bash
+$ mkdir ipex_bundle
+$ cd ipex_bundle
+$ wget .../compile_bundle.sh
+$ bash compile_bundle.sh
+$ ls
+compile_bundle.sh  intel_extension_for_pytorch  llvm-project
+$ tree -L 3 .
+.
+├── intel_extension_for_pytorch
+│   ├── dist
+│   │   └── intel_extension_for_pytorch-....whl
+│   ├ build.log
+│   └ ...
+└── llvm-project
+    └ ...
+```
+
+## Install via Docker container
+
+### Build Docker container from Dockerfile
+
+Run the following commands to build the `pip` based deployment container:
+
+```console
+$ cd docker
+$ DOCKER_BUILDKIT=1 docker build -f Dockerfile.pip -t intel-extension-for-pytorch:pip .
+$ docker run --rm intel-extension-for-pytorch:pip python -c "import torch; import intel_extension_for_pytorch as ipex; print('torch:', torch.__version__,' ipex:',ipex.__version__)"
+```
+
+Run the following commands to build the `conda` based development container:
+
+```console
+$ cd docker
+$ DOCKER_BUILDKIT=1 docker build -f Dockerfile.conda -t intel-extension-for-pytorch:conda .
+$ docker run --rm intel-extension-for-pytorch:conda python -c "import torch; import intel_extension_for_pytorch as ipex; print('torch:', torch.__version__,' ipex:',ipex.__version__)"
+```
+
+### Get docker container from dockerhub
+
+Pre-built docker images are available at [DockerHub](https://hub.docker.com/r/intel/intel-optimized-pytorch/tags).
+
+Run the following command to pull the image to your local machine.
+
+```console
+docker pull intel/intel-optimized-pytorch:latest
+```
+
+## Install C++ SDK
+
+|Version|Pre-cxx11 ABI|cxx11 ABI|
+|--|--|--|
+| 2.0.100 | [libintel-ext-pt-2.0.100+cpu.run](https://intel-extension-for-pytorch.s3.amazonaws.com/libipex/cpu/libintel-ext-pt-2.0.100%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-2.0.100+cpu.run](https://intel-extension-for-pytorch.s3.amazonaws.com/libipex/cpu/libintel-ext-pt-cxx11-abi-2.0.100%2Bcpu.run) |
+| 2.0.0 | [libintel-ext-pt-2.0.0+cpu.run](https://intel-extension-for-pytorch.s3.amazonaws.com/libipex/cpu/libintel-ext-pt-2.0.0%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-2.0.0+cpu.run](https://intel-extension-for-pytorch.s3.amazonaws.com/libipex/cpu/libintel-ext-pt-cxx11-abi-2.0.0%2Bcpu.run) |
+| 1.13.100 | [libintel-ext-pt-1.13.100+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-1.13.100%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.13.100+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-1.13.100%2Bcpu.run) |
+| 1.13.0 | [libintel-ext-pt-1.13.0+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-1.13.0%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.13.0+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-1.13.0%2Bcpu.run) |
+| 1.12.300 | [libintel-ext-pt-1.12.300+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-1.12.300%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.12.300+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-1.12.300%2Bcpu.run) |
+| 1.12.100 | [libintel-ext-pt-1.12.100+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-1.12.100%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.12.100+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-1.12.100%2Bcpu.run) |
+| 1.12.0 | [libintel-ext-pt-1.12.0+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-1.12.0%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.12.0+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-1.12.0%2Bcpu.run) |
+| 1.11.200 | [libintel-ext-pt-1.11.200+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-shared-with-deps-1.11.200%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.11.200+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-shared-with-deps-1.11.200%2Bcpu.run) |
+| 1.11.0 | [libintel-ext-pt-1.11.0+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-1.11.0%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.11.0+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-1.11.0%2Bcpu.run) |
+| 1.10.100 | [libtorch-shared-with-deps-1.10.0%2Bcpu-intel-ext-pt-cpu-1.10.100.zip](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libtorch-shared-with-deps-1.10.0%2Bcpu-intel-ext-pt-cpu-1.10.100.zip) | [libtorch-cxx11-abi-shared-with-deps-1.10.0%2Bcpu-intel-ext-pt-cpu-1.10.100.zip](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libtorch-cxx11-abi-shared-with-deps-1.10.0%2Bcpu-intel-ext-pt-cpu-1.10.100.zip) |
+| 1.10.0 | [intel-ext-pt-cpu-libtorch-shared-with-deps-1.10.0+cpu.zip](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/intel-ext-pt-cpu-libtorch-shared-with-deps-1.10.0%2Bcpu.zip) | [intel-ext-pt-cpu-libtorch-cxx11-abi-shared-with-deps-1.10.0+cpu.zip](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/intel-ext-pt-cpu-libtorch-cxx11-abi-shared-with-deps-1.10.0%2Bcpu.zip) |
+
+**Usage:** For version newer than 1.11.0, download one run file above according to your scenario, run the following command to install it and follow the [C++ example](./examples.md#c).
+```
+bash <libintel-ext-pt-name>.run install <libtorch_path>
+```
+
+You can get full usage help message by running the run file alone, as the following command.
+
+```
+bash <libintel-ext-pt-name>.run
+```
+
+**Usage:** For version before 1.11.0, download one zip file above according to your scenario, unzip it and follow the [C++ example](./examples.md#c).
diff --git a/docs/tutorials/license.md b/docs/tutorials/license.md
new file mode 100644
index 00000000..3ea34ef6
--- /dev/null
+++ b/docs/tutorials/license.md
@@ -0,0 +1,9 @@
+License
+=======
+
+Intel® Extension for PyTorch\* is licensed under [Apache License Version 2.0](http://www.apache.org/licenses/LICENSE-2.0). This software includes components that have separate copyright notices and licensing terms. Your use of the source code for these components is subject to the terms and conditions of the following licenses.
+
+Apache License Version 2.0:
+
+[Intel® Extension for PyTorch\* LICENSE](https://github.com/intel/intel-extension-for-pytorch/blob/master/LICENSE.txt)
+
diff --git a/docs/tutorials/performance.md b/docs/tutorials/performance.md
new file mode 100644
index 00000000..f2ebf16c
--- /dev/null
+++ b/docs/tutorials/performance.md
@@ -0,0 +1,622 @@
+Performance
+===========
+
+## Overview
+
+This page shows performance boost with Intel® Extension for PyTorch\* on several popular topologies.
+
+## Performance Data for Intel® AI Data Center Products
+
+Find the latest performance data for 4th gen Intel® Xeon® Scalable processors and 3rd gen Intel® Xeon® processors, including detailed hardware and software configurations, at [Intel® Developer Zone article](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/performance.html).
+
+## INT8 with v1.11
+
+### Performance Numbers
+
+<table border="1" cellpadding="10" align="center" class="perf_table">
+<tbody>
+  <col>
+  <col>
+  <col>
+  <colgroup span="2"></colgroup>
+  <colgroup span="2"></colgroup>
+  <col>
+  <col>
+  <col>
+  <tr>
+    <th rowspan="2" scope="col">Hardware</th>
+    <th rowspan="2" scope="col">Workload<sup>1</sup></th>
+    <th rowspan="2" scope="col">Precision</th>
+    <th colspan="2" scope="colgroup">Throughput Inference<sup>2</sup></th>
+    <th colspan="2" scope="colgroup">Realtime Inference<sup>3</sup></th>
+    <th rowspan="2" scope="col">Model Type</th>
+    <th rowspan="2" scope="col">Dataset</th>
+    <th rowspan="2" scope="col">Input Data Shape</th>
+    <th rowspan="2" scope="col">Tunable Parameters</th>
+  </tr>
+  <tr>
+    <th scope="col">Batch Size</th>
+    <th scope="col">Boost Ratio</th>
+    <th scope="col">Batch Size</th>
+    <th scope="col">Boost Ratio</th>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" rowspan="10" scope="col">Intel(R) Xeon(R) Platinum 8380 CPU @ 2.30GHz</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ResNet50</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">INT8</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">80</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.83x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.44x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.11-models/quickstart/image_recognition/pytorch/resnet50/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">SSD-ResNet34</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">INT8</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">80</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">2.16x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.83x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">COCO</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 1200, 1200]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.11-models/quickstart/object_detection/pytorch/ssd-resnet34/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">ResNext 32x16d</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">INT8</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">80</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.81x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.21x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.11-models/quickstart/image_recognition/pytorch/resnext-32x16d/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">VGG-11</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">INT8</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">80</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.75x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.19x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.11-models/quickstart/image_recognition/pytorch/vgg11/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">ShuffleNetv2_x1.0</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">INT8</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">80</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">2.07x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.47x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">BERT-Large</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">INT8</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">80</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">2.78x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">2.04x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">NLP</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Squad</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">max_seq_len=384<br />Task: Question Answering</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Jemalloc;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.11-models/quickstart/language_modeling/pytorch/bert_large/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">Bert-Base</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">INT8</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">80</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">2.05x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.96x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">NLP</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">MRPC</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">max_seq_len=128<br />Task: Text Classification</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Jemalloc;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.11-models/quickstart/language_modeling/pytorch/bert_base/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">DistilBERT-Base</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">INT8</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">80</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">2.12x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.57x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">NLP</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Squad</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">max_seq_len=384<br />Task: Question Answering</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Jemalloc;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.11-models/quickstart/language_modeling/pytorch/distilbert_base/inference/cpu">inference scripts</a></td>
+  </tr>
+</tbody>
+</table>
+
+<br />
+<sup>1. <a href="https://github.com/IntelAI/models/tree/pytorch-r1.11-models">Model Zoo for Intel® Architecture</a></sup>
+<br />
+<sup>2. Throughput inference runs with single instance per socket.</sup>
+<br />
+<sup>3. Realtime inference runs with multiple instances, 4 cores per instance.</sup>
+<br />
+
+*Note:* Performance numbers with stock PyTorch are measured with its most performant configuration.
+
+*Note:* Environment variable *DNNL_PRIMITIVE_CACHE_CAPACITY* is set to *1024*.
+
+### Accuracy
+
+<table border="1" cellpadding="10" align="center" class="perf_table">
+<tbody>
+  <tr>
+    <th>Workload</th>
+    <th>Metric</th>
+    <th>FP32</th>
+    <th>INT8</th>
+    <th>INT8/FP32</th>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle">BERT-base_text_classification</td>
+    <td style="text-align: center; vertical-align: middle">f1</td>
+    <td style="text-align: center; vertical-align: middle">0.81</td>
+    <td style="text-align: center; vertical-align: middle">0.81</td>
+    <td style="text-align: center; vertical-align: middle">99.79%</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle">BERT-Large</td>
+    <td style="text-align: center; vertical-align: middle">f1</td>
+    <td style="text-align: center; vertical-align: middle">93.16</td>
+    <td style="text-align: center; vertical-align: middle">93.02</td>
+    <td style="text-align: center; vertical-align: middle">99.85%</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle">Distilbert-base</td>
+    <td style="text-align: center; vertical-align: middle">f1</td>
+    <td style="text-align: center; vertical-align: middle">86.84</td>
+    <td style="text-align: center; vertical-align: middle">86.13</td>
+    <td style="text-align: center; vertical-align: middle">99.19%</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle">ResNet50</td>
+    <td style="text-align: center; vertical-align: middle">Top1</td>
+    <td style="text-align: center; vertical-align: middle">76.15</td>
+    <td style="text-align: center; vertical-align: middle">75.98</td>
+    <td style="text-align: center; vertical-align: middle">99.78%</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle">ResNext 32x16d</td>
+    <td style="text-align: center; vertical-align: middle">Top1</td>
+    <td style="text-align: center; vertical-align: middle">84.17</td>
+    <td style="text-align: center; vertical-align: middle">84.05</td>
+    <td style="text-align: center; vertical-align: middle">99.86%</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle">SSD-ResNet34</td>
+    <td style="text-align: center; vertical-align: middle">mAP</td>
+    <td style="text-align: center; vertical-align: middle">0.200</td>
+    <td style="text-align: center; vertical-align: middle">0.199</td>
+    <td style="text-align: center; vertical-align: middle">99.48%</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle">VGG11</td>
+    <td style="text-align: center; vertical-align: middle">Top1</td>
+    <td style="text-align: center; vertical-align: middle">69.04</td>
+    <td style="text-align: center; vertical-align: middle">67.96</td>
+    <td style="text-align: center; vertical-align: middle">98.44%</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle">Shufflenetv2_x1.0</td>
+    <td style="text-align: center; vertical-align: middle">Top1</td>
+    <td style="text-align: center; vertical-align: middle">69.36</td>
+    <td style="text-align: center; vertical-align: middle">67.92</td>
+    <td style="text-align: center; vertical-align: middle">97.93%<sup>1</sup></td>
+  </tr>
+</tbody>
+</table>
+
+<br />
+<sup>1. ShuffleNet INT8 accuracy is expected to improve w/o performance trade-off via histogram calibration algorithm.</sup>
+<br />
+
+### Configuration
+
+#### Software Version
+
+| Software | Version |
+| :-: | :-: |
+| PyTorch | [v1.11.0](https://pytorch.org/get-started/locally/) |
+| Intel® Extension for PyTorch\* | [v1.11.0](https://github.com/intel/intel-extension-for-pytorch/releases) |
+
+#### Hardware Configuration
+
+| | 3rd Generation Intel® Xeon® Scalable Processors |
+| :-: | :-: |
+| CPU | Intel(R) Xeon(R) Platinum 8380 CPU @ 2.30GHz |
+| Number of nodes | 1 |
+| Number of sockets | 2 |
+| Cores/Socket | 40 |
+| Threads/Core | 2 |
+| uCode | 0xd0002a0 |
+| Hyper-Threading | ON |
+| TurboBoost | ON |
+| BIOS version | 04.12.02 |
+| Number of DDR Memory slots | 16 |
+| Capacity of DDR memory per slot | 16GB |
+| DDR frequency | 3200 |
+| Total Memory/Node (DDR+DCPMM) | 256GB |
+| Host OS | CentOS Linux release 8.4.2105 |
+| Host Kernel | 4.18.0-305.10.2.el8\_4.x86\_64 |
+| Docker OS | Ubuntu 18.04.5 LTS |
+| [Spectre-Meltdown Mitigation](https://github.com/speed47/spectre-meltdown-checker) | Mitigated |
+
+## FP32 with v1.11.200 on an AWS EC2 C6i.2xlarge instance
+
+### Performance Numbers
+
+<table border="1" cellpadding="10" align="center" class="perf_table">
+<tbody>
+  <col>
+  <col>
+  <col>
+  <colgroup span="2"></colgroup>
+  <colgroup span="2"></colgroup>
+  <col>
+  <col>
+  <col>
+  <tr>
+    <th rowspan="2" scope="col">Hardware</th>
+    <th rowspan="2" scope="col">Workload<sup>1</sup></th>
+    <th rowspan="2" scope="col">Precision</th>
+    <th colspan="2" scope="colgroup">Throughput Inference<sup>2</sup></th>
+    <th colspan="2" scope="colgroup">Real-time Inference<sup>3</sup></th>
+    <th rowspan="2" scope="col">Model Type</th>
+    <th rowspan="2" scope="col">Dataset</th>
+    <th rowspan="2" scope="col">Input Data Shape</th>
+    <th rowspan="2" scope="col">Tunable Parameters</th>
+  </tr>
+  <tr>
+    <th scope="col">Batch Size</th>
+    <th scope="col">Boost Ratio</th>
+    <th scope="col">Batch Size</th>
+    <th scope="col">Boost Ratio</th>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" rowspan="10" scope="col">AWS EC2 C6i.2xlarge</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ResNet50</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">64</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.24x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.31x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/image_recognition/pytorch/resnet50/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">ResNext 32x16d</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">64</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.07x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.05x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/image_recognition/pytorch/resnext-32x16d/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">VGG-11</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">64</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.15x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.21x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/image_recognition/pytorch/vgg11/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">ShuffleNetv2_x1.0</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">64</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.12x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.30x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">MobileNet v2</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">64</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.08x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.12x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">BERT-Large</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">64</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.05x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.03x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">NLP</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Squad</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">max_seq_len=384<br />Task: Question Answering</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/language_modeling/pytorch/bert_large/inference/cpu">inference scripts</a>;<br />Recommend to set auto_kernel_selection to ON when seq_len exceeds 64</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">Bert-Base</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">64</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.08x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.09x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">NLP</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">MRPC</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">max_seq_len=128<br />Task: Text Classification</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Jemalloc;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/language_modeling/pytorch/bert_base/inference/cpu">inference scripts</a>;<br />Recommend to set auto_kernel_selection to ON when seq_len exceeds 128</td>
+  </tr>
+</tbody>
+</table>
+
+<br />
+<sup>1. <a href="https://github.com/IntelAI/models/tree/pytorch-r1.11-models">Model Zoo for Intel® Architecture</a></sup>
+<br />
+<sup>2. Throughput inference runs with single instance per socket.</sup>
+<br />
+<sup>3. Realtime inference runs with multiple instances, 4 cores per instance.</sup>
+<br />
+
+*Note:* Performance numbers with stock PyTorch are measured with its most performant configuration.
+
+*Note:* Environment variable *DNNL_PRIMITIVE_CACHE_CAPACITY* is set to *1024*.
+
+### Configuration
+
+#### Software Version
+
+| Software | Version |
+| :-: | :-: |
+| PyTorch | [v1.11.0](https://pytorch.org/get-started/locally/) |
+| Intel® Extension for PyTorch\* | [v1.11.200](https://github.com/intel/intel-extension-for-pytorch/releases) |
+
+## FP32 and BFloat16 with v1.10
+
+### Performance Numbers
+
+<table border="1" cellpadding="10" align="center" class="perf_table">
+<tbody>
+  <col>
+  <col>
+  <col>
+  <colgroup span="2"></colgroup>
+  <colgroup span="2"></colgroup>
+  <col>
+  <col>
+  <col>
+  <tr>
+    <th rowspan="2" scope="col">Hardware</th>
+    <th rowspan="2" scope="col">Workload<sup>1</sup></th>
+    <th rowspan="2" scope="col">Precision</th>
+    <th colspan="2" scope="colgroup">Throughput Inference<sup>2</sup></th>
+    <th colspan="2" scope="colgroup">Real-time Inference<sup>3</sup></th>
+    <th rowspan="2" scope="col">Model Type</th>
+    <th rowspan="2" scope="col">Dataset</th>
+    <th rowspan="2" scope="col">Input Data Shape</th>
+    <th rowspan="2" scope="col">Tunable Parameters</th>
+  </tr>
+  <tr>
+    <th scope="col">Batch Size</th>
+    <th scope="col">Boost Ratio</th>
+    <th scope="col">Batch Size</th>
+    <th scope="col">Boost Ratio</th>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" rowspan="10" scope="col">Intel(R) Xeon(R) Platinum 8380 CPU @ 2.30GHz</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ResNet50</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">80</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.39x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.35x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/image_recognition/pytorch/resnet50/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">SSD-ResNet34</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">160</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.55x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.06x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">COCO</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 1200, 1200]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/object_detection/pytorch/ssd-resnet34/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">ResNext 32x16d</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">80</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.08x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.08x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/image_recognition/pytorch/resnext-32x16d/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">Faster R-CNN ResNet50 FPN</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">80</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.71x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.07x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">COCO</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 1200, 1200]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/object_detection/pytorch/maskrcnn_resnet50_fpn/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">VGG-11</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">160</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.20x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.13x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/image_recognition/pytorch/vgg11/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">ShuffleNetv2_x1.0</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">160</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.32x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.20x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">MobileNet v2</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">160</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.48x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.12x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Computer Vision</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">ImageNet</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Input shape<br />[3, 224, 224]</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">DLRM</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">80</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.11x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">-</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Recommendation</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Terabyte</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">-</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/recommendation/pytorch/dlrm/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">BERT-Large</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">80</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.14x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.02x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">NLP</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Squad</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">max_seq_len=384<br />Task: Question Answering</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Default memory allocator;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/language_modeling/pytorch/bert_large/inference/cpu">inference scripts</a>;<br />Recommend to set auto_kernel_selection to ON when seq_len exceeds 64</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">Bert-Base</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Float32</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">160</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.10x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.33x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">NLP</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">MRPC</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">max_seq_len=128<br />Task: Text Classification</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Jemalloc;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/language_modeling/pytorch/bert_base/inference/cpu">inference scripts</a>;<br />Recommend to set auto_kernel_selection to ON when seq_len exceeds 128</td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" rowspan="2" scope="col">Intel(R) Xeon(R) Platinum 8380H CPU @ 2.90GHz</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">BERT-Large</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">BFloat16</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">56</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.67x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.45x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">NLP</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Squad</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">max_seq_len=384<br />Task: Question Answering</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Jemalloc;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/language_modeling/pytorch/bert_large/inference/cpu">inference scripts</a></td>
+  </tr>
+  <tr>
+    <td style="text-align: center; vertical-align: middle" scope="col">Bert-Base</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">BFloat16</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">112</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.77x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">1.18x</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">NLP</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">MRPC</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">max_seq_len=128<br />Task: Text Classification</td>
+    <td style="text-align: center; vertical-align: middle" scope="col">Jemalloc;<br />Intel(R) OpenMP;<br /><a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models/quickstart/language_modeling/pytorch/bert_base/inference/cpu">inference scripts</a></td>
+  </tr>
+</tbody>
+</table>
+
+<br />
+<sup>1. <a href="https://github.com/IntelAI/models/tree/pytorch-r1.10-models">Model Zoo for Intel® Architecture</a></sup>
+<br />
+<sup>2. Throughput inference runs with single instance per socket.</sup>
+<br />
+<sup>3. Realtime inference runs with multiple instances, 4 cores per instance.</sup>
+<br />
+
+*Note:* Performance numbers with stock PyTorch are measured with its most performant configuration.
+
+*Note:* Environment variable *DNNL_PRIMITIVE_CACHE_CAPACITY* is set to *1024*.
+
+### Configuration
+
+#### Software Version
+
+| Software | Version |
+| :-: | :-: |
+| PyTorch | [v1.10.1](https://pytorch.org/get-started/locally/) |
+| Intel® Extension for PyTorch\* | [v1.10.100](https://github.com/intel/intel-extension-for-pytorch/releases) |
+
+#### Hardware Configuration
+
+| | 3rd Generation Intel® Xeon® Scalable Processors | Products formerly Cooper Lake |
+| :-: | :-: | :-: |
+| CPU | Intel(R) Xeon(R) Platinum 8380 CPU @ 2.30GHz | Intel(R) Xeon(R) Platinum 8380H CPU @ 2.90GHz
+|
+| Number of nodes | 1 | 1 |
+| Number of sockets | 2 | 2 |
+| Cores/Socket | 40 | 28 |
+| Threads/Core | 2 | 2 |
+| uCode | 0xd0002a0 | 0x700001c |
+| Hyper-Threading | ON | ON |
+| TurboBoost | ON | ON |
+| BIOS version | 04.12.02 | WLYDCRB1.SYS.0016.P29.2006080250 |
+| Number of DDR Memory slots | 16 | 12 |
+| Capacity of DDR memory per slot | 16GB | 64GB |
+| DDR frequency | 3200 | 3200 |
+| Total Memory/Node (DDR+DCPMM) | 256GB | 768GB |
+| Host OS | CentOS Linux release 8.4.2105 | Ubuntu 18.04.4 LTS |
+| Host Kernel | 4.18.0-305.10.2.el8\_4.x86\_64 | 4.15.0-76-generic |
+| Docker OS | Ubuntu 18.04.5 LTS | Ubuntu 18.04.5 LTS |
+| [Spectre-Meltdown Mitigation](https://github.com/speed47/spectre-meltdown-checker) | Mitigated | Mitigated |
diff --git a/docs/tutorials/performance_tuning.rst b/docs/tutorials/performance_tuning.rst
new file mode 100644
index 00000000..b2f0aa99
--- /dev/null
+++ b/docs/tutorials/performance_tuning.rst
@@ -0,0 +1,17 @@
+Performance Tuning Guide
+========================
+
+Intel® Extension for PyTorch\* should yield a satisfying performance with its default configuration for general use cases. To squeeze usage of hardware resources further, there are still several configurations that users can tune. This page shows tutorials for performance tuning guides, as well as an introduction of an easy-to-use tool.
+
+- `Performance Tuning Guide <performance_tuning/tuning_guide.html>`_
+- `Launch Script Usage Guide <performance_tuning/launch_script.html>`_
+- `TorchServe with Intel® Extension for PyTorch* <performance_tuning/torchserve.html>`_
+- `Known Issues <performance_tuning/known_issues.html>`_
+
+.. toctree::
+   :hidden:
+
+   performance_tuning/tuning_guide
+   performance_tuning/launch_script
+   performance_tuning/torchserve
+   performance_tuning/known_issues
diff --git a/docs/tutorials/performance_tuning/known_issues.md b/docs/tutorials/performance_tuning/known_issues.md
new file mode 100644
index 00000000..a93d246a
--- /dev/null
+++ b/docs/tutorials/performance_tuning/known_issues.md
@@ -0,0 +1,117 @@
+Known Issues
+============
+
+## Usage
+
+- There might be Python packages having PyTorch as their hard dependency. If you installed `+cpu` version of PyTorch, installation of these packages might replace the `+cpu` version with the default version released on Pypi.org. If anything goes wrong, please reinstall the `+cpu` version back.
+
+- If you found the workload runs with Intel® Extension for PyTorch\* occupies a remarkably large amount of memory, you can try to reduce the occupied memory size by setting the `--weights_prepack` parameter of the `ipex.optimize()` function to `False`.
+
+- If inference is done with a custom function, `conv+bn` folding feature of the `ipex.optimize()` function doesn't work.
+
+  ```
+  import torch
+  import intel_pytorch_extension as ipex
+
+  class Module(torch.nn.Module):
+      def __init__(self):
+          super(Module, self).__init__()
+          self.conv = torch.nn.Conv2d(1, 10, 5, 1)
+          self.bn = torch.nn.BatchNorm2d(10)
+          self.relu = torch.nn.ReLU()
+
+      def forward(self, x):
+          x = self.conv(x)
+          x = self.bn(x)
+          x = self.relu(x)
+          return x
+
+      def inference(self, x):
+          return self.forward(x)
+
+  if __name__ == '__main__':
+      m = Module()
+      m.eval()
+      m = ipex.optimize(m, dtype=torch.float32, level="O0")
+      d = torch.rand(1, 1, 112, 112)
+      with torch.no_grad():
+        m.inference(d)
+  ```
+
+  This is a PyTorch FX limitation. You can avoid this error by calling `m = ipex.optimize(m, level="O0")`, which doesn't apply ipex optimization, or disable `conv+bn` folding by calling `m = ipex.optimize(m, level="O1", conv_bn_folding=False)`.
+
+## TorchDynamo
+
+- The support of torch.compile() with ipex as the backend is still an experimental feature. If the workload fails to run or demonstrates poor performance, you can use the `torch.jit` APIs and graph optimization APIs of ipex. Currently, the below HuggingFace models fail to run using torch.compile() with ipex backend due to memory issues:
+  - masked-language-modeling+xlm-roberta-base
+  - casual-language-modeling+gpt2
+  - casual-language-modeling+xlm-roberta-base
+  - summarization+t5-base
+  - text-classification+allenai-longformer-base-409
+
+## Dynamic Shape
+
+- When working with an NLP model inference with dynamic input data length appling with TorchScript (either `torch.jit.trace` or `torch.jit.script`), performance with Intel® Extension for PyTorch\* is possible to be less than that without Intel® Extension for PyTorch\*. In this case, adding the workarounds below would help solve this issue.
+  - Python interface
+    ```python
+    torch._C._jit_set_texpr_fuser_enabled(False)
+    ```
+  - C++ interface
+    ```c++
+    #include <torch/csrc/jit/passes/tensorexpr_fuser.h>
+    torch::jit::setTensorExprFuserEnabled(false);
+    ```
+
+## INT8
+
+- Low performance with INT8 support for dynamic shapes
+
+  The support for dynamic shapes in Intel® Extension for PyTorch\* INT8 integration is still work in progress. When the input shapes are dynamic, for example inputs of variable image sizes in an object detection task or of variable sequence lengths in NLP tasks, the Intel® Extension for PyTorch\* INT8 path may slow down the model inference. In this case, use stock PyTorch INT8 functionality.
+
+  **Note**: Using Runtime Extension feature if batch size cannot be divided by number of streams, because mini batch size on each stream are not equivalent, scripts run into this issues.
+
+- Supporting of EmbeddingBag with INT8 when bag size > 1 is working in progress.
+
+- `RuntimeError: Overflow when unpacking long` when a tensor's min max value exceeds int range while performing int8 calibration. Please customize QConfig to use min-max calibration method.
+
+- For models with dynamic control flow, please try dynamic quantization. Users are likely to get performance gain for GEMM models.
+
+- Calibrating with quantize_per_tensor, when benchmarking with 1 OpenMP\* thread, results might be incorrect with large tensors (find more detailed info [here](https://github.com/pytorch/pytorch/issues/80501). Editing your code following the pseudocode below can workaround this issue, if you do need to explicitly set OMP_NUM_THREAEDS=1 for benchmarking. However, there could be a performance regression if oneDNN graph compiler prototype feature is utilized.
+
+  Workaround pseudocode:
+  ```
+  # perform convert/trace/freeze with omp_num_threads > 1(N)
+  torch.set_num_threads(N)
+  prepared_model = prepare(model, input)
+  converted_model = convert(prepared_model)
+  traced_model = torch.jit.trace(converted_model, input)
+  freezed_model = torch.jit.freeze(traced_model)
+  # run freezed model to apply optimization pass
+  freezed_model(input)
+  
+  # benchmarking with omp_num_threads = 1
+  torch.set_num_threads(1)
+  run_benchmark(freezed_model, input)
+  ```
+
+## BFloat16
+
+- BF16 AMP(auto-mixed-precision) runs abnormally with the extension on the AVX2-only machine if the topology contains `Conv`, `Matmul`, `Linear`, and `BatchNormalization`
+
+## Runtime Extension
+
+- Runtime extension of MultiStreamModule doesn't support DLRM inference, since the input of DLRM (EmbeddingBag specifically) can't be simplely batch split.
+
+- Runtime extension of MultiStreamModule has poor performance of RNNT Inference comparing with native throughput mode. Only part of the RNNT models (joint_net specifically) can be jit traced into graph. However, in one batch inference, `joint_net` is invoked multi times. It increases the overhead of MultiStreamModule as input batch split, thread synchronization and output concat.
+
+## Correctness
+
+- Incorrect Conv and Linear result if the number of OMP threads is changed at runtime
+
+  The oneDNN memory layout depends on the number of OMP threads, which requires the caller to detect the changes for the # of OMP threads while this release has not implemented it yet.
+
+## Float32 Training
+
+- Low throughput with DLRM FP32 Train
+
+  A 'Sparse Add' [PR](https://github.com/pytorch/pytorch/pull/23057) is pending on review. The issue will be fixed when the PR is merged.
diff --git a/docs/tutorials/performance_tuning/launch_script.md b/docs/tutorials/performance_tuning/launch_script.md
new file mode 100644
index 00000000..db7f1557
--- /dev/null
+++ b/docs/tutorials/performance_tuning/launch_script.md
@@ -0,0 +1,534 @@
+Launch Script Usage Guide
+=========================
+
+## Overview
+
+As introduced in the [Performance Tuning Guide](tuning_guide.md), there are several factors that influence performance. Setting configuration options properly contributes to a performance boost. However, there is no unified configuration that is optimal to all topologies. Users need to try different combinations by themselves. A *launch* script is provided to automate these configuration settings to free users from this complicated work. This guide helps you to learn some common usage examples that cover many optimized configuration cases.
+
+The configurations are mainly around the following perspectives.
+1. OpenMP library: [**Intel OpenMP library** (default) | GNU OpenMP library]
+2. Memory allocator: [PyTorch default memory allocator | Jemalloc | **TCMalloc** (default)]
+3. Number of instances: [**Single instance** (default) | Multiple instances]
+
+## Usage of launch script
+
+The *launch* script is provided as a module of *intel_extension_for_pytorch*. You can take advantage of it with the following command:
+
+```
+ipexrun [knobs] <your_pytorch_script> [args]
+```
+
+Available option settings (knobs) are listed below:
+
+| knob | type | default value | help |
+| :-- | :--: | :--: | :-- |
+| `-h`, `--help` | - | - | show this help message and exit |
+| `-m`, `--module` | - | False | Changes each process to interpret the launch script  as a python module, executing with the same behavior as 'python -m'. |
+| `--no-python` | - | False | Avoid applying `python` to execute `program`. |
+| `--log-dir` | str | '' | The log file directory. Setting it to empty ('') disables logging to files. |
+| `--log-file-prefix` | str | 'run' | log file name prefix |
+
+Launcher Common Arguments:
+
+| knob | type | default value | help |
+| :-- | :--: | :--: | :-- |
+| `--ncores-per-instance` | int | 0 | Number of cores per instance |
+| `--nodes-list` | str | '' | Specify nodes list for multiple instances to run on, in format of list of single node ids "node_id,node_id,..." or list of node ranges "node_id-node_id,...". By default all nodes will be used. |
+| `--use-e-cores` | - | False | Use Efficient-Cores on the workloads or not. By default, only Performance-Cores are used. |
+| `--memory-allocator` | str | 'auto' | Choose which memory allocator to run the workloads with. Supported choices are ['auto', 'default', 'tcmalloc', 'jemalloc']. |
+| `--omp-runtime` | str | 'auto' | Choose which OpenMP runtime to run the workloads with. Supported choices are ['auto', 'default', 'intel']. |
+
+Multi-instance Arguments:
+
+| knob | type | default value | help |
+| :-- | :--: | :--: | :-- |
+| `--ninstances` | int | 0 | Number of instances |
+| `--instance-idx` | int | -1 | Inside the multi instance list, execute a specific instance at index. If it is set to -1, run all of them. |
+| `--use-logical-cores` | - | False | Use logical cores on the workloads or not. By default, only physical cores are used. |
+| `--skip-cross-node-cores` | - | False | Allow instances to be executed on cores across NUMA nodes. |
+| `--multi-task-manager` | str | 'auto' | Choose which multi task manager to run the workloads with. Supported choices are ['auto', 'none', 'numactl', 'taskset']. |
+| `--latency-mode` | - | False | Use 4 cores per instance over all physical cores. |
+| `--throughput-mode` | - | False | Run one instance per node with all physical cores. |
+| `--cores-list` | str | '' | Specify cores list for multiple instances to run on, in format of list of single core ids "core_id,core_id,..." or list of core ranges "core_id-core_id,...". By default all cores will be used. |
+| `--benchmark` | - | False | Enable benchmark config. JeMalloc's MALLOC_CONF has been tuned for low latency. Recommend to use this for benchmarking purpose; for other use cases, this MALLOC_CONF may cause Out-of-Memory crash. |
+
+Distributed Training Arguments With oneCCL backend:
+
+| knob | type | default value | help |
+| :-- | :--: | :--: | :-- |
+| `--nnodes` | int | 0 | Number of machines/devices to use for distributed training |
+| `--nprocs-per-node` | int | 0 | Number of processes run on each machine/device. It is by default the number of available nodes when set to `0`. Argument `--nodes-list` affects this default value. |
+| `--ccl-worker-count` | int | 4 | Number of cores per rank for ccl communication |
+| `--logical-cores-for-ccl` | - | False | Use logical cores for the ccl worker. |
+| `--master-addr` | str | 127.0.0.1 | Address of master node (rank 0), should be either IP address or hostname of node 0. For single node multi-proc training, the --master-addr can simply be 127.0.0.1. |
+| `--master-port` | int | 29500 | Port on master node (rank 0) for communication during distributed training. |
+| `--hostfile` | str | 'hostfile' | Set the hostfile for multi-node multi-proc training. The hostfile includes a node address list containing either IP addresses or hostnames of computation nodes. |
+| `--extra-mpi-params` | str | '' | Extra parameters for mpiexec.hydra except for -np -ppn -hostfile and -genv I_MPI_PIN_DOMAIN |
+
+[Codeless Optimization feature](../features/codeless_optimization.md) related option settings (knobs) are listed below:
+
+| knob | type | default value | help |
+| :-- | :--: | :--: | :-- |
+| `--auto-ipex` | - | False | Auto enabled the ipex optimization feature |
+| `--dtype` | string | False | data type, can choose from ['float32', 'bfloat16'] |
+| `--auto-ipex-verbose` | - | False | This flag is only used for debug and UT of auto ipex. |
+| `--disable-ipex-graph-mode` | - | False | Enable the Graph Mode for `ipex.optimize()` function |
+
+**Note:** `--latency-mode` and `--throughput-mode` are exclusive knobs to `--ninstances`, `--ncores-per-instance` and `--use-logical-cores`. I.e., setting either of `--latency-mode` or `--throughput-mode` overwrites settings of `--ninstances`, `--ncores-per-instance` and `--use-logical-cores` if they are explicitly set in command line. `--latency-mode` and `--throughput-mode` are mutually exclusive.
+
+The *launch* script respects existing environment variables when it get launched, except for *LD_PRELOAD*. If you have your favorite values for certain environment variables, you can set them before running the *launch* script. Intel OpenMP library uses an environment variable *KMP_AFFINITY* to control its behavior. Different settings result in different performance numbers. By default, if you enable Intel OpenMP library, the *launch* script will set *KMP_AFFINITY* to `granularity=fine,compact,1,0`. If you want to try with other values, you can use `export` command on Linux to set *KMP_AFFINITY* before you run the *launch* script. In this case, the script will not set the default value but take the existing value of *KMP_AFFINITY*, and print a message to stdout.
+
+Execution via the *launch* script can dump logs into files under a designated log directory so you can do some investigations afterward. By default, it is disabled to avoid undesired log files. You can enable logging by setting knob `--log-dir` to be:
+
+- directory to store log files. It can be an absolute path or relative path.
+- types of log files to generate. One file (`<prefix>_timestamp_instances.log`) contains command and information when the script was launched. Another type of file (`<prefix>_timestamp_instance_#_core#-core#....log`) contain stdout print of each instance.
+
+For example:
+
+```
+run_20210712212258_instances.log
+run_20210712212258_instance_0_cores_0-43.log
+```
+
+## Usage Examples
+
+Example script [resnet50.py](https://github.com/intel/intel-extension-for-pytorch/tree/v2.0.100+cpu/examples/cpu/inference/resnet50_general_inference_script.py) will be used in this guide.
+
+- Single instance for inference
+  - [I. Use all physical cores](#i-use-all-physical-cores)
+  - [II. Use all cores including logical cores](#ii-use-all-cores-including-logical-cores)
+  - [III. Use physical cores on 1 node](#iii-use-physical-cores-on-1-node)
+  - [IV. Use your designated number of cores](#iv-use-your-designated-number-of-cores)
+- Multiple instances for inference
+  - [V. Throughput mode (i.e. number of numa node instances, each instance runs on 1 numa node)](#v-throughput-mode)
+  - [VI. Latency mode (Use 4 cores for each instance)](#vi-latency-mode)
+  - [VII. Your designated number of instances](#vii-your-designated-number-of-instances)
+  - [VIII. Your designated number of instances and instance index](#viii-your-designated-number-of-instances-and-instance-index)
+- Usage of Jemalloc/TCMalloc/Default memory allocator
+  - [Jemalloc](#jemalloc)
+  - [TCMalloc](#tcmalloc)
+  - [Default memory allocator](#default-memory-allocator)
+- Usage of GNU OpenMP library
+  - [Intel OpenMP library](#intel-openmp-library)
+  - [GNU OpenMP library](#gnu-openmp-library)
+
+__Note:__ GIF files below illustrate CPU usage ONLY. Do NOT infer performance numbers.
+
+### Single instance for inference
+
+#### I. Use all physical cores
+
+```
+ipexrun --log-dir ./logs resnet50.py
+```
+
+CPU usage is shown as below. 1 main worker thread was launched, then it launched physical core number of threads on all physical cores.
+
+![Single instance all physical cores](../../../images/launch_script/1ins_phy.gif)
+
+If you check your log directory, you will find directory structure as below.
+
+```
+.
+├── resnet50.py
+└── logs
+    ├── run_20210712212258_instance_0_cores_0-43.log
+    └── run_20210712212258_instances.log
+```
+
+The `run_20210712212258_instances.log` contains information and command that were used for this execution launch.
+
+```
+$ cat logs/run_20210712212258_instances.log
+2021-07-12 21:22:58,764 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/<user>/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
+2021-07-12 21:22:58,764 - __main__ - INFO - OMP_NUM_THREADS=44
+2021-07-12 21:22:58,764 - __main__ - INFO - Using Intel OpenMP
+2021-07-12 21:22:58,764 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2021-07-12 21:22:58,764 - __main__ - INFO - KMP_BLOCKTIME=1
+2021-07-12 21:22:58,764 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so
+2021-07-12 21:22:58,764 - __main__ - WARNING - Numa Aware: cores:['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43'] on different NUMA nodes
+2021-07-12 21:22:58,764 - __main__ - INFO - numactl -C 0-43 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712212258_instance_0_cores_0-43.log
+```
+
+#### II. Use all cores including logical cores
+
+```
+ipexrun --use-logical-core --log-dir ./logs resnet50.py
+```
+
+CPU usage is shown as below. 1 main worker thread was launched, then it launched threads on all cores, including logical cores.
+
+![Single instance logical cores](../../../images/launch_script/1ins_log.gif)
+
+If you check your log directory, you will find directory structure as below.
+
+```
+.
+├── resnet50.py
+└── logs
+    ├── run_20210712223308_instances.log
+    └── run_20210712223308_instance_0_cores_0-87.log
+```
+
+The `run_20210712223308_instances.log` contains information and command that were used for this execution launch.
+
+```
+$ cat logs/run_20210712223308_instances.log
+2021-07-12 22:33:08,117 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/<user>/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
+2021-07-12 22:33:08,117 - __main__ - INFO - OMP_NUM_THREADS=88
+2021-07-12 22:33:08,117 - __main__ - INFO - Using Intel OpenMP
+2021-07-12 22:33:08,118 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2021-07-12 22:33:08,118 - __main__ - INFO - KMP_BLOCKTIME=1
+2021-07-12 22:33:08,118 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so
+2021-07-12 22:33:08,118 - __main__ - WARNING - Numa Aware: cores:['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87'] on different NUMA nodes
+2021-07-12 22:33:08,118 - __main__ - INFO - numactl -C 0-87 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712223308_instance_0_cores_0-87.log
+```
+
+#### III. Use physical cores on designated nodes
+
+```
+ipexrun --nodes-list 1 --log-dir ./logs resnet50.py
+```
+
+CPU usage is shown as below. 1 main worker thread was launched, then it launched threads on all other cores on the same numa node.
+
+![Single instance all physical cores](../../../images/launch_script/1ins_soc.gif)
+
+If you check your log directory, you will find directory structure as below.
+
+```
+.
+├── resnet50.py
+└── logs
+    ├── run_20210712214504_instances.log
+    └── run_20210712214504_instance_0_cores_22-43.log
+
+```
+
+The `run_20210712214504_instances.log` contains information and command that were used for this execution launch.
+
+```
+$ cat logs/run_20210712214504_instances.log
+2021-07-12 21:45:04,512 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/<user>/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
+2021-07-12 21:45:04,513 - __main__ - INFO - OMP_NUM_THREADS=22
+2021-07-12 21:45:04,513 - __main__ - INFO - Using Intel OpenMP
+2021-07-12 21:45:04,513 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2021-07-12 21:45:04,513 - __main__ - INFO - KMP_BLOCKTIME=1
+2021-07-12 21:45:04,513 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so
+2021-07-12 21:45:04,513 - __main__ - INFO - numactl -C 22-43 -m 1 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712214504_instance_0_cores_22-43.log
+```
+
+#### IV. Use your designated number of cores
+
+```
+ipexrun --ninstances 1 --ncores-per-instance 10 --log-dir ./logs resnet50.py
+```
+
+CPU usage is shown as below. 1 main worker thread was launched, then it launched threads on other 9 physical cores.
+
+![Single instance designated number of cores](../../../images/launch_script/1ins_cus.gif)
+
+If you check your log directory, you will find directory structure as below.
+
+```
+.
+├── resnet50.py
+└── logs
+    ├── run_20210712220928_instances.log
+    └── run_20210712220928_instance_0_cores_0-9.log
+```
+
+The `run_20210712220928_instances.log` contains information and command that were used for this execution launch.
+
+```
+$ cat logs/run_20210712220928_instances.log
+2021-07-12 22:09:28,355 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/<user>/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
+2021-07-12 22:09:28,355 - __main__ - INFO - OMP_NUM_THREADS=10
+2021-07-12 22:09:28,355 - __main__ - INFO - Using Intel OpenMP
+2021-07-12 22:09:28,355 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2021-07-12 22:09:28,356 - __main__ - INFO - KMP_BLOCKTIME=1
+2021-07-12 22:09:28,356 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so
+2021-07-12 22:09:28,356 - __main__ - INFO - numactl -C 0-9 -m 0 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712220928_instance_0_cores_0-9.log
+```
+
+You can also specify the cores to be utilized using `--cores-list` argument. For example, if core id 11-20 are desired instead of the first 10 cores, the launch command would be as below.
+
+```
+ipexrun --ncores-per-instance 10 --cores-list "11-20" --log-dir ./logs resnet50.py
+```
+
+Please notice that when specifying `--cores-list`, a correspondant `--ncores-per-instance` argument is required for instance number deduction.
+
+In this case the log directory should be like
+```
+.
+├── resnet50.py
+└── logs
+    ├── run_20210712221615_instances.log
+    └── run_20210712221615_instance_0_cores_11-20.log
+```
+
+The `run_20210712221615_instances.log` contains information and command that were used for this execution launch.
+
+```
+$ cat logs/run_20210712221615_instances.log
+2021-07-12 22:16:15,591 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/<user>/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
+2021-07-12 22:16:15,591 - __main__ - INFO - OMP_NUM_THREADS=10
+2021-07-12 22:16:15,591 - __main__ - INFO - Using Intel OpenMP
+2021-07-12 22:16:15,591 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2021-07-12 22:16:15,591 - __main__ - INFO - KMP_BLOCKTIME=1
+2021-07-12 22:16:15,591 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so
+2021-07-12 22:16:15,591 - __main__ - INFO - numactl -C 11-20 -m 0 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221615_instance_0_cores_11-20.log
+```
+
+### Multiple instances for inference
+
+#### V. Throughput mode
+
+```
+ipexrun --throughput-mode --log-dir ./logs resnet50.py
+```
+
+CPU usage is shown as below. 2 main worker threads were launched on 2 numa nodes respectively, then they launched threads on other physical cores.
+
+![Multiple instance throughput mode](../../../images/launch_script/nins_thr.gif)
+
+If you check your log directory, you will find directory structure as below.
+
+```
+.
+├── resnet50.py
+└── logs
+    ├── run_20210712221150_instances.log
+    ├── run_20210712221150_instance_0_cores_0-21.log
+    └── run_20210712221150_instance_1_cores_22-43.log
+```
+
+The `run_20210712221150_instances.log` contains information and command that were used for this execution launch.
+
+```
+$ cat logs/run_20210712221150_instances.log
+2021-07-12 22:11:50,233 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/<user>/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
+2021-07-12 22:11:50,233 - __main__ - INFO - OMP_NUM_THREADS=22
+2021-07-12 22:11:50,233 - __main__ - INFO - Using Intel OpenMP
+2021-07-12 22:11:50,233 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2021-07-12 22:11:50,233 - __main__ - INFO - KMP_BLOCKTIME=1
+2021-07-12 22:11:50,233 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so
+2021-07-12 22:11:50,233 - __main__ - INFO - numactl -C 0-21 -m 0 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221150_instance_0_cores_0-21.log
+2021-07-12 22:11:50,236 - __main__ - INFO - numactl -C 22-43 -m 1 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221150_instance_1_cores_22-43.log
+```
+
+#### VI. Latency mode
+
+```
+ipexrun --latency-mode --log-dir ./logs resnet50.py
+```
+
+CPU usage is shown as below. 4 cores are used for each instance.
+
+![Multiple instances latency mode](../../../images/launch_script/nins_lat.gif)
+
+If you check your log directory, you will find directory structure as below.
+
+```
+.
+├── resnet50.py
+└── logs
+    ├── run_20210712221415_instances.log
+    ├── run_20210712221415_instance_0_cores_0-3.log
+    ├── run_20210712221415_instance_1_cores_4-7.log
+    ├── run_20210712221415_instance_2_cores_8-11.log
+    ├── run_20210712221415_instance_3_cores_12-15.log
+    ├── run_20210712221415_instance_4_cores_16-19.log
+    ├── run_20210712221415_instance_5_cores_20-23.log
+    ├── run_20210712221415_instance_6_cores_24-27.log
+    ├── run_20210712221415_instance_7_cores_28-31.log
+    ├── run_20210712221415_instance_8_cores_32-35.log
+    ├── run_20210712221415_instance_9_cores_36-39.log
+    └── run_20210712221415_instance_10_cores_40-43.log
+```
+
+The `run_20210712221415_instances.log` contains information and command that were used for this execution launch.
+
+```
+$ cat logs/run_20210712221415_instances.log
+2021-07-12 22:14:15,140 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/<user>/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
+2021-07-12 22:14:15,140 - __main__ - INFO - OMP_NUM_THREADS=4
+2021-07-12 22:14:15,140 - __main__ - INFO - Using Intel OpenMP
+2021-07-12 22:14:15,140 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2021-07-12 22:14:15,140 - __main__ - INFO - KMP_BLOCKTIME=1
+2021-07-12 22:14:15,140 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so
+2021-07-12 22:14:15,140 - __main__ - INFO - numactl -C 0-3 -m 0 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221415_instance_0_cores_0-3.log
+2021-07-12 22:14:15,143 - __main__ - INFO - numactl -C 4-7 -m 0 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221415_instance_1_cores_4-7.log
+2021-07-12 22:14:15,146 - __main__ - INFO - numactl -C 8-11 -m 0 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221415_instance_2_cores_8-11.log
+2021-07-12 22:14:15,149 - __main__ - INFO - numactl -C 12-15 -m 0 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221415_instance_3_cores_12-15.log
+2021-07-12 22:14:15,151 - __main__ - INFO - numactl -C 16-19 -m 0 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221415_instance_4_cores_16-19.log
+2021-07-12 22:14:15,154 - __main__ - WARNING - Numa Aware: cores:['20', '21', '22', '23'] on different NUMA nodes
+2021-07-12 22:14:15,154 - __main__ - INFO - numactl -C 20-23 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221415_instance_5_cores_20-23.log
+2021-07-12 22:14:15,157 - __main__ - INFO - numactl -C 24-27 -m 1 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221415_instance_6_cores_24-27.log
+2021-07-12 22:14:15,159 - __main__ - INFO - numactl -C 28-31 -m 1 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221415_instance_7_cores_28-31.log
+2021-07-12 22:14:15,162 - __main__ - INFO - numactl -C 32-35 -m 1 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221415_instance_8_cores_32-35.log
+2021-07-12 22:14:15,164 - __main__ - INFO - numactl -C 36-39 -m 1 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221415_instance_9_cores_36-39.log
+2021-07-12 22:14:15,167 - __main__ - INFO - numactl -C 40-43 -m 1 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221415_instance_10_cores_40-43.log
+```
+
+#### VII. Your designated number of instances
+
+```
+ipexrun --ninstances 4 --log-dir ./logs resnet50.py
+```
+
+CPU usage is shown as below. 4 main worker thread were launched, then they launched threads on all other physical cores.
+
+![Multiple instances designated number of instances](../../../images/launch_script/nins_cus.gif)
+
+If you check your log directory, you will find directory structure as below.
+
+```
+.
+├── resnet50.py
+└── logs
+    ├── run_20210712221305_instances.log
+    ├── run_20210712221305_instance_0_cores_0-10.log
+    ├── run_20210712221305_instance_1_cores_11-21.log
+    ├── run_20210712221305_instance_2_cores_22-32.log
+    └── run_20210712221305_instance_3_cores_33-43.log
+```
+
+The `run_20210712221305_instances.log` contains information and command that were used for this execution launch.
+
+```
+$ cat logs/run_20210712221305_instances.log
+2021-07-12 22:13:05,470 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/<user>/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
+2021-07-12 22:13:05,470 - __main__ - INFO - OMP_NUM_THREADS=11
+2021-07-12 22:13:05,470 - __main__ - INFO - Using Intel OpenMP
+2021-07-12 22:13:05,470 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2021-07-12 22:13:05,470 - __main__ - INFO - KMP_BLOCKTIME=1
+2021-07-12 22:13:05,470 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so
+2021-07-12 22:13:05,471 - __main__ - INFO - numactl -C 0-10 -m 0 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221305_instance_0_cores_0-10.log
+2021-07-12 22:13:05,473 - __main__ - INFO - numactl -C 11-21 -m 0 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221305_instance_1_cores_11-21.log
+2021-07-12 22:13:05,476 - __main__ - INFO - numactl -C 22-32 -m 1 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221305_instance_2_cores_22-32.log
+2021-07-12 22:13:05,479 - __main__ - INFO - numactl -C 33-43 -m 1 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210712221305_instance_3_cores_33-43.log
+```
+
+#### VIII. Your designated number of instances and instance index
+
+Launcher by default runs all `ninstances` for multi-instance inference/training as shown above. You can specify `instance_idx` to independently run that instance only among `ninstances`
+
+```
+ipexrun --ninstances 4 --instance-idx 0 --log-dir ./logs resnet50.py
+```
+
+you can confirm usage in log file:
+
+```
+2022-01-06 13:01:51,175 - __main__ - INFO - OMP_NUM_THREADS=14
+2022-01-06 13:01:51,176 - __main__ - INFO - Using Intel OpenMP
+2022-01-06 13:01:51,177 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2022-01-06 13:01:51,177 - __main__ - INFO - KMP_BLOCKTIME=1
+2022-01-06 13:01:51,177 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so
+2022-01-06 13:01:51,177 - __main__ - INFO - numactl -C 0-10 -m 0 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20220106130151_instance_0_cores_0-13.log
+```
+
+```
+ipexrun --ninstances 4 --instance-idx 1 --log-dir ./logs resnet50.py
+```
+
+you can confirm usage in log file:
+
+```
+2022-01-06 13:01:51,175 - __main__ - INFO - OMP_NUM_THREADS=14
+2022-01-06 13:01:51,176 - __main__ - INFO - Using Intel OpenMP
+2022-01-06 13:01:51,177 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2022-01-06 13:01:51,177 - __main__ - INFO - KMP_BLOCKTIME=1
+2022-01-06 13:01:51,177 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so
+2022-01-06 13:01:51,177 - __main__ - INFO - numactl -C 11-21 -m 0 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20220106130151_instance_0_cores_0-13.log
+```
+
+### Usage of Jemalloc/TCMalloc/Default memory allocator
+
+Memory allocator influences performance sometime. If users do not designate desired memory allocator, the *launch* script searches them in the order of TCMalloc > Jemalloc > PyTorch default memory allocator, and takes the first matched one.
+
+#### Jemalloc
+
+__Note:__ You can set your favorite value to *MALLOC_CONF* before running the *launch* script if you do not want to use its default setting.
+
+```
+ipexrun --memory-allocator jemalloc --log-dir ./logs resnet50.py
+```
+
+you can confirm usage in log file:
+
+```
+2021-07-13 15:30:48,235 - __main__ - INFO - Use JeMallocl memory allocator
+2021-07-13 15:30:48,235 - __main__ - INFO - MALLOC_CONF=oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000
+2021-07-13 15:30:48,235 - __main__ - INFO - OMP_NUM_THREADS=44
+2021-07-13 15:30:48,235 - __main__ - INFO - Using Intel OpenMP
+2021-07-13 15:30:48,235 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2021-07-13 15:30:48,235 - __main__ - INFO - KMP_BLOCKTIME=1
+2021-07-13 15:30:48,235 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so:<VIRTUAL_ENV>/lib/libjemalloc.so
+2021-07-13 15:30:48,236 - __main__ - WARNING - Numa Aware: cores:['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43'] on different NUMA nodes
+2021-07-13 15:30:48,236 - __main__ - INFO - numactl -C 0-43 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210713153048_instance_0_cores_0-43.log
+```
+
+#### TCMalloc
+
+```
+ipexrun --memory-allocator tcmalloc --log-dir ./logs resnet50.py
+```
+
+you can confirm usage in log file:
+
+```
+2021-07-13 15:33:33,654 - __main__ - INFO - Use TCMalloc memory allocator
+2021-07-13 15:33:33,654 - __main__ - INFO - OMP_NUM_THREADS=44
+2021-07-13 15:33:33,654 - __main__ - INFO - Using Intel OpenMP
+2021-07-13 15:33:33,654 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2021-07-13 15:33:33,654 - __main__ - INFO - KMP_BLOCKTIME=1
+2021-07-13 15:33:33,654 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so:<VIRTUAL_ENV>/lib/libtcmalloc.so
+2021-07-13 15:33:33,654 - __main__ - WARNING - Numa Aware: cores:['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43'] on different NUMA nodes
+2021-07-13 15:33:33,655 - __main__ - INFO - numactl -C 0-43 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210713153333_instance_0_cores_0-43.log
+```
+
+#### Default memory allocator
+
+```
+ipexrun --memory-allocator default --log-dir ./logs resnet50.py
+```
+
+you can confirm usage in log file:
+
+```
+2021-07-13 15:36:59,784 - __main__ - INFO - OMP_NUM_THREADS=44
+2021-07-13 15:36:59,784 - __main__ - INFO - Using Intel OpenMP
+2021-07-13 15:36:59,784 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2021-07-13 15:36:59,784 - __main__ - INFO - KMP_BLOCKTIME=1
+2021-07-13 15:36:59,784 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so
+2021-07-13 15:36:59,784 - __main__ - WARNING - Numa Aware: cores:['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43'] on different NUMA nodes
+2021-07-13 15:36:59,784 - __main__ - INFO - numactl -C 0-43 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210713153659_instance_0_cores_0-43.log
+```
+
+### Usage of OpenMP library
+
+#### Intel OpenMP Library
+
+Generally, Intel OpenMP library brings better performance. Thus, in the *launch* script, Intel OpenMP library is used by default, if it is available. Intel OpenMP library takes environment variables like *KMP_AFFINITY* and *KMP_BLOCKTIME* to control its behavior. You can set your favorite values to them before running the *launch* script if you do not want to use the default settings.
+
+#### GNU OpenMP Library
+
+It is, however, not always that Intel OpenMP library brings better performance comparing to GNU OpenMP library. In this case, you can use knob `--omp-runtime default` to switch active OpenMP library to the GNU one. GNU OpenMP specific environment variables, *OMP_SCHEDULE* and *OMP_PROC_BIND*, for setting CPU affinity are set automatically.
+
+```
+ipexrun --omp-runtime default --log-dir ./logs resnet50.py
+```
+
+you can confirm usage in log file:
+
+```
+2021-07-13 15:25:00,760 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/<user>/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
+2021-07-13 15:25:00,761 - __main__ - INFO - OMP_SCHEDULE=STATIC
+2021-07-13 15:25:00,761 - __main__ - INFO - OMP_PROC_BIND=CLOSE
+2021-07-13 15:25:00,761 - __main__ - INFO - OMP_NUM_THREADS=44
+2021-07-13 15:25:00,761 - __main__ - WARNING - Numa Aware: cores:['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43'] on different NUMA nodes
+2021-07-13 15:25:00,761 - __main__ - INFO - numactl -C 0-43 <VIRTUAL_ENV>/bin/python resnet50.py 2>&1 | tee ./logs/run_20210713152500_instance_0_cores_0-43.log
+```
diff --git a/docs/tutorials/performance_tuning/torchserve.md b/docs/tutorials/performance_tuning/torchserve.md
new file mode 100644
index 00000000..a5d8d694
--- /dev/null
+++ b/docs/tutorials/performance_tuning/torchserve.md
@@ -0,0 +1,322 @@
+# TorchServe with Intel® Extension for PyTorch\*
+
+TorchServe can be used with Intel® Extension for PyTorch\* to give performance boost on Intel hardware.<sup>1</sup>
+Here we show how to use TorchServe with Intel® Extension for PyTorch\*.
+
+<sup>1. While Intel® Extension for PyTorch\* benefits all platforms, platforms with AVX512 benefit the most. </sup>
+
+## Contents of this Document
+* [Install Intel® Extension for PyTorch\*](#install-intel-extension-for-pytorch)
+* [Serving model with Intel® Extension for PyTorch\*](#serving-model-with-intel-extension-for-pytorch)
+* [TorchServe with Launcher](#torchserve-with-launcher)
+* [Creating and Exporting INT8 model for Intel® Extension for PyTorch\*](#creating-and-exporting-int8-model-for-intel-extension-for-pytorch)
+* [Benchmarking with Launcher](#benchmarking-with-launcher)
+* [Performance Boost with Intel® Extension for PyTorch\* and Launcher](#performance-boost-with-intel-extension-for-pytorch-and-launcher)
+
+
+## Install Intel® Extension for PyTorch\*
+Refer to the documentation [here](../installation.md).
+
+## Serving model with Intel® Extension for PyTorch\*
+After installation, all it needs to use TorchServe with Intel® Extension for PyTorch\* is to enable it in `config.properties`.
+```
+ipex_enable=true
+```
+Once Intel® Extension for PyTorch\* is enabled, deploying PyTorch model follows the same procedure shown [here](https://pytorch.org/serve/use_cases.html). TorchServe with Intel® Extension for PyTorch\* can deploy any model and do inference.
+
+## TorchServe with Launcher
+Launcher is a script to automate the process of tunining configuration setting on Intel hardware to boost performance. Tuning configurations such as OMP_NUM_THREADS, thread affinity, memory allocator can have a dramatic effect on performance. Refer to [Performance Tuning Guide](./tuning_guide.md) and [Launch Script Usage Guide](./launch_script.md) for details on performance tuning with launcher.
+
+All it needs to use TorchServe with launcher is to set its configuration in `config.properties`.
+
+Add the following lines in `config.properties` to use launcher with its default configuration.
+```
+ipex_enable=true
+cpu_launcher_enable=true
+```
+
+Launcher by default uses `numactl` if it's installed to ensure socket is pinned and thus memory is allocated from local numa node. To use launcher without numactl, add the following lines in `config.properties`.
+```
+ipex_enable=true
+cpu_launcher_enable=true
+cpu_launcher_args=--disable_numactl
+```
+
+Launcher by default uses only non-hyperthreaded cores if hyperthreading is present to avoid core compute resource sharing. To use launcher with all cores, both physical and logical, add the following lines in `config.properties`.
+```
+ipex_enable=true
+cpu_launcher_enable=true
+cpu_launcher_args=--use_logical_core
+```
+
+Below is an example of passing multiple args to `cpu_launcher_args`.
+```
+ipex_enable=true
+cpu_launcher_enable=true
+cpu_launcher_args=--use_logical_core --disable_numactl
+```
+
+Below are some useful `cpu_launcher_args` to note. Italic values are default if applicable.
+1. Memory Allocator: [ PTMalloc `--use_default_allocator` | *TCMalloc `--enable_tcmalloc`* | JeMalloc `--enable_jemalloc`]
+   * PyTorch by default uses PTMalloc. TCMalloc/JeMalloc generally gives better performance.
+2. OpenMP library: [GNU OpenMP `--disable_iomp` | *Intel OpenMP*]
+   * PyTorch by default uses GNU OpenMP. Launcher by default uses Intel OpenMP. Intel OpenMP library generally gives better performance.
+3. Node id: [`--node_id`]
+   * Launcher by default uses all NUMA nodes. Limit memory access to local memories on the Nth Numa node to avoid Non-Uniform Memory Access (NUMA).
+
+Refer to [Launch Script Usage Guide](./launch_script.md) for a full list of tunable configuration of launcher. And refer to [Performance Tuning Guide](./tuning_guide.md) for more details.
+
+### Launcher Core Pinning to Boost Performance of TorchServe Multi Worker Inference
+When running [multi-worker inference](https://pytorch.org/serve/management_api.html#scale-workers) with Torchserve (Required torchserve>=0.6.1), launcher pin cores to workers to boost performance. Internally, launcher equally divides the number of cores by the number of workers such that each worker is pinned to assigned cores. Doing so avoids core overlap among workers which can signficantly boost performance for TorchServe multi-worker inference. For example, assume running 4 workers on a machine with Intel(R) Xeon(R) Platinum 8180 CPU, 2 sockets, 28 cores per socket, 2 threads per core. Launcher will bind worker 0 to cores 0-13, worker 1 to cores 14-27, worker 2 to cores 28-41, and worker 3 to cores 42-55.
+
+CPU usage is shown below. 4 main worker threads were launched, each launching 14 threads affinitized to the assigned physical cores.
+![26](https://user-images.githubusercontent.com/93151422/170373651-fd8a0363-febf-4528-bbae-e1ddef119358.gif)
+
+
+#### Scaling workers
+Additionally when dynamically [scaling the number of workers](https://pytorch.org/serve/management_api.html#scale-workers), cores that were pinned to killed workers by the launcher could be left unutilized. To address this problem, launcher internally restarts the workers to re-distribute cores that were pinned to killed workers to the remaining, alive workers. This is taken care internally, so users do not have to worry about this.
+
+Continuing with the above example with 4 workers, assume killing workers 2 and 3. If cores were not re-distributed after the scale down, cores 28-55 would be left unutilized. Instead, launcher re-distributes cores 28-55 to workers 0 and 1 such that now worker 0 binds to cores 0-27 and worker 1 binds to cores 28-55.<sup>2</sup>
+
+CPU usage is shown below. 4 main worker threads were initially launched. Then after scaling down the number of workers from 4 to 2, 2 main worker threads were launched, each launching 28 threads affinitized to the assigned physical cores.
+![worker_scaling](https://user-images.githubusercontent.com/93151422/170374697-7497c2d5-4c17-421b-9993-1434d1f722f6.gif)
+
+<sup>2. Serving is interrupted for few seconds while re-distributing cores to scaled workers.</sup>
+
+Again, all it needs to use TorchServe with launcher core pinning for multiple workers as well as scaling workers is to set its configuration in `config.properties`.
+
+Add the following lines in `config.properties` to use launcher with its default configuration.
+```
+cpu_launcher_enable=true
+```
+
+## Creating and Exporting INT8 model for Intel® Extension for PyTorch\*
+Intel® Extension for PyTorch\* supports both eager and torchscript mode. In this section, we show how to deploy INT8 model for Intel® Extension for PyTorch\*. Refer to [here](../features/int8_overview.md) for more details on Intel® Extension for PyTorch\* optimizations for quantization.
+
+### 1. Creating a serialized file
+First create `.pt` serialized file using Intel® Extension for PyTorch\* INT8 inference. Here we show two examples with BERT and ResNet50.
+
+#### BERT
+
+```
+import torch
+import intel_extension_for_pytorch as ipex
+from transformers import BertModel
+
+# load the model
+model = BertModel.from_pretrained('bert-base-uncased')
+model = model.eval()
+
+# define dummy input tensor to use for the model's forward call to record operations in the model for tracing
+vocab_size = model.config.vocab_size
+batch_size = 1
+seq_length = 384
+dummy_tensor = torch.randint(vocab_size, size=[batch_size, seq_length])
+
+from intel_extension_for_pytorch.quantization import prepare, convert
+
+# ipex supports two quantization schemes: static and dynamic
+# default dynamic qconfig
+qconfig = ipex.quantization.default_dynamic_qconfig
+
+# prepare and calibrate
+model = prepare(model, qconfig, example_inputs=dummy_tensor)
+
+# convert and deploy
+model = convert(model)
+
+with torch.no_grad():
+    model = torch.jit.trace(model, dummy_tensor, check_trace=False, strict=False)
+    model = torch.jit.freeze(model)
+
+torch.jit.save(model, 'bert_int8_jit.pt')
+```
+
+#### ResNet50
+
+```
+import torch
+import intel_extension_for_pytorch as ipex
+import torchvision.models as models
+
+# load the model
+model = models.resnet50(pretrained=True)
+model = model.eval()
+
+# define dummy input tensor to use for the model's forward call to record operations in the model for tracing
+N, C, H, W = 1, 3, 224, 224
+dummy_tensor = torch.randn(N, C, H, W)
+
+from intel_extension_for_pytorch.quantization import prepare, convert
+
+# ipex supports two quantization schemes: static and dynamic
+# default static qconfig
+qconfig = ipex.quantization.default_static_qconfig
+
+# prepare and calibrate
+model = prepare(model, qconfig, example_inputs=dummy_tensor, inplace=False)
+
+n_iter = 100
+for i in range(n_iter):
+    model(dummy_tensor)
+
+# convert and deploy
+model = convert(model)
+
+with torch.no_grad():
+    model = torch.jit.trace(model, dummy_tensor)
+    model = torch.jit.freeze(model)
+
+torch.jit.save(model, 'rn50_int8_jit.pt')
+```
+
+### 2. Creating a Model Archive
+Once the serialized file ( `.pt`) is created, it can be used with `torch-model-archiver` as ususal.
+
+Use the following command to package `rn50_int8_jit.pt` into `rn50_ipex_int8.mar`.
+```
+torch-model-archiver --model-name rn50_ipex_int8 --version 1.0 --serialized-file rn50_int8_jit.pt --handler image_classifier
+```
+Similarly, use the following command in the [Huggingface_Transformers directory](https://github.com/pytorch/serve/tree/master/examples/Huggingface_Transformers) to package `bert_int8_jit.pt` into `bert_ipex_int8.mar`.
+
+```
+torch-model-archiver --model-name bert_ipex_int8 --version 1.0 --serialized-file bert_int8_jit.pt --handler ./Transformer_handler_generalized.py --extra-files "./setup_config.json,./Seq_classification_artifacts/index_to_name.json"
+```
+
+### 3. Start TorchServe to serve the model
+Make sure to set `ipex_enable=true` in `config.properties`. Use the following command to start TorchServe with Intel® Extension for PyTorch\*.
+```
+torchserve --start --ncs --model-store model_store --ts-config config.properties
+```
+
+### 4. Registering and Deploying model
+Registering and deploying the model follows the same steps shown [here](https://pytorch.org/serve/use_cases.html).
+
+## Benchmarking with Launcher
+Launcher can be used with TorchServe official [benchmark](https://github.com/pytorch/serve/tree/master/benchmarks) to launch server and benchmark requests with optimal configuration on Intel hardware.
+
+In this section we provide examples of benchmarking with launcher with its default configuration.
+
+Add the following lines to `config.properties` in the benchmark directory to use launcher with its default setting.
+```
+ipex_enable=true
+cpu_launcher_enable=true
+```
+
+The rest of the steps for benchmarking follows the same steps shown [here](https://github.com/pytorch/serve/tree/master/benchmarks).
+
+`model_log.log` contains information and command that were used for this execution launch.
+
+
+CPU usage on a machine with Intel(R) Xeon(R) Platinum 8180 CPU, 2 sockets, 28 cores per socket, 2 threads per core is shown as below:
+![launcher_default_2sockets](https://user-images.githubusercontent.com/93151422/144373537-07787510-039d-44c4-8cfd-6afeeb64ac78.gif)
+
+```
+$ cat logs/model_log.log
+2021-12-01 21:22:40,096 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/<user>/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
+2021-12-01 21:22:40,096 - __main__ - INFO - OMP_NUM_THREADS=56
+2021-12-01 21:22:40,096 - __main__ - INFO - Using Intel OpenMP
+2021-12-01 21:22:40,096 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2021-12-01 21:22:40,096 - __main__ - INFO - KMP_BLOCKTIME=1
+2021-12-01 21:22:40,096 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so
+2021-12-01 21:22:40,096 - __main__ - WARNING - Numa Aware: cores:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55] in different NUMA node
+```
+
+CPU usage on a machine with Intel(R) Xeon(R) Platinum 8375C CPU, 1 socket, 2 cores per socket, 2 threads per socket is shown as below:
+![launcher_default_1socket](https://user-images.githubusercontent.com/93151422/144372993-92b2ca96-f309-41e2-a5c8-bf2143815c93.gif)
+
+```
+$ cat logs/model_log.log
+2021-12-02 06:15:03,981 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/<user>/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
+2021-12-02 06:15:03,981 - __main__ - INFO - OMP_NUM_THREADS=2
+2021-12-02 06:15:03,982 - __main__ - INFO - Using Intel OpenMP
+2021-12-02 06:15:03,982 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2021-12-02 06:15:03,982 - __main__ - INFO - KMP_BLOCKTIME=1
+2021-12-02 06:15:03,982 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so
+
+```
+
+### Benchmarking with Launcher Core Pinning
+As described previously in [TorchServe with Launcher](#torchserve-with-launcher), launcher core pinning boosts performance of multi-worker inference. We'll demonstrate launcher core pinning with TorchServe benchmark, but keep in mind that launcher core pinning is a generic feature applicable to any TorchServe multi-worker inference use casese.
+
+For example, assume running 4 workers
+```
+python benchmark-ab.py --workers 4
+```
+on a machine with Intel(R) Xeon(R) Platinum 8180 CPU, 2 sockets, 28 cores per socket, 2 threads per core. Launcher will bind worker 0 to cores 0-13, worker 1 to cores 14-27, worker 2 to cores 28-41, and worker 3 to cores 42-55.
+
+All it needs to use TorchServe with launcher's core pinning is to enable launcher in `config.properties`.
+
+Add the following lines to `config.properties` in the benchmark directory to use launcher's core pinning:
+```
+cpu_launcher_enable=true
+```
+
+CPU usage is shown as below:
+![launcher_core_pinning](https://user-images.githubusercontent.com/93151422/159063975-e7e8d4b0-e083-4733-bdb6-4d92bdc10556.gif)
+
+4 main worker threads were launched, then each launched a num_physical_cores/num_workers number (14) of threads affinitized to the assigned physical cores.
+
+<pre><code>
+$ cat logs/model_log.log
+2022-03-24 10:41:32,223 - __main__ - INFO - Use TCMalloc memory allocator
+2022-03-24 10:41:32,223 - __main__ - INFO - OMP_NUM_THREADS=14
+2022-03-24 10:41:32,223 - __main__ - INFO - Using Intel OpenMP
+2022-03-24 10:41:32,223 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2022-03-24 10:41:32,223 - __main__ - INFO - KMP_BLOCKTIME=1
+2022-03-24 10:41:32,223 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so:<VIRTUAL_ENV>/lib/libtcmalloc.so
+2022-03-24 10:41:32,223 - __main__ - INFO - <b>numactl -C 0-13 -m 0</b> <VIRTUAL_ENV>/bin/python -u <VIRTUAL_ENV>/lib/python/site-packages/ts/model_service_worker.py --sock-type unix --sock-name /tmp/.ts.sock.9000
+
+2022-03-24 10:49:03,760 - __main__ - INFO - Use TCMalloc memory allocator
+2022-03-24 10:49:03,761 - __main__ - INFO - OMP_NUM_THREADS=14
+2022-03-24 10:49:03,762 - __main__ - INFO - Using Intel OpenMP
+2022-03-24 10:49:03,762 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2022-03-24 10:49:03,762 - __main__ - INFO - KMP_BLOCKTIME=1
+2022-03-24 10:49:03,762 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so:<VIRTUAL_ENV>/lib/libtcmalloc.so
+2022-03-24 10:49:03,763 - __main__ - INFO - <b>numactl -C 14-27 -m 0</b> <VIRTUAL_ENV>/bin/python -u <VIRTUAL_ENV>/lib/python/site-packages/ts/model_service_worker.py --sock-type unix --sock-name /tmp/.ts.sock.9001
+
+2022-03-24 10:49:26,274 - __main__ - INFO - Use TCMalloc memory allocator
+2022-03-24 10:49:26,274 - __main__ - INFO - OMP_NUM_THREADS=14
+2022-03-24 10:49:26,274 - __main__ - INFO - Using Intel OpenMP
+2022-03-24 10:49:26,274 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2022-03-24 10:49:26,274 - __main__ - INFO - KMP_BLOCKTIME=1
+2022-03-24 10:49:26,274 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so:<VIRTUAL_ENV>/lib/libtcmalloc.so
+2022-03-24 10:49:26,274 - __main__ - INFO - <b>numactl -C 28-41 -m 1</b> <VIRTUAL_ENV>/bin/python -u <VIRTUAL_ENV>/lib/python/site-packages/ts/model_service_worker.py --sock-type unix --sock-name /tmp/.ts.sock.9002
+
+2022-03-24 10:49:42,975 - __main__ - INFO - Use TCMalloc memory allocator
+2022-03-24 10:49:42,975 - __main__ - INFO - OMP_NUM_THREADS=14
+2022-03-24 10:49:42,975 - __main__ - INFO - Using Intel OpenMP
+2022-03-24 10:49:42,975 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
+2022-03-24 10:49:42,975 - __main__ - INFO - KMP_BLOCKTIME=1
+2022-03-24 10:49:42,975 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so:<VIRTUAL_ENV>/lib/libtcmalloc.so
+2022-03-24 10:49:42,975 - __main__ - INFO - <b>numactl -C 42-55 -m 1</b> <VIRTUAL_ENV>/bin/python -u <VIRTUAL_ENV>/lib/python/site-packages/ts/model_service_worker.py --sock-type unix --sock-name /tmp/.ts.sock.9003
+</code></pre>
+
+## Performance Boost with Intel® Extension for PyTorch\* and Launcher
+
+![pdt_perf](https://user-images.githubusercontent.com/93151422/159067306-dfd604e3-8c66-4365-91ae-c99f68d972d5.png)
+
+
+Above shows performance improvement of Torchserve with Intel® Extension for PyTorch\* and launcher on ResNet50 and BERT-base-uncased. Torchserve official [apache-bench benchmark](https://github.com/pytorch/serve/tree/master/benchmarks#benchmarking-with-apache-bench) on Amazon EC2 m6i.24xlarge was used to collect the results<sup>2</sup>. Add the following lines in ```config.properties``` to reproduce the results. Notice that launcher is configured such that a single instance uses all physical cores on a single socket to avoid cross socket communication and core overlap.
+
+```
+ipex_enable=true
+cpu_launcher_enable=true
+cpu_launcher_args=--node_id 0 --enable_jemalloc
+```
+Use the following command to reproduce the results.
+```
+python benchmark-ab.py --url {modelUrl} --input {inputPath} --concurrency 1
+```
+
+For example, run the following command to reproduce latency performance of ResNet50 with data type of Intel® Extension for PyTorch\* int8 and batch size of 1. Refer to [Creating and Exporting INT8 model for Intel® Extension for PyTorch\*](#creating-and-exporting-int8-model-for-intel-extension-for-pytorch) for steps to creating ```rn50_ipex_int8.mar``` file for ResNet50 with Intel® Extension for PyTorch\* int8 data type.
+```
+python benchmark-ab.py --url 'file:///model_store/rn50_ipex_int8.mar' --concurrency 1
+```
+
+For example, run the following command to reproduce latency performance of BERT with data type of Intel® Extension for PyTorch\* int8 and batch size of 1. Refer to [Creating and Exporting INT8 model for Intel® Extension for PyTorch\*](#creating-and-exporting-int8-model-for-intel-extension-for-pytorch) for steps to creating ```bert_ipex_int8.mar``` file for BERT with Intel® Extension for PyTorch\* int8 data type.
+```
+python benchmark-ab.py --url 'file:///model_store/bert_ipex_int8.mar' --input '../examples/Huggingface_Transformers/Seq_classification_artifacts/sample_text_captum_input.txt' --concurrency 1
+```
+
+<sup>3. Amazon EC2 m6i.24xlarge was used for benchmarking purpose only. For multi-core instances, Intel® Extension for PyTorch\* optimizations automatically scale and leverage full instance resources.</sup>
diff --git a/docs/tutorials/performance_tuning/tuning_guide.md b/docs/tutorials/performance_tuning/tuning_guide.md
new file mode 100644
index 00000000..6dadcfb7
--- /dev/null
+++ b/docs/tutorials/performance_tuning/tuning_guide.md
@@ -0,0 +1,264 @@
+Performance Tuning Guide
+========================
+
+## Overview
+
+Intel® Extension for PyTorch\* is a Python package to extend official PyTorch. It makes the out-of-box user experience of PyTorch CPU better while achieving good performance. To fully utilize the power of Intel® architecture and thus yield high performance, PyTorch, as well as Intel® Extension for PyTorch\*, are powered by [oneAPI Deep Neural Network Library (oneDNN)](https://github.com/oneapi-src/oneDNN), an open-source cross-platform performance library of basic building blocks for deep learning applications. It is developed and optimized for Intel Architecture Processors, Intel Processor Graphics, and Xe architecture-based Graphics.
+
+Although default primitives of PyTorch and Intel® Extension for PyTorch\* are highly optimized, there are things users can do improve performance. Most optimized configurations can be automatically set by the launcher script. This article introduces common methods recommended by Intel developers.
+
+## Contents of this Document
+* [Hardware Configuration](#hardware-configuration)
+  * [Intel CPU Structure](#intel-cpu-structure)
+  * [Non-Uniform Memory Access (NUMA)](#non-uniform-memory-access-numa)
+* [Software Configuration](#software-configuration)
+  * [Channels Last](#channels-last)
+  * [Numactl](#numactl)
+  * [OpenMP](#openmp)
+    * [OMP_NUM_THREADS](#omp-num-threads)
+    * [OMP_THREAD_LIMIT](#omp-thread-limit)
+    * [GNU OpenMP](#gnu-openmp)
+    * [Intel OpenMP](#intel-openmp)
+  * [Memory Allocator](#memory-allocator)
+    * [Jemalloc](#jemalloc)
+    * [TCMalloc](#tcmalloc)
+  * [Denormal Number](#denormal-number)
+  * [OneDNN primitive cache](#onednn-primitive-cache)
+
+## Hardware Configuration
+
+This section briefly introduces the structure of Intel CPUs, as well as concept of Non-Uniform Memory Access (NUMA).
+
+### Intel CPU Structure
+
+There are many families of Intel CPUs. We'll use Intel® Xeon® processor Scalable family as an example to discuss an Intel CPU and how it works. Understanding this background knowledge is helpful to understand the PyTorch optimization methodologies that Intel engineers recommend.
+
+On the Intel® Xeon® Scalable Processors with Intel® C620 Series Chipsets, (formerly Purley) platform, each chip provides up to 28 cores. Each core has a non-inclusive last-level cache and an 1MB L2 cache. The CPU features fast 2666 MHz DDR4 memory, six memory channels per CPU, Intel Ultra Path Interconnect (UPI) high speed point-to-point processor interconnect, and more. Figure 1 shows microarchitecture of the Intel® Xeon® processor Scalable family chips. Each CPU chip consists of a number of cores, along with core-specific cache. 6 channels of DDR4 memory are connected to the chip directly. Meanwhile, chips communicates through the Intel UPI interconnect, which features a transfer speed of up to 10.4 GT/s.
+
+<div align="center">
+
+![Block Diagram of the Intel® Xeon® processor Scalable family microarchitecture](https://software.intel.com/content/dam/develop/external/us/en/images/xeon-processor-scalable-family-tech-overview-fig03-737410.png)
+
+Figure 1: Block Diagram of the Intel® Xeon® processor Scalable family microarchitecture.
+
+</div>
+
+Usually, a CPU chip is called a socket. A typical two-socket configuration is illustrated as Figure 2. Two CPU sockets are equipped on one motherboard. Each socket is connected to up to 6 channels of memory, called its local memory, from socket perspective. Sockets are connected to each other via Intel UPI. It is possible for each socket to access memories attached on other sockets, usually called remote memory access. Local memory access is always faster than remote memory access. Meanwhile, cores on one socket share a space of high speed cache memory, which is much faster than communication via Intel UPI. Figure 3 shows an ASUS Z11PA-D8 Intel® Xeon® server motherboard, equipping with two sockets for Intel® Xeon® processor Scalable family CPUs.
+
+<div align="center">
+
+![Typical two-socket configuration](https://software.intel.com/content/dam/develop/external/us/en/images/xeon-processor-scalable-family-tech-overview-fig06-737410.png)
+
+Figure 2: Typical two-socket configuration.
+
+![ASUS Z11PA-D8 Intel® Xeon® server motherboard](https://dlcdnimgs.asus.com/websites/global/products/MCCApMgGOdr9WJxN/MB-Z11PAD8-overview-01-s.jpg)
+
+Figure 3: An ASUS Z11PA-D8 Intel® Xeon® server motherboard. It contains two sockets for Intel® Xeon® processor Scalable family CPUs.
+
+</div>
+
+### Non-Uniform Memory Access (NUMA)
+
+It is a good thing that more and more CPU cores are provided to users in one socket, because this brings more computation resources. However, this also brings memory access competitions. Program can stall because memory is busy to visit. To address this problem, Non-Uniform Memory Access (NUMA) was introduced. Comparing to Uniform Memory Access (UMA), in which scenario all memories are connected to all cores equally, NUMA tells memories into multiple groups. Certain number of memories are directly attached to one socket's integrated memory controller to become local memory of this socket. As described in the previous section, local memory access is much faster than remote memory access.
+
+Users can get CPU information with `lscpu` command on Linux to learn how many cores, sockets there on the machine. Also, NUMA information like how CPU cores are distributed can also be retrieved. The following is an example of `lscpu` execution on a machine with two Intel(R) Xeon(R) Platinum 8180M CPUs. 2 sockets were detected. Each socket has 28 physical cores onboard. Since Hyper-Threading is enabled, each core can run 2 threads. I.e. each socket has another 28 logical cores. Thus, there are 112 CPU cores on service. When indexing CPU cores, usually physical cores are indexed before logical core. In this case, the first 28 cores (0-27) are physical cores on the first NUMA socket (node), the second 28 cores (28-55) are physical cores on the second NUMA socket (node). Logical cores are indexed afterward. 56-83 are 28 logical cores on the first NUMA socket (node), 84-111 are the second 28 logical cores on the second NUMA socket (node). Typically, running Intel® Extension for PyTorch\* should avoid using logical cores to get a good performance.
+
+```
+$ lscpu
+...
+CPU(s):              112
+On-line CPU(s) list: 0-111
+Thread(s) per core:  2
+Core(s) per socket:  28
+Socket(s):           2
+NUMA node(s):        2
+...
+Model name:          Intel(R) Xeon(R) Platinum 8180M CPU @ 2.50GHz
+...
+NUMA node0 CPU(s):   0-27,56-83
+NUMA node1 CPU(s):   28-55,84-111
+...
+```
+
+## Software Configuration
+
+This section introduces software configurations that helps to boost performance.
+
+### Channels Last
+
+Take advantage of **Channels Last** memory format for image processing tasks. Comparing to PyTorch default NCHW (`torch.contiguous_format`) memory format, NHWC (`torch.channels_last`) is more friendly to Intel platforms, and thus generally yields better performance. More detailed introduction can be found at [Channels Last page](../features/nhwc.md). You can get sample codes with Resnet50 at [Example page](../examples.md).
+
+### Numactl
+
+Since NUMA largely influences memory access performance, this functionality should also be implemented in software side.
+
+During development of Linux kernels, more and more sophisticated implementations/optimizations/strategies had been brought out. Version 2.5 of the Linux kernel already contained basic NUMA support, which was further improved in subsequent kernel releases. Version 3.8 of the Linux kernel brought a new NUMA foundation that allowed development of more efficient NUMA policies in later kernel releases. Version 3.13 of the Linux kernel brought numerous policies that aim at putting a process near its memory, together with the handling of cases such as having memory pages shared between processes, or the use of transparent huge pages. New sysctl settings allow NUMA balancing to be enabled or disabled, as well as the configuration of various NUMA memory balancing parameters.[1] Behavior of Linux kernels are thus different according to kernel version. Newer Linux kernels may contain further optimizations of NUMA strategies, and thus have better performances. For some workloads, NUMA strategy influences performance great.
+
+Linux provides a tool, `numactl`, that allows user control of NUMA policy for processes or shared memory. It runs processes with a specific NUMA scheduling or memory placement policy. As described in previous section, cores share high-speed cache in one socket, thus it is a good idea to avoid cross socket computations. From a memory access perspective, bounding memory access locally is much faster than accessing remote memories.
+
+The following is an example of numactl usage to run a workload on the Nth socket and limit memory access to its local memories on the Nth socket. More detailed description of numactl command can be found [on the numactl man page](https://linux.die.net/man/8/numactl).
+
+```numactl --cpunodebind N --membind N python <script>```
+
+Assume core 0-3 are on socket 0, the following command binds script execution on core 0-3, and binds memory access to socket 0 local memories.
+
+```numactl --membind 0 -C 0-3 python <script>```
+
+[1] [Wikipedia - Non-uniform memory access](https://en.wikipedia.org/wiki/Non-uniform_memory_access)
+
+### OpenMP
+
+OpenMP is an implementation of multithreading, a method of parallelizing where a primary thread (a series of instructions executed consecutively) forks a specified number of sub-threads and the system divides a task among them. The threads then run concurrently, with the runtime environment allocating threads to different processors.[2] Figure 4 illustrates fork-join model of OpenMP execution.
+
+<div align="center">
+
+![A number of parallel block execution threads are forked from primary thread](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/Fork_join.svg/1920px-Fork_join.svg.png)
+
+Figure 4: A number of parallel block execution threads are forked from primary thread.
+
+</div>
+
+Users can control OpenMP behaviors through some environment variables to fit for their workloads. Also, beside GNU OpenMP library ([libgomp](https://gcc.gnu.org/onlinedocs/libgomp/)), Intel provides another OpenMP implementation [libiomp](https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support.html) for users to choose from. Environment variables that control behavior of OpenMP threads may differ from libgomp and libiomp. They will be introduced separately in sections below.
+
+GNU OpenMP (libgomp) is the default multi-threading library for both PyTorch and Intel® Extension for PyTorch\*.
+
+[2] [Wikipedia - OpenMP](https://en.wikipedia.org/wiki/OpenMP)
+
+#### OMP_NUM_THREADS
+
+Environment variable `OMP_NUM_THREADS` sets the number of threads used for parallel regions. By default, it is set to be the number of available physical cores. It can be used along with numactl settings, as the following example. If cores 0-3 are on socket 0, this example command runs \<script\> on cores 0-3, with 4 OpenMP threads.
+
+This environment variable works on both libgomp and libiomp.
+
+```
+export OMP_NUM_THREADS=4
+numactl -C 0-3 --membind 0 python <script>
+```
+
+#### OMP_THREAD_LIMIT
+
+Environment variable `OMP_THREAD_LIMIT` specifies the number of threads to use for the whole program. The value of this variable shall be a positive integer. If undefined, the number of threads is not limited.
+
+Please make sure `OMP_THREAD_LIMIT` is set to a number equal to or larger than `OMP_NUM_THREADS` to avoid backward propagation hanging issues.
+
+#### GNU OpenMP
+
+Beside `OMP_NUM_THREADS`, other GNU OpenMP specific environment variables are commonly used to improve performance:
+
+- `GOMP_CPU_AFFINITY`: Binds threads to specific CPUs. The variable should contain a space-separated or comma-separated list of CPUs.
+- `OMP_PROC_BIND`: Specifies whether threads may be moved between processors. Setting it to CLOSE keeps OpenMP threads close to the primary thread in contiguous place partitions.
+- `OMP_SCHEDULE`: Determine how OpenMP threads are scheduled.
+
+Here are recommended settings of these environment variables:
+
+```
+export GOMP_CPU_AFFINITY="0-3"
+export OMP_PROC_BIND=CLOSE
+export OMP_SCHEDULE=STATIC
+```
+
+#### Intel OpenMP
+
+By default, PyTorch uses GNU OpenMP (GNU libgomp) for parallel computation. On Intel platforms, Intel OpenMP Runtime Library (libiomp) provides OpenMP API specification support. It sometimes brings more performance benefits compared to libgomp. Utilizing environment variable `LD_PRELOAD` can switch OpenMP library to libiomp:
+
+```
+export LD_PRELOAD=<path>/libiomp5.so:$LD_PRELOAD
+```
+
+Similar to GNU OpenMP, beside `OMP_NUM_THREADS`, there are other Intel OpenMP specific environment variables that control behavior of OpenMP threads:
+
+- `KMP_AFFINITY`
+
+  `KMP_AFFINITY` controls how to to bind OpenMP threads to physical processing units. Depending on the system (machine) topology, application, and operating system, thread affinity can have a dramatic effect on the application speed.
+
+  A common usage scenario is to bind consecutive threads close together, as is done with KMP_AFFINITY=compact. By doing this, communication overhead, cache line invalidation overhead, and page thrashing are minimized. Now, suppose the application also had a number of parallel regions that did not utilize all of the available OpenMP threads. A thread normally executes faster on a core where it is not competing for resources with another active thread on the same core. It is always good to avoid binding multiple threads to the same core while leaving other cores unused. This can be achieved by the following command. Figure 5 illustrates this strategy.
+
+  ```
+  export KMP_AFFINITY=granularity=fine,compact,1,0
+  ```
+
+  <div align="center">
+
+  ![KMP_AFFINITY=granularity=fine,compact,1,0](../../../images/performance_tuning_guide/kmp_affinity.jpg)
+
+  Figure 5: *KMP_AFFINITY=granularity=fine,compact,1,0* 
+
+  </div>
+
+  The OpenMP thread n+1 is bound to a thread context as close as possible to OpenMP thread n, but on a different core. Once each core has been assigned one OpenMP thread, the subsequent OpenMP threads are assigned to the available cores in the same order, but they are assigned on different thread contexts.
+
+
+  It is also possible to bind OpenMP threads to certain CPU cores with the following command.
+
+  ```
+  export KMP_AFFINITY=granularity=fine,proclist=[N-M],explicit
+  ```
+
+  More detailed information about `KMP_AFFINITY` can be found in the [Intel CPP Compiler Developer Guide](https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support/openmp-library-support/thread-affinity-interface-linux-and-windows.html).
+
+- `KMP_BLOCKTIME`
+
+  `KMP_BLOCKTIME` sets the time, in milliseconds, that a thread, after completing the execution of a parallel region, should wait before sleeping. The default value is 200ms.
+
+  After completing the execution of a parallel region, threads wait for new parallel work to become available. After a certain period of time has elapsed, they stop waiting and sleep. Sleeping allows the threads to be used, until more parallel work becomes available, by non-OpenMP threaded code that may execute between parallel regions, or by other applications. A small `KMP_BLOCKTIME` value may offer better overall performance if application contains non-OpenMP threaded code that executes between parallel regions. A larger `KMP_BLOCKTIME` value may be more appropriate if threads are to be reserved solely for use for OpenMP execution, but may penalize other concurrently-running OpenMP or threaded applications. It is suggested to be set to 0 or 1 for convolutional neural network (CNN) based models.
+
+  ```
+  export KMP_BLOCKTIME=0 (or 1)
+  ```
+
+### Memory Allocator
+
+Memory allocator plays an important role from performance perspective as well. A more efficient memory usage reduces overhead on unnecessary memory allocations or destructions, and thus results in a faster execution. From practical experiences, for deep learning workloads, Jemalloc or TCMalloc can get better performance by reusing memory as much as possible than default malloc function.
+
+It is as simple as adding path of Jemalloc/TCMalloc dynamic library to environment variable `LD_PRELOAD` to switch memory allocator to one of them.
+
+```
+export LD_PRELOAD=<jemalloc.so/tcmalloc.so>:$LD_PRELOAD
+```
+
+#### Jemalloc
+
+[Jemalloc](https://github.com/jemalloc/jemalloc) is a general purpose malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support. More detailed introduction of performance tuning with Jemalloc can be found at [Jemalloc tuning guide](https://android.googlesource.com/platform/external/jemalloc_new/+/6e6a93170475c05ebddbaf3f0df6add65ba19f01/TUNING.md)
+
+A recommended setting for `MALLOC_CONF` is `oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000` from performance perspective. However, in some cases the `dirty_decay_ms:9000000000,mmuzzy_decay_ms:9000000000` may cause Out-of-Memory crash. Try `oversize_threshold:1,background_thread:true,metadata_thp:auto` instead in this case.
+
+Getting Jemalloc is straight-forward.
+
+```
+conda install -c conda-forge jemalloc
+```
+
+#### TCMalloc
+
+[TCMalloc](https://github.com/google/tcmalloc) also features a couple of optimizations to speed up program executions. One of them is holding memory in caches to speed up access of commonly-used objects. Holding such caches even after deallocation also helps avoid costly system calls if such memory is later re-allocated. It is part of [gpertools](https://github.com/gperftools/gperftools), a collection of a high-performance multi-threaded malloc() implementation, plus some pretty nifty performance analysis tools.
+
+Getting TCMalloc is also not complicated.
+
+```
+conda install -c conda-forge gperftools
+```
+
+### Denormal Number
+
+[Denormal number](https://en.wikipedia.org/wiki/Denormal_number) is used to store extremely small numbers that are close to 0. Computations with denormal numbers are remarkably slower than normalized number. To solve the low performance issue caused by denormal numbers, users can use the following PyTorch API function.
+
+```
+torch.set_flush_denormal(True)
+```
+
+### OneDNN primitive cache
+
+Intel® Extension for PyTorch\* is using OneDNN backend for those most computing bound PyTorch operators such as Linear and Convolution.
+
+To achieve better performance, OneDNN backend is using its [primitive cache](https://oneapi-src.github.io/oneDNN/dev_guide_primitive_cache.html) to store those created primitives for different input shapes during warm-up stage (default primitive cache size is 1024, i.e., 1024 cached primitives). Therefore, when the total size of the primitives created by all the input shapes is within the default threshold, Intel® Extension for PyTorch\* could get fully computation performance from OneDNN kernels.
+
+Different input shapes usualy come from dynamic shapes of datasets. Dynamic shapes commonly exist in [MaskRCNN model](https://github.com/matterport/Mask_RCNN) (object detection), [Transformers](https://github.com/huggingface/transformers/) Wav2vec2 model (speech-recognition) and other speech/text-generation related Transformers models.
+
+However, we might meet the fact that model would need to cache a large amount of various input shapes, which would even exceed the default primitive cache size. In such case, we recommend tuning the OneDNN primitive cache by setting `ONEDNN_PRIMITIVE_CACHE_CAPACITY` environment variable to get better performance (Note that it is at the cost of increased memory usage):
+
+```
+export ONEDNN_PRIMITIVE_CACHE_CAPACITY = {Tuning size}
+//Note that {Tuning size} has an upper limit 65536 cached primitives
+```
+
+Take Transformers [Wav2vec2 for speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition) as an example, the dataset “common voice” used for inference has a large amount of difference shapes for Convolution operator. In our experiment, the best primitive cache size is 4096, and the model runs with its full speed after being warmed up with inputs of all the shape sizes.
diff --git a/docs/tutorials/releases.md b/docs/tutorials/releases.md
new file mode 100644
index 00000000..a6f4a9ce
--- /dev/null
+++ b/docs/tutorials/releases.md
@@ -0,0 +1,892 @@
+Releases
+=============
+
+## 2.0.100
+
+### Highlights
+
+- Enhanced the functionality of Intel® Extension for PyTorch as a backend of `torch.compile`: [#1568](https://github.com/intel/intel-extension-for-pytorch/commit/881c6fe0e6f8ab84a564b02216ddb96a3589363e) [#1585](https://github.com/intel/intel-extension-for-pytorch/commit/f5ce6193496ae68a57d688a3b3bbff541755e4ce) [#1590](https://github.com/intel/intel-extension-for-pytorch/commit/d8723df73358ae495ae5f62b5cdc90ae08920d27)
+- Fixed the Stable Diffusion fine-tuning accuracy issue [#1587](https://github.com/intel/intel-extension-for-pytorch/commit/bc76ab133b7330852931db9cda8dca7c69a0b594) [#1594](https://github.com/intel/intel-extension-for-pytorch/commit/b2983b4d35fc0ea7f5bdaf37f6e269256f8c36c4)
+- Fixed the ISA check on old hypervisor based VM [#1513](https://github.com/intel/intel-extension-for-pytorch/commit/a34eab577c4efa1c336b1f91768075bb490c1f14)
+- Addressed the excessive memory usage in weight prepack [#1593](https://github.com/intel/intel-extension-for-pytorch/commit/ee7dc343790d1d63bab1caf71e57dd3f7affdce9)
+- Fixed the weight prepack of convolution when `padding_mode` is not `'zeros'` [#1580](https://github.com/intel/intel-extension-for-pytorch/commit/02449ccb3a6b475643116532a4cffbe1f974c1d9)
+- Optimized the INT8 LSTM performance [#1566](https://github.com/intel/intel-extension-for-pytorch/commit/fed42b17391fed477ae8adec83d920f8f8fb1a80)
+- Fixed TransNetV2 calibration failure [#1564](https://github.com/intel/intel-extension-for-pytorch/commit/046f7dfbaa212389ac58ae219597c16403e66bad)
+- Fixed BF16 RNN-T inference when `AVX512_CORE_VNNI` ISA is used [#1592](https://github.com/intel/intel-extension-for-pytorch/commit/023c104ab5953cf63b84efeb5176007d876015a2)
+- Fixed the ROIAlign operator [#1589](https://github.com/intel/intel-extension-for-pytorch/commit/6beb3d4661f09f55d031628ebe9fa6d63f04cab1)
+- Enabled execution on designated numa nodes with launch script [#1517](https://github.com/intel-innersource/frameworks.ai.pytorch.ipex-cpu/commit/2ab3693d50d6edd4bfae766f75dc273396a79488)
+
+**Full Changelog**: https://github.com/intel/intel-extension-for-pytorch/compare/v2.0.0+cpu...v2.0.100+cpu
+
+## 2.0.0
+
+We are pleased to announce the release of Intel® Extension for PyTorch\* 2.0.0-cpu which accompanies PyTorch 2.0. This release mainly brings in our latest optimization on NLP (BERT), support of PyTorch 2.0's hero API –- torch.compile as one of its backend, together with a set of bug fixing and small optimization.
+
+### Highlights
+
+- **Fast BERT optimization (Experimental)**: Intel introduced a new technique to speed up BERT workloads. Intel® Extension for PyTorch\* integrated this implementation, which benefits BERT model especially training. A new API `ipex.fast_bert` is provided to try this new optimization. More detailed information can be found at [Fast Bert Feature](./features/fast_bert.md).
+
+- **MHA optimization with Flash Attention**: Intel optimized MHA module with Flash Attention technique as inspired by [Stanford paper](https://arxiv.org/abs/2205.14135). This brings less memory consumption for LLM, and also provides better inference performance for models like BERT, Stable Diffusion, etc.
+
+- **Work with torch.compile as an backend (Experimental)**: PyTorch 2.0 introduces a new feature, `torch.compile`, to speed up PyTorch execution. We've enabled Intel® Extension for PyTorch as a backend of torch.compile, which can leverage this new PyTorch API's power of graph capture and provide additional optimization based on these graphs.
+The usage of this new feature is quite simple as below: 
+
+```python
+import torch
+import intel_extension_for_pytorch as ipex
+...
+model = ipex.optimize(model)
+model = torch.compile(model, backend='ipex')
+```
+
+- **Bug fixing and other optimization**
+
+  - Supported [RMSNorm](https://arxiv.org/abs/1910.07467) which is widely used in the t5 model of huggingface [#1341](https://github.com/intel/intel-extension-for-pytorch/commit/d1de1402a8d6b9ca49b9c9a45a92899f7566866a)
+  - Optimized InstanceNorm [#1330](https://github.com/intel/intel-extension-for-pytorch/commit/8b97d2998567cc2fda6eb008194cd64f624e857f)
+  - Fixed the quantization of LSTM [#1414](https://github.com/intel/intel-extension-for-pytorch/commit/a4f93c09855679d2b424ca5be81930e3a4562cef) [#1473](https://github.com/intel/intel-extension-for-pytorch/commit/5b44996dc0fdb5c45995d403e18a44f2e1a11b3d)
+  - Fixed the correctness issue of unpacking non-contiguous Linear weight [#1419](https://github.com/intel/intel-extension-for-pytorch/commit/84d413d6c10e16c025c407b68652b1769597e016) 
+  - oneDNN update [#1488](https://github.com/intel/intel-extension-for-pytorch/commit/fd5c10b664d19c87f8d94cf293077f65f78c3937)
+
+### Known Issues
+
+Please check at [Known Issues webpage](./performance_tuning/known_issues.md).
+
+## 1.13.100
+
+### Highlights
+
+- Quantization optimization with more fusion, op and auto channels last support [#1318](https://github.com/intel/intel-extension-for-pytorch/commit/5dd3a6ed9017197dea5c05c3af6d330336ed8eff) [#1353](https://github.com/intel/intel-extension-for-pytorch/commit/461b867021e1471c93a1a2a96255247c9d2ab45b) [#1328](https://github.com/intel/intel-extension-for-pytorch/commit/ff3f527025d2102898df9d02977df955e31ddf69) [#1355](https://github.com/intel/intel-extension-for-pytorch/commit/d21111565a179bb8f7ef6db3c04fafbe94871b61) [#1367](https://github.com/intel/intel-extension-for-pytorch/commit/2b898a935e597cfa92ee01a064a626763657c952) [#1384](https://github.com/intel/intel-extension-for-pytorch/commit/a81bd7023e9a119d1ce5f86307865b443034909e)
+- Installation and build enhancement [#1295](https://github.com/intel/intel-extension-for-pytorch/commit/9da7844b75b7cf22d9f4f5401178948919c40914) [#1392](https://github.com/intel/intel-extension-for-pytorch/commit/ef12c70c3ed496e723ac087ea5703dae7df0358d)
+- OneDNN graph and OneDNN update [#1376](https://github.com/intel/intel-extension-for-pytorch/commit/dab9dc18659da53e624637166283ccc8db1373f9)
+- Misc fix and enhancement [#1373](https://github.com/intel/intel-extension-for-pytorch/commit/085ba5d93773ab283e954a4fce75468708b74d3a) [#1338](https://github.com/intel/intel-extension-for-pytorch/commit/0bdf4b27dc445eb8fd0d59f46d157949db597953) [#1391](https://github.com/intel/intel-extension-for-pytorch/commit/2e8289967472553a049158d55e60835371829925) [#1322](https://github.com/intel/intel-extension-for-pytorch/commit/f69492345eb8a9383a67d9416146c2b73de19d8d)
+
+**Full Changelog**: https://github.com/intel/intel-extension-for-pytorch/compare/v1.13.0+cpu...v1.13.100+cpu
+
+## 1.13.0
+
+We are pleased to announce the release of Intel® Extension for PyTorch\* 1.13.0-cpu which accompanies PyTorch 1.13. This release is highlighted with quite a few usability features which help users to get good performance and accuracy on CPU with less effort. We also added a couple of performance features as always. Check out the feature summary below.
+- Usability Features
+1. **Automatic channels last format conversion**: Channels last conversion is now applied automatically to PyTorch modules with `ipex.optimize` by default. Users don't have to explicitly convert input and weight for CV models.
+2. **Code-free optimization** (experimental): `ipex.optimize` is automatically applied to PyTorch modules without the need of code changes when the PyTorch program is started with the Intel® Extension for PyTorch\* launcher via the new `--auto-ipex` option.
+3. **Graph capture mode** of `ipex.optimize` (experimental): A new boolean flag `graph_mode` (default off) was added to `ipex.optimize`, when turned on, converting the eager-mode PyTorch module into graph(s) to get the best of graph optimization.
+4. **INT8 quantization accuracy autotune** (experimental): A new quantization API `ipex.quantization.autotune` was added to refine the default Intel® Extension for PyTorch\* quantization recipe via autotuning algorithms for better accuracy.
+5. **Hypertune** (experimental) is a new tool added on top of Intel® Extension for PyTorch\* launcher to automatically identify the good configurations for best throughput via hyper-parameter tuning.
+6. **ipexrun**: The counterpart of **torchrun**, is a shortcut added for invoking Intel® Extension for PyTorch\* launcher.
+- Performance Features
+1. Packed MKL SGEMM landed as the default kernel option for FP32 Linear, bringing up-to 20% geomean speedup for real-time NLP tasks.
+2. DL compiler is now turned on by default with oneDNN fusion and gives additional performance boost for INT8 models.
+
+### Highlights
+* **Automatic channels last format conversion**: Channels last conversion is now applied to PyTorch modules automatically with `ipex.optimize` by default for both training and inference scenarios. Users don't have to explicitly convert input and weight for CV models.
+  ```python
+  import intel_extension_for_pytorch as ipex
+  # No need to do explicitly format conversion
+  # m = m.to(format=torch.channels_last)
+  # x = x.to(format=torch.channels_last)
+  # for inference
+  m = ipex.optimize(m)
+  m(x)
+  # for training
+  m, optimizer = ipex.optimize(m, optimizer)
+  m(x)
+  ```
+* **Code-free optimization** (experimental): `ipex.optimize` is automatically applied to PyTorch modules without the need of code changes when the PyTorch program is started with the Intel® Extension for PyTorch\* launcher via the new `--auto-ipex` option.
+
+  Example: QA case in HuggingFace
+
+  ```bash
+  # original command
+  ipexrun --use_default_allocator --ninstance 2 --ncore_per_instance 28 run_qa.py \
+    --model_name_or_path bert-base-uncased --dataset_name squad --do_eval \
+    --per_device_train_batch_size 12 --learning_rate 3e-5 --num_train_epochs 2 \
+    --max_seq_length 384 --doc_stride 128 --output_dir /tmp/debug_squad/
+  
+  # automatically apply bfloat16 optimization (--auto-ipex --dtype bfloat16)
+  ipexrun --use_default_allocator --ninstance 2 --ncore_per_instance 28 --auto_ipex --dtype bfloat16 run_qa.py \
+    --model_name_or_path bert-base-uncased --dataset_name squad --do_eval \
+    --per_device_train_batch_size 12 --learning_rate 3e-5 --num_train_epochs 2 \
+    --max_seq_length 384 --doc_stride 128 --output_dir /tmp/debug_squad/
+  ```
+
+* **Graph capture mode** of `ipex.optimize` (experimental): A new boolean flag `graph_mode` (default off) was added to `ipex.optimize`, when turned on, converting the eager-mode PyTorch module into graph(s) to get the best of graph optimization. Under the hood, it combines the goodness of both TorchScript tracing and TorchDynamo to get as max graph scope as possible. Currently, it only supports FP32 and BF16 inference. INT8 inference and training support are under way.
+  ```python
+  import intel_extension_for_pytorch as ipex
+  model = ...
+  model.load_state_dict(torch.load(PATH))
+  model.eval()
+  optimized_model = ipex.optimize(model, graph_mode=True)
+  ```
+
+* **INT8 quantization accuracy autotune** (experimental): A new quantization API `ipex.quantization.autotune` was added to refine the default Intel® Extension for PyTorch\* quantization recipe via autotuning algorithms for better accuracy. This is an optional API to invoke (after `prepare` and before `convert`) for scenarios when the accuracy of default quantization recipe of Intel® Extension for PyTorch\* cannot meet the requirement. The current implementation is powered by Intel® Neural Compressor.
+  ```python
+  import intel_extension_for_pytorch as ipex
+  # Calibrate the model
+  qconfig = ipex.quantization.default_static_qconfig
+  calibrated_model = ipex.quantization.prepare(model_to_be_calibrated, qconfig, example_inputs=example_inputs)
+  for data in calibration_data_set:
+      calibrated_model(data)
+  # Autotune the model
+  calib_dataloader = torch.utils.data.DataLoader(...)
+  def eval_func(model):
+      # Return accuracy value
+      ...
+      return accuracy
+  tuned_model = ipex.quantization.autotune(
+                   calibrated_model, calib_dataloader, eval_func,
+                   sampling_sizes=[100], accuracy_criterion={'relative': 0.01}, tuning_time=0
+                )
+  # Convert the model to jit model
+  quantized_model = ipex.quantization.convert(tuned_model)
+  with torch.no_grad():
+      traced_model = torch.jit.trace(quantized_model, example_input)
+      traced_model = torch.jit.freeze(traced_model)
+  # Do inference
+  y = traced_model(x)
+  ```
+
+* **Hypertune** (experimental) is a new tool added on top of Intel® Extension for PyTorch\* launcher to automatically identify the good configurations for best throughput via hyper-parameter tuning.
+  ```bash
+  python -m intel_extension_for_pytorch.cpu.launch.hypertune --conf_file <your_conf_file> <your_python_script> [args]
+  ```
+
+### Known Issues
+
+Please check at [Known Issues webpage](./performance_tuning/known_issues.md).
+
+## 1.12.300
+
+### Highlights
+
+- Optimize BF16 MHA fusion to avoid transpose overhead to boost BERT-\* BF16 performance [#992](https://github.com/intel/intel-extension-for-pytorch/commit/7076524601f42a9b60402019af21b32782c2c203)
+- Remove 64bytes alignment constraint for FP32 and BF16 AddLayerNorm fusion [#992](https://github.com/intel/intel-extension-for-pytorch/commit/7076524601f42a9b60402019af21b32782c2c203)
+- Fix INT8 RetinaNet accuracy issue [#1032](https://github.com/intel/intel-extension-for-pytorch/commit/e0c719be8246041f8b7bc5feca9cf9c2f599210a)
+- Fix `Cat.out` issue that does not update the `out` tensor (#1053) [#1074](https://github.com/intel/intel-extension-for-pytorch/commit/4381f9126bbb65aab2daf034299c3bf3d307e6e2)
+
+**Full Changelog**: https://github.com/intel/intel-extension-for-pytorch/compare/v1.12.100...v1.12.300
+
+## 1.12.100
+
+This is a patch release to fix the AVX2 issue that blocks running on non-AVX512 platforms.
+
+## 1.12.0
+
+We are excited to bring you the release of Intel® Extension for PyTorch\* 1.12.0-cpu, by tightly following PyTorch [1.12](https://github.com/pytorch/pytorch/releases/tag/v1.12.0) release. In this release, we matured the automatic int8 quantization and made it a stable feature. We stabilized runtime extension and brought about a MultiStreamModule feature to further boost throughput in offline inference scenario. We also brought about various enhancements in operation and graph which are positive for performance of broad set of workloads.
+
+Highlights include:
+- Automatic INT8 quantization became a stable feature baking into a well-tuned default quantization recipe, supporting both static and dynamic quantization and a wide range of calibration algorithms.
+- Runtime Extension, featured MultiStreamModule, became a stable feature, could further enhance throughput in offline inference scenario.
+- More optimizations in graph and operations to improve performance of broad set of models, examples include but not limited to wave2vec, T5, Albert etc.
+- Pre-built experimental binary with oneDNN Graph Compiler tuned on would deliver additional performance gain for Bert, Albert, Roberta in INT8 inference.
+
+### Highlights
+
+- Matured automatic INT8 quantization feature baking into a well-tuned default quantization recipe. We facilitated the user experience and provided a wide range of calibration algorithms like Histogram, MinMax, MovingAverageMinMax, etc. Meanwhile, We polished the static quantization with better flexibility and enabled dynamic quantization as well. Compared to the previous version, the brief changes are as follows. Refer to [tutorial page](features/int8_overview.md) for more details.
+
+  <table align="center">
+  <tbody>
+  <tr>
+  <td>v1.11.0-cpu</td>
+  <td>v1.12.0-cpu</td>
+  </tr>
+  <tr>
+  <td valign="top">
+  
+  ```python
+  import intel_extension_for_pytorch as ipex
+  # Calibrate the model
+  qconfig = ipex.quantization.QuantConf(qscheme=torch.per_tensor_affine)
+  for data in calibration_data_set:
+      with ipex.quantization.calibrate(qconfig):
+          model_to_be_calibrated(x)
+  qconfig.save('qconfig.json')
+  # Convert the model to jit model
+  conf = ipex.quantization.QuantConf('qconfig.json')
+  with torch.no_grad():
+      traced_model = ipex.quantization.convert(model, conf, example_input)
+  # Do inference 
+  y = traced_model(x)
+  ```
+  
+  </td>
+  <td valign="top">
+  
+  ```python
+  import intel_extension_for_pytorch as ipex
+  # Calibrate the model
+  qconfig = ipex.quantization.default_static_qconfig # Histogram calibration algorithm and 
+  calibrated_model = ipex.quantization.prepare(model_to_be_calibrated, qconfig, example_inputs=example_inputs)
+  for data in calibration_data_set:
+      calibrated_model(data)
+  # Convert the model to jit model
+  quantized_model = ipex.quantization.convert(calibrated_model)
+  with torch.no_grad():
+      traced_model = torch.jit.trace(quantized_model, example_input)
+      traced_model = torch.jit.freeze(traced_model)
+  # Do inference 
+  y = traced_model(x)
+  ```
+  
+  </td>
+  </tr>
+  </tbody>
+  </table>
+
+- Runtime Extension, featured MultiStreamModule, became a stable feature. In this release, we enhanced the heuristic rule to further enhance throughput in offline inference scenario. Meanwhile, we also provide the `ipex.cpu.runtime.MultiStreamModuleHint` to custom how to split the input into streams and concat the output for each steam.
+
+  <table align="center">
+  <tbody>
+  <tr>
+  <td>v1.11.0-cpu</td>
+  <td>v1.12.0-cpu</td>
+  </tr>
+  <tr>
+  <td valign="top">
+  
+  ```python
+  import intel_extension_for_pytorch as ipex
+  # Create CPU pool
+  cpu_pool = ipex.cpu.runtime.CPUPool(node_id=0)
+  # Create multi-stream model
+  multi_Stream_model = ipex.cpu.runtime.MultiStreamModule(model, num_streams=2, cpu_pool=cpu_pool)
+  ```
+  
+  </td>
+  <td valign="top">
+  
+  ```python
+  import intel_extension_for_pytorch as ipex
+  # Create CPU pool
+  cpu_pool = ipex.cpu.runtime.CPUPool(node_id=0)
+  # Optional
+  multi_stream_input_hint = ipex.cpu.runtime.MultiStreamModuleHint(0)
+  multi_stream_output_hint = ipex.cpu.runtime.MultiStreamModuleHint(0)
+  # Create multi-stream model
+  multi_Stream_model = ipex.cpu.runtime.MultiStreamModule(model, num_streams=2, cpu_pool=cpu_pool,
+    multi_stream_input_hint,   # optional
+    multi_stream_output_hint ) # optional
+  ```
+  
+  </td>
+  </tr>
+  </tbody>
+  </table>
+
+- Polished the `ipex.optimize` to accept the input shape information which would conclude the optimal memory layout for better kernel efficiency.
+
+  <table align="center">
+  <tbody>
+  <tr>
+  <td>v1.11.0-cpu</td>
+  <td>v1.12.0-cpu</td>
+  </tr>
+  <tr>
+  <td valign="top">
+  
+  ```python
+  import intel_extension_for_pytorch as ipex
+  model = ...
+  model.load_state_dict(torch.load(PATH))
+  model.eval()
+  optimized_model = ipex.optimize(model, dtype=torch.bfloat16)
+  ```
+  
+  </td>
+  <td valign="top">
+  
+  ```python
+  import intel_extension_for_pytorch as ipex
+  model = ...
+  model.load_state_dict(torch.load(PATH))
+  model.eval()
+  optimized_model = ipex.optimize(model, dtype=torch.bfloat16, sample_input=input)
+  ```
+  
+  </td>
+  </tr>
+  </tbody>
+  </table>
+
+- Provided more optimizations in graph and operations
+  - Fuse Adam to improve training performance [#822](https://github.com/intel/intel-extension-for-pytorch/commit/d3f714e54dc8946675259ea7a445b26a2460b523)
+  - Enable Normalization operators to support channels-last 3D [#642](https://github.com/intel/intel-extension-for-pytorch/commit/ae268ac1760d598a29584de5c99bfba46c6554ae)
+  - Support Deconv3D to serve most models and implement most fusions like Conv
+  - Enable LSTM to support static and dynamic quantization [#692](https://github.com/intel/intel-extension-for-pytorch/commit/2bf8dba0c380a26bbb385e253adbfaa2a033a785)
+  - Enable Linear to support dynamic quantization [#787](https://github.com/intel/intel-extension-for-pytorch/commit/ff231fb55e33c37126a0ef7f0e739cd750d1ef6c)
+  - Fusions.
+    - Fuse `Add` + `Swish` to accelerate FSI Riskful model [#551](https://github.com/intel/intel-extension-for-pytorch/commit/cc855ff2bafd245413a6111f3d21244d0bcbb6f6)
+    - Fuse `Conv` + `LeakyReLU` [#589](https://github.com/intel/intel-extension-for-pytorch/commit/dc6ed1a5967c644b03874fd1f8a503f0b80be6bd)
+    - Fuse `BMM` + `Add` [#407](https://github.com/intel/intel-extension-for-pytorch/commit/d1379aa565cc84b4a61b537ba2c9a046b7652f1a)
+    - Fuse `Concat` + `BN` + `ReLU` [#647](https://github.com/intel/intel-extension-for-pytorch/commit/cad3f82f6b7efed0c08b2f0c11117a4720f58df4)
+    - Optimize `Convolution1D` to support channels last memory layout and fuse `GeLU` as its post operation. [#657](https://github.com/intel/intel-extension-for-pytorch/commit/a0c063bdf4fd1a7e66f8a23750ac0c2fe471a559)
+    - Fuse `Einsum` + `Add` to boost Alphafold2 [#674](https://github.com/intel/intel-extension-for-pytorch/commit/3094f346a67c81ad858ad2a80900fab4c3b4f4e9)
+    - Fuse `Linear` + `Tanh` [#711](https://github.com/intel/intel-extension-for-pytorch/commit/b24cc530b1fd29cb161a76317891e361453333c9)
+
+### Known Issues
+- `RuntimeError: Overflow when unpacking long` when a tensor's min max value exceeds int range while performing int8 calibration. Please customize QConfig to use min-max calibration method.
+- Calibrating with quantize_per_tensor, when benchmarking with 1 OpenMP\* thread, results might be incorrect with large tensors (find more detailed info [here](https://github.com/pytorch/pytorch/issues/80501). Editing your code following the pseudocode below can workaround this issue, if you do need to explicitly set OMP_NUM_THREAEDS=1 for benchmarking. However, there could be a performance regression if oneDNN graph compiler prototype feature is utilized.
+
+  Workaround pseudocode:
+  ```
+  # perform convert/trace/freeze with omp_num_threads > 1(N)
+  torch.set_num_threads(N)
+  prepared_model = prepare(model, input)
+  converted_model = convert(prepared_model)
+  traced_model = torch.jit.trace(converted_model, input)
+  freezed_model = torch.jit.freeze(traced_model)
+  # run freezed model to apply optimization pass
+  freezed_model(input)
+
+  # benchmarking with omp_num_threads = 1
+  torch.set_num_threads(1)
+  run_benchmark(freezed_model, input)
+  ```
+- Low performance with INT8 support for dynamic shapes
+  The support for dynamic shapes in Intel® Extension for PyTorch\* INT8 integration is still work in progress. When the input shapes are dynamic, for example inputs of variable image sizes in an object detection task or of variable sequence lengths in NLP tasks, the Intel® Extension for PyTorch\* INT8 path may slow down the model inference. In this case, use stock PyTorch INT8 functionality.
+  **Note**: Using Runtime Extension feature if batch size cannot be divided by number of streams, because mini batch size on each stream are not equivalent, scripts run into this issues.
+- BF16 AMP(auto-mixed-precision) runs abnormally with the extension on the AVX2-only machine if the topology contains `Conv`, `Matmul`, `Linear`, and `BatchNormalization`
+- Runtime extension of MultiStreamModule doesn't support DLRM inference, since the input of DLRM (EmbeddingBag specifically) can't be simplely batch split.
+- Runtime extension of MultiStreamModule has poor performance of RNNT Inference comparing with native throughput mode. Only part of the RNNT models (joint_net specifically) can be jit traced into graph. However, in one batch inference, `joint_net` is invoked multi times. It increases the overhead of MultiStreamModule as input batch split, thread synchronization and output concat.
+- Incorrect Conv and Linear result if the number of OMP threads is changed at runtime
+  The oneDNN memory layout depends on the number of OMP threads, which requires the caller to detect the changes for the # of OMP threads while this release has not implemented it yet.
+- Low throughput with DLRM FP32 Train
+  A 'Sparse Add' [PR](https://github.com/pytorch/pytorch/pull/23057) is pending on review. The issue will be fixed when the PR is merged.
+- If inference is done with a custom function, `conv+bn` folding feature of the `ipex.optimize()` function doesn't work.
+  ```
+  import torch
+  import intel_pytorch_extension as ipex
+  class Module(torch.nn.Module):
+      def __init__(self):
+          super(Module, self).__init__()
+          self.conv = torch.nn.Conv2d(1, 10, 5, 1)
+          self.bn = torch.nn.BatchNorm2d(10)
+          self.relu = torch.nn.ReLU()
+      def forward(self, x):
+          x = self.conv(x)
+          x = self.bn(x)
+          x = self.relu(x)
+          return x
+      def inference(self, x):
+          return self.forward(x)
+  if __name__ == '__main__':
+      m = Module()
+      m.eval()
+      m = ipex.optimize(m, dtype=torch.float32, level="O0")
+      d = torch.rand(1, 1, 112, 112)
+      with torch.no_grad():
+        m.inference(d)
+  ```
+  This is a PyTorch FX limitation. You can avoid this error by calling `m = ipex.optimize(m, level="O0")`, which doesn't apply ipex optimization, or disable `conv+bn` folding by calling `m = ipex.optimize(m, level="O1", conv_bn_folding=False)`.
+
+## 1.11.200
+
+### Highlights
+
+- Enable more fused operators to accelerate particular models.
+- Fuse `Convolution` and `LeakyReLU` ([#648](https://github.com/intel/intel-extension-for-pytorch/commit/d7603133f37375b3aba7bf744f1095b923ba979e))
+- Support [`torch.einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html) and fuse it with `add` ([#684](https://github.com/intel/intel-extension-for-pytorch/commit/b66d6d8d0c743db21e534d13be3ee75951a3771d))
+- Fuse `Linear` and `Tanh` ([#685](https://github.com/intel/intel-extension-for-pytorch/commit/f0f2bae96162747ed2a0002b274fe7226a8eb200))
+- In addition to the original installation methods, this release provides Docker installation from [DockerHub](https://hub.docker.com/).
+- Provided the <a class="reference external" href="installation.html#installation_onednn_graph_compiler">evaluation wheel packages</a> that could boost performance for selective topologies on top of oneDNN graph compiler prototype feature.
+***NOTE***: This is still at an early development stage and not fully mature yet, but feel free to reach out through [GitHub issues](https://github.com/intel/intel-extension-for-pytorch/issues) if you have any suggestions.
+
+**[Full Changelog](https://github.com/intel/intel-extension-for-pytorch/compare/v1.11.0...v1.11.200)**
+
+## 1.11.0
+
+We are excited to announce Intel® Extension for PyTorch\* 1.11.0-cpu release by tightly following PyTorch 1.11 release. Along with extension 1.11, we focused on continually improving OOB user experience and performance. Highlights include:
+
+* Support a single binary with runtime dynamic dispatch based on AVX2/AVX512 hardware ISA detection
+* Support install binary from `pip` with package name only (without the need of specifying the URL)
+* Provide the C++ SDK installation to facilitate ease of C++ app development and deployment
+* Add more optimizations, including graph fusions for speeding up Transformer-based models and CNN, etc
+* Reduce the binary size for both the PIP wheel and C++ SDK (2X to 5X reduction from the previous version)
+
+### Highlights
+- Combine the AVX2 and AVX512 binary as a single binary and automatically dispatch to different implementations based on hardware ISA detection at runtime. The typical case is to serve the data center that mixtures AVX2-only and AVX512 platforms. It does not need to deploy the different ISA binary now compared to the previous version
+
+    ***NOTE***:  The extension uses the oneDNN library as the backend. However, the BF16 and INT8 operator sets and features are different between AVX2 and AVX512. Refer to [oneDNN document](https://oneapi-src.github.io/oneDNN/dev_guide_int8_computations.html#processors-with-the-intel-avx2-or-intel-avx-512-support) for more details. 
+
+    > When one input is of type u8, and the other one is of type s8, oneDNN assumes the user will choose the quantization parameters so no overflow/saturation occurs. For instance, a user can use u7 [0, 127] instead of u8 for the unsigned input, or s7 [-64, 63] instead of the s8 one. It is worth mentioning that this is required only when the Intel AVX2 or Intel AVX512 Instruction Set is used.
+
+- The extension wheel packages have been uploaded to [pypi.org](https://pypi.org/project/intel-extension-for-pytorch/). The user could directly install the extension by `pip/pip3` without explicitly specifying the binary location URL.
+
+<table align="center">
+<tbody>
+<tr>
+<td>v1.10.100-cpu</td>
+<td>v1.11.0-cpu</td>
+</tr>
+<tr>
+<td>
+
+```python
+python -m pip install intel_extension_for_pytorch==1.10.100 -f https://software.intel.com/ipex-whl-stable
+```
+</td>
+<td>
+
+```python
+pip install intel_extension_for_pytorch
+```
+</td>
+</tr>
+</tbody>
+</table>
+
+- Compared to the previous version, this release provides a dedicated installation file for the C++ SDK. The installation file automatically detects the PyTorch C++ SDK location and installs the extension C++ SDK files to the PyTorch C++ SDK. The user does not need to manually add the extension C++ SDK source files and CMake to the PyTorch SDK. In addition to that, the installation file reduces the C++ SDK binary size from ~220MB to ~13.5MB. 
+
+<table align="center">
+<tbody>
+<tr>
+<td>v1.10.100-cpu</td>
+<td>v1.11.0-cpu</td>
+</tr>
+<tr>
+<td>
+
+```python
+intel-ext-pt-cpu-libtorch-shared-with-deps-1.10.0+cpu.zip (220M)
+intel-ext-pt-cpu-libtorch-cxx11-abi-shared-with-deps-1.10.0+cpu.zip (224M)
+```
+</td>
+<td>
+
+```python
+libintel-ext-pt-1.11.0+cpu.run (13.7M)
+libintel-ext-pt-cxx11-abi-1.11.0+cpu.run (13.5M)
+```
+</td>
+</tr>
+</tbody>
+</table>
+
+- Add more optimizations, including more custom operators and fusions.
+    - Fuse the QKV linear operators as a single Linear to accelerate the Transformer\*(BERT-\*) encoder part  - [#278](https://github.com/intel/intel-extension-for-pytorch/commit/0f27c269cae0f902973412dc39c9a7aae940e07b).
+    - Remove Multi-Head-Attention fusion limitations to support the 64bytes unaligned tensor shape. [#531](https://github.com/intel/intel-extension-for-pytorch/commit/dbb10fedb00c6ead0f5b48252146ae9d005a0fad)
+    - Fold the binary operator to Convolution and Linear operator to reduce computation. [#432](https://github.com/intel/intel-extension-for-pytorch/commit/564588561fa5d45b8b63e490336d151ff1fc9cbc) [#438](https://github.com/intel/intel-extension-for-pytorch/commit/b4e7dacf08acd849cecf8d143a11dc4581a3857f) [#602](https://github.com/intel/intel-extension-for-pytorch/commit/74aa21262938b923d3ed1e6929e7d2b629b3ff27)
+    - Replace the outplace operators with their corresponding in-place version to reduce memory footprint. The extension currently supports the operators including `sliu`, `sigmoid`, `tanh`, `hardsigmoid`, `hardswish`, `relu6`, `relu`, `selu`, `softmax`. [#524](https://github.com/intel/intel-extension-for-pytorch/commit/38647677e8186a235769ea519f4db65925eca33c)
+    - Fuse the Concat + BN + ReLU as a single operator. [#452](https://github.com/intel/intel-extension-for-pytorch/commit/275ff503aea780a6b741f04db5323d9529ee1081)
+    - Optimize Conv3D for both imperative and JIT by enabling NHWC and pre-packing the weight. [#425](https://github.com/intel/intel-extension-for-pytorch/commit/ae33faf62bb63b204b0ee63acb8e29e24f6076f3)
+- Reduce the binary size. C++ SDK is reduced from ~220MB to ~13.5MB while the wheel packaged is reduced from ~100MB to ~40MB.
+- Update oneDNN and oneDNN graph to [2.5.2](https://github.com/oneapi-src/oneDNN/releases/tag/v2.5.2) and [0.4.2](https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.4.2) respectively.
+
+### What's Changed
+**Full Changelog**: https://github.com/intel/intel-extension-for-pytorch/compare/v1.10.100...v1.11.0
+
+## 1.10.100
+
+This release is meant to fix the following issues:
+- Resolve the issue that the PyTorch Tensor Expression(TE) did not work after importing the extension.
+- Wraps the BactchNorm(BN) as another operator to break the TE's BN-related fusions. Because the BatchNorm performance of PyTorch Tensor Expression can not achieve the same performance as PyTorch ATen BN.
+- Update the [documentation](https://intel.github.io/intel-extension-for-pytorch/)
+    - Fix the INT8 quantization example issue #205
+    - Polish the installation guide
+
+## 1.10.0
+
+The Intel® Extension for PyTorch\* 1.10 is on top of PyTorch 1.10. In this release, we polished the front end APIs. The APIs are more simple, stable, and straightforward now. According to PyTorch community recommendation, we changed the underhood device from `XPU` to `CPU`. With this change, the model and tensor does not need to be converted to the extension device to get performance improvement. It simplifies the model changes.
+
+Besides that, we continuously optimize the Transformer\* and CNN models by fusing more operators and applying NHWC. We measured the 1.10 performance on Torchvison and HugginFace. As expected, 1.10 can speed up the two model zones.
+
+### Highlights
+
+- Change the package name to `intel_extension_for_pytorch` while the original package name is `intel_pytorch_extension`. This change targets to avoid any potential legal issues.
+
+<table align="center">
+<tbody>
+<tr>
+<td>v1.9.0-cpu</td>
+<td>v1.10.0-cpu</td>
+</tr>
+<tr>
+<td>
+
+```
+import intel_extension_for_pytorch as ipex
+```
+</td>
+<td>
+
+```
+import intel_extension_for_pytorch as ipex
+```
+</td>
+</tr>
+</tbody>
+</table>
+
+- The underhood device is changed from the extension-specific device(`XPU`) to the standard CPU device that aligns with the PyTorch CPU device design, regardless of the dispatch mechanism and operator register mechanism. The means the model does not need to be converted to the extension device explicitly.
+
+<table align="center">
+<tbody>
+<tr>
+<td>v1.9.0-cpu</td>
+<td>v1.10.0-cpu</td>
+</tr>
+<tr>
+<td>
+
+```
+import torch
+import torchvision.models as models
+
+# Import the extension
+import intel_extension_for_pytorch as ipex
+
+resnet18 = models.resnet18(pretrained = True)
+
+# Explicitly convert the model to the extension device
+resnet18_xpu = resnet18.to(ipex.DEVICE)
+```
+</td>
+<td>
+
+```
+import torch
+import torchvision.models as models
+
+# Import the extension
+import intel_extension_for_pytorch as ipex
+
+resnet18 = models.resnet18(pretrained = True)
+```
+</td>
+</tr>
+</tbody>
+</table>
+
+- Compared to v1.9.0, v1.10.0 follows PyTorch AMP API(`torch.cpu.amp`) to support auto-mixed-precision. `torch.cpu.amp` provides convenience for auto data type conversion at runtime. Currently, `torch.cpu.amp` only supports `torch.bfloat16`. It is the default lower precision floating point data type when `torch.cpu.amp` is enabled. `torch.cpu.amp` primarily benefits on Intel CPU with BFloat16 instruction set support.
+
+```
+import torch
+class SimpleNet(torch.nn.Module):
+    def __init__(self):
+        super(SimpleNet, self).__init__()
+        self.conv = torch.nn.Conv2d(64, 128, (3, 3), stride=(2, 2), padding=(1, 1), bias=False)
+
+    def forward(self, x):
+        return self.conv(x)
+```
+
+<table align="center">
+<tbody>
+<tr>
+<td>v1.9.0-cpu</td>
+<td>v1.10.0-cpu</td>
+</tr>
+<tr>
+<td>
+
+```
+# Import the extension
+import intel_extension_for_pytorch as ipex
+
+# Automatically mix precision
+ipex.enable_auto_mixed_precision(mixed_dtype = torch.bfloat16)
+
+model = SimpleNet().eval()
+x = torch.rand(64, 64, 224, 224)
+with torch.no_grad():
+    model = torch.jit.trace(model, x)
+    model = torch.jit.freeze(model)
+    y = model(x)
+```
+</td>
+<td>
+
+```
+# Import the extension
+import intel_extension_for_pytorch as ipex
+
+model = SimpleNet().eval()
+x = torch.rand(64, 64, 224, 224)
+with torch.cpu.amp.autocast(), torch.no_grad():
+    model = torch.jit.trace(model, x)
+    model = torch.jit.freeze(model)
+    y = model(x)
+```
+</td>
+</tr>
+</tbody>
+</table>
+
+- The 1.10 release provides the INT8 calibration as an experimental feature while it only supports post-training static quantization now. Compared to 1.9.0, the fronted APIs for quantization is more straightforward and ease-of-use.
+
+```
+import torch
+import torch.nn as nn
+import intel_extension_for_pytorch as ipex
+
+class MyModel(nn.Module):
+    def __init__(self):
+        super(MyModel, self).__init__()
+        self.conv = nn.Conv2d(10, 10, 3)
+
+    def forward(self, x):
+        x = self.conv(x)
+        return x
+
+model = MyModel().eval()
+
+# user dataset for calibration.
+xx_c = [torch.randn(1, 10, 28, 28) for i in range(2))
+# user dataset for validation.
+xx_v = [torch.randn(1, 10, 28, 28) for i in range(20))
+```
+  - Clibration
+<table align="center">
+<tbody>
+<tr>
+<td>v1.9.0-cpu</td>
+<td>v1.10.0-cpu</td>
+</tr>
+<tr>
+<td>
+
+```
+# Import the extension
+import intel_extension_for_pytorch as ipex
+
+# Convert the model to the Extension device
+model = Model().to(ipex.DEVICE)
+
+# Create a configuration file to save quantization parameters.
+conf = ipex.AmpConf(torch.int8)
+with torch.no_grad():
+    for x in xx_c:
+        # Run the model under calibration mode to collect quantization parameters
+        with ipex.AutoMixPrecision(conf, running_mode='calibration'):
+            y = model(x.to(ipex.DEVICE))
+# Save the configuration file
+conf.save('configure.json')
+```
+</td>
+<td>
+
+```
+# Import the extension
+import intel_extension_for_pytorch as ipex
+
+conf = ipex.quantization.QuantConf(qscheme=torch.per_tensor_affine)
+with torch.no_grad():
+    for x in xx_c:
+        with ipex.quantization.calibrate(conf):
+            y = model(x)
+
+conf.save('configure.json')
+```
+</td>
+</tr>
+</tbody>
+</table>
+
+ - Inference
+ <table align="center">
+<tbody>
+<tr>
+<td>v1.9.0-cpu</td>
+<td>v1.10.0-cpu</td>
+</tr>
+<tr>
+<td>
+
+```
+# Import the extension
+import intel_extension_for_pytorch as ipex
+
+# Convert the model to the Extension device
+model = Model().to(ipex.DEVICE)
+conf = ipex.AmpConf(torch.int8, 'configure.json')
+with torch.no_grad():
+    for x in cali_dataset:
+        with ipex.AutoMixPrecision(conf, running_mode='inference'):
+            y = model(x.to(ipex.DEVICE))
+```
+</td>
+<td>
+
+```
+# Import the extension
+import intel_extension_for_pytorch as ipex
+
+conf = ipex.quantization.QuantConf('configure.json')
+
+with torch.no_grad():
+    trace_model = ipex.quantization.convert(model, conf, example_input)
+    for x in xx_v:
+        y = trace_model(x)
+```
+</td>
+</tr>
+</tbody>
+</table>
+
+
+- This release introduces the `optimize` API at python front end to optimize the model and optimizer for training. The new API both supports FP32 and BF16, inference and training.
+
+- Runtime Extension (Experimental) provides a runtime CPU pool API to bind threads to cores. It also features async tasks. **Note**: Intel® Extension for PyTorch\* Runtime extension is still in the **experimental** stage. The API is subject to change. More detailed descriptions are available in the extension documentation.
+
+### Known Issues
+
+- `omp_set_num_threads` function failed to change OpenMP threads number of oneDNN operators if it was set before.
+
+  `omp_set_num_threads` function is provided in Intel® Extension for PyTorch\* to change the number of threads used with OpenMP. However, it failed to change the number of OpenMP threads if it was set before.
+
+  pseudo-code:
+
+  ```
+  omp_set_num_threads(6)
+  model_execution()
+  omp_set_num_threads(4)
+  same_model_execution_again()
+  ```
+
+  **Reason:** oneDNN primitive descriptor stores the omp number of threads. Current oneDNN integration caches the primitive descriptor in IPEX. So if we use runtime extension with oneDNN based pytorch/ipex operation, the runtime extension fails to change the used omp number of threads.
+
+- Low performance with INT8 support for dynamic shapes
+
+  The support for dynamic shapes in Intel® Extension for PyTorch\* INT8 integration is still work in progress. When the input shapes are dynamic, for example, inputs of variable image sizes in an object detection task or of variable sequence lengths in NLP tasks, the Intel® Extension for PyTorch\* INT8 path may slow down the model inference. In this case, use stock PyTorch INT8 functionality.
+
+- Low throughput with DLRM FP32 Train
+
+  A 'Sparse Add' [PR](https://github.com/pytorch/pytorch/pull/23057) is pending review. The issue will be fixed when the PR is merged.
+
+### What's Changed
+**Full Changelog**: https://github.com/intel/intel-extension-for-pytorch/compare/v1.9.0...v1.10.0+cpu-rc3
+
+## 1.9.0
+
+### What's New
+
+* Rebased the Intel Extension for Pytorch from PyTorch-1.8.0 to the official PyTorch-1.9.0 release.
+* Support binary installation.
+
+  `python -m pip install torch_ipex==1.9.0 -f https://software.intel.com/ipex-whl-stable`
+* Support the C++ library. The third party App can link the Intel-Extension-for-PyTorch C++ library to enable the particular optimizations.
+
+## 1.8.0
+
+### What's New
+
+* Rebased the Intel Extension for Pytorch from Pytorch -1.7.0 to the official Pytorch-1.8.0 release. The new XPU device type has been added into Pytorch-1.8.0(49786), don’t need to patch PyTorch to enable Intel Extension for Pytorch anymore
+* Upgraded the oneDNN from v1.5-rc to v1.8.1
+* Updated the README file to add the sections to introduce supported customized operators, supported fusion patterns, tutorials, and joint blogs with stakeholders
+
+## 1.2.0
+
+### What's New
+
+* We rebased the Intel Extension for pytorch from Pytorch -1.5rc3 to the official Pytorch-1.7.0 release. It will have performance improvement with the new Pytorch-1.7 support.
+* Device name was changed from DPCPP to XPU.
+
+  We changed the device name from DPCPP to XPU to align with the future Intel GPU product for heterogeneous computation.
+* Enabled the launcher for end users.
+* We enabled the launch script that helps users launch the program for training and inference, then automatically setup the strategy for multi-thread, multi-instance, and memory allocator. Refer to the launch script comments for more details.
+
+### Performance Improvement
+
+* This upgrade provides better INT8 optimization with refined auto mixed-precision API.
+* More operators are optimized for the int8 inference and bfp16 training of some key workloads, like MaskRCNN, SSD-ResNet34, DLRM, RNNT.
+
+### Others
+
+* Bug fixes
+  * This upgrade fixes the issue that saving the model trained by Intel extension for PyTorch caused errors.
+  * This upgrade fixes the issue that Intel extension for PyTorch was slower than pytorch proper for Tacotron2.
+* New custom operators
+
+  This upgrade adds several custom operators: ROIAlign, RNN, FrozenBatchNorm, nms.
+* Optimized operators/fusion
+
+  This upgrade optimizes several operators: tanh, log_softmax, upsample, and embeddingbad and enables int8 linear fusion.
+* Performance
+
+  The release has daily automated testing for the supported models: ResNet50, ResNext101, Huggingface Bert, DLRM, Resnext3d, MaskRNN, SSD-ResNet34. With the extension imported, it can bring up to 2x INT8 over FP32 inference performance improvements on the 3rd Gen Intel Xeon scalable processors (formerly codename Cooper Lake).
+
+### Known issues
+
+* Multi-node training still encounter hang issues after several iterations. The fix will be included in the next official release.
+
+## 1.1.0
+
+### What's New
+
+* Added optimization for training with FP32 data type & BF16 data type. All the optimized FP32/BF16 backward operators include:
+  * Conv2d
+  * Relu
+  * Gelu
+  * Linear
+  * Pooling
+  * BatchNorm
+  * LayerNorm
+  * Cat
+  * Softmax
+  * Sigmoid
+  * Split
+  * Embedding_bag
+  * Interaction
+  * MLP
+* More fusion patterns are supported and validated in the release, see table:
+
+  |Fusion Patterns|Release|
+  |--|--|
+  |Conv + Sum|v1.0|
+  |Conv + BN|v1.0|
+  |Conv + Relu|v1.0|
+  |Linear + Relu|v1.0|
+  |Conv + Eltwise|v1.1|
+  |Linear + Gelu|v1.1|
+
+* Add docker support
+* [Alpha] Multi-node training with oneCCL support.
+* [Alpha] INT8 inference optimization.
+
+### Performance
+
+* The release has daily automated testing for the supported models: ResNet50, ResNext101, [Huggingface Bert](https://github.com/huggingface/transformers), [DLRM](https://github.com/intel/optimized-models/tree/master/pytorch/dlrm), [Resnext3d](https://github.com/XiaobingSuper/Resnext3d-for-video-classification), [Transformer](https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py). With the extension imported, it can bring up to 1.2x~1.7x BF16 over FP32 training performance improvements on the 3rd Gen Intel Xeon scalable processors (formerly codename Cooper Lake).
+
+### Known issue
+
+* Some workloads may crash after several iterations on the extension with [jemalloc](https://github.com/jemalloc/jemalloc) enabled.
+
+## 1.0.2
+
+* Rebase torch CCL patch to PyTorch 1.5.0-rc3
+
+## 1.0.1-Alpha
+
+* Static link oneDNN library
+* Check AVX512 build option
+* Fix the issue that cannot normally invoke `enable_auto_optimization`
+
+## 1.0.0-Alpha
+
+### What's New
+
+* Auto Operator Optimization
+
+  Intel Extension for PyTorch will automatically optimize the operators of PyTorch when importing its python package. It will significantly improve the computation performance if the input tensor and the model is converted to the extension device.
+
+* Auto Mixed Precision
+  Currently, the extension has supported bfloat16. It streamlines the work to enable a bfloat16 model. The feature is controlled by `enable_auto_mix_precision`. If you enable it, the extension will run the operator with bfloat16 automatically to accelerate the operator computation.
+
+### Performance Result
+
+We collected the performance data of some models on the Intel Cooper Lake platform with 1 socket and 28 cores. Intel Cooper Lake introduced AVX512 BF16 instructions that could improve the bfloat16 computation significantly. The detail is as follows (The data is the speedup ratio and the baseline is upstream PyTorch).
+
+||Imperative - Operator Injection|Imperative - Mixed Precision|JIT- Operator Injection|JIT - Mixed Precision|
+|:--:|:--:|:--:|:--:|:--:|
+|RN50|2.68|5.01|5.14|9.66|
+|ResNet3D|3.00|4.67|5.19|8.39|
+|BERT-LARGE|0.99|1.40|N/A|N/A|
+
+We also measured the performance of ResNeXt101, Transformer-FB, DLRM, and YOLOv3 with the extension. We observed that the performance could be significantly improved by the extension as expected.
+
+### Known issue
+
+* [#10](https://github.com/intel/intel-extension-for-pytorch/issues/10) All data types have not been registered for DPCPP
+* [#37](https://github.com/intel/intel-extension-for-pytorch/issues/37) MaxPool can't get nan result when input's value is nan
+
+### NOTE
+
+The extension supported PyTorch v1.5.0-rc3. Support for other PyTorch versions is working in progress.
diff --git a/examples/cpu/inference/python/llm/README.md b/examples/cpu/inference/python/llm/README.md
deleted file mode 100644
index 71f051b7..00000000
--- a/examples/cpu/inference/python/llm/README.md
+++ /dev/null
@@ -1,173 +0,0 @@
-# Text Generation
-We provide the inference benchmarking script `run_generation.py` for large language models text generation.<br/>
-Support large language models, such as GPT-J, LLaMA, GPT-Neox, OPT.<br/>
-And script `run_generation_with_deepspeed.py` for distributed with DeepSpeed.<br/>
-And script `run_model_int8.py` for int8.<br/>
-
-## Setup
-```bash
-WORK_DIR=$PWD
-# GCC 12.3 is required, please set it firstly
-# Create environment (conda recommended)
-conda create -n llm python=3.9 -y
-# install deps
-conda install gcc=12.3 gxx=12.3 cxx-compiler -c conda-forge -y
-conda install cmake ninja mkl mkl-include -y
-conda install gperftools -c conda-forge -y
-
-# Install PyTorch
-python -m pip install torch==2.1.0.dev20230711+cpu torchvision==0.16.0.dev20230711+cpu torchaudio==2.1.0.dev20230711+cpu --index-url https://download.pytorch.org/whl/nightly/cpu
-
-# Install IPEX with semi-compiler, require gcc 12.3
-rm -rf llvm-project && mkdir llvm-project && cd llvm-project
-wget https://github.com/llvm/llvm-project/releases/download/llvmorg-16.0.6/cmake-16.0.6.src.tar.xz
-wget https://github.com/llvm/llvm-project/releases/download/llvmorg-16.0.6/llvm-16.0.6.src.tar.xz
-tar -xf cmake-16.0.6.src.tar.xz && mv cmake-16.0.6.src cmake
-tar -xf llvm-16.0.6.src.tar.xz && mv llvm-16.0.6.src llvm
-mkdir build && cd build
-cmake ../llvm -DCMAKE_INSTALL_PREFIX=${PWD}/_install/llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_TARGETS_TO_BUILD=X86 -DLLVM_INCLUDE_TESTS=OFF -DLLVM_INCLUDE_EXAMPLES=OFF -DLLVM_ENABLE_TERMINFO=OFF -DLLVM_INCLUDE_BENCHMARKS=OFF -DCMAKE_CXX_FLAGS="-D_GLIBCXX_USE_CXX11_ABI=0"
-make install -j$(nproc)
-ln -s ${PWD}/_install/llvm/bin/llvm-config ${CONDA_PREFIX}/bin/llvm-config-13
-cd ../../
-
-git clone --branch llm_feature_branch https://github.com/intel-innersource/frameworks.ai.pytorch.ipex-cpu
-cd frameworks.ai.pytorch.ipex-cpu
-git submodule sync && git submodule update --init --recursive
-export DNNL_GRAPH_BUILD_COMPILER_BACKEND=1
-export CXXFLAGS="${CXXFLAGS} -D__STDC_FORMAT_MACROS"
-python setup.py install
-cd ../
-
-# Used for accuracy test only
-git clone https://github.com/EleutherAI/lm-evaluation-harness
-cd lm-evaluation-harness
-pip install -e .
-
-# Install transformers (version 4.31.0 is required for testing LLaMA2 70B model)
-pip install transformers==4.28.1
-# Install others deps
-pip install cpuid accelerate datasets sentencepiece protobuf==3.20.3
-
-# Setup environment variables for performance on Xeon
-export LD_PRELOAD=${CONDA_PREFIX}/lib/libstdc++.so.6
-export KMP_BLOCKTIME=INF
-export KMP_TPAUSE=0
-export KMP_SETTINGS=1
-export KMP_AFFINITY=granularity=fine,compact,1,0
-export KMP_FORJOIN_BARRIER_PATTERN=dist,dist
-export KMP_PLAIN_BARRIER_PATTERN=dist,dist
-export KMP_REDUCTION_BARRIER_PATTERN=dist,dist
-export LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libiomp5.so # Intel OpenMP
-# Tcmalloc is a recommended malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support.
-export LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libtcmalloc.so
-
-# [Optional] install neural-compressor for GPT-J INT8 only
-pip install neural-compressor==2.2
-
-# [Optional] The following is only for DeepSpeed case
-git clone https://github.com/delock/DeepSpeedSYCLSupport
-cd DeepSpeedSYCLSupport
-git checkout gma/run-opt-branch
-python -m pip install -r requirements/requirements.txt
-python setup.py install
-cd ../
-git clone https://github.com/oneapi-src/oneCCL.git
-cd oneCCL
-mkdir build
-cd build
-cmake ..
-make -j install
-source _install/env/setvars.sh
-cd ../..
-
-# Get the sample prompt.json
-# Make sure the downloaded prompt.json file is under the same directory as that of the python scripts mentioned above.
-wget https://intel-extension-for-pytorch.s3.amazonaws.com/miscellaneous/llm/prompt.json
-
-```
-
-
-## Supported Model List
-```
-<MODEL ID> in
-(1) "EleutherAI/gpt-j-6b" (model id from transformers Hub)
-(2) "EleutherAI/gpt-neox-20b" (model id from transformers Hub)
-(3) Llama 2 Model directory path
-(4) "facebook/opt-30b" (model id from transformers Hub)
-Note: Above models are well supported with all optimizations like indirect access KV cache, fused ROPE, and prepacked TPP Linear (fp32/bf16). For other LLM models (like Bloom...), we could still run with this BKC, and may get parts of optimizations like prepacked TPP Linear (fp32/bf16), and we are working in progress to cover all optimizations to these other LLM models, which will expand the model list above.
-```
-* Llama 2 model conversion steps:
-    1) [transformers conversion tool](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py) (Verified [meta-llama/Llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat) and [meta-llama/Llama-2-13b-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)).
-    2) Follow [instructions](https://github.com/facebookresearch/llama#access-on-hugging-face) to download model files for conversion.
-    3) Decompress the downloaded model file.
-    4) Follow [instructions](https://github.com/facebookresearch/llama-recipes#model-conversion-to-hugging-face) to convert the model.
-    5) Launch example scripts with the place holder <MODEL_ID> substituted by the --output_dir argument value of the conversion script.
-
-
-## Single Instance Performance
-```bash
-# Get prompt file to the path of scripts
-mv PATH/TO/prompt.json WORK_DIR
-
-# bfloat16 benchmark
-OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run_generation.py --benchmark -m <MODEL_ID> --dtype bfloat16 --ipex --jit
-
-# int8 benchmark
-## (1) Do quantization to get the quantized model
-mkdir saved_results
-
-## GPT-J quantization
-python run_gpt-j_int8.py --ipex-smooth-quant --lambada --output-dir "saved_results" --jit --int8-bf16-mixed -m <GPTJ MODEL_ID>
-## Llama 2 quantization
-python run_llama_int8.py --ipex-smooth-quant --lambada --output-dir "saved_results" --jit --int8-bf16-mixed -m <LLAMA MODEL_ID>
-## GPT-NEOX quantization
-python run_gpt-neox_int8.py --ipex-weight-only-quantization --lambada --output-dir "saved_results" --jit --int8 -m <GPT-NEOX MODEL_ID>
-
-## (2) Run int8 performance test (note that GPT-NEOX uses --int8 instead of --int8-bf16-mixed)
-OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <cpu list> python run_<MODEL>_int8.py -m <MODEL_ID> --quantized-model-path "./saved_results/best_model.pt" --benchmark --jit --int8-bf16-mixed
-```
-## Single Instance Accuracy
-```bash
-Accuracy test {TASK_NAME}, choice in this [link](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md), by default we use "lambada_openai"
-
-# bfloat16
-OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run_accuracy.py --accuracy-only -m <MODEL_ID> --dtype bfloat16 --ipex --jit --tasks {TASK_NAME}
-
-# Quantization as a performance part
-# (1) Do quantization to get the quantized model as mentioned above
-# (2) Run int8 accuracy test (note that GPT-NEOX please remove --int8-bf16-mixed)
-OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <cpu list> python run_accuracy.py --model <MODEL ID> --quantized-model-path "./saved_results/best_model.pt" --dtype int8 --accuracy-only --jit --int8-bf16-mixed --tasks {TASK_NAME}
-```
-## Shard model for Distributed Performance
-```
-# We need to make sure the model is well shard before we test Distributed Performance with DeepSpeed (saving memory usage purpose)
-python create_shard_model.py -m <MODEL ID>  --save-path <SHARD MODEL NEW PATH>
-# After sharding the model, using -m <SHARD MODEL NEW PATH> in later tests.
-```
-## Distributed Performance with DeepSpeed (autoTP)
-```bash
-export DS_SHM_ALLREDUCE=1
-unset KMP_AFFINITY
-
-# Get prompt file to the path of scripts
-mv PATH/TO/prompt.json WORK_DIR
-
-# Run GPTJ/LLAMA/OPT with bfloat16  DeepSpeed
-deepspeed --bind_cores_to_rank run_generation_with_deepspeed.py --benchmark -m <MODEL_ID> --dtype bfloat16 --ipex --jit
-
-# Run GPT-NeoX with ipex weight only quantization
-deepspeed --bind_cores_to_rank run_generation_with_deepspeed.py --benchmark -m EleutherAI/gpt-neox-20b --dtype float32 --ipex --jit --ipex-weight-only-quantization
-```
-
-## Distributed Accuracy with DeepSpeed (autoTP)
-```bash
-# Run distributed accuracy with 2 ranks of one node for bfloat16 with ipex and jit 
-source ${ONECCL_DIR}/build/_install/env/setvars.sh
-
-export LD_PRELOAD=${CONDA_PREFIX}/lib/libiomp5.so:${CONDA_PREFIX}/lib/libtcmalloc.so
-export LD_LIBRARY_PATH=${ONECCL_DIR}/lib:$LD_LIBRARY_PATH
-unset KMP_AFFINITY
-
-deepspeed  --num_gpus 2 --master_addr `hostname -I | sed -e 's/\s.*$//'` --bind_cores_to_rank run_accuracy_with_deepspeed.py --device cpu --model <MODEL_ID> --dtype bfloat16 --ipex --jit --tasks <TASK_NAME> --accuracy-only
-
-```
diff --git a/examples/cpu/inference/python/llm/create_shard_model.py b/examples/cpu/inference/python/llm/create_shard_model.py
deleted file mode 100644
index e492736c..00000000
--- a/examples/cpu/inference/python/llm/create_shard_model.py
+++ /dev/null
@@ -1,66 +0,0 @@
-import torch
-import argparse
-
-from transformers import (
-    AutoConfig,
-    AutoModelForCausalLM,
-    AutoTokenizer,
-    LlamaTokenizer
-)
-# supported models
-MODEL_CLASSES = {
-    "gpt-j": (AutoModelForCausalLM, AutoTokenizer),
-    "gpt-neox": (AutoModelForCausalLM, AutoTokenizer),
-    "llama": (AutoModelForCausalLM, LlamaTokenizer),
-    "opt": (AutoModelForCausalLM, AutoTokenizer),
-    "auto": (AutoModelForCausalLM, AutoTokenizer),
-}
-
-# args
-parser = argparse.ArgumentParser("shard model weight script", add_help=False)
-parser.add_argument(
-    "-m",
-    "--model-id",
-    type=str,
-    default="EleutherAI/gpt-j-6B",
-    help="the huggingface mdoel id",
-)
-parser.add_argument(
-    "--save-path",
-    type=str,
-    default="./",
-    help="saving path",
-)
-parser.add_argument(
-    "--dtype",
-    type=str,
-    choices=["float32", "bfloat16", "float16"],
-    default="bfloat16",
-    help="bfloat16, float32, float16",
-)
-parser.add_argument(
-    "--max-shard-size",
-    type=str,
-    default="500MB",
-)
-args = parser.parse_args()
-print(args)
-model_type = next(
-    (x for x in MODEL_CLASSES.keys() if x in args.model_id.lower()), "auto"
-)
-model_class = MODEL_CLASSES[model_type]
-
-load_dtype = torch.float32
-if args.dtype == "float16":
-    load_dtype = torch.half
-elif args.dtype == "bfloat16":
-    load_dtype = torch.bfloat16
-
-tokenizer = model_class[1].from_pretrained(args.model_id)
-model = model_class[0].from_pretrained(
-    args.model_id, torch_dtype=load_dtype, low_cpu_mem_usage=True
-)
-
-model.save_pretrained(save_directory=args.save_path, max_shard_size=args.max_shard_size)
-tokenizer.save_pretrained(save_directory=args.save_path)
-
diff --git a/examples/cpu/inference/python/llm/run_accuracy.py b/examples/cpu/inference/python/llm/run_accuracy.py
deleted file mode 100644
index 7597a449..00000000
--- a/examples/cpu/inference/python/llm/run_accuracy.py
+++ /dev/null
@@ -1,549 +0,0 @@
-import os
-import argparse
-import json
-import re
-import gc
-import torch
-from pathlib import Path
-import intel_extension_for_pytorch as ipex
-
-from transformers import (
-    AutoConfig,
-    AutoModelForCausalLM,
-    LlamaForCausalLM,
-    T5ForConditionalGeneration,
-    AutoTokenizer,
-    LlamaTokenizer,
-)
-
-
-MODEL_CLASSES = {
-    "gpt-j": (AutoModelForCausalLM, AutoTokenizer),
-    "gpt-neox": (AutoModelForCausalLM, AutoTokenizer),
-    "opt": (AutoModelForCausalLM, AutoTokenizer),
-    "bloom": (AutoModelForCausalLM, AutoTokenizer),
-    "llama": (AutoModelForCausalLM, LlamaTokenizer),
-    "auto": (AutoModelForCausalLM, AutoTokenizer),
-}
-
-parser = argparse.ArgumentParser()
-parser.add_argument("-m", "--model", nargs="?", default="EleutherAI/gpt-j-6b")
-parser.add_argument("--output_dir", nargs="?", default="./saved_results")
-parser.add_argument("--device", default="cpu", type=str, help="cpu")
-parser.add_argument(
-    "--dtype", default="bfloat16", type=str, help="float32 or bfloat16 or int8"
-)
-parser.add_argument("--accuracy-only", action="store_true")
-parser.add_argument(
-    "--batch-size", default=1, type=int, help="For accuracy measurement only."
-)
-parser.add_argument(
-    "--save-accuracy-path", default=None, help="Save accuracy results path."
-)
-parser.add_argument(
-    "--ipex", action="store_true", help="use intel extension for pytorch."
-)
-parser.add_argument(
-    "--jit", action="store_true", help="convert model to torchscript mode."
-)
-parser.add_argument("--int8-bf16-mixed", action="store_true", help="int8 mixed bf16")
-parser.add_argument("--quantized-model-path", default="./saved_result/best_model.pt")
-parser.add_argument(
-    "--tasks",
-    nargs="+",
-    default=[
-        "lambada_openai",
-    ],
-    type=str,
-    help="tasks list for accuracy validation, only enabled lambada_openai and lambada_standard at present",
-)
-
-
-args = parser.parse_args()
-
-
-if args.accuracy_only:
-    import lm_eval
-    from lm_eval import tasks, evaluator, models
-    from lm_eval.base import BaseLM
-    from typing import Union, List, Optional
-    from transformers import BatchEncoding
-
-    TokenSequence = Union[List[int], torch.LongTensor, torch.Tensor, BatchEncoding]
-
-    class HuggingFaceModel(BaseLM):
-        _DEFAULT_MAX_LENGTH = 2048
-
-        def __init__(
-            self,
-            device="cpu",
-            model_id="",
-            with_ipex=True,
-            with_jit=True,
-            with_greedy=False,
-            batch_size=1,
-            max_length=None,
-            dtype: Optional[Union[str, torch.dtype]] = "auto",
-        ):
-            super().__init__()
-
-            self._device = device
-            self._batch_size = batch_size
-            self._with_jit = with_jit
-            self._with_ipex = with_ipex
-            self._with_greedy = with_greedy
-            self._max_length = max_length
-            self._dtype = dtype
-
-            if dtype == "float16":
-                load_dtype = torch.half
-                infer_dtype = torch.half
-            elif dtype == "bfloat16":
-                load_dtype = torch.bfloat16
-                infer_dtype = torch.bfloat16
-            elif dtype == "int8":
-                load_dtype = torch.float32
-                infer_dtype = torch.int8
-            elif dtype == "float32":
-                load_dtype = torch.float32
-                infer_dtype = torch.float32
-
-            amp_enabled = True if dtype != "float32" else False
-            amp_dtype = getattr(torch, dtype)
-
-            model_type = next(
-                (x for x in MODEL_CLASSES.keys() if x in model_id.lower()), "auto"
-            )
-            model_class = MODEL_CLASSES[model_type]
-            self.tokenizer = model_class[1].from_pretrained(model_id)
-
-            self.config = AutoConfig.from_pretrained(model_id, torchscript=with_jit)
-
-            self.model = model_class[0].from_pretrained(
-                model_id,
-                low_cpu_mem_usage=True,
-                config=self.config,
-                torch_dtype=load_dtype,
-            )
-
-            self.model = self.model.eval()
-
-            if with_ipex:
-                self.model = ipex._optimize_transformers(
-                    self.model.eval(), dtype=infer_dtype, inplace=True
-                )
-
-            self.base_model = self.model
-
-            self.iter = 0
-            self.num_beams = 1 if with_greedy else 4
-            self.tp_number = 1
-
-        def _model_call(
-            self, inputs: TokenSequence, labels: Optional[TokenSequence] = None
-        ) -> TokenSequence:
-            _attention_mask = []
-            _position_ids = []
-
-
-            if self._with_jit:
-                for text in inputs:
-                    input_ids = text.to(self._device)
-                    input_bs = inputs.shape[0] * self.num_beams
-                    if re.search("GPTJ", self.base_model.config.architectures[0]):
-                        beam_idx_tmp = torch.zeros(
-                            (2048, int(input_bs)), dtype=torch.long
-                        ).contiguous()
-                        past_key_values = tuple(
-                            [
-                                (
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.n_head
-                                                / self.tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.n_embd
-                                                / self.base_model.config.n_head
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.n_head
-                                                / self.tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.n_embd
-                                                / self.base_model.config.n_head
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    beam_idx_tmp,
-                                    torch.zeros(1, dtype=torch.long).contiguous(),
-                                )
-                                for i in range(self.base_model.config.n_layer)
-                            ]
-                        )
-                    elif re.search(
-                        "llama", self.base_model.config.architectures[0], re.IGNORECASE
-                    ):
-                        beam_idx_tmp = torch.zeros(
-                            (2048, int(input_bs)), dtype=torch.long
-                        ).contiguous()
-                        past_key_values = tuple(
-                            [
-                                (
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.num_attention_heads
-                                                / self.tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.num_attention_heads
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.num_attention_heads
-                                                / self.tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.num_attention_heads
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    beam_idx_tmp,
-                                    torch.zeros(1, dtype=torch.long).contiguous(),
-                                )
-                                for i in range(self.base_model.config.num_hidden_layers)
-                            ]
-                        )
-                    elif re.search(
-                        "gptneox", self.base_model.config.architectures[0], re.IGNORECASE
-                    ):
-                        beam_idx_tmp = torch.zeros(
-                            (2048, int(input_bs)), dtype=torch.long
-                        ).contiguous()
-                        past_key_values = tuple(
-                            [
-                                (
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.num_attention_heads
-                                                / self.tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.num_attention_heads
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.num_attention_heads
-                                                / self.tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.num_attention_heads
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    beam_idx_tmp,
-                                    torch.zeros(1, dtype=torch.long).contiguous(),
-                                )
-                                for i in range(self.base_model.config.num_hidden_layers)
-                            ]
-                        )
-                    elif re.search(
-                        "bloom", self.base_model.config.architectures[0], re.IGNORECASE
-                    ):
-                        past_key_values = tuple(
-                            [
-                                (
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(self.base_model.config.n_head),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.n_head
-                                            ),
-                                        ]
-                                    ),
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(self.base_model.config.n_head),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.n_head
-                                            ),
-                                        ]
-                                    ),
-                                )
-                                for i in range(self.base_model.config.n_layer)
-                            ]
-                        )
-                    elif re.search(
-                        "OPT", self.base_model.config.architectures[0], re.IGNORECASE
-                    ):
-                        beam_idx_tmp = torch.zeros(
-                            (2048, int(input_bs)), dtype=torch.long
-                        ).contiguous()
-                        past_key_values = tuple(
-                            [
-                                (
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.num_attention_heads
-                                                / self.tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.num_attention_heads
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.num_attention_heads
-                                                / self.tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.num_attention_heads
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    beam_idx_tmp,
-                                    torch.zeros(1, dtype=torch.long).contiguous(),
-                                )
-                                for i in range(self.base_model.config.num_hidden_layers)
-                            ]
-                        )
-
-                    position_ids = torch.arange(len(input_ids))
-                    attention_mask = torch.ones(len(input_ids))
-
-                    _attention_mask.append(attention_mask)
-                    _position_ids.append(position_ids)
-
-                attention_mask_batched = torch.stack(_attention_mask)
-                position_ids_batched = torch.stack(_position_ids)
-
-            if self._with_jit and self.iter == 0 and self._dtype == "int8":
-                with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
-                    enabled=True
-                    if args.int8_bf16_mixed or self._dtype == torch.bfloat16
-                    else False,
-                    dtype=torch.bfloat16,
-                ):
-                    if self._dtype != "int8":
-                        if re.search(
-                            "bloom",
-                            self.base_model.config.architectures[0],
-                            re.IGNORECASE,
-                        ) or re.search(
-                            "OPT",
-                            self.base_model.config.architectures[0],
-                            re.IGNORECASE,
-                        ):
-                            example_dict = {
-                                "input_ids": inputs,
-                                "attention_mask": attention_mask_batched,
-                                "past_key_values": past_key_values,
-                            }
-                        else:
-                            example_dict = {
-                                "input_ids": inputs,
-                                "attention_mask": attention_mask_batched,
-                                "position_ids": position_ids_batched,
-                                "past_key_values": past_key_values,
-                            }
-
-                            self.model = torch.jit.trace(
-                                self.model.eval(),
-                                example_kwarg_inputs=example_dict,
-                                strict=False,
-                                check_trace=False,
-                            )
-                            self.model = torch.jit.freeze(self.model.eval())
-                    else:
-                        self.model = torch.jit.load(args.quantized_model_path)
-                        self.model = torch.jit.freeze(self.model.eval())
-
-                    if re.search(
-                        "bloom", self.base_model.config.architectures[0], re.IGNORECASE
-                    ) or re.search(
-                        "OPT", self.base_model.config.architectures[0], re.IGNORECASE
-                    ):
-                        self.model(
-                            inputs,
-                            past_key_values=past_key_values,
-                            attention_mask=attention_mask_batched,
-                        )
-                        self.model(
-                            inputs,
-                            past_key_values=past_key_values,
-                            attention_mask=attention_mask_batched,
-                        )
-                    else:
-                        self.model(
-                            inputs,
-                            past_key_values=past_key_values,
-                            attention_mask=attention_mask_batched,
-                            position_ids=position_ids_batched,
-                        )
-                        self.model(
-                            inputs,
-                            past_key_values=past_key_values,
-                            attention_mask=attention_mask_batched,
-                            position_ids=position_ids_batched,
-                        )
-
-                self.iter = self.iter + 1
-
-            if re.search(
-                "bloom", self.base_model.config.architectures[0], re.IGNORECASE
-            ) or re.search(
-                "OPT", self.base_model.config.architectures[0], re.IGNORECASE
-            ):
-                with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
-                    enabled=True
-                    if args.int8_bf16_mixed or self._dtype == torch.bfloat16
-                    else False,
-                    dtype=torch.bfloat16,
-                ):
-                    if self._with_jit:
-                        output = self.model(
-                            inputs,
-                            past_key_values=past_key_values,
-                            attention_mask=attention_mask_batched,
-                        )
-                    else:
-                        output = self.base_model(
-                            inputs,
-                        )
-            else:
-                with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
-                    enabled=True
-                    if args.int8_bf16_mixed or self._dtype == torch.bfloat16
-                    else False,
-                    dtype=torch.bfloat16,
-                ):
-                    if self._with_jit:
-                        output = self.model(
-                            inputs,
-                            past_key_values=past_key_values,
-                            attention_mask=attention_mask_batched,
-                            position_ids=position_ids_batched,
-                        )
-                    else:
-                        output = self.base_model(
-                            inputs,
-                        )
-
-            if isinstance(output, tuple):
-                return output[0]
-
-            return output["logits"]
-
-        @property
-        def eot_token_id(self):
-            # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*
-            return self.tokenizer.eos_token_id
-
-        @property
-        def max_length(self):
-            if self._max_length:  # if max length manually set, return it
-                return self._max_length
-            seqlen_config_attrs = ("n_positions", "max_position_embeddings", "n_ctx")
-            for attr in seqlen_config_attrs:
-                if hasattr(self.config, attr):
-                    return getattr(self.config, attr)
-            if hasattr(self.tokenizer, "model_max_length"):
-                if self.tokenizer.model_max_length == 1000000000000000019884624838656:
-                    return self._DEFAULT_MAX_LENGTH
-                return self.tokenizer.model_max_length
-
-            return self._DEFAULT_MAX_LENGTH
-
-        @property
-        def max_gen_toks(self):
-            return 256
-
-        @property
-        def batch_size(self):
-            # TODO: fix multi-gpu
-            return self._batch_size  # * gpus
-
-        @property
-        def device(self):
-            # TODO: fix multi-gpu
-            return self._device
-
-        def tok_encode(self, string: str):
-            return self.tokenizer.encode(string, add_special_tokens=False)
-
-        def tok_decode(self, tokens):
-            return self.tokenizer.decode(tokens)
-
-        def _model_generate(self, context, max_length, eos_token_id):
-            generation_kwargs = {"do_sample": False, "max_length": max_length}
-            if eos_token_id is not None:
-                generation_kwargs["eos_token_id"] = eos_token_id
-                generation_kwargs[
-                    "pad_token_id"
-                ] = eos_token_id  # setting eos_token_id as pad token
-            return self.model.generate(context, **generation_kwargs)
-
-    task_dict = lm_eval.tasks.get_task_dict(args.tasks)
-
-    hfmodel = HuggingFaceModel(
-        model_id=args.model,
-        device="cpu",
-        batch_size=args.batch_size,
-        with_ipex=args.ipex,
-        with_jit=args.jit,
-        dtype=args.dtype,
-    )
-
-    results = evaluator.evaluate(
-        hfmodel,
-        task_dict,
-        #        bootstrap_iters=1000,
-        #        limit=100
-    )
-
-    print(evaluator.make_table(results))
diff --git a/examples/cpu/inference/python/llm/run_accuracy_with_deepspeed.py b/examples/cpu/inference/python/llm/run_accuracy_with_deepspeed.py
deleted file mode 100644
index f3c7261a..00000000
--- a/examples/cpu/inference/python/llm/run_accuracy_with_deepspeed.py
+++ /dev/null
@@ -1,590 +0,0 @@
-import os
-import argparse
-import json
-import re
-import gc
-import torch
-from pathlib import Path
-import intel_extension_for_pytorch as ipex
-
-import deepspeed
-from deepspeed.accelerator import get_accelerator
-import deepspeed.comm as dist
-from huggingface_hub import snapshot_download
-from transformers.utils import is_offline_mode
-from transformers import (
-    AutoConfig,
-    AutoModelForCausalLM,
-    LlamaForCausalLM,
-    T5ForConditionalGeneration,
-    AutoTokenizer,
-    LlamaTokenizer,
-)
-
-
-MODEL_CLASSES = {
-    "gpt-j": (AutoModelForCausalLM, AutoTokenizer),
-    "gpt-neox": (AutoModelForCausalLM, AutoTokenizer),
-    "opt": (AutoModelForCausalLM, AutoTokenizer),
-    "bloom": (AutoModelForCausalLM, AutoTokenizer),
-    "llama": (AutoModelForCausalLM, LlamaTokenizer),
-    "auto": (AutoModelForCausalLM, AutoTokenizer),
-}
-
-parser = argparse.ArgumentParser()
-parser.add_argument("--model", nargs="?", default="EleutherAI/gpt-j-6b")
-parser.add_argument("--output_dir", nargs="?", default="./saved_results")
-parser.add_argument("--device", default="cpu", type=str, help="cpu")
-parser.add_argument(
-    "--dtype", default="bfloat16", type=str, help="float32 or bfloat16 or int8"
-)
-parser.add_argument("--accuracy-only", action="store_true")
-parser.add_argument(
-    "--batch-size", default=1, type=int, help="For accuracy measurement only."
-)
-parser.add_argument(
-    "--save-accuracy-path", default=None, help="Save accuracy results path."
-)
-parser.add_argument(
-    "--ipex", action="store_true", help="use intel extension for pytorch."
-)
-parser.add_argument(
-    "--jit", action="store_true", help="convert model to torchscript mode."
-)
-parser.add_argument("--int8-bf16-mixed", action="store_true", help="int8 mixed bf16")
-parser.add_argument("--quantized-model-path", default="./saved_result/best_model.pt")
-parser.add_argument(
-    "--tasks",
-    nargs="+",
-    default=[
-        "lambada_openai",
-    ],
-    type=str,
-    help="tasks list for accuracy validation, only enabled lambada_openai and lambada_standard at present",
-)
-parser.add_argument(
-    "--local_rank", required=False, type=int, help="used by dist launchers"
-)
-
-args = parser.parse_args()
-
-def get_int_from_env(env_keys, default):
-    """Returns the first positive env value found in the `env_keys` list or the default."""
-    for e in env_keys:
-        val = int(os.environ.get(e, -1))
-        if val >= 0:
-            return val
-    return default
-
-
-
-local_rank = get_int_from_env(["LOCAL_RANK", "MPI_LOCALRANKID"], "0")
-world_size = get_int_from_env(["WORLD_SIZE", "PMI_SIZE"], "1")
-
-deepspeed.init_distributed(get_accelerator().communication_backend_name())
-
-print("init_distributed done")
-
-if args.accuracy_only:
-    import lm_eval
-    from lm_eval import tasks, evaluator, models
-    from lm_eval.base import BaseLM
-    from typing import Union, List, Optional
-    from transformers import BatchEncoding
-
-    TokenSequence = Union[List[int], torch.LongTensor, torch.Tensor, BatchEncoding]
-
-    class HuggingFaceModel(BaseLM):
-        _DEFAULT_MAX_LENGTH = 2048
-
-        def __init__(
-            self,
-            device="cpu",
-            model_id="",
-            with_ipex=True,
-            with_jit=True,
-            with_greedy=False,
-            batch_size=1,
-            max_length=None,
-            dtype: Optional[Union[str, torch.dtype]] = "auto",
-            tp_number = 1,
-        ):
-            super().__init__()
-            
-            self._device = device
-            self._batch_size = batch_size
-            self._with_jit = with_jit
-            self._with_ipex = with_ipex
-            self._with_greedy = with_greedy
-            self._max_length = max_length
-            self._dtype = dtype
-            self._tp_number = tp_number
-
-            if dtype == "float16":
-                load_dtype = torch.half
-                infer_dtype = torch.half
-            elif dtype == "bfloat16":
-                load_dtype = torch.bfloat16
-                infer_dtype = torch.bfloat16
-            elif dtype == "int8":
-                load_dtype = torch.float32
-                infer_dtype = torch.int8
-            elif dtype == "float32":
-                load_dtype = torch.float32
-                infer_dtype = torch.float32
-
-            amp_enabled = True if dtype != "float32" else False
-            amp_dtype = getattr(torch, dtype)
-
-            model_type = next(
-                (x for x in MODEL_CLASSES.keys() if x in model_id.lower()), "auto"
-            )
-            model_class = MODEL_CLASSES[model_type]
-
-            self.tokenizer = model_class[1].from_pretrained(model_id)
-
-            self.config = AutoConfig.from_pretrained(model_id, torchscript=with_jit)
-
-            with deepspeed.OnDevice(dtype=load_dtype, device="meta"):
-                if model_class[0] == AutoModelForCausalLM:
-                    self.model = model_class[0].from_config(self.config).to(load_dtype)
-                else: 
-                    self.model = model_class[0].from_pretrained(
-                        model_id,
-                        low_cpu_mem_usage=True,
-                        config=self.config,
-                        torch_dtype=load_dtype,
-                    )
-
-            self.model = self.model.eval()
-          
-            checkpoints_json = "checkpoints.json"
-            def get_repo_root(model_name_or_path):
-                local_prefix = ("/", "./", "../")
-                if model_name_or_path.startswith(local_prefix):
-                    return model_name_or_path
-                # checks if online or not
-                if is_offline_mode():
-                    print_rank0("Offline mode: forcing local_files_only=True")
-                # download only on first process
-                allow_patterns = ["*.bin", "*.model", "*.json", "*.txt", "*.py", "*LICENSE"]
-                if local_rank == 0:
-                    snapshot_download(
-                        model_name_or_path,
-                        local_files_only=is_offline_mode(),
-                        cache_dir=os.getenv("TRANSFORMERS_CACHE", None),
-                        allow_patterns=allow_patterns,
-                        # ignore_patterns=["*.safetensors"],
-                    )
-            
-                dist.barrier()
-            
-                return snapshot_download(
-                    model_name_or_path,
-                    local_files_only=is_offline_mode(),
-                    cache_dir=os.getenv("TRANSFORMERS_CACHE", None),
-                    allow_patterns=allow_patterns,
-                    # ignore_patterns=["*.safetensors"],
-                )
-            
-            
-            def get_checkpoint_files(model_name_or_path):
-                cached_repo_dir = get_repo_root(model_name_or_path)
-            
-                # extensions: .bin | .pt
-                # creates a list of paths from all downloaded files in cache dir
-                file_list = [
-                    str(entry)
-                    for entry in Path(cached_repo_dir).rglob("*.[bp][it][n]")
-                    if entry.is_file()
-                ]
-                return file_list
-            
-
-            def write_checkpoints_json():
-                checkpoint_files = get_checkpoint_files(model_id)
-                if local_rank == 0:
-                    # model.config.model_type.upper()
-                    data = {"type": "BLOOM", "checkpoints": checkpoint_files, "version": 1.0}
-                    json.dump(data, open(checkpoints_json, "w"))
-
-            repo_root = get_repo_root(model_id)
-            write_checkpoints_json()
-
-            self.model = deepspeed.init_inference(
-                self.model,
-                mp_size=tp_number,
-                base_dir=repo_root,
-                dtype=infer_dtype,
-                checkpoint=checkpoints_json,
-            )
-
-            self.model = self.model.module
-
-            if with_ipex:
-                self.model = ipex._optimize_transformers(
-                    self.model.eval(), dtype=infer_dtype, inplace=True
-                )
-
-            self.base_model = self.model
-
-            self.num_beams = 1 if with_greedy else 4
-            self.iter = 0
-
-        def _model_call(
-            self, inputs: TokenSequence, labels: Optional[TokenSequence] = None
-        ) -> TokenSequence:
-            _attention_mask = []
-            _position_ids = []
-
-            if self._with_jit:
-                for text in inputs:
-                    input_ids = text.to(self._device)
-                    input_bs = inputs.shape[0] * self.num_beams
-                    if re.search("GPTJ", self.base_model.config.architectures[0]):
-                        beam_idx_tmp = torch.zeros(
-                            (2048, int(input_bs)), dtype=torch.long
-                        ).contiguous()
-                        past_key_values = tuple(
-                            [
-                                (
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.n_head
-                                                / self._tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.n_embd
-                                                / self.base_model.config.n_head
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.n_head
-                                                / self._tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.n_embd
-                                                / self.base_model.config.n_head
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    beam_idx_tmp,
-                                    torch.zeros(1, dtype=torch.long).contiguous(),
-                                )
-                                for i in range(self.base_model.config.n_layer)
-                            ]
-                        )
-                    elif re.search(
-                        "llama", self.base_model.config.architectures[0], re.IGNORECASE
-                    ):
-                        beam_idx_tmp = torch.zeros(
-                            (2048, int(input_bs)), dtype=torch.long
-                        ).contiguous()
-                        past_key_values = tuple(
-                            [
-                                (
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.num_attention_heads
-                                                / self._tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.num_attention_heads
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.num_attention_heads
-                                                / self._tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.num_attention_heads
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    beam_idx_tmp,
-                                    torch.zeros(1, dtype=torch.long).contiguous(),
-                                )
-                                for i in range(self.base_model.config.num_hidden_layers)
-                            ]
-                        )
-                    elif re.search(
-                        "gptneox", self.base_model.config.architectures[0], re.IGNORECASE
-                    ):
-                        beam_idx_tmp = torch.zeros(
-                            (2048, int(input_bs)), dtype=torch.long
-                        ).contiguous()
-                        past_key_values = tuple(
-                            [
-                                (
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.num_attention_heads
-                                                / self._tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.num_attention_heads
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(
-                                                self.base_model.config.num_attention_heads
-                                                / self._tp_number
-                                            ),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.num_attention_heads
-                                            ),
-                                        ]
-                                    ).contiguous(),
-                                    beam_idx_tmp,
-                                    torch.zeros(1, dtype=torch.long).contiguous(),
-                                )
-                                for i in range(self.base_model.config.num_hidden_layers)
-                            ]
-                        )
-                    elif re.search(
-                        "bloom", self.base_model.config.architectures[0], re.IGNORECASE
-                    ):
-                        past_key_values = tuple(
-                            [
-                                (
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(self.base_model.config.n_head),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.n_head
-                                            ),
-                                        ]
-                                    ),
-                                    torch.zeros(
-                                        [
-                                            1,
-                                            int(self.base_model.config.n_head),
-                                            1,
-                                            int(
-                                                self.base_model.config.hidden_size
-                                                / self.base_model.config.n_head
-                                            ),
-                                        ]
-                                    ),
-                                )
-                                for i in range(self.base_model.config.n_layer)
-                            ]
-                        )
-
-                    position_ids = torch.arange(len(input_ids))
-                    attention_mask = torch.ones(len(input_ids))
-
-                    _attention_mask.append(attention_mask)
-                    _position_ids.append(position_ids)
-
-                attention_mask_batched = torch.stack(_attention_mask)
-                position_ids_batched = torch.stack(_position_ids)
-
-            if self._with_jit and self.iter == 0 and self._dtype == "int8":
-                with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
-                    enabled=True
-                    if args.int8_bf16_mixed or self._dtype == torch.bfloat16
-                    else False,
-                    dtype=torch.bfloat16,
-                ):
-                    if self._dtype != "int8":
-                        if re.search(
-                            "bloom",
-                            self.base_model.config.architectures[0],
-                            re.IGNORECASE,
-                        ):
-                            example_dict = {
-                                "input_ids": inputs,
-                                "attention_mask": attention_mask_batched,
-                                "past_key_values": past_key_values,
-                            }
-                        else:
-                            example_dict = {
-                                "input_ids": inputs,
-                                "attention_mask": attention_mask_batched,
-                                "position_ids": position_ids_batched,
-                                "past_key_values": past_key_values,
-                            }
-
-                            self.model = torch.jit.trace(
-                                self.model.eval(),
-                                example_kwarg_inputs=example_dict,
-                                strict=False,
-                                check_trace=False,
-                            )
-                            self.model = torch.jit.freeze(self.model.eval())
-                    else:
-                        self.model = torch.jit.load(args.quantized_model_path)
-                        self.model = torch.jit.freeze(self.model.eval())
-
-                    if re.search(
-                        "bloom", self.base_model.config.architectures[0], re.IGNORECASE
-                    ):
-                        self.model(
-                            inputs,
-                            past_key_values=past_key_values,
-                            attention_mask=attention_mask_batched,
-                        )
-                        self.model(
-                            inputs,
-                            past_key_values=past_key_values,
-                            attention_mask=attention_mask_batched,
-                        )
-                    else:
-                        self.model(
-                            inputs,
-                            past_key_values=past_key_values,
-                            attention_mask=attention_mask_batched,
-                            position_ids=position_ids_batched,
-                        )
-                        self.model(
-                            inputs,
-                            past_key_values=past_key_values,
-                            attention_mask=attention_mask_batched,
-                            position_ids=position_ids_batched,
-                        )
-
-                self.iter = self.iter + 1
-
-            if re.search(
-                "bloom", self.base_model.config.architectures[0], re.IGNORECASE
-            ):
-                with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
-                    enabled=True
-                    if args.int8_bf16_mixed or self._dtype == torch.bfloat16
-                    else False,
-                    dtype=torch.bfloat16,
-                ):
-                    if self._with_jit:
-                        output = self.model(
-                            inputs,
-                            past_key_values=past_key_values,
-                            attention_mask=attention_mask_batched,
-                        )
-                    else:
-                        output = self.base_model(
-                            inputs,
-                        )
-            else:
-                with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
-                    enabled=True
-                    if args.int8_bf16_mixed or self._dtype == torch.bfloat16
-                    else False,
-                    dtype=torch.bfloat16,
-                ):
-                    if self._with_jit:
-                        output = self.model(
-                            inputs,
-                            past_key_values=past_key_values,
-                            attention_mask=attention_mask_batched,
-                            position_ids=position_ids_batched,
-                        )
-                    else:
-                        output = self.base_model(
-                            inputs,
-                        )
- 
-            if isinstance(output, tuple):
-                return output[0]
-
-            return output["logits"]
-
-        @property
-        def eot_token_id(self):
-            # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*
-            return self.tokenizer.eos_token_id
-
-        @property
-        def max_length(self):
-            if self._max_length:  # if max length manually set, return it
-                return self._max_length
-            seqlen_config_attrs = ("n_positions", "max_position_embeddings", "n_ctx")
-            for attr in seqlen_config_attrs:
-                if hasattr(self.config, attr):
-                    return getattr(self.config, attr)
-            if hasattr(self.tokenizer, "model_max_length"):
-                if self.tokenizer.model_max_length == 1000000000000000019884624838656:
-                    return self._DEFAULT_MAX_LENGTH
-                return self.tokenizer.model_max_length
-
-            return self._DEFAULT_MAX_LENGTH
-
-        @property
-        def max_gen_toks(self):
-            return 256
-
-        @property
-        def batch_size(self):
-            # TODO: fix multi-gpu
-            return self._batch_size  # * gpus
-
-        @property
-        def device(self):
-            # TODO: fix multi-gpu
-            return self._device
-
-        def tok_encode(self, string: str):
-            return self.tokenizer.encode(string, add_special_tokens=False)
-
-        def tok_decode(self, tokens):
-            return self.tokenizer.decode(tokens)
-
-        def _model_generate(self, context, max_length, eos_token_id):
-            generation_kwargs = {"do_sample": False, "max_length": max_length}
-            if eos_token_id is not None:
-                generation_kwargs["eos_token_id"] = eos_token_id
-                generation_kwargs[
-                    "pad_token_id"
-                ] = eos_token_id  # setting eos_token_id as pad token
-            return self.model.generate(context, **generation_kwargs)
-
-    task_dict = lm_eval.tasks.get_task_dict(args.tasks)
-
-    hfmodel = HuggingFaceModel(
-        model_id=args.model,
-        device="cpu",
-        batch_size=args.batch_size,
-        with_ipex=args.ipex,
-        with_jit=args.jit,
-        dtype=args.dtype,
-        tp_number=world_size,
-    )
-
-    results = evaluator.evaluate(
-        hfmodel,
-        task_dict,
-        #        bootstrap_iters=1000,
-        #        limit=100
-    )
-
-    print(evaluator.make_table(results))
diff --git a/examples/cpu/inference/python/llm/run_generation.py b/examples/cpu/inference/python/llm/run_generation.py
deleted file mode 100644
index 50efe4cc..00000000
--- a/examples/cpu/inference/python/llm/run_generation.py
+++ /dev/null
@@ -1,468 +0,0 @@
-import torch
-import time
-import json
-import pathlib
-import argparse
-
-from torch.nn.functional import pad
-from datasets import load_dataset
-from torch.utils.data import DataLoader
-
-from transformers import (
-    AutoConfig,
-    AutoModelForCausalLM,
-    LlamaForCausalLM,
-    AutoTokenizer,
-    LlamaTokenizer,
-)
-
-
-# supported models
-MODEL_CLASSES = {
-    "gpt-j": (AutoModelForCausalLM, AutoTokenizer),
-    "gpt-neox": (AutoModelForCausalLM, AutoTokenizer),
-    "llama": (AutoModelForCausalLM, LlamaTokenizer),
-    "opt": (AutoModelForCausalLM, AutoTokenizer),
-    "auto": (AutoModelForCausalLM, AutoTokenizer),
-}
-
-# args
-parser = argparse.ArgumentParser("Generation script (fp32/bf16 path)", add_help=False)
-parser.add_argument(
-    "-m",
-    "--model-id",
-    type=str,
-    default="EleutherAI/gpt-j-6B",
-    help="the huggingface mdoel id",
-)
-parser.add_argument(
-    "--device",
-    type=str,
-    choices=["cpu"],
-    default="cpu",
-    help="cpu",
-)
-parser.add_argument(
-    "--dtype",
-    type=str,
-    choices=["float32", "bfloat16"],
-    default="bfloat16",
-    help="bfloat16, float32",
-)
-parser.add_argument(
-    "--input-tokens",
-    default="32",
-    type=str,
-    help="input tokens length if needed from prompt.json",
-)
-parser.add_argument(
-    "--max-new-tokens", default=32, type=int, help="output max new tokens"
-)
-parser.add_argument(
-    "--prompt", default=None, type=str, help="input prompt for self-defined if needed"
-)
-parser.add_argument("--greedy", action="store_true")
-parser.add_argument("--ipex", action="store_true")
-parser.add_argument("--jit", action="store_true")
-parser.add_argument("--profile", action="store_true")
-parser.add_argument("--benchmark", action="store_true")
-parser.add_argument("--lambada", action="store_true")
-parser.add_argument("--dataset", default="lambada", type=str)
-parser.add_argument("--accuracy-only", action="store_true")
-parser.add_argument("--num-iter", default=100, type=int, help="num iter")
-parser.add_argument("--num-warmup", default=10, type=int, help="num warmup")
-parser.add_argument("--batch-size", default=1, type=int, help="batch size")
-parser.add_argument(
-    "--token-latency", action="store_true", help="get token latency breakdown"
-)
-args = parser.parse_args()
-print(args)
-
-# device
-device = torch.device(args.device)
-
-
-if not args.ipex or not args.jit:
-    print("Please use --ipex and --jit to re-run this script, aborting...")
-    exit(0)
-
-# import ipex
-if args.ipex:
-    import intel_extension_for_pytorch as ipex
-
-    try:
-        ipex._C.disable_jit_linear_repack()
-    except Exception:
-        pass
-
-if args.jit:
-    torch._C._jit_set_texpr_fuser_enabled(False)
-
-# dtype
-amp_enabled = True if args.dtype != "float32" else False
-amp_dtype = getattr(torch, args.dtype)
-
-# load model
-model_type = next(
-    (x for x in MODEL_CLASSES.keys() if x in args.model_id.lower()), "auto"
-)
-model_class = MODEL_CLASSES[model_type]
-config = AutoConfig.from_pretrained(args.model_id, torchscript=args.jit)
-if not hasattr(config, "text_max_length") and args.prompt is None:
-    config.text_max_length = int(args.input_tokens) + int(args.max_new_tokens)
-model = model_class[0].from_pretrained(
-    args.model_id, torch_dtype=amp_dtype, config=config, low_cpu_mem_usage=True
-)
-tokenizer = model_class[1].from_pretrained(args.model_id)
-model = model.eval().to(device)
-model = model.to(memory_format=torch.channels_last)
-
-# to ipex
-if args.ipex:
-    model = ipex._optimize_transformers(model.eval(), dtype=amp_dtype, inplace=True)
-
-num_beams = 1 if args.greedy else 4
-# generate args
-generate_kwargs = dict(do_sample=False, temperature=0.9, num_beams=num_beams)
-
-# dummy past key values
-past_key_values = None
-import re
-
-if re.search("GPTJ", model.config.architectures[0]):
-    beam_idx_tmp = torch.zeros(
-        (2048, int(args.batch_size * num_beams)), dtype=torch.long
-    ).contiguous()
-    past_key_values = tuple(
-        [
-            (
-                torch.zeros([1, 1, 1, 1]).contiguous(),
-                torch.zeros([1, 1, 1, 1]).contiguous(),
-                beam_idx_tmp,
-                torch.zeros(1, dtype=torch.long).contiguous(),
-            )
-            for i in range(model.config.n_layer)
-        ]
-    )
-elif re.search("llama", model.config.architectures[0], re.IGNORECASE):
-    beam_idx_tmp = torch.zeros(
-        (2048, int(args.batch_size * num_beams)), dtype=torch.long
-    ).contiguous()
-    past_key_values = tuple(
-        [
-            (
-                torch.zeros([1, 1, 1, 1]).contiguous(),
-                torch.zeros([1, 1, 1, 1]).contiguous(),
-                beam_idx_tmp,
-                torch.zeros(1, dtype=torch.long).contiguous(),
-            )
-            for i in range(model.config.num_hidden_layers)
-        ]
-    )
-elif re.search("gptneox", model.config.architectures[0], re.IGNORECASE):
-    beam_idx_tmp = torch.zeros(
-        (2048, int(args.batch_size * num_beams)), dtype=torch.long
-    ).contiguous()
-    past_key_values = tuple(
-        [
-            (
-                torch.zeros([1, 1, 1, 1]).contiguous(),
-                torch.zeros([1, 1, 1, 1]).contiguous(),
-                beam_idx_tmp,
-                torch.zeros(1, dtype=torch.long).contiguous(),
-            )
-            for i in range(model.config.num_hidden_layers)
-        ]
-    )
-elif re.search("opt", model.config.architectures[0], re.IGNORECASE):
-    beam_idx_tmp = torch.zeros(
-        (2048, int(args.batch_size * num_beams)), dtype=torch.long
-    ).contiguous()
-    past_key_values = tuple(
-        [
-            (
-                torch.zeros([1, 1, 1, 1]).contiguous(),
-                torch.zeros([1, 1, 1, 1]).contiguous(),
-                beam_idx_tmp,
-                torch.zeros(1, dtype=torch.long).contiguous(),
-            )
-            for i in range(model.config.num_hidden_layers)
-        ]
-    )
-else:
-    print(
-        "Currently we only support jit path on GPTJ, llama, and gpt_neox models for IPEX new API ipex._optimize_transformers(), please re-run without jit "
-    )
-    exit(0)
-
-if not hasattr(model, "trace_graph") and args.jit and args.benchmark and args.ipex:
-    example_inputs = None
-    input_ids = torch.ones(32).to(torch.long)
-    attention_mask = torch.ones(len(input_ids))
-    position_ids = torch.arange(len(input_ids))
-    if re.search("opt", model.config.architectures[0], re.IGNORECASE):
-        example_inputs = {
-            "input_ids": input_ids.unsqueeze(0),
-            "attention_mask": attention_mask.unsqueeze(0),
-            "past_key_values": past_key_values,
-        }
-    else:
-        example_inputs = {
-            "input_ids": input_ids.unsqueeze(0),
-            "attention_mask": attention_mask.unsqueeze(0),
-            "position_ids": position_ids.unsqueeze(0),
-            "past_key_values": past_key_values,
-        }
-
-    with torch.inference_mode(), torch.no_grad(), torch.autocast(
-        device_type=args.device,
-        enabled=amp_enabled,
-        dtype=amp_dtype if amp_enabled else None,
-    ):
-        trace_model = torch.jit.trace(
-            model, example_kwarg_inputs=example_inputs, strict=False, check_trace=False
-        )
-        trace_model = torch.jit.freeze(trace_model)
-        setattr(model, "trace_graph", trace_model)
-
-
-class Evaluator:
-    def __init__(self, dataset, tokenizer, batch_size=8, pad_val=1, pad_max=196):
-        self.dataset = dataset
-        self.tokenizer = tokenizer
-        self.batch_size = batch_size
-        self.pad_val = pad_val
-        self.pad_max = pad_max
-
-        # tokenize the dataset
-        self.dataset = self.dataset.map(self.tokenize_function, batched=True)
-        self.dataset.set_format(type="torch", columns=["input_ids"])
-
-    @torch.no_grad()
-    def tokenize_function(self, examples):
-        example = self.tokenizer(examples["text"])
-        return example
-
-    @torch.no_grad()
-    def collate_batch(self, batch):
-        position_ids_padded = []
-        input_ids_padded = []
-        last_ind = []
-        attention_mask_padded = []
-        for text in batch:
-            # we cut the sentence if it exceeds pad_max, we are using tuned max 196 from gptj model; TODO: tune best pad_max
-            input_ids = (
-                text["input_ids"]
-                if text["input_ids"].shape[0] <= self.pad_max
-                else text["input_ids"][0 : int(self.pad_max - 1)]
-            )
-            pad_len = self.pad_max - input_ids.shape[0]
-            last_ind.append(input_ids.shape[0] - 1)
-            attention_mask = torch.ones(len(input_ids))
-            position_ids = torch.arange(len(input_ids))
-            input_ids = pad(input_ids, (0, pad_len), value=self.pad_val)
-            input_ids_padded.append(input_ids)
-            attention_mask = pad(attention_mask, (0, pad_len), value=0)
-            attention_mask_padded.append(attention_mask)
-            position_ids = pad(position_ids, (0, pad_len), value=self.pad_val)
-            position_ids_padded.append(position_ids)
-        return (
-            (
-                torch.vstack(input_ids_padded),
-                torch.vstack(attention_mask_padded),
-                torch.vstack(position_ids_padded),
-                tuple(past_key_values),
-            ),
-            torch.tensor(last_ind),
-        )
-
-    @torch.no_grad()
-    def evaluate(self, model):
-        model.eval()
-        # The task is to predict the last word of the input.
-        total, hit = 0, 0
-        latency = 0
-        test_dataloader = DataLoader(
-            self.dataset,
-            batch_size=self.batch_size,
-            shuffle=False,
-            collate_fn=self.collate_batch,
-        )
-
-        for i, (
-            (input_ids, attention_mask, position_ids, past_key_values),
-            last_ind,
-        ) in enumerate(test_dataloader):
-            label = input_ids[torch.arange(len(last_ind)), last_ind]
-            input_ids[torch.arange(len(last_ind)), last_ind] = self.pad_val
-            pad_len = self.pad_max - last_ind - 1
-            start = time.time()
-
-            outputs = model(
-                input_ids,
-                attention_mask=attention_mask,
-                position_ids=position_ids,
-                past_key_values=past_key_values,
-            )
-
-            latency += time.time() - start
-
-            last_token_logits = outputs[0][torch.arange(len(last_ind)), -2 - pad_len, :]
-
-            pred = last_token_logits.argmax(dim=-1)
-            total += label.size(0)
-            hit += (pred == label).sum().item()
-            if i % 50 == 0:
-                print(hit / total)
-                print("Processed minibatch:", i)
-
-        acc = hit / total
-        print(acc)
-        lantecy = latency / len(self.dataset)
-        return acc, lantecy
-
-
-if args.lambada:
-    full_dataset = load_dataset(args.dataset)
-    dataset = full_dataset["validation"]
-
-    model.eval()
-    evaluator = Evaluator(dataset, tokenizer, args.batch_size)
-
-    test_dataloader = DataLoader(
-        evaluator.dataset,
-        batch_size=args.batch_size,
-        shuffle=False,
-        collate_fn=evaluator.collate_batch,
-    )
-
-
-def eval_func(traced_model):
-    acc, latency = evaluator.evaluate(traced_model)
-    print("Accuracy:", acc)
-    print("Latency (sec):", latency)
-    return acc
-
-
-if args.accuracy_only:
-    if args.jit and args.ipex:
-        input_ids = torch.ones(32).to(torch.long)
-        attention_mask = torch.ones(len(input_ids))
-        position_ids = torch.arange(len(input_ids))
-        example_inputs = (
-            input_ids.unsqueeze(0),
-            attention_mask.unsqueeze(0),
-            position_ids.unsqueeze(0),
-            tuple(past_key_values),
-        )
-        with torch.no_grad(), torch.autocast(
-            device_type=args.device,
-            enabled=amp_enabled,
-            dtype=amp_dtype if amp_enabled else None,
-        ):
-            model = torch.jit.trace(model.eval(), example_inputs, strict=False)
-            model = torch.jit.freeze(model.eval())
-
-    with torch.autocast(
-        device_type=args.device,
-        enabled=amp_enabled,
-        dtype=amp_dtype if amp_enabled else None,
-    ):
-        eval_func(model)
-
-
-def trace_handler(prof):
-    print(prof.key_averages().table(
-        sort_by="self_cpu_time_total", row_limit=-1))
-
-if args.benchmark:
-    if args.token_latency:
-        if not hasattr(model.config, "token_latency"):
-            model.config.token_latency = True
-    # input prompt
-    current_path = pathlib.Path(__file__).parent.resolve()
-    with open(str(current_path) + "/prompt.json") as f:
-        prompt_pool = json.load(f)
-    if args.prompt is not None:
-        prompt = args.prompt
-    elif model_type == "auto":
-        raise SystemExit(
-            "[ERROR] model prompt is not supported, please use --prompt for this model: "
-            + args.model_id
-        )
-    elif int(args.input_tokens) > 8192:
-        prompt = prompt_pool[model_type]["8192"] * int(int(args.input_tokens) / 8192)
-    elif args.input_tokens in prompt_pool[model_type]:
-        prompt = prompt_pool[model_type][args.input_tokens]
-    else:
-        raise SystemExit("[ERROR] Plese use --prompt if want to use custom input.")
-
-    input_size = tokenizer(prompt, return_tensors="pt").input_ids.size(dim=1)
-    print("---- Prompt size:", input_size)
-
-    # start
-    total_time = 0.0
-    num_iter = args.num_iter
-    num_warmup = args.num_warmup
-    prompt = [prompt] * args.batch_size
-    total_list = []
-    with torch.inference_mode(), torch.no_grad(), torch.autocast(
-        device_type=args.device,
-        enabled=amp_enabled,
-        dtype=amp_dtype if amp_enabled else None,
-    ):
-        if args.profile:
-            with torch.profiler.profile(
-                activities=[torch.profiler.ProfilerActivity.CPU],
-                schedule=torch.profiler.schedule(
-                    wait=1,
-                    warmup=3,
-                    active=1),
-                on_trace_ready=trace_handler
-            ) as prof:
-                for i in range(5):
-                    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
-                    output = model.generate(
-                        input_ids, max_new_tokens=args.max_new_tokens, **generate_kwargs
-                    )
-                    prof.step()
-        for i in range(num_iter):
-            tic = time.time()
-            input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
-            output = model.generate(
-                input_ids, max_new_tokens=args.max_new_tokens, **generate_kwargs
-            )
-            gen_ids = output[0] if args.token_latency else output
-            gen_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
-            toc = time.time()
-            input_tokens_lengths = [x.shape[0] for x in input_ids]
-            output_tokens_lengths = [x.shape[0] for x in gen_ids]
-            total_new_tokens = [
-                o - i if model.config.model_type != "t5" else o
-                for i, o in zip(input_tokens_lengths, output_tokens_lengths)
-            ]
-            print(gen_text, total_new_tokens, flush=True)
-            print("Iteration: %d, Time: %.6f sec" % (i, toc - tic), flush=True)
-            if i >= num_warmup:
-                total_time += toc - tic
-                if args.token_latency:
-                    total_list.append(output[1])
-
-    print("\n", "-" * 10, "Summary:", "-" * 10)
-    latency = total_time / (num_iter - num_warmup)
-    print("Inference latency: %.3f sec." % latency)
-
-    if args.token_latency:
-        import numpy as np
-        from itertools import chain
-
-        first_latency = np.mean([x[0] for x in total_list])
-        average_2n = list(chain(*[x[1:] for x in total_list]))
-        average_2n.sort()
-        average_2n_latency = np.mean(average_2n)
-        p90_latency = average_2n[int(len(average_2n) * 0.9)]
-        p99_latency = average_2n[int(len(average_2n) * 0.99)]
-        print("First token average latency: %.3f sec." % first_latency)
-        print("Average 2... latency: %.3f sec." % average_2n_latency)
-        print("P90 2... latency: %.3f sec." % p90_latency)
-        print("P99 2... latency: %.3f sec." % p99_latency)
diff --git a/examples/cpu/inference/python/llm/run_generation_with_deepspeed.py b/examples/cpu/inference/python/llm/run_generation_with_deepspeed.py
deleted file mode 100644
index 48468c20..00000000
--- a/examples/cpu/inference/python/llm/run_generation_with_deepspeed.py
+++ /dev/null
@@ -1,556 +0,0 @@
-import gc
-import json
-import math
-import pathlib
-import os
-import time
-from argparse import ArgumentParser
-from pathlib import Path
-import torch
-
-import deepspeed
-from deepspeed.accelerator import get_accelerator
-import deepspeed.comm as dist
-from huggingface_hub import snapshot_download
-from transformers.models.bloom.modeling_bloom import BloomBlock as BloomBlock
-from transformers.utils import is_offline_mode
-from transformers import (
-    AutoConfig,
-    AutoModelForCausalLM,
-    LlamaForCausalLM,
-    AutoTokenizer,
-    LlamaTokenizer,
-)
-
-
-# supported models now
-MODEL_CLASSES = {
-    "gpt-j": (AutoModelForCausalLM, AutoTokenizer),
-    "gpt-neox": (AutoModelForCausalLM, AutoTokenizer),
-    "llama": (AutoModelForCausalLM, LlamaTokenizer),
-    "opt": (AutoModelForCausalLM, AutoTokenizer),
-    "auto": (AutoModelForCausalLM, AutoTokenizer),
-}
-
-# the Deepspeed team made these so it's super fast to load (~1 minute), rather than wait 10-20min loading time.
-tp_presharded_models = [
-    "microsoft/bloom-deepspeed-inference-int8",
-    "microsoft/bloom-deepspeed-inference-fp16",
-]
-
-t_start = time.time()
-
-parser = ArgumentParser()
-
-parser.add_argument(
-    "-m",
-    "--model-id",
-    type=str,
-    default="bigscience/bloom",
-    help="the huggingface mdoel id",
-)
-parser.add_argument(
-    "--device",
-    type=str,
-    choices=["cpu"],
-    help="cpu",
-    default="cpu",
-)
-parser.add_argument(
-    "--dtype",
-    type=str,
-    help="float16 or bfloat16 or int8",
-    choices=["int8", "float16", "bfloat16", "float32"],
-    default="float16",
-)
-parser.add_argument(
-    "--local_rank", required=False, type=int, help="used by dist launchers"
-)
-parser.add_argument(
-    "--batch-size", "--batch-size", default=1, type=int, help="batch size"
-)
-parser.add_argument("--num-iter", default=50, type=int, help="num iter")
-parser.add_argument("--num-warmup", default=5, type=int, help="num warmup")
-parser.add_argument(
-    "--benchmark", action="store_true", help="additionally run benchmark"
-)
-parser.add_argument("--greedy", action="store_true")
-parser.add_argument("--profile", action="store_true")
-parser.add_argument("--ki", action="store_true")
-parser.add_argument(
-    "--max-new-tokens", default=32, type=int, help="output max new tokens"
-)
-parser.add_argument("--input-tokens", default="32", type=str)
-parser.add_argument("--prompt", default=None, type=str)
-parser.add_argument("--ipex", action="store_true", help="ipex is not enabled now")
-parser.add_argument(
-    "--ipex-weight-only-quantization",
-    action="store_true",
-    help="use ipex weight-only quantization",
-)
-parser.add_argument("--jit", action="store_true")
-parser.add_argument("--print-memory", action="store_true")
-parser.add_argument("--token-latency", action="store_true")
-parser.add_argument(
-    "--lowp-mode",
-    choices=["BF16","FP32","INT8","FP16"], 
-    default="BF16",
-    type=str,
-    help="low precision mode for weight only quantization"
-)
-args = parser.parse_args()
-
-
-num_tokens = args.max_new_tokens
-# import extension
-if args.ipex:
-    import intel_extension_for_pytorch as ipex
-
-    try:
-        ipex._C.disable_jit_linear_repack()
-    except Exception:
-        pass
-
-
-def get_int_from_env(env_keys, default):
-    """Returns the first positive env value found in the `env_keys` list or the default."""
-    for e in env_keys:
-        val = int(os.environ.get(e, -1))
-        if val >= 0:
-            return val
-    return default
-
-
-local_rank = get_int_from_env(["LOCAL_RANK", "MPI_LOCALRANKID"], "0")
-world_size = get_int_from_env(["WORLD_SIZE", "PMI_SIZE"], "1")
-
-deepspeed.init_distributed(get_accelerator().communication_backend_name())
-
-
-def print_rank0(*msg):
-    if local_rank != 0:
-        return
-    print(*msg)
-
-
-### Model loading and instantiating on GPUs
-def get_repo_root(model_name_or_path):
-    local_prefix = ("/", "./", "../")
-    if model_name_or_path.startswith(local_prefix):
-        return model_name_or_path
-    # checks if online or not
-    if is_offline_mode():
-        print_rank0("Offline mode: forcing local_files_only=True")
-    # download only on first process
-    allow_patterns = ["*.bin", "*.model", "*.json", "*.txt", "*.py", "*LICENSE"]
-    if local_rank == 0:
-        snapshot_download(
-            model_name_or_path,
-            local_files_only=is_offline_mode(),
-            cache_dir=os.getenv("TRANSFORMERS_CACHE", None),
-            allow_patterns=allow_patterns,
-            # ignore_patterns=["*.safetensors"],
-        )
-
-    dist.barrier()
-
-    return snapshot_download(
-        model_name_or_path,
-        local_files_only=is_offline_mode(),
-        cache_dir=os.getenv("TRANSFORMERS_CACHE", None),
-        allow_patterns=allow_patterns,
-        # ignore_patterns=["*.safetensors"],
-    )
-
-
-def get_checkpoint_files(model_name_or_path):
-    cached_repo_dir = get_repo_root(model_name_or_path)
-
-    # extensions: .bin | .pt
-    # creates a list of paths from all downloaded files in cache dir
-    file_list = [
-        str(entry)
-        for entry in Path(cached_repo_dir).rglob("*.[bp][it][n]")
-        if entry.is_file()
-    ]
-    return file_list
-
-
-model_name = args.model_id
-if args.dtype == "float16":
-    load_dtype = torch.half
-    infer_dtype = torch.half
-elif args.dtype == "bfloat16":
-    load_dtype = torch.bfloat16
-    infer_dtype = torch.bfloat16
-elif args.dtype == "int8":
-    load_dtype = torch.half
-    infer_dtype = torch.int8
-elif args.dtype == "float32":
-    load_dtype = torch.float32
-    infer_dtype = torch.float32
-
-tp_presharded_mode = True if model_name in tp_presharded_models else False
-
-# print(get_checkpoint_files(model_name))
-
-print_rank0(f"*** Loading the model {model_name}")
-model_type = next((x for x in MODEL_CLASSES.keys() if x in model_name.lower()), "auto")
-model_class = MODEL_CLASSES[model_type]
-tokenizer = model_class[1].from_pretrained(model_name)
-config = AutoConfig.from_pretrained(model_name, torchscript=args.jit)
-if not hasattr(config, "text_max_length") and args.prompt is None:
-    config.text_max_length = int(args.input_tokens) + int(args.max_new_tokens)
-
-# XXX: can't automatically derive dtype via config's `from_pretrained`
-# dtype = torch.bfloat16 if model_name in ["bigscience/bloom", "bigscience/bigscience-small-testing"] else torch.float16
-
-
-# use one of these args to `init_inference`
-# 1. injection_policy is the slower version, but it's plain pytorch so it'll always work
-# 2. replace_with_kernel_inject is the faster one (fast fused kernels)
-kernel_inject = args.ki
-
-if args.benchmark:
-    get_accelerator().empty_cache()
-    gc.collect()
-    deepspeed.runtime.utils.see_memory_usage("pre-from-pretrained", force=True)
-
-# Construct model with fake meta tensors, later will be replaced during ds-inference ckpt load
-with deepspeed.OnDevice(dtype=load_dtype, device="meta"):
-    # Even inside the meta device context, from_pretrained still loads the
-    # model to cpu instead of meta device. Use from_config instead to solve the issue for big models.
-    # We add the instance type check here since some of the models haven't yet supported from_config.
-    if model_class[0] == AutoModelForCausalLM:
-        model = model_class[0].from_config(config).to(load_dtype)
-    else:
-        model = model_class[0].from_pretrained(
-            model_name, config=config, low_cpu_mem_usage=True, torch_dtype=load_dtype
-        )
-
-if args.benchmark:
-    deepspeed.runtime.utils.see_memory_usage("post-from-pretrained", force=True)
-
-model = model.eval()
-model = model.to(memory_format=torch.channels_last)
-
-if args.benchmark:
-    get_accelerator().empty_cache()
-    gc.collect()
-    deepspeed.runtime.utils.see_memory_usage("post-init-ds-zero-init", force=True)
-
-### Deepspeed-Inference Loading
-
-checkpoints_json = "checkpoints.json"
-
-
-def write_checkpoints_json():
-    checkpoint_files = get_checkpoint_files(model_name)
-    if local_rank == 0:
-        # model.config.model_type.upper()
-        data = {"type": "BLOOM", "checkpoints": checkpoint_files, "version": 1.0}
-        json.dump(data, open(checkpoints_json, "w"))
-
-
-if args.benchmark:
-    get_accelerator().empty_cache()
-    gc.collect()
-    deepspeed.runtime.utils.see_memory_usage("pre-ds-inference-init", force=True)
-
-if kernel_inject:
-    kwargs = dict(replace_with_kernel_inject=True)
-else:
-    kwargs = dict(replace_with_kernel_inject=False)
-
-repo_root = get_repo_root(model_name)
-if tp_presharded_mode:
-    # tp presharded repos come with their own checkpoints config file
-    checkpoints_json = os.path.join(repo_root, "ds_inference_config.json")
-else:
-    # for normal bloom repo we need to write the checkpoints config file
-    write_checkpoints_json()
-    dist.barrier()
-
-model = deepspeed.init_inference(
-    model,
-    mp_size=world_size,
-    base_dir=repo_root,
-    dtype=infer_dtype,
-    checkpoint=checkpoints_json,
-    **kwargs,
-)
-
-if args.benchmark:
-    get_accelerator().empty_cache()
-    gc.collect()
-    deepspeed.runtime.utils.see_memory_usage("post-ds-inference-init", force=True)
-
-
-model = model.module
-
-if args.benchmark:
-    t_ready = time.time()
-
-# to ipex
-if args.ipex:
-    ipex_woq_enabled = args.ipex_weight_only_quantization and args.dtype == "float32"
-    model = ipex._optimize_transformers(
-        model.eval(),
-        dtype=infer_dtype if not ipex_woq_enabled else torch.int8,
-        inplace=True,
-    )
-    if ipex_woq_enabled:
-        from intel_extension_for_pytorch.quantization import convert, prepare
-
-        if args.lowp_mode == "INT8":
-            lowp_mode = ipex.quantization.WoqLowpMode.INT8
-        elif args.lowp_mode == "FP32":
-            lowp_mode = ipex.quantization.WoqLowpMode.NONE
-        elif args.lowp_mode == "FP16":
-            lowp_mode = ipex.quantization.WoqLowpMode.FP16
-        else:
-            lowp_mode = ipex.quantization.WoqLowpMode.BF16
-    
-        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-            lowp_mode=lowp_mode
-        )
-        model = prepare(model.eval(), qconfig, inplace=True, bn_folding=False)
-        with torch.no_grad():
-            model = convert(model.eval(), inplace=True).eval()
-
-### Generate
-
-
-print_rank0(f"*** Starting to generate {num_tokens} tokens with bs={args.batch_size}")
-
-# input tokens
-input_sentences = []
-current_path = pathlib.Path(__file__).parent.resolve()
-with open(str(current_path) + "/prompt.json") as f:
-    prompt_pool = json.load(f)
-if args.prompt is not None:
-    input_sentences.append(args.prompt)
-elif model_type == "auto":
-    raise SystemExit(
-        "[ERROR] model prompt is not supported, please use --prompt for this model: "
-        + args.model_id
-    )
-elif int(args.input_tokens) > 8192:
-    input_sentences.append(prompt_pool[model_type]["8192"] * int(int(args.input_tokens) / 8192))
-elif args.input_tokens in prompt_pool[model_type]:
-    input_sentences.append(prompt_pool[model_type][args.input_tokens])
-else:
-    raise SystemExit("[ERROR] Plese use --prompt if want to use custom input.")
-
-
-if args.batch_size > len(input_sentences):
-    # dynamically extend to support larger bs by repetition
-    input_sentences *= math.ceil(args.batch_size / len(input_sentences))
-num_beams = 1 if args.greedy else 4
-generate_kwargs = dict(max_new_tokens=num_tokens, do_sample=False, num_beams=num_beams)
-if args.token_latency:
-    if not hasattr(model.config, "token_latency"):
-        model.config.token_latency = True
-
-if args.jit:
-    torch._C._jit_set_texpr_fuser_enabled(False)
-    past_key_values = None
-    import re
-
-    if re.search("GPTJ", model.config.architectures[0]):
-        beam_idx_tmp = torch.zeros(
-            (2048, int(args.batch_size * num_beams)), dtype=torch.long
-        ).contiguous()
-        past_key_values = tuple(
-            [
-                (
-                    torch.zeros([1, 1, 1, 1]).contiguous(),
-                    torch.zeros([1, 1, 1, 1]).contiguous(),
-                    beam_idx_tmp,
-                    torch.zeros(1, dtype=torch.long).contiguous(),
-                )
-                for i in range(model.config.n_layer)
-            ]
-        )
-    elif re.search("llama", model.config.architectures[0], re.IGNORECASE):
-        beam_idx_tmp = torch.zeros(
-            (2048, int(args.batch_size * num_beams)), dtype=torch.long
-        ).contiguous()
-        past_key_values = tuple(
-            [
-                (
-                    torch.zeros([1, 32, 1, 128]).contiguous(),
-                    torch.zeros([1, 32, 1, 128]).contiguous(),
-                    beam_idx_tmp,
-                    torch.zeros(1, dtype=torch.long).contiguous(),
-                )
-                for i in range(model.config.num_hidden_layers)
-            ]
-        )
-    elif re.search("gptneox", model.config.architectures[0], re.IGNORECASE):
-        beam_idx_tmp = torch.zeros(
-            (2048, int(args.batch_size * num_beams)), dtype=torch.long
-        ).contiguous()
-        past_key_values = tuple(
-            [
-                (
-                    torch.zeros([1, 1, 1, 1]).contiguous(),
-                    torch.zeros([1, 1, 1, 1]).contiguous(),
-                    beam_idx_tmp,
-                    torch.zeros(1, dtype=torch.long).contiguous(),
-                )
-                for i in range(model.config.num_hidden_layers)
-            ]
-        )
-    elif re.search("opt", model.config.architectures[0], re.IGNORECASE):
-        beam_idx_tmp = torch.zeros(
-            (2048, int(args.batch_size * num_beams)), dtype=torch.long
-        ).contiguous()
-        past_key_values = tuple(
-            [
-                (
-                    torch.zeros([1, 1, 1, 1]).contiguous(),
-                    torch.zeros([1, 1, 1, 1]).contiguous(),
-                    beam_idx_tmp,
-                    torch.zeros(1, dtype=torch.long).contiguous(),
-                )
-                for i in range(model.config.num_hidden_layers)
-            ]
-        )
-    else:
-        print("does not support jit yet on your model, please re-run without jit")
-        exit(0)
-    example_inputs = None
-    input_ids = torch.ones(32).to(torch.long)
-    attention_mask = torch.ones(len(input_ids))
-    position_ids = torch.arange(len(input_ids))
-    if re.search("opt", model.config.architectures[0], re.IGNORECASE):
-        example_inputs = {
-            "input_ids": input_ids.unsqueeze(0),
-            "attention_mask": attention_mask.unsqueeze(0),
-            "past_key_values": past_key_values,
-        }
-    else:
-        example_inputs = {
-            "input_ids": input_ids.unsqueeze(0),
-            "attention_mask": attention_mask.unsqueeze(0),
-            "position_ids": position_ids.unsqueeze(0),
-            "past_key_values": past_key_values,
-        }
-
-    with torch.inference_mode(), torch.no_grad(), torch.autocast(
-        device_type=args.device,
-        enabled=infer_dtype is torch.bfloat16,
-        dtype=infer_dtype if infer_dtype is torch.bfloat16 else None,
-    ):
-        trace_model = torch.jit.trace(
-            model, example_kwarg_inputs=example_inputs, strict=False, check_trace=False
-        )
-        trace_model = torch.jit.freeze(trace_model)
-        setattr(model, "trace_graph", trace_model)
-print_rank0(f"Generate args {generate_kwargs}")
-
-
-inputs = input_sentences[: args.batch_size]
-input_size = tokenizer.batch_encode_plus(inputs, return_tensors="pt").input_ids.size(
-    dim=1
-)
-print("*** Prompt size: ", input_size)
-
-
-def generate():
-    """returns a list of zipped inputs, outputs and number of new tokens"""
-
-    input_tokens = tokenizer.batch_encode_plus(inputs, return_tensors="pt")
-    for t in input_tokens:
-        if torch.is_tensor(input_tokens[t]):
-            input_tokens[t] = input_tokens[t].to(
-                get_accelerator().current_device_name()
-            )
-
-    outputs = model.generate(**input_tokens, **generate_kwargs)
-    gen_ids = outputs[0] if args.token_latency else outputs
-
-    input_tokens_lengths = [x.shape[0] for x in input_tokens.input_ids]
-    output_tokens_lengths = [x.shape[0] for x in gen_ids]
-
-    total_new_tokens = [
-        o - i if model.config.model_type != "t5" else o
-        for i, o in zip(input_tokens_lengths, output_tokens_lengths)
-    ]
-    gen_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
-
-    return zip(inputs, gen_text, total_new_tokens), outputs
-
-
-def trace_handler(prof):
-    print(prof.key_averages().table(
-        sort_by="self_cpu_time_total", row_limit=-1))
-
-# warmup is a must if measuring speed as it's when all the optimizations are performed
-# e.g. on 8x80 a100 the first pass of 100 tokens takes 23sec, and the next one is 4secs
-if not args.benchmark:
-    print_rank0("*** Running generate warmup")
-    generated, _ = generate()
-
-    print_rank0("*** Running generate")
-    t_generate_start = time.time()
-    generated, _ = generate()
-    t_generate_span = time.time() - t_generate_start
-    for i, o, _ in generated:
-        print_rank0(f"{'-'*60}\nin={i}\nout={o}\n")
-
-### Benchmark
-# benchmark it!
-else:
-    get_accelerator().empty_cache()
-    gc.collect()
-    deepspeed.runtime.utils.see_memory_usage("end-of-run", force=True)
-
-    print_rank0("*** Running benchmark")
-    total_time = 0.0
-    cycles = args.num_iter
-    warmup = args.num_warmup
-    total_list = []
-    if args.profile:
-        with torch.profiler.profile(
-            activities=[torch.profiler.ProfilerActivity.CPU],
-            schedule=torch.profiler.schedule(
-                wait=1,
-                warmup=3,
-                active=1),
-            on_trace_ready=trace_handler
-        ) as prof:
-            for i in range(5):
-                gen_ids, outputs = generate()
-                prof.step()
-    # latency
-    for i in range(cycles):
-        t0 = time.time()
-        gen_ids, outputs = generate()
-        t1 = time.time()
-        gen_ids = list(gen_ids)
-        print_rank0(gen_ids[0][1:])
-        print_rank0("Iteration: %d, Time: %.6f sec" % (i, t1 - t0))
-        if i >= warmup:
-            total_time += t1 - t0
-            if args.token_latency:
-                total_list.append(outputs[1])
-
-    latency = total_time / (cycles - warmup)
-    print_rank0("\n", "-" * 10, "Summary:", "-" * 10)
-    print_rank0("Inference latency: %.3f sec." % latency)
-    if args.token_latency:
-        import numpy as np
-        from itertools import chain
-
-        first_latency = np.mean([x[0] for x in total_list])
-        average_2n = list(chain(*[x[1:] for x in total_list]))
-        average_2n.sort()
-        average_2n_latency = np.mean(average_2n)
-        p90_latency = average_2n[int(len(average_2n) * 0.9)]
-        p99_latency = average_2n[int(len(average_2n) * 0.99)]
-        print_rank0("First token average latency: %.3f sec." % first_latency)
-        print_rank0("Average 2... latency: %.3f sec." % average_2n_latency)
-        print_rank0("P90 2... latency: %.3f sec." % p90_latency)
-        print_rank0("P99 2... latency: %.3f sec." % p99_latency)
diff --git a/examples/cpu/inference/python/llm/run_gpt-j_int8.py b/examples/cpu/inference/python/llm/run_gpt-j_int8.py
deleted file mode 100644
index 1aab487e..00000000
--- a/examples/cpu/inference/python/llm/run_gpt-j_int8.py
+++ /dev/null
@@ -1,488 +0,0 @@
-import argparse
-import time
-import json
-import pathlib
-import torch
-from datasets import load_dataset
-from torch.nn.functional import pad
-from torch.utils.data import DataLoader
-import intel_extension_for_pytorch as ipex
-from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
-
-parser = argparse.ArgumentParser("GPT-J generation script (int8 path)", add_help=False)
-parser.add_argument(
-    "-m",
-    "--model-id",
-    type=str,
-    default="EleutherAI/gpt-j-6B",
-    help="the huggingface mdoel id",
-)
-parser.add_argument(
-    "--device",
-    type=str,
-    choices=["cpu"],
-    help="cpu",
-    default="cpu",
-)
-parser.add_argument("--dtype", type=str, default="int8")
-parser.add_argument(
-    "--max-new-tokens", default=32, type=int, help="output max new tokens"
-)
-parser.add_argument("--output-dir", nargs="?", default="./saved_results")
-parser.add_argument("--lambada", action="store_true")
-parser.add_argument(
-    "--ipex-weight-only-quantization",
-    action="store_true",
-    help="use ipex weight-only quantization",
-)
-parser.add_argument("--jit", action="store_true")
-parser.add_argument("--int8", action="store_true")
-parser.add_argument("--ipex-smooth-quant", action="store_true")
-parser.add_argument(
-    "--int8-bf16-mixed",
-    action="store_true",
-    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like SPR)",
-)
-parser.add_argument("--quantized-model-path", default="./saved_results/best_model.pt")
-parser.add_argument("--accuracy-only", action="store_true")
-parser.add_argument("--benchmark", action="store_true")
-parser.add_argument("--input-tokens", default="32", type=str)
-parser.add_argument("--prompt", default=None, type=str)
-parser.add_argument("--num-iter", default=100, type=int, help="num iter")
-parser.add_argument("--num-warmup", default=10, type=int, help="num warmup")
-parser.add_argument("--batch-size", default=1, type=int, help="batch size")
-parser.add_argument("--token-latency", action="store_true")
-parser.add_argument("--greedy", action="store_true")
-parser.add_argument("--profile", action="store_true")
-parser.add_argument(
-    "--lowp-mode",
-    choices=["BF16","FP32","INT8","FP16"], 
-    default="BF16",
-    type=str,
-    help="low precision mode for weight only quantization"
-)
-args = parser.parse_args()
-
-
-# disable
-try:
-    ipex._C.disable_jit_linear_repack()
-except Exception:
-    pass
-
-# beam search = 4
-num_beams = 1 if args.greedy else 4
-generate_kwargs = dict(do_sample=False, temperature=0.9, num_beams=num_beams)
-
-# load model
-config = AutoConfig.from_pretrained(args.model_id, torchscript=args.jit)
-if not hasattr(config, "text_max_length") and args.prompt is None:
-    config.text_max_length = int(args.input_tokens) + int(args.max_new_tokens)
-
-if args.benchmark and args.jit and not args.ipex_weight_only_quantization:
-    try:
-        with ipex._IPEXOnDevice(dtype=torch.float, device="meta"):
-            user_model = AutoModelForCausalLM.from_config(config)
-    except:
-        user_model = AutoModelForCausalLM.from_config(config)
-else:
-    user_model = AutoModelForCausalLM.from_pretrained(
-        args.model_id, torch_dtype=torch.float, config=config, low_cpu_mem_usage=True
-    )
-
-tokenizer = AutoTokenizer.from_pretrained(args.model_id)
-print("Data type of the model:", user_model.dtype)
-
-# to channels last
-user_model = user_model.to(memory_format=torch.channels_last)
-user_model.eval()
-# calling _optimize_transformers for int8 path
-if args.ipex_weight_only_quantization:
-    user_model.config.weight_only_quantization = True
-user_model = ipex._optimize_transformers(
-    user_model.eval(), dtype=torch.int8, inplace=True
-)
-
-beam_idx_tmp = torch.zeros((2048, int(args.batch_size * num_beams)), dtype=torch.long).contiguous()
-global_past_key_value = [(torch.zeros([1,user_model.config.num_attention_heads,1,int(user_model.config.hidden_size/user_model.config.num_attention_heads)]).contiguous(),
-                           torch.zeros([1,user_model.config.num_attention_heads,1,int(user_model.config.hidden_size/user_model.config.num_attention_heads)]).contiguous(), beam_idx_tmp, torch.zeros(1, dtype=torch.long).contiguous()) for i in range(user_model.config.num_hidden_layers)]
-                           
-# amp autocast
-if args.int8_bf16_mixed:
-    amp_enabled = True
-    amp_dtype = torch.bfloat16
-else:
-    amp_enabled = False
-    amp_dtype = torch.float32
-
-if args.lambada:
-
-    class Evaluator:
-        def __init__(
-            self, dataset, tokenizer, args, batch_size=8, pad_val=1, pad_max=196
-        ):
-            self.dataset = dataset
-            self.tokenizer = tokenizer
-            self.batch_size = batch_size
-            self.pad_val = pad_val
-            self.pad_max = pad_max
-            self.args = args
-
-            # tokenize the dataset
-            self.dataset = self.dataset.map(self.tokenize_function, batched=True)
-            self.dataset.set_format(type="torch", columns=["input_ids"])
-
-        @torch.no_grad()
-        def tokenize_function(self, examples):
-            example = self.tokenizer(examples["text"])
-            return example
-
-        @torch.no_grad()
-        def collate_batch(self, batch):
-            input_ids_padded = []
-            last_ind = []
-            past_key_values = []
-            attention_mask_padded = []
-            position_ids_padded = []
-
-            for text in batch:
-                input_ids = text["input_ids"]
-                pad_len = self.pad_max - input_ids.shape[0]
-                last_ind.append(input_ids.shape[0] - 1)
-
-                attention_mask = torch.ones(len(input_ids))
-
-                position_ids = torch.arange(len(input_ids))
-
-                input_ids = pad(input_ids, (0, pad_len), value=self.pad_val)
-                input_ids_padded.append(input_ids)
-                attention_mask = pad(attention_mask, (0, pad_len), value=0)
-                attention_mask_padded.append(attention_mask)
-                position_ids = pad(position_ids, (0, pad_len), value=self.pad_val)
-
-                position_ids_padded.append(position_ids)
-                # dummy past key value
-                beam_idx_tmp = torch.zeros(
-                    (2048, int(self.args.batch_size * num_beams)), dtype=torch.long
-                ).contiguous()
-                past_key_value = [
-                    (
-                        torch.zeros([1, 16, 1, 256]).contiguous(),
-                        torch.zeros([1, 16, 1, 256]).contiguous(),
-                        beam_idx_tmp,
-                        torch.zeros(1, dtype=torch.long).contiguous(),
-                    )
-                    for i in range(28)
-                ]
-
-            return (
-                (
-                    torch.vstack(input_ids_padded),
-                    torch.vstack(attention_mask_padded),
-                    torch.vstack(position_ids_padded),
-                    tuple(past_key_value),
-                ),
-                torch.tensor(last_ind),
-            )
-
-        @torch.no_grad()
-        def evaluate(self, model):
-            model.eval()
-            # The task is to predict the last word of the input.
-            total, hit = 0, 0
-            latency = 0
-            test_dataloader = DataLoader(
-                self.dataset,
-                batch_size=self.batch_size,
-                shuffle=False,
-                collate_fn=self.collate_batch,
-            )
-            for i, (
-                (input_ids, attention_mask, position_ids, past_key_values),
-                last_ind,
-            ) in enumerate(test_dataloader):
-                label = input_ids[torch.arange(len(last_ind)), last_ind]
-                input_ids[torch.arange(len(last_ind)), last_ind] = self.pad_val
-                pad_len = self.pad_max - last_ind - 1
-                start = time.time()
-                outputs = model(
-                    input_ids,
-                    past_key_values=past_key_values,
-                    attention_mask=attention_mask,
-                    position_ids=position_ids,
-                )
-                latency += time.time() - start
-                last_token_logits = outputs[0][
-                    torch.arange(len(last_ind)), -2 - pad_len, :
-                ]
-                pred = last_token_logits.argmax(dim=-1)
-                total += label.size(0)
-                hit += (pred == label).sum().item()
-                if i % 50 == 0:
-                    print(hit / total, flush=True)
-                    print("Processed minibatch:", i, flush=True)
-
-            acc = hit / total
-            print(acc)
-            lantecy = latency / len(self.dataset)
-            return acc, lantecy
-
-    full_dataset = load_dataset("lambada")
-    dataset = full_dataset["validation"]
-    calib_dataset = full_dataset["train"]
-    evaluator = Evaluator(dataset, tokenizer, args, batch_size=args.batch_size)
-    calib_evaluator = Evaluator(
-        calib_dataset, tokenizer, args, batch_size=args.batch_size
-    )
-
-    calib_dataloader = DataLoader(
-        calib_evaluator.dataset,
-        batch_size=args.batch_size,
-        shuffle=False,
-        collate_fn=evaluator.collate_batch,
-    )
-
-
-if args.jit and args.benchmark:
-    torch._C._jit_set_texpr_fuser_enabled(False)
-    if args.benchmark and (args.int8 or args.int8_bf16_mixed):
-        if not hasattr(user_model, "trace_graph"):
-            print("load_int8_model")
-            self_jit = torch.jit.load(args.quantized_model_path)
-            self_jit = torch.jit.freeze(self_jit.eval())
-            setattr(user_model, "trace_graph", self_jit)
-
-
-def calib_func(prepared_model):
-    for i, (
-        (input_ids, attention_mask, position_ids, past_key_values),
-        last_ind,
-    ) in enumerate(calib_dataloader):
-        if i == 100:
-            break
-        prepared_model(
-            input_ids=input_ids,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_values=past_key_values,
-        )
-
-
-def eval_func(traced_model):
-    acc, latency = evaluator.evaluate(traced_model)
-    print("Accuracy:", acc)
-    print("Latency (sec):", latency)
-    return acc
-
-
-if args.ipex_smooth_quant:
-    from neural_compressor import PostTrainingQuantConfig, quantization
-
-    op_type_dict = {
-        "add": {"weight": {"dtype": ["fp32"]}, "activation": {"dtype": ["fp32"]}},
-        "linear": {
-            "weight": {
-                "dtype": ["int8"],
-                "scheme": ["sym"],
-                "granularity": ["per_channel"],
-                "algorithm": ["minmax"],
-            },
-            "activation": {
-                "dtype": ["uint8"],
-                "scheme": ["asym"],
-                "granularity": ["per_tensor"],
-                "algorithm": ["kl"],
-            },
-        },
-    }
-
-    excluded_precisions = [] if args.int8_bf16_mixed else ["bf16"]
-    recipes = {"smooth_quant": True, "smooth_quant_args": {"alpha": "auto"}}
-
-    recipes["smooth_quant_args"]["folding"] = True
-
-    print("smooth_quant_args:", recipes)
-    conf = PostTrainingQuantConfig(
-        backend="ipex",
-        excluded_precisions=excluded_precisions,
-        op_type_dict=op_type_dict,
-        recipes=recipes,
-    )
-
-    q_model = quantization.fit(
-        user_model,
-        conf,
-        calib_dataloader=calib_dataloader,
-        calib_func=calib_func,
-    )
-
-    q_model.save(args.output_dir)
-
-if args.ipex_weight_only_quantization:
-
-    def convert_woq(m, qconfig, inplace=True):
-        import copy
-
-        def _convert(m):
-            from intel_extension_for_pytorch.nn.modules import IpexWoqLinear
-
-            if isinstance(m, torch.nn.Linear):
-                m.qconfig = qconfig.global_qconfig
-                m_new = IpexWoqLinear.from_float(m)
-                return m_new
-            m_new = m
-
-            for name, child in m.named_children():
-                setattr(m_new, name, _convert(child))
-            return m_new
-
-        if not inplace:
-            m_new = copy.deepcopy(m)
-        else:
-            m_new = m
-        return _convert(m_new)
-
-    example_inputs = None
-    input_ids = torch.ones(32).to(torch.long)
-    attention_mask = torch.ones(len(input_ids))
-    position_ids = torch.arange(len(input_ids))
-    example_inputs = (
-        input_ids.unsqueeze(0),
-        attention_mask.unsqueeze(0),
-        position_ids.unsqueeze(0),
-        tuple(global_past_key_value),
-    )
-
-    from intel_extension_for_pytorch.quantization import prepare, convert
-    
-    if args.lowp_mode == "INT8":
-        lowp_mode = ipex.quantization.WoqLowpMode.INT8
-    elif args.lowp_mode == "FP32":
-        lowp_mode = ipex.quantization.WoqLowpMode.NONE
-    elif args.lowp_mode == "FP16":
-        lowp_mode = ipex.quantization.WoqLowpMode.FP16
-    else:
-        lowp_mode = ipex.quantization.WoqLowpMode.BF16
-
-    qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-        lowp_mode=lowp_mode
-    )
-    with torch.no_grad():
-        convert_model = convert_woq(user_model.eval(), qconfig)
-    with torch.no_grad(), torch.autocast(
-        device_type=args.device,
-        enabled=amp_enabled,
-        dtype=amp_dtype if amp_enabled else None,
-    ):
-        if amp_enabled:
-            convert_model = ipex.optimize(convert_model, dtype=torch.bfloat16, inplace=True, concat_linear=False)
-        self_jit = torch.jit.trace(convert_model.eval(), example_inputs, strict=False)
-        self_jit = torch.jit.freeze(self_jit.eval())
-        self_jit.save(args.output_dir + "/best_model.pt")
-
-if args.accuracy_only:
-    if args.int8 or args.int8_bf16_mixed:
-        user_model = torch.jit.load(args.quantized_model_path)
-        user_model = torch.jit.freeze(user_model.eval())
-
-    with torch.autocast(
-        device_type=args.device,
-        enabled=amp_enabled,
-        dtype=amp_dtype if amp_enabled else None,
-    ):
-        eval_func(user_model)
-
-if args.benchmark:
-    # input prompt
-    current_path = pathlib.Path(__file__).parent.resolve()
-    with open(str(current_path) + "/prompt.json") as f:
-        prompt_pool = json.load(f)
-    if args.prompt is not None:
-        prompt = args.prompt
-    elif int(args.input_tokens) > 8192:
-        prompt = prompt_pool["gpt-j"]["8192"] * int(int(args.input_tokens) / 8192)
-    elif args.input_tokens in prompt_pool["gpt-j"]:
-        prompt = prompt_pool["gpt-j"][args.input_tokens]
-    else:
-        raise SystemExit("[ERROR] Plese use --prompt if want to use custom input.")
-
-    input_size = tokenizer(prompt, return_tensors="pt").input_ids.size(dim=1)
-    print("---- Prompt size:", input_size)
-
-    if args.token_latency:
-        if not hasattr(user_model.config, "token_latency"):
-            user_model.config.token_latency = True
-
-    # start
-    total_time = 0.0
-    num_iter = args.num_iter
-    num_warmup = args.num_warmup
-    prompt = [prompt] * args.batch_size
-    total_list = []
-    with torch.inference_mode(), torch.no_grad(), torch.autocast(
-        device_type=args.device,
-        enabled=amp_enabled,
-        dtype=torch.bfloat16 if amp_enabled else None,
-    ):
-        for i in range(num_iter):
-            tic = time.time()
-            input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(args.device)
-            output = user_model.generate(
-                input_ids, max_new_tokens=args.max_new_tokens, **generate_kwargs
-            )
-            gen_ids = output[0] if args.token_latency else output
-            gen_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
-            toc = time.time()
-            input_tokens_lengths = [x.shape[0] for x in input_ids]
-            output_tokens_lengths = [x.shape[0] for x in gen_ids]
-            total_new_tokens = [
-                o - i if user_model.config.model_type != "t5" else o
-                for i, o in zip(input_tokens_lengths, output_tokens_lengths)
-            ]
-            print(gen_text, total_new_tokens, flush=True)
-            print("Iteration: %d, Time: %.6f sec" % (i, toc - tic), flush=True)
-            # if user_model.config.model_type != 't5':
-            #     assert total_new_tokens[0] == args.max_new_tokens, "Generated new tokens != max new tokens"
-            if i >= num_warmup:
-                total_time += toc - tic
-                if args.token_latency:
-                    total_list.append(output[1])
-
-    if args.profile:
-        def trace_handler(prof):
-            print(prof.key_averages().table(
-                sort_by="self_cpu_time_total", row_limit=-1))
-        with torch.profiler.profile(
-            activities=[torch.profiler.ProfilerActivity.CPU],
-            schedule=torch.profiler.schedule(
-                wait=1,
-                warmup=3,
-                active=1),
-            on_trace_ready=trace_handler
-        ) as prof:
-            for i in range(5):
-                input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(args.device)
-                output = user_model.generate(
-                    input_ids, max_new_tokens=args.max_new_tokens, **generate_kwargs
-                )
-                gen_ids = output[0] if args.token_latency else output
-                gen_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
-                prof.step()
-
-    print("\n", "-" * 10, "Summary:", "-" * 10)
-    latency = total_time / (num_iter - num_warmup)
-    print("Inference latency: %.3f sec." % latency)
-    if args.token_latency:
-        import numpy as np
-        from itertools import chain
-
-        first_latency = np.mean([x[0] for x in total_list])
-        average_2n = list(chain(*[x[1:] for x in total_list]))
-        average_2n.sort()
-        average_2n_latency = np.mean(average_2n)
-        p90_latency = average_2n[int(len(average_2n) * 0.9)]
-        p99_latency = average_2n[int(len(average_2n) * 0.99)]
-        print("First token average latency: %.3f sec." % first_latency)
-        print("Average 2... latency: %.3f sec." % average_2n_latency)
-        print("P90 2... latency: %.3f sec." % p90_latency)
-        print("P99 2... latency: %.3f sec." % p99_latency)
diff --git a/examples/cpu/inference/python/llm/run_gpt-neox_int8.py b/examples/cpu/inference/python/llm/run_gpt-neox_int8.py
deleted file mode 100644
index 11d4c68d..00000000
--- a/examples/cpu/inference/python/llm/run_gpt-neox_int8.py
+++ /dev/null
@@ -1,507 +0,0 @@
-import os
-import psutil
-import argparse
-import time
-import json
-from pathlib import Path
-import pathlib
-
-from datasets import load_dataset
-from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
-import torch
-from torch.nn.functional import pad
-from torch.utils.data import DataLoader
-
-import intel_extension_for_pytorch as ipex
-from intel_extension_for_pytorch.quantization import convert, prepare
-
-
-parser = argparse.ArgumentParser(
-    "GPT-neox generation script (int8 path)", add_help=False
-)
-parser.add_argument(
-    "-m",
-    "--model-id",
-    default=None,
-    type=str,
-    required=True,
-    help="your GPT-neox model",
-)
-parser.add_argument(
-    "--device",
-    type=str,
-    choices=["cpu"],
-    help="cpu",
-    default="cpu",
-)
-parser.add_argument("--dtype", type=str, default="int8")
-parser.add_argument(
-    "--max-new-tokens", default=32, type=int, help="output max new tokens"
-)
-parser.add_argument("--dataset", nargs="?", default="lambada", const="lambada")
-parser.add_argument("--split", nargs="?", default="validation", const="validation")
-parser.add_argument("--output-dir", nargs="?", default="./saved_results")
-parser.add_argument(
-    "--ipex-weight-only-quantization",
-    action="store_true",
-    help="use ipex weight-only quantization",
-)
-parser.add_argument("--jit", action="store_true")
-parser.add_argument("--int8", action="store_true")
-parser.add_argument(
-    "--int8-bf16-mixed",
-    action="store_true",
-    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like SPR)",
-)
-parser.add_argument("--quantized-model-path", default="./saved_results/best_model.pt")
-parser.add_argument("--ipex-smooth-quant", action="store_true")
-parser.add_argument("--lambada", action="store_true")
-parser.add_argument("--accuracy-only", action="store_true")
-parser.add_argument("--benchmark", action="store_true")
-parser.add_argument("--input-tokens", default="32", type=str)
-parser.add_argument("--prompt", default=None, type=str)
-parser.add_argument("--num-iter", default=100, type=int, help="num iter")
-parser.add_argument("--num-warmup", default=10, type=int, help="num warmup")
-parser.add_argument("--batch-size", default=1, type=int, help="batch size")
-parser.add_argument("--token-latency", action="store_true")
-parser.add_argument("--greedy", action="store_true")
-parser.add_argument("--profile", action="store_true")
-parser.add_argument(
-    "--lowp-mode",
-    choices=["BF16","FP32","INT8","FP16"], 
-    default="BF16",
-    type=str,
-    help="low precision mode for weight only quantization"
-)
-args = parser.parse_args()
-
-
-# disable
-try:
-    ipex._C.disable_jit_linear_repack()
-except Exception:
-    pass
-
-device = torch.device(args.device)
-args.dtype = "int8" if args.int8 or args.int8_bf16_mixed else args.dtype
-
-# amp autocast
-if args.int8_bf16_mixed:
-    amp_enabled = True
-    amp_dtype = torch.bfloat16
-else:
-    amp_enabled = False
-    amp_dtype = torch.float32
-
-num_beams = 1 if args.greedy else 4
-generate_kwargs = dict(do_sample=False, temperature=0.9, num_beams=num_beams)
-
-# load model
-config = AutoConfig.from_pretrained(args.model_id, torchscript=args.jit)
-if not hasattr(config, "text_max_length") and args.prompt is None:
-    config.text_max_length = int(args.input_tokens) + int(args.max_new_tokens)
-if args.benchmark and args.jit and not args.ipex_weight_only_quantization:
-    try:
-        with ipex._IPEXOnDevice(dtype=torch.float, device="meta"):
-            user_model = AutoModelForCausalLM.from_config(config)
-    except:
-        user_model = AutoModelForCausalLM.from_config(config)
-else:
-    user_model = AutoModelForCausalLM.from_pretrained(
-        args.model_id, torch_dtype=torch.float, config=config, low_cpu_mem_usage=True
-    )
-
-tokenizer = AutoTokenizer.from_pretrained(args.model_id)
-print("Data type of the model:", user_model.dtype)
-# calling _optimize_transformers for int8 path
-user_model = ipex._optimize_transformers(
-    user_model.eval(), dtype=torch.int8, inplace=True
-)
-# dummy past key value
-beam_idx_tmp = torch.zeros(
-    (2048, int(args.batch_size * num_beams)), dtype=torch.long
-).contiguous()
-global_past_key_value = [
-    (
-        torch.zeros(
-            [
-                1,
-                user_model.config.num_attention_heads,
-                1,
-                int(
-                    user_model.config.hidden_size
-                    / user_model.config.num_attention_heads
-                ),
-            ]
-        ).contiguous(),
-        torch.zeros(
-            [
-                1,
-                user_model.config.num_attention_heads,
-                1,
-                int(
-                    user_model.config.hidden_size
-                    / user_model.config.num_attention_heads
-                ),
-            ]
-        ).contiguous(),
-        beam_idx_tmp,
-        torch.zeros(1, dtype=torch.long).contiguous(),
-    )
-    for i in range(user_model.config.num_hidden_layers)
-]
-
-
-class Evaluator:
-    def __init__(self, dataset, tokenizer, batch_size=8, pad_val=1, pad_max=196):
-        self.dataset = dataset
-        self.tokenizer = tokenizer
-        self.batch_size = batch_size
-        self.pad_val = pad_val
-        self.pad_max = pad_max
-
-        # tokenize the dataset
-        self.dataset = self.dataset.map(self.tokenize_function, batched=True)
-        self.dataset.set_format(type="torch", columns=["input_ids"])
-
-    @torch.no_grad()
-    def tokenize_function(self, examples):
-        example = self.tokenizer(examples["text"])
-        return example
-
-    @torch.no_grad()
-    def collate_batch(self, batch):
-        position_ids_padded = []
-        input_ids_padded = []
-        last_ind = []
-        attention_mask_padded = []
-        for text in batch:
-            # we cut the sentence if it exceeds pad_max, we are using tuned max 196 from gptj model; TODO: tune best pad_max
-            input_ids = (
-                text["input_ids"]
-                if text["input_ids"].shape[0] <= self.pad_max
-                else text["input_ids"][0 : int(self.pad_max - 1)]
-            )
-            pad_len = self.pad_max - input_ids.shape[0]
-            last_ind.append(input_ids.shape[0] - 1)
-            attention_mask = torch.ones(len(input_ids))
-            position_ids = torch.arange(len(input_ids))
-            input_ids = pad(input_ids, (0, pad_len), value=self.pad_val)
-            input_ids_padded.append(input_ids)
-            attention_mask = pad(attention_mask, (0, pad_len), value=0)
-            attention_mask_padded.append(attention_mask)
-            position_ids = pad(position_ids, (0, pad_len), value=self.pad_val)
-            position_ids_padded.append(position_ids)
-        return (
-            (
-                torch.vstack(input_ids_padded),
-                torch.vstack(attention_mask_padded),
-                torch.vstack(position_ids_padded),
-                tuple(global_past_key_value),
-            ),
-            torch.tensor(last_ind),
-        )
-
-    @torch.no_grad()
-    def evaluate(self, model):
-        model.eval()
-        # The task is to predict the last word of the input.
-        total, hit = 0, 0
-        latency = 0
-        test_dataloader = DataLoader(
-            self.dataset,
-            batch_size=self.batch_size,
-            shuffle=False,
-            collate_fn=self.collate_batch,
-        )
-
-        for i, (
-            (input_ids, attention_mask, position_ids, past_key_values),
-            last_ind,
-        ) in enumerate(test_dataloader):
-            label = input_ids[torch.arange(len(last_ind)), last_ind]
-            input_ids[torch.arange(len(last_ind)), last_ind] = self.pad_val
-            pad_len = self.pad_max - last_ind - 1
-            start = time.time()
-
-            outputs = model(
-                input_ids,
-                attention_mask=attention_mask,
-                position_ids=position_ids,
-                past_key_values=past_key_values,
-            )
-
-            latency += time.time() - start
-
-            last_token_logits = outputs[0][torch.arange(len(last_ind)), -2 - pad_len, :]
-
-            pred = last_token_logits.argmax(dim=-1)
-            total += label.size(0)
-            hit += (pred == label).sum().item()
-            if i % 50 == 0:
-                print(hit / total, flush=True)
-                print("Processed minibatch:", i, flush=True)
-
-        acc = hit / total
-        print(acc)
-        lantecy = latency / len(self.dataset)
-        return acc, lantecy
-
-
-if args.lambada:
-    full_dataset = load_dataset(args.dataset)
-    dataset = full_dataset["validation"]
-    calib_dataset = full_dataset["train"]
-
-    user_model.eval()
-    evaluator = Evaluator(dataset, tokenizer, args.batch_size)
-    calib_evaluator = Evaluator(calib_dataset, tokenizer, args.batch_size)
-
-    calib_dataloader = DataLoader(
-        calib_evaluator.dataset,
-        batch_size=args.batch_size,
-        shuffle=False,
-        collate_fn=evaluator.collate_batch,
-    )
-
-    test_dataloader = DataLoader(
-        evaluator.dataset,
-        batch_size=args.batch_size,
-        shuffle=False,
-        collate_fn=evaluator.collate_batch,
-    )
-
-
-def calib_func(prepared_model):
-    for i, (
-        (input_ids, attention_mask, position_ids, past_key_values),
-        last_ind,
-    ) in enumerate(calib_dataloader):
-        if i == 8:
-            break
-        prepared_model(
-            input_ids,
-            position_ids=position_ids,
-            attention_mask=attention_mask,
-            past_key_values=past_key_values,
-        )
-
-
-def eval_func(traced_model):
-    acc, latency = evaluator.evaluate(traced_model)
-    print("Accuracy:", acc)
-    print("Latency (sec):", latency)
-    return acc
-
-
-if args.jit and args.benchmark:
-    torch._C._jit_set_texpr_fuser_enabled(False)
-    if args.benchmark and (args.int8 or args.int8_bf16_mixed):
-        if not hasattr(user_model, "trace_graph"):
-            print("load_int8_model")
-            self_jit = torch.jit.load(args.quantized_model_path)
-            self_jit = torch.jit.freeze(self_jit.eval())
-            setattr(user_model, "trace_graph", self_jit)
-
-if args.ipex_smooth_quant:
-    from neural_compressor import PostTrainingQuantConfig, quantization
-
-    op_type_dict = {
-        "add": {"weight": {"dtype": ["fp32"]}, "activation": {"dtype": ["fp32"]}},
-        "linear": {
-            "weight": {
-                "dtype": ["int8"],
-                "scheme": ["sym"],
-                "granularity": ["per_channel"],
-                "algorithm": ["minmax"],
-            },
-            "activation": {
-                "dtype": ["uint8"],
-                "scheme": ["asym"],
-                "granularity": ["per_tensor"],
-                "algorithm": ["kl"],
-            },
-        },
-    }
-
-    excluded_precisions = [] if args.int8_bf16_mixed else ["bf16"]
-    recipes = {"smooth_quant": True, "smooth_quant_args": {"alpha": "auto"}}
-
-    recipes["smooth_quant_args"]["folding"] = True
-
-    print("smooth_quant_args:", recipes)
-    conf = PostTrainingQuantConfig(
-        backend="ipex",
-        excluded_precisions=excluded_precisions,
-        op_type_dict=op_type_dict,
-        recipes=recipes,
-    )
-
-    q_model = quantization.fit(
-        user_model,
-        conf,
-        calib_dataloader=calib_dataloader,
-        calib_func=calib_func,
-    )
-
-    q_model.save(args.output_dir)
-
-if args.ipex_weight_only_quantization:
-
-    def convert_woq(m, qconfig, inplace=True):
-        import copy
-
-        def _convert(m):
-            from intel_extension_for_pytorch.nn.modules import IpexWoqLinear
-
-            if isinstance(m, torch.nn.Linear):
-                m.qconfig = qconfig.global_qconfig
-                m_new = IpexWoqLinear.from_float(m)
-                return m_new
-            m_new = m
-
-            for name, child in m.named_children():
-                setattr(m_new, name, _convert(child))
-            return m_new
-
-        if not inplace:
-            m_new = copy.deepcopy(m)
-        else:
-            m_new = m
-        return _convert(m_new)
-
-    example_inputs = None
-    input_ids = torch.ones(32).to(torch.long)
-    attention_mask = torch.ones(len(input_ids))
-    position_ids = torch.arange(len(input_ids))
-    example_inputs = (
-        input_ids.unsqueeze(0),
-        attention_mask.unsqueeze(0),
-        position_ids.unsqueeze(0),
-        tuple(global_past_key_value),
-    )
-
-    from intel_extension_for_pytorch.quantization import prepare, convert
-
-    if args.lowp_mode == "INT8":
-        lowp_mode = ipex.quantization.WoqLowpMode.INT8
-    elif args.lowp_mode == "FP32":
-        lowp_mode = ipex.quantization.WoqLowpMode.NONE
-    elif args.lowp_mode == "FP16":
-        lowp_mode = ipex.quantization.WoqLowpMode.FP16
-    else:
-        lowp_mode = ipex.quantization.WoqLowpMode.BF16  
-      
-    qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-        lowp_mode=lowp_mode
-    )
-    with torch.no_grad():
-        convert_model = convert_woq(user_model.eval(), qconfig)
-        self_jit = torch.jit.trace(convert_model.eval(), example_inputs, strict=False)
-        self_jit = torch.jit.freeze(self_jit.eval())
-        self_jit.save(args.output_dir + "/best_model.pt")
-
-
-if args.accuracy_only:
-    if args.int8 or args.int8_bf16_mixed:
-        user_model = torch.jit.load(args.quantized_model_path)
-        user_model = torch.jit.freeze(user_model.eval())
-
-    with torch.autocast(
-        device_type=args.device,
-        enabled=amp_enabled,
-        dtype=amp_dtype if amp_enabled else None,
-    ):
-        eval_func(user_model)
-
-if args.benchmark:
-    # input prompt
-    current_path = pathlib.Path(__file__).parent.resolve()
-    with open(str(current_path) + "/prompt.json") as f:
-        prompt_pool = json.load(f)
-    if args.prompt is not None:
-        prompt = args.prompt
-    elif int(args.input_tokens) > 8192:
-        prompt = prompt_pool["gpt-neox"]["8192"] * int(int(args.input_tokens) / 8192)
-    elif args.input_tokens in prompt_pool["gpt-neox"]:
-        prompt = prompt_pool["gpt-neox"][args.input_tokens]
-    else:
-        raise SystemExit("[ERROR] Plese use --prompt if want to use custom input.")
-
-    input_size = tokenizer(prompt, return_tensors="pt").input_ids.size(dim=1)
-    print("---- Prompt size:", input_size)
-    if args.token_latency:
-        if not hasattr(user_model.config, "token_latency"):
-            user_model.config.token_latency = True
-
-    total_time = 0.0
-    num_iter = args.num_iter
-    num_warmup = args.num_warmup
-    prompt = [prompt] * args.batch_size
-    total_list = []
-    with torch.inference_mode(), torch.no_grad(), torch.autocast(
-        device_type=args.device,
-        enabled=amp_enabled,
-        dtype=torch.bfloat16 if amp_enabled else None,
-    ):
-        for i in range(num_iter):
-            tic = time.time()
-            input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
-            output = user_model.generate(
-                input_ids, max_new_tokens=args.max_new_tokens, **generate_kwargs
-            )
-            gen_ids = output[0] if args.token_latency else output
-            gen_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
-            toc = time.time()
-            input_tokens_lengths = [x.shape[0] for x in input_ids]
-            output_tokens_lengths = [x.shape[0] for x in gen_ids]
-            total_new_tokens = [
-                o - i if user_model.config.model_type != "t5" else o
-                for i, o in zip(input_tokens_lengths, output_tokens_lengths)
-            ]
-            print(gen_text, total_new_tokens, flush=True)
-            print("Iteration: %d, Time: %.6f sec" % (i, toc - tic), flush=True)
-            if i >= num_warmup:
-                total_time += toc - tic
-                if args.token_latency:
-                    total_list.append(output[1])
-
-    if args.profile:
-        def trace_handler(prof):
-            print(prof.key_averages().table(
-                sort_by="self_cpu_time_total", row_limit=-1))
-        with torch.profiler.profile(
-            activities=[torch.profiler.ProfilerActivity.CPU],
-            schedule=torch.profiler.schedule(
-                wait=1,
-                warmup=3,
-                active=1),
-            on_trace_ready=trace_handler
-        ) as prof:
-            for i in range(5):
-                input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
-                output = user_model.generate(
-                    input_ids, max_new_tokens=args.max_new_tokens, **generate_kwargs
-                )
-                gen_ids = output[0] if args.token_latency else output
-                gen_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
-                prof.step()
-
-    print("\n", "-" * 10, "Summary:", "-" * 10)
-    latency = total_time / (num_iter - num_warmup)
-    print("Inference latency: %.3f sec." % latency)
-    if args.token_latency:
-        import numpy as np
-        from itertools import chain
-
-        first_latency = np.mean([x[0] for x in total_list])
-        average_2n = list(chain(*[x[1:] for x in total_list]))
-        average_2n.sort()
-        average_2n_latency = np.mean(average_2n)
-        p90_latency = average_2n[int(len(average_2n) * 0.9)]
-        p99_latency = average_2n[int(len(average_2n) * 0.99)]
-        print("First token average latency: %.3f sec." % first_latency)
-        print("Average 2... latency: %.3f sec." % average_2n_latency)
-        print("P90 2... latency: %.3f sec." % p90_latency)
-        print("P99 2... latency: %.3f sec." % p99_latency)
-
diff --git a/examples/cpu/inference/python/llm/run_llama_int8.py b/examples/cpu/inference/python/llm/run_llama_int8.py
deleted file mode 100644
index ed2b2438..00000000
--- a/examples/cpu/inference/python/llm/run_llama_int8.py
+++ /dev/null
@@ -1,501 +0,0 @@
-import os
-import psutil
-import argparse
-import time
-import json
-from pathlib import Path
-import pathlib
-
-from datasets import load_dataset
-from transformers import LlamaForCausalLM, LlamaTokenizer, AutoConfig
-
-import torch
-from torch.nn.functional import pad
-from torch.utils.data import DataLoader
-
-import intel_extension_for_pytorch as ipex
-from intel_extension_for_pytorch.quantization import convert, prepare
-
-
-parser = argparse.ArgumentParser("LLaMA generation script (int8 path)", add_help=False)
-parser.add_argument(
-    "-m", "--model-id", default=None, type=str, required=True, help="your llama model"
-)
-parser.add_argument(
-    "--device",
-    type=str,
-    choices=["cpu"],
-    help="cpu",
-    default="cpu",
-)
-parser.add_argument("--dtype", type=str, default="int8")
-parser.add_argument(
-    "--max-new-tokens", default=32, type=int, help="output max new tokens"
-)
-parser.add_argument("--dataset", nargs="?", default="lambada", const="lambada")
-parser.add_argument("--split", nargs="?", default="validation", const="validation")
-parser.add_argument("--output-dir", nargs="?", default="./saved_results")
-parser.add_argument("--ipex-smooth-quant", action="store_true")
-parser.add_argument(
-    "--ipex-weight-only-quantization",
-    action="store_true",
-    help="use ipex weight-only quantization",
-)
-parser.add_argument("--jit", action="store_true")
-parser.add_argument("--int8", action="store_true")
-parser.add_argument(
-    "--int8-bf16-mixed",
-    action="store_true",
-    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like SPR)",
-)
-parser.add_argument("--quantized-model-path", default="./saved_results/best_model.pt")
-parser.add_argument("--lambada", action="store_true")
-parser.add_argument("--accuracy-only", action="store_true")
-parser.add_argument("--benchmark", action="store_true")
-parser.add_argument("--input-tokens", default="32", type=str)
-parser.add_argument("--prompt", default=None, type=str)
-parser.add_argument("--num-iter", default=100, type=int, help="num iter")
-parser.add_argument("--num-warmup", default=10, type=int, help="num warmup")
-parser.add_argument("--batch-size", default=1, type=int, help="batch size")
-parser.add_argument("--token-latency", action="store_true")
-parser.add_argument("--greedy", action="store_true")
-parser.add_argument("--profile", action="store_true")
-parser.add_argument(
-    "--lowp-mode",
-    choices=["BF16","FP32","INT8","FP16"], 
-    default="BF16",
-    type=str,
-    help="low precision mode for weight only quantization"
-)
-args = parser.parse_args()
-
-
-# disable
-try:
-    ipex._C.disable_jit_linear_repack()
-except Exception:
-    pass
-
-device = torch.device(args.device)
-args.dtype = "int8" if args.int8 or args.int8_bf16_mixed else args.dtype
-
-# amp autocast
-if args.int8_bf16_mixed:
-    amp_enabled = True
-    amp_dtype = torch.bfloat16
-else:
-    amp_enabled = False
-    amp_dtype = torch.float32
-
-
-num_beams = 1 if args.greedy else 4
-generate_kwargs = dict(do_sample=False, temperature=0.9, num_beams=num_beams)
-
-
-# load model
-config = AutoConfig.from_pretrained(args.model_id, torchscript=args.jit)
-if not hasattr(config, "text_max_length") and args.prompt is None:
-    config.text_max_length = int(args.input_tokens) + int(args.max_new_tokens)
-
-if args.benchmark and args.jit and not args.ipex_weight_only_quantization:
-    try:
-        with ipex._IPEXOnDevice(dtype=torch.float, device="meta"):
-            user_model = LlamaForCausalLM._from_config(config)
-    except:
-        user_model = LlamaForCausalLM.from_pretrained(
-            args.model_id, config=config, low_cpu_mem_usage=True, torch_dtype=torch.half
-        )
-else:
-    user_model = LlamaForCausalLM.from_pretrained(
-        args.model_id, config=config, low_cpu_mem_usage=True, torch_dtype=torch.float
-    )
-
-tokenizer = LlamaTokenizer.from_pretrained(args.model_id)
-print("Data type of the model:", user_model.dtype)
-# calling _optimize_transformers for int8 path
-user_model = ipex._optimize_transformers(
-    user_model.eval(), dtype=torch.int8, inplace=True
-)
-# dummy past key value
-beam_idx_tmp = torch.zeros(
-    (2048, int(args.batch_size * num_beams)), dtype=torch.long
-).contiguous()
-global_past_key_value = [
-    (
-        torch.zeros(
-            [
-                1,
-                user_model.config.num_attention_heads,
-                1,
-                int(
-                    user_model.config.hidden_size
-                    / user_model.config.num_attention_heads
-                ),
-            ]
-        ).contiguous(),
-        torch.zeros(
-            [
-                1,
-                user_model.config.num_attention_heads,
-                1,
-                int(
-                    user_model.config.hidden_size
-                    / user_model.config.num_attention_heads
-                ),
-            ]
-        ).contiguous(),
-        beam_idx_tmp,
-        torch.zeros(1, dtype=torch.long).contiguous(),
-    )
-    for i in range(user_model.config.num_hidden_layers)
-]
-
-
-class Evaluator:
-    def __init__(self, dataset, tokenizer, batch_size=8, pad_val=1, pad_max=196):
-        self.dataset = dataset
-        self.tokenizer = tokenizer
-        self.batch_size = batch_size
-        self.pad_val = pad_val
-        self.pad_max = pad_max
-
-        # tokenize the dataset
-        self.dataset = self.dataset.map(self.tokenize_function, batched=True)
-        self.dataset.set_format(type="torch", columns=["input_ids"])
-
-    @torch.no_grad()
-    def tokenize_function(self, examples):
-        example = self.tokenizer(examples["text"])
-        return example
-
-    @torch.no_grad()
-    def collate_batch(self, batch):
-        position_ids_padded = []
-        input_ids_padded = []
-        last_ind = []
-        attention_mask_padded = []
-        for text in batch:
-            # we cut the sentence if it exceeds pad_max, we are using tuned max 196 from gptj model; TODO: tune best pad_max
-            input_ids = (
-                text["input_ids"]
-                if text["input_ids"].shape[0] <= self.pad_max
-                else text["input_ids"][0 : int(self.pad_max - 1)]
-            )
-            pad_len = self.pad_max - input_ids.shape[0]
-            last_ind.append(input_ids.shape[0] - 1)
-            attention_mask = torch.ones(len(input_ids))
-            position_ids = torch.arange(len(input_ids))
-            input_ids = pad(input_ids, (0, pad_len), value=self.pad_val)
-            input_ids_padded.append(input_ids)
-            attention_mask = pad(attention_mask, (0, pad_len), value=0)
-            attention_mask_padded.append(attention_mask)
-            position_ids = pad(position_ids, (0, pad_len), value=self.pad_val)
-            position_ids_padded.append(position_ids)
-        return (
-            (
-                torch.vstack(input_ids_padded),
-                torch.vstack(attention_mask_padded),
-                torch.vstack(position_ids_padded),
-                tuple(global_past_key_value),
-            ),
-            torch.tensor(last_ind),
-        )
-
-    @torch.no_grad()
-    def evaluate(self, model):
-        model.eval()
-        # The task is to predict the last word of the input.
-        total, hit = 0, 0
-        latency = 0
-        test_dataloader = DataLoader(
-            self.dataset,
-            batch_size=self.batch_size,
-            shuffle=False,
-            collate_fn=self.collate_batch,
-        )
-        for i, (
-                (input_ids, attention_mask, position_ids, past_key_values),
-                last_ind,
-            ) in enumerate(test_dataloader):
-                label = input_ids[torch.arange(len(last_ind)), last_ind]
-                input_ids[torch.arange(len(last_ind)), last_ind] = self.pad_val
-                pad_len = self.pad_max - last_ind - 1
-                start = time.time()
-                outputs = model(
-                    input_ids,
-                    attention_mask=attention_mask,
-                    position_ids=position_ids,
-                    past_key_values=past_key_values,
-                )
-                latency += time.time() - start
-                last_token_logits = outputs[0][torch.arange(len(last_ind)), -2 - pad_len, :]
-                pred = last_token_logits.argmax(dim=-1)
-                total += label.size(0)
-                hit += (pred == label).sum().item()
-                if i % 50 == 0:
-                    print(hit / total, flush=True)
-                    print("Processed minibatch:", i, flush=True)
-
-        acc = hit / total
-        print(acc)
-        lantecy = latency / len(self.dataset)
-        return acc, lantecy
-
-
-if args.lambada:
-    full_dataset = load_dataset(args.dataset)
-    dataset = full_dataset["validation"]
-    calib_dataset = full_dataset["train"]
-
-    user_model.eval()
-    evaluator = Evaluator(dataset, tokenizer, args.batch_size)
-    calib_evaluator = Evaluator(calib_dataset, tokenizer, args.batch_size)
-
-    calib_dataloader = DataLoader(
-        calib_evaluator.dataset,
-        batch_size=args.batch_size,
-        shuffle=False,
-        collate_fn=evaluator.collate_batch,
-    )
-
-    test_dataloader = DataLoader(
-        evaluator.dataset,
-        batch_size=args.batch_size,
-        shuffle=False,
-        collate_fn=evaluator.collate_batch,
-    )
-
-
-def calib_func(prepared_model):
-    for i, (
-        (input_ids, attention_mask, position_ids, past_key_values),
-        last_ind,
-    ) in enumerate(calib_dataloader):
-        if i == 8:
-            break
-        prepared_model(
-            input_ids,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_values=past_key_values,
-        )
-
-
-def eval_func(traced_model):
-    acc, latency = evaluator.evaluate(traced_model)
-    print("Accuracy:", acc)
-    print("Latency (sec):", latency)
-    return acc
-
-
-if args.jit and args.benchmark:
-    torch._C._jit_set_texpr_fuser_enabled(False)
-    if args.benchmark and (args.int8 or args.int8_bf16_mixed):
-        if not hasattr(user_model, "trace_graph"):
-            print("load_int8_model")
-            self_jit = torch.jit.load(args.quantized_model_path)
-            self_jit = torch.jit.freeze(self_jit.eval())
-            setattr(user_model, "trace_graph", self_jit)
-
-
-if args.ipex_smooth_quant:
-    example_inputs = None
-    for i, (
-        (input_ids, attention_mask, position_ids, past_key_values),
-        last_ind,
-    ) in enumerate(calib_dataloader):
-        example_inputs = (input_ids, attention_mask, position_ids, past_key_values)
-        break
-    from intel_extension_for_pytorch.quantization import prepare, convert
-
-    qconfig = ipex.quantization.get_smooth_quant_qconfig_mapping()
-    prepared_model = prepare(user_model.eval(), qconfig, example_inputs=example_inputs)
-    with torch.no_grad():
-        for i, (
-            (input_ids, attention_mask, position_ids, past_key_values),
-            last_ind,
-        ) in enumerate(calib_dataloader):
-            if i == 8:
-                break
-            prepared_model(
-                input_ids,
-                position_ids=position_ids,
-                attention_mask=attention_mask,
-                past_key_values=past_key_values,
-            )
-    with torch.no_grad(), torch.autocast(
-        device_type=args.device,
-        enabled=amp_enabled,
-        dtype=torch.bfloat16 if amp_enabled else None,
-    ):
-        convert_model = convert(prepared_model.eval()).eval()
-        self_jit = torch.jit.trace(convert_model.eval(), example_inputs, strict=False)
-        self_jit = torch.jit.freeze(self_jit.eval())
-        self_jit.save(args.output_dir + "/best_model.pt")
-
-
-if args.ipex_weight_only_quantization:
-
-    def convert_woq(m, qconfig, inplace=True):
-        import copy
-
-        def _convert(m):
-            from intel_extension_for_pytorch.nn.modules import IpexWoqLinear
-
-            if isinstance(m, torch.nn.Linear):
-                m.qconfig = qconfig.global_qconfig
-                m_new = IpexWoqLinear.from_float(m)
-                return m_new
-            m_new = m
-
-            for name, child in m.named_children():
-                setattr(m_new, name, _convert(child))
-            return m_new
-
-        if not inplace:
-            m_new = copy.deepcopy(m)
-        else:
-            m_new = m
-        return _convert(m_new)
-
-    example_inputs = None
-    input_ids = torch.ones(32).to(torch.long)
-    attention_mask = torch.ones(len(input_ids))
-    position_ids = torch.arange(len(input_ids))
-    example_inputs = (
-        input_ids.unsqueeze(0),
-        attention_mask.unsqueeze(0),
-        position_ids.unsqueeze(0),
-        tuple(global_past_key_value),
-    )
-
-    from intel_extension_for_pytorch.quantization import prepare, convert
-    
-    if args.lowp_mode == "INT8":
-        lowp_mode = ipex.quantization.WoqLowpMode.INT8
-    elif args.lowp_mode == "FP32":
-        lowp_mode = ipex.quantization.WoqLowpMode.NONE
-    elif args.lowp_mode == "FP16":
-        lowp_mode = ipex.quantization.WoqLowpMode.FP16
-    else:
-        lowp_mode = ipex.quantization.WoqLowpMode.BF16
-
-    qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-        lowp_mode=lowp_mode
-    )
-    with torch.no_grad():
-        convert_model = convert_woq(user_model.eval(), qconfig)
-    with torch.no_grad(), torch.autocast(
-        device_type=args.device,
-        enabled=amp_enabled,
-        dtype=amp_dtype if amp_enabled else None,
-    ):
-        convert_model = convert_woq(user_model.eval(), qconfig)
-        self_jit = torch.jit.trace(convert_model.eval(), example_inputs, strict=False)
-        self_jit = torch.jit.freeze(self_jit.eval())
-        self_jit.save(args.output_dir + "/best_model.pt")
-
-
-if args.accuracy_only:
-    if args.int8 or args.int8_bf16_mixed:
-        user_model = torch.jit.load(args.quantized_model_path)
-        user_model = torch.jit.freeze(user_model.eval())
-
-    with torch.autocast(
-        device_type=args.device,
-        enabled=amp_enabled,
-        dtype=torch.bfloat16 if amp_enabled else None,
-    ):
-        eval_func(user_model)
-        
-
-if args.benchmark:
-    # input prompt
-    current_path = pathlib.Path(__file__).parent.resolve()
-    with open(str(current_path) + "/prompt.json") as f:
-        prompt_pool = json.load(f)
-    if args.prompt is not None:
-        prompt = args.prompt
-    elif int(args.input_tokens) > 8192:
-        prompt = prompt_pool["llama"]["8192"] * int(int(args.input_tokens) / 8192)
-    elif args.input_tokens in prompt_pool["llama"]:
-        prompt = prompt_pool["llama"][args.input_tokens]
-    else:
-        raise SystemExit("[ERROR] Plese use --prompt if want to use custom input.")
-
-    input_size = tokenizer(prompt, return_tensors="pt").input_ids.size(dim=1)
-    print("---- Prompt size:", input_size)
-    if args.token_latency:
-        if not hasattr(user_model.config, "token_latency"):
-            user_model.config.token_latency = True
-
-    total_time = 0.0
-    num_iter = args.num_iter
-    num_warmup = args.num_warmup
-    prompt = [prompt] * args.batch_size
-    total_list = []
-    with torch.inference_mode(), torch.no_grad(), torch.autocast(
-        device_type=args.device,
-        enabled=amp_enabled,
-        dtype=torch.bfloat16 if amp_enabled else None,
-    ):
-        for i in range(num_iter):
-            tic = time.time()
-            input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
-            output = user_model.generate(
-                input_ids, max_new_tokens=args.max_new_tokens, **generate_kwargs
-            )
-            gen_ids = output[0] if args.token_latency else output
-            gen_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
-            toc = time.time()
-            input_tokens_lengths = [x.shape[0] for x in input_ids]
-            output_tokens_lengths = [x.shape[0] for x in gen_ids]
-            total_new_tokens = [
-                o - i if user_model.config.model_type != "t5" else o
-                for i, o in zip(input_tokens_lengths, output_tokens_lengths)
-            ]
-            print(gen_text, total_new_tokens, flush=True)
-            print("Iteration: %d, Time: %.6f sec" % (i, toc - tic), flush=True)
-            if i >= num_warmup:
-                total_time += toc - tic
-                if args.token_latency:
-                    total_list.append(output[1])
-
-    if args.profile:
-        def trace_handler(prof):
-            print(prof.key_averages().table(
-                sort_by="self_cpu_time_total", row_limit=-1))
-        with torch.profiler.profile(
-            activities=[torch.profiler.ProfilerActivity.CPU],
-            schedule=torch.profiler.schedule(
-                wait=1,
-                warmup=3,
-                active=1),
-            on_trace_ready=trace_handler
-        ) as prof:
-            for i in range(5):
-                input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
-                output = user_model.generate(
-                    input_ids, max_new_tokens=args.max_new_tokens, **generate_kwargs
-                )
-                gen_ids = output[0] if args.token_latency else output
-                gen_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
-                prof.step()
-
-    print("\n", "-" * 10, "Summary:", "-" * 10)
-    latency = total_time / (num_iter - num_warmup)
-    print("Inference latency: %.3f sec." % latency)
-    if args.token_latency:
-        import numpy as np
-        from itertools import chain
-
-        first_latency = np.mean([x[0] for x in total_list])
-        average_2n = list(chain(*[x[1:] for x in total_list]))
-        average_2n.sort()
-        average_2n_latency = np.mean(average_2n)
-        p90_latency = average_2n[int(len(average_2n) * 0.9)]
-        p99_latency = average_2n[int(len(average_2n) * 0.99)]
-        print("First token average latency: %.3f sec." % first_latency)
-        print("Average 2... latency: %.3f sec." % average_2n_latency)
-        print("P90 2... latency: %.3f sec." % p90_latency)
-        print("P99 2... latency: %.3f sec." % p99_latency)
diff --git a/intel_extension_for_pytorch/__init__.py b/intel_extension_for_pytorch/__init__.py
index 51b7331c..0fcdf342 100644
--- a/intel_extension_for_pytorch/__init__.py
+++ b/intel_extension_for_pytorch/__init__.py
@@ -23,7 +23,6 @@ from . import jit
 from . import optim
 from . import fx
 from . import _meta_registrations
-
 try:
     from .cpu import tpp
 except BaseException:
@@ -32,10 +31,6 @@ except BaseException:
     )
 
 from .frontend import optimize
-from .cpu.transformers import (
-    _optimize_transformers,
-    _set_optimized_model_for_generation,
-)
 from .frontend import enable_auto_channels_last, disable_auto_channels_last
 from .frontend import set_fp32_math_mode, get_fp32_math_mode, FP32MathMode
 from .cpu._auto_kernel_selection import _enable_dnnl, _disable_dnnl, _using_dnnl
@@ -44,7 +39,6 @@ from .cpu.tpp.fused_bert import fast_bert
 from ._inductor.compiler import _set_compiler_backend, _get_compiler_backend, compile
 from ._inductor.dynamo_backends import *
 from .cpu.onednn_fusion import enable_onednn_fusion
-from ._init_on_device import _IPEXOnDevice
 
 from . import _C
 from ._version import (
diff --git a/intel_extension_for_pytorch/_init_on_device.py b/intel_extension_for_pytorch/_init_on_device.py
deleted file mode 100644
index d3e3ba07..00000000
--- a/intel_extension_for_pytorch/_init_on_device.py
+++ /dev/null
@@ -1,98 +0,0 @@
-# Copyright (c) Microsoft Corporation.
-# SPDX-License-Identifier: Apache-2.0
-# DeepSpeed Team
-# https://github.com/microsoft/DeepSpeed/blob/55243f3bc8d4e751734ee2000fe3979bd4b6228c/deepspeed/utils/init_on_device.py#L12
-
-# Copyright (c) 2023 Intel Corporation
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-import torch
-from typing import Callable
-from torch import Tensor
-from packaging import version as pkg_version
-
-
-class _IPEXOnDevice(object):
-    """
-    Create modules/tensors w. specific devices and dtypes. Examples:
-
-    Create MyModule which consists of many different sub-modules and parameters. In this case we can create
-    MyModule as a collection of 'meta' tensors by passing `device='meta'` or we can create the module _directly_
-    on a CUDA device by passing `device=f'cuda:{local_rank}'` (where `local_rank` is the local GPU id.
-
-    with _IPEXOnDevice(dtype=torch.float16, device='meta'):
-        model = MyModel()
-
-    with _IPEXOnDevice(dtype=torch.float16, device=f'cuda:{local_rank}'):
-        model = MyModel()
-
-    """
-
-    _orig_torch_empty = torch.empty
-    _orig_torch_zeros = torch.zeros
-    _orig_torch_ones = torch.ones
-    _orig_torch_full = torch.full
-
-    def __init__(self, dtype, device="meta", enabled=True):
-        self.dtype = dtype
-        self.enabled = enabled
-        self.device = device
-
-        if device == "meta":
-            if pkg_version.parse("1.10") > pkg_version.parse(torch.__version__):
-                raise NotImplementedError(
-                    "Meta tensor support is not available, please upgrade to torch 1.10+"
-                )
-
-    def fp_tensor_constructor(
-        self, fn: Callable, target_fp_dtype: torch.dtype
-    ) -> Callable:
-        def wrapped_fn(*args, **kwargs) -> Tensor:
-            if kwargs.get("device", None) is None:
-                kwargs["device"] = self.device
-            tensor: Tensor = fn(*args, **kwargs)
-            if tensor.is_floating_point():
-                tensor = tensor.to(target_fp_dtype)
-            return tensor
-
-        return wrapped_fn
-
-    def get_new_tensor_fn_for_dtype(self, dtype: torch.dtype) -> Callable:
-        def new_tensor(cls, *args) -> Tensor:
-            tensor = _IPEXOnDevice._orig_torch_empty(0, device=self.device).new_empty(
-                *args
-            )
-            if tensor.is_floating_point():
-                tensor = tensor.to(dtype)
-            return tensor
-
-        return new_tensor
-
-    def __enter__(self):
-        if not self.enabled:
-            return
-        torch.Tensor.__old_new__ = torch.Tensor.__new__
-        torch.Tensor.__new__ = self.get_new_tensor_fn_for_dtype(self.dtype)
-        torch.empty = self.fp_tensor_constructor(self._orig_torch_empty, self.dtype)
-        torch.zeros = self.fp_tensor_constructor(self._orig_torch_zeros, self.dtype)
-        torch.ones = self.fp_tensor_constructor(self._orig_torch_ones, self.dtype)
-        torch.full = self.fp_tensor_constructor(self._orig_torch_full, self.dtype)
-
-    def __exit__(self, exc_type, exc_value, traceback):
-        if not self.enabled:
-            return
-        torch.Tensor.__new__ = torch.Tensor.__old_new__
-        torch.empty = self._orig_torch_empty
-        torch.zeros = self._orig_torch_zeros
-        torch.ones = self._orig_torch_ones
-        torch.full = self._orig_torch_full
diff --git a/intel_extension_for_pytorch/_meta_registrations.py b/intel_extension_for_pytorch/_meta_registrations.py
index 2612d012..2711a5cc 100644
--- a/intel_extension_for_pytorch/_meta_registrations.py
+++ b/intel_extension_for_pytorch/_meta_registrations.py
@@ -5,17 +5,22 @@ import torch
 import torch.library
 from torch._prims_common import IntLike
 
+
 @functools.lru_cache(None)
 def get_meta_lib():
     return torch.library.Library("torch_ipex", "IMPL", "Meta")
 
+
 def register_meta(op_name, overload_name="default"):
     def wrapper(fn):
-        get_meta_lib().impl(getattr(getattr(torch.ops.torch_ipex, op_name), overload_name), fn)
+        get_meta_lib().impl(
+            getattr(getattr(torch.ops.torch_ipex, op_name), overload_name), fn
+        )
         return fn
 
     return wrapper
 
+
 def calc_conv_nd_return_shape(
     input_tensor,
     weight_size,
@@ -114,12 +119,15 @@ def calc_conv_nd_return_shape(
 
     return ret_shape
 
+
 def is_channels_last(ten):
     return torch._prims_common.suggest_memory_format(ten) == torch.channels_last
 
+
 def is_channels_last_3d(ten):
     return torch._prims_common.suggest_memory_format(ten) == torch.channels_last_3d
 
+
 @register_meta("convolution_forward")
 def meta_convolution_forward(
     input,
@@ -143,7 +151,9 @@ def meta_convolution_forward(
         None,
     )
 
-    use_channels_last = is_channels_last(input) or is_channels_last_3d(input) or weight_channels_last
+    use_channels_last = (
+        is_channels_last(input) or is_channels_last_3d(input) or weight_channels_last
+    )
     memory_format = torch.contiguous_format
     if use_channels_last:
         if input.dim() == 4:
@@ -155,6 +165,43 @@ def meta_convolution_forward(
     out = out.to(memory_format=memory_format)  # type: ignore[call-overload]
     return out
 
+
+@register_meta("convolution_backward")
+def meta_convolution_backward(
+    input,
+    weight,
+    bias,
+    grad_output,
+    out_mask,
+    W_prepack,
+    weight_channels_last,
+):
+    use_channels_last = (
+        is_channels_last(input) or is_channels_last_3d(input) or weight_channels_last
+    )
+    memory_format = torch.contiguous_format
+    if use_channels_last:
+        if input.dim() == 4:
+            memory_format = torch.channels_last
+        elif input.dim() == 5:
+            memory_format = torch.channels_last_3d
+
+    backend_grad_input = None
+    backend_grad_weight = None
+    backend_grad_bias = None
+
+    if out_mask[0]:
+        backend_grad_input = grad_output.new_empty(input.size()).to(
+            memory_format=memory_format
+        )
+    if out_mask[1]:
+        backend_grad_weight = grad_output.new_empty(weight.size())
+    if out_mask[2]:
+        backend_grad_bias = grad_output.new_empty(bias.size())
+
+    return (backend_grad_input, backend_grad_weight, backend_grad_bias)
+
+
 @register_meta("conv_transpose")
 def meta_conv_transpose(
     input,
@@ -180,7 +227,9 @@ def meta_conv_transpose(
         output_padding,
     )
 
-    use_channels_last = is_channels_last(input) or is_channels_last_3d(input) or weight_channels_last
+    use_channels_last = (
+        is_channels_last(input) or is_channels_last_3d(input) or weight_channels_last
+    )
     memory_format = torch.contiguous_format
     if use_channels_last:
         if input.dim() == 4:
@@ -192,6 +241,43 @@ def meta_conv_transpose(
     out = out.to(memory_format=memory_format)  # type: ignore[call-overload]
     return out
 
+
+@register_meta("conv_transpose_backward")
+def meta_conv_transpose_backward(
+    input,
+    weight,
+    bias,
+    grad_output,
+    out_mask,
+    W_prepack,
+    weight_channels_last,
+):
+    use_channels_last = (
+        is_channels_last(input) or is_channels_last_3d(input) or weight_channels_last
+    )
+    memory_format = torch.contiguous_format
+    if use_channels_last:
+        if input.dim() == 4:
+            memory_format = torch.channels_last
+        elif input.dim() == 5:
+            memory_format = torch.channels_last_3d
+
+    backend_grad_input = None
+    backend_grad_weight = None
+    backend_grad_bias = None
+
+    if out_mask[0]:
+        backend_grad_input = grad_output.new_empty(input.size()).to(
+            memory_format=memory_format
+        )
+    if out_mask[1]:
+        backend_grad_weight = grad_output.new_empty(weight.size())
+    if out_mask[2]:
+        backend_grad_bias = grad_output.new_empty(bias.size())
+
+    return (backend_grad_input, backend_grad_weight, backend_grad_bias)
+
+
 @register_meta("ipex_linear")
 def meta_ipex_linear(
     input,
@@ -202,6 +288,30 @@ def meta_ipex_linear(
 ):
     return input.new_empty((*input.shape[:-1], out_features))
 
+
+@register_meta("linear_backward")
+def meta_linear_backward(
+    input,
+    weight,
+    bias,
+    grad_output,
+    out_mask,
+    W_prepack,
+):
+    backend_grad_input = None
+    backend_grad_weight = None
+    backend_grad_bias = None
+
+    if out_mask[0]:
+        backend_grad_input = grad_output.new_empty(input.size())
+    if out_mask[1]:
+        backend_grad_weight = grad_output.new_empty(weight.size())
+    if out_mask[2]:
+        backend_grad_bias = grad_output.new_empty(bias.size())
+
+    return (backend_grad_input, backend_grad_weight, backend_grad_bias)
+
+
 @register_meta("ipex_MKLSGEMM")
 def meta_ipex_MKLSGEMM(
     input,
@@ -212,6 +322,7 @@ def meta_ipex_MKLSGEMM(
 ):
     return input.new_empty((*input.shape[:-1], out_features))
 
+
 @register_meta("embedding_bag")
 def meta_embedding_bag(
     weight,
@@ -226,6 +337,7 @@ def meta_embedding_bag(
     shape_out = [num_bags, weight.shape[1]]
     return weight.new_empty(shape_out)
 
+
 @register_meta("ipex_lstm")
 def meta_ipex_lstm(
     input,
@@ -245,14 +357,33 @@ def meta_ipex_lstm(
     cy = hx[1].new_empty(hx[1].size())
     return (out, hy, cy)
 
+
 @register_meta("ROIAlign_forward")
 def meta_ROIAlign_forward(
-    input,
+    input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned
+):
+    return input.new_empty(
+        (rois.shape[0], input.shape[1], pooled_height, pooled_width)
+    ).to(memory_format=torch._prims_common.suggest_memory_format(input))
+
+
+@register_meta("ROIAlign_backward")
+def meta_ROIAlign_backward(
+    grad,
     rois,
     spatial_scale,
     pooled_height,
     pooled_width,
+    batch_size,
+    channels,
+    height,
+    width,
     sampling_ratio,
-    aligned
+    aligned,
+    is_channels_last,
 ):
-    return input.new_empty((rois.shape[0], input.shape[1], pooled_height, pooled_width))
\ No newline at end of file
+    return grad.new_empty((batch_size, channels, height, width)).to(
+        memory_format=torch.channels_last
+        if is_channels_last
+        else torch.contiguous_format
+    )
diff --git a/intel_extension_for_pytorch/cpu/__init__.py b/intel_extension_for_pytorch/cpu/__init__.py
index c822eefc..9b2ff9ad 100644
--- a/intel_extension_for_pytorch/cpu/__init__.py
+++ b/intel_extension_for_pytorch/cpu/__init__.py
@@ -1,4 +1,3 @@
 from . import runtime
 from . import autocast
 from . import auto_ipex
-from . import transformers
diff --git a/intel_extension_for_pytorch/cpu/_auto_kernel_selection.py b/intel_extension_for_pytorch/cpu/_auto_kernel_selection.py
index c7d68fa4..189dfae8 100644
--- a/intel_extension_for_pytorch/cpu/_auto_kernel_selection.py
+++ b/intel_extension_for_pytorch/cpu/_auto_kernel_selection.py
@@ -14,20 +14,3 @@ def _disable_dnnl():
 def _using_dnnl():
     global _use_dnnl
     return _use_dnnl
-
-_use_tpp = False
-
-
-def _enable_tpp():
-    global _use_tpp
-    _use_tpp = True
-
-
-def _disable_tpp():
-    global _use_tpp
-    _use_tpp = False
-
-
-def _using_tpp():
-    global _use_tpp
-    return _use_tpp
\ No newline at end of file
diff --git a/intel_extension_for_pytorch/cpu/launch/launch.py b/intel_extension_for_pytorch/cpu/launch/launch.py
index 1fde11e3..fb513d06 100644
--- a/intel_extension_for_pytorch/cpu/launch/launch.py
+++ b/intel_extension_for_pytorch/cpu/launch/launch.py
@@ -268,12 +268,24 @@ def process_deprecated_params(args, logger):
             "Arguments --enable_tcmalloc, --enable_jemalloc and --use_default_allocator \
                 are deprecated by --memory-allocator."
         )
-        if args.use_default_allocator:
-            args.memory_allocator = "default"
-        if args.enable_jemalloc:
-            args.memory_allocator = "jemalloc"
-        if args.enable_tcmalloc:
-            args.memory_allocator = "tcmalloc"
+        if (
+            (args.enable_tcmalloc and args.enable_jemalloc)
+            or (args.enable_tcmalloc and args.use_default_allocator)
+            or (args.enable_jemalloc and args.use_default_allocator)
+            or (
+                args.enable_tcmalloc
+                and args.enable_jemalloc
+                and args.use_default_allocator
+            )
+        ):
+            args.memory_allocator = "auto"
+        else:
+            if args.enable_tcmalloc:
+                args.memory_allocator = "tcmalloc"
+            if args.enable_jemalloc:
+                args.memory_allocator = "jemalloc"
+            if args.use_default_allocator:
+                args.memory_allocator = "default"
     if args.disable_numactl:
         logger.warning(
             "Argument --disable_numactl is deprecated by --multi-task-manager."
diff --git a/intel_extension_for_pytorch/cpu/tpp/__init__.py b/intel_extension_for_pytorch/cpu/tpp/__init__.py
index c1eba8d0..6e60cbc2 100644
--- a/intel_extension_for_pytorch/cpu/tpp/__init__.py
+++ b/intel_extension_for_pytorch/cpu/tpp/__init__.py
@@ -4,4 +4,3 @@ from . import fused_bert
 from . import utils
 from . import optim
 from .utils.blocked_layout import block_model_params as block
-from .fused_llm import Apply_TPP_optimization
diff --git a/intel_extension_for_pytorch/cpu/tpp/fused_llm.py b/intel_extension_for_pytorch/cpu/tpp/fused_llm.py
deleted file mode 100644
index 9b6bc7c5..00000000
--- a/intel_extension_for_pytorch/cpu/tpp/fused_llm.py
+++ /dev/null
@@ -1,383 +0,0 @@
-import torch
-from torch import nn
-from typing import Optional, Tuple, Union
-
-
-def GPTNeoXMLP_forward(self, hidden_states):
-    hidden_states = self.dense_h_to_4h(hidden_states)
-    hidden_states = self.act(hidden_states)
-    hidden_states = self.dense_4h_to_h(hidden_states)
-    return hidden_states
-
-
-def GPTNeoXLayer_forward(
-    self,
-    hidden_states: Optional[torch.FloatTensor],
-    attention_mask: Optional[torch.FloatTensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    head_mask: Optional[torch.FloatTensor] = None,
-    use_cache: Optional[bool] = False,
-    layer_past: Optional[Tuple[torch.Tensor]] = None,
-    output_attentions: Optional[bool] = False,
-):
-    attention_layer_outputs = self.attention(
-        self.input_layernorm(hidden_states),
-        attention_mask=attention_mask,
-        position_ids=position_ids,
-        layer_past=layer_past,
-        head_mask=head_mask,
-        use_cache=use_cache,
-        output_attentions=output_attentions,
-    )
-    attn_output = attention_layer_outputs[
-        0
-    ]  # output_attn: attn_output, present, (attn_weights)
-    outputs = attention_layer_outputs[1:]
-
-    if self.use_parallel_residual:
-        # pseudocode:
-        # x = x + attn(ln1(x)) + mlp(ln2(x))
-        mlp_output = self.mlp(self.post_attention_layernorm(hidden_states))
-        hidden_states = mlp_output + attn_output + hidden_states
-    else:
-        # pseudocode:
-        # x = x + attn(ln1(x))
-        # x = x + mlp(ln2(x))
-        attn_output = attn_output + hidden_states
-        mlp_output = self.mlp(self.post_attention_layernorm(attn_output))
-        hidden_states = mlp_output + attn_output
-
-    if use_cache:
-        outputs = (hidden_states,) + outputs  # hidden_states, present, (attn_weights)
-    else:
-        outputs = (hidden_states,) + outputs[1:]  # hidden_states, (attn_weights)
-
-    return outputs
-
-
-def LlamaMLP_forward_distributed(self, x):
-    gate = torch.ops.torch_ipex.tpp_linear_silu(
-        x, self.gate_proj.weight, x.new_empty(0)
-    )
-    up = torch.ops.torch_ipex.tpp_linear_mul(
-        x, gate, self.up_proj.weight, x.new_empty(0)
-    )
-    return self.down_proj(up)
-
-
-def LlamaMLP_forward(self, x):
-    gate = torch.ops.torch_ipex.tpp_linear_silu(
-        x, self.gate_proj.weight, x.new_empty(0)
-    )
-    up = torch.ops.torch_ipex.tpp_linear_mul(
-        x, gate, self.up_proj.weight, x.new_empty(0)
-    )
-    return up
-
-
-def LlamaDecoderLayer_forward(
-    self,
-    hidden_states: torch.Tensor,
-    attention_mask: Optional[torch.Tensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    past_key_value: Optional[Tuple[torch.Tensor]] = None,
-    output_attentions: Optional[bool] = False,
-    use_cache: Optional[bool] = False,
-) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
-    residual = hidden_states
-
-    hidden_states = self.input_layernorm(hidden_states)
-
-    # Self Attention
-    hidden_states, self_attn_weights, present_key_value = self.self_attn(
-        hidden_states=hidden_states,
-        attention_mask=attention_mask,
-        position_ids=position_ids,
-        past_key_value=past_key_value,
-        output_attentions=output_attentions,
-        use_cache=use_cache,
-    )
-    hidden_states = residual + hidden_states
-
-    # Fully Connected
-    residual = hidden_states
-    hidden_states = self.post_attention_layernorm(hidden_states)
-    hidden_states = torch.ops.torch_ipex.tpp_linear_add(
-        self.mlp(hidden_states),
-        residual,
-        self.mlp.down_proj.weight,
-        hidden_states.new_empty(0),
-        1.0,
-    )
-
-    outputs = (hidden_states,)
-
-    if output_attentions:
-        outputs += (self_attn_weights,)
-
-    if use_cache:
-        outputs += (present_key_value,)
-
-    return outputs
-
-
-def OPTDecoderLayer_forward(
-    self,
-    hidden_states: torch.Tensor,
-    attention_mask: Optional[torch.Tensor] = None,
-    layer_head_mask: Optional[torch.Tensor] = None,
-    output_attentions: Optional[bool] = False,
-    use_cache: Optional[bool] = False,
-    past_key_value: Optional[Tuple[torch.Tensor]] = None,
-) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
-    residual = hidden_states
-
-    # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
-    if self.do_layer_norm_before:
-        hidden_states = self.self_attn_layer_norm(hidden_states)
-
-    # Self Attention
-    hidden_states, self_attn_weights, present_key_value = self.self_attn(
-        hidden_states=hidden_states,
-        past_key_value=past_key_value,
-        attention_mask=attention_mask,
-        layer_head_mask=layer_head_mask,
-        output_attentions=output_attentions,
-    )
-    hidden_states = torch.ops.torch_ipex.tpp_linear_add(
-        hidden_states,
-        residual,
-        self.self_attn.out_proj.weight,
-        self.self_attn.out_proj.bias,
-        1.0,
-    )
-
-    # 350m applies layer norm AFTER attention
-    if not self.do_layer_norm_before:
-        hidden_states = self.self_attn_layer_norm(hidden_states)
-
-    # Fully Connected
-    hidden_states_shape = hidden_states.shape
-    # TPP only supports 3d inputs for now
-    # hidden_states = hidden_states.reshape(-1, hidden_states.size(-1))
-    residual = hidden_states
-
-    # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
-    if self.do_layer_norm_before:
-        hidden_states = self.final_layer_norm(hidden_states)
-
-    hidden_states = torch.ops.torch_ipex.tpp_linear_relu(
-        hidden_states, self.fc1.weight, self.fc1.bias
-    )
-
-    hidden_states = torch.ops.torch_ipex.tpp_linear_add(
-        hidden_states,
-        residual,
-        self.fc2.weight,
-        self.fc2.bias,
-        1.0,
-    ).view(hidden_states_shape)
-
-    # 350m applies layer norm AFTER attention
-    if not self.do_layer_norm_before:
-        hidden_states = self.final_layer_norm(hidden_states)
-
-    outputs = (hidden_states,)
-
-    if output_attentions:
-        outputs += (self_attn_weights,)
-
-    if use_cache:
-        outputs += (present_key_value,)
-
-    return outputs
-
-
-def OPTDecoderLayer_forward_distributed(
-    self,
-    hidden_states: torch.Tensor,
-    attention_mask: Optional[torch.Tensor] = None,
-    layer_head_mask: Optional[torch.Tensor] = None,
-    output_attentions: Optional[bool] = False,
-    use_cache: Optional[bool] = False,
-    past_key_value: Optional[Tuple[torch.Tensor]] = None,
-) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
-    residual = hidden_states
-
-    # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
-    if self.do_layer_norm_before:
-        hidden_states = self.self_attn_layer_norm(hidden_states)
-
-    # Self Attention
-    hidden_states, self_attn_weights, present_key_value = self.self_attn(
-        hidden_states=hidden_states,
-        past_key_value=past_key_value,
-        attention_mask=attention_mask,
-        layer_head_mask=layer_head_mask,
-        output_attentions=output_attentions,
-    )
-    hidden_states = self.self_attn.out_proj(hidden_states)
-    hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
-    hidden_states = residual + hidden_states
-
-    # 350m applies layer norm AFTER attention
-    if not self.do_layer_norm_before:
-        hidden_states = self.self_attn_layer_norm(hidden_states)
-
-    # Fully Connected
-    hidden_states_shape = hidden_states.shape
-    # TPP only supports 3d inputs for now
-    # hidden_states = hidden_states.reshape(-1, hidden_states.size(-1))
-    residual = hidden_states
-
-    # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
-    if self.do_layer_norm_before:
-        hidden_states = self.final_layer_norm(hidden_states)
-
-    hidden_states = torch.ops.torch_ipex.tpp_linear_relu(
-        hidden_states, self.fc1.weight, self.fc1.bias
-    )
-
-    hidden_states = self.fc2(hidden_states)
-    hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
-    hidden_states = (residual + hidden_states).view(hidden_states_shape)
-
-    # 350m applies layer norm AFTER attention
-    if not self.do_layer_norm_before:
-        hidden_states = self.final_layer_norm(hidden_states)
-
-    outputs = (hidden_states,)
-
-    if output_attentions:
-        outputs += (self_attn_weights,)
-
-    if use_cache:
-        outputs += (present_key_value,)
-
-    return outputs
-
-def GPTJMLP_forward(
-    self, hidden_states: Optional[torch.FloatTensor]
-) -> torch.FloatTensor:
-    hidden_states = torch.ops.torch_ipex.tpp_linear_gelu(
-        hidden_states, self.fc_in.weight, self.fc_in.bias
-    )
-    return hidden_states
-
-
-def GPTJMLP_forward_distributed(
-    self, hidden_states: Optional[torch.FloatTensor]
-) -> torch.FloatTensor:
-    hidden_states = torch.ops.torch_ipex.tpp_linear_gelu(
-        hidden_states, self.fc_in.weight, self.fc_in.bias
-    )
-    hidden_states = self.fc_out(hidden_states)
-    hidden_states = self.dropout(hidden_states)
-    return hidden_states
-
-
-def GPTJBlock_forward(
-    self,
-    hidden_states: Optional[torch.FloatTensor],
-    layer_past: Optional[Tuple[torch.Tensor]] = None,
-    attention_mask: Optional[torch.FloatTensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    head_mask: Optional[torch.FloatTensor] = None,
-    use_cache: Optional[bool] = False,
-    output_attentions: Optional[bool] = False,
-) -> Union[
-    Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]
-]:
-    residual = hidden_states
-    hidden_states = self.ln_1(hidden_states)
-    attn_outputs = self.attn(
-        hidden_states=hidden_states,
-        layer_past=layer_past,
-        attention_mask=attention_mask,
-        position_ids=position_ids,
-        head_mask=head_mask,
-        use_cache=use_cache,
-        output_attentions=output_attentions,
-    )
-    attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)
-    outputs = attn_outputs[1:]
-
-    feed_forward_hidden_states = self.mlp(hidden_states)
-    hidden_states = torch.ops.torch_ipex.tpp_linear_add_add(
-        feed_forward_hidden_states,
-        attn_output,
-        residual,
-        self.mlp.fc_out.weight,
-        self.mlp.fc_out.bias,
-        1.0,
-    )
-
-    if use_cache:
-        outputs = (hidden_states,) + outputs
-    else:
-        outputs = (hidden_states,) + outputs[1:]
-
-    return outputs  # hidden_states, present, (attentions)
-
-
-def Apply_TPP_optimization(model, dtype, distributed=False):
-    def convert_forward(m, target_m, new_forward):
-        for _, sub_m in m.named_children():
-            if isinstance(sub_m, target_m):
-                bound_method = new_forward.__get__(sub_m, sub_m.__class__)
-                setattr(sub_m, "forward", bound_method)
-            convert_forward(sub_m, target_m, new_forward)
-
-    import warnings
-
-    warnings.warn(
-        "Apply_TPP_optimization is a temp API, will be removed soon, please check the NEW ipex.optimze_transformers API"
-    )
-    from intel_extension_for_pytorch.cpu._auto_kernel_selection import _enable_tpp
-
-    _enable_tpp()
-    try:
-        # tpp rope optimization has transformers version requirements
-        import pkg_resources
-
-        installed_pkg = {pkg.key for pkg in pkg_resources.working_set}
-        min_version = "4.28.0"
-        max_version = "4.30.0"
-        if "transformers" not in installed_pkg:
-            raise RuntimeError(
-                "tpp rope optimization requires transformers package and its version between {} and {}, fallback due to not meet".format(
-                    min_version, max_version
-                )
-            )
-
-        import transformers
-        from packaging import version
-
-        trans_version = transformers.__version__
-        if version.parse(trans_version) < version.parse(min_version) or version.parse(
-            trans_version
-        ) > version.parse(max_version):
-            raise RuntimeError(
-                "tpp rope optimization requires the transformers with version: between {} and {} while now transformers== {}, fallback due to not meet".format(
-                    min_version, max_version, trans_version
-                )
-            )
-
-        if not distributed:
-            convert_forward(
-                model,
-                transformers.models.gptj.modeling_gptj.GPTJBlock,
-                GPTJBlock_forward,
-            )
-            convert_forward(
-                model, transformers.models.gptj.modeling_gptj.GPTJMLP, GPTJMLP_forward
-            )
-        else:
-            convert_forward(
-                model,
-                transformers.models.gptj.modeling_gptj.GPTJMLP,
-                GPTJMLP_forward_distributed,
-            )
-
-    except RuntimeError:
-        pass
diff --git a/intel_extension_for_pytorch/cpu/tpp/utils/blocked_layout.py b/intel_extension_for_pytorch/cpu/tpp/utils/blocked_layout.py
index 4d029ea1..1141d9e4 100644
--- a/intel_extension_for_pytorch/cpu/tpp/utils/blocked_layout.py
+++ b/intel_extension_for_pytorch/cpu/tpp/utils/blocked_layout.py
@@ -377,7 +377,7 @@ class TestModule(BlockedModule):
     def __init__(self):
         super(BlockedModule, self).__init__()
         self.param1 = BlockedParameter(torch.arange(10.0))
-        self.param1.set_blocking_param(
+        self.param1.set_blcoking_param(
             (
                 [5],
                 [1, 0],
diff --git a/intel_extension_for_pytorch/cpu/transformers/__init__.py b/intel_extension_for_pytorch/cpu/transformers/__init__.py
deleted file mode 100644
index 43464e70..00000000
--- a/intel_extension_for_pytorch/cpu/transformers/__init__.py
+++ /dev/null
@@ -1,2 +0,0 @@
-from .optimize import _optimize_transformers
-from .optimize import _set_optimized_model_for_generation
diff --git a/intel_extension_for_pytorch/cpu/transformers/attentions.py b/intel_extension_for_pytorch/cpu/transformers/attentions.py
deleted file mode 100644
index 59dcc7e0..00000000
--- a/intel_extension_for_pytorch/cpu/transformers/attentions.py
+++ /dev/null
@@ -1,1151 +0,0 @@
-import torch
-from torch import nn
-from typing import Optional, Tuple, Union
-import math
-from transformers.models.llama.configuration_llama import LlamaConfig
-
-
-def _make_causal_mask(
-    input_ids_shape: torch.Size,
-    dtype: torch.dtype,
-    device: torch.device,
-    past_key_values_length: int = 0,
-):
-    """
-    Make causal mask used for bi-directional self-attention.
-    """
-    bsz, tgt_len = input_ids_shape
-    mask = torch.full(
-        (tgt_len, tgt_len),
-        torch.tensor(torch.finfo(dtype).min, device=device),
-        device=device,
-    )
-    mask_cond = torch.arange(mask.size(-1), device=device)
-    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)
-    mask = mask.to(dtype)
-
-    if past_key_values_length > 0:
-        mask = torch.cat(
-            [
-                torch.zeros(
-                    tgt_len, past_key_values_length, dtype=dtype, device=device
-                ),
-                mask,
-            ],
-            dim=-1,
-        )
-    return mask[None, None, :, :].expand(
-        bsz, 1, tgt_len, tgt_len + past_key_values_length
-    )
-
-
-def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):
-    """
-    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.
-    """
-    bsz, src_len = mask.size()
-    tgt_len = tgt_len if tgt_len is not None else src_len
-
-    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
-
-    inverted_mask = 1.0 - expanded_mask
-
-    return inverted_mask.masked_fill(
-        inverted_mask.to(torch.bool), torch.finfo(dtype).min
-    )
-
-
-def _prepare_decoder_attention_mask(
-    self, attention_mask, input_shape, inputs_embeds, past_key_values_length
-):
-    # create causal mask
-    # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
-    combined_attention_mask = None
-    if input_shape[-1] > 1:
-        combined_attention_mask = _make_causal_mask(
-            input_shape,
-            inputs_embeds.dtype,
-            device=inputs_embeds.device,
-            past_key_values_length=past_key_values_length,
-        )
-
-    if attention_mask is not None:
-        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
-        expanded_attn_mask = _expand_mask(
-            attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]
-        ).to(inputs_embeds.device)
-        combined_attention_mask = (
-            expanded_attn_mask
-            if combined_attention_mask is None
-            else torch.tensor(expanded_attn_mask)
-            + torch.tensor(combined_attention_mask)
-        )
-
-    return combined_attention_mask
-
-
-def create_sinusoidal_positions(num_pos: int, dim: int) -> torch.Tensor:
-    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2) / dim))
-    sinusoid_inp = torch.einsum(
-        "i , j -> i j", torch.arange(num_pos, dtype=torch.float), inv_freq
-    ).float()
-    return torch.cat((torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)), dim=1)
-
-
-class _GPTJAttention(nn.Module):
-    def __init__(self, module, config):
-        super().__init__()
-
-        max_positions = config.max_position_embeddings
-        self.register_buffer(
-            "bias",
-            torch.tril(
-                torch.ones((max_positions, max_positions), dtype=torch.bool)
-            ).view(1, 1, max_positions, max_positions),
-        )
-        self.register_buffer("masked_bias", torch.tensor(-1e9))
-
-        self.attn_dropout = nn.Dropout(config.attn_pdrop)
-        self.resid_dropout = nn.Dropout(config.resid_pdrop)
-
-        self.embed_dim = module.embed_dim
-        self.num_attention_heads = module.num_attention_heads
-        self.head_dim = self.embed_dim // self.num_attention_heads
-        if self.head_dim * self.num_attention_heads != self.embed_dim:
-            raise ValueError(
-                f"embed_dim must be divisible by num_attention_heads (got `embed_dim`: {self.embed_dim} and"
-                f" `num_attention_heads`: {self.num_attention_heads})."
-            )
-        self.scale_attn = torch.sqrt(
-            torch.tensor(self.head_dim, dtype=torch.float32)
-        ).to(torch.get_default_dtype())
-
-        self.enable_concat_linear = getattr(config, "weight_only_quantization", False)
-        self.k_proj = module.k_proj
-        self.v_proj = module.v_proj
-        self.q_proj = module.q_proj
-
-        if self.enable_concat_linear:
-            weights = torch.cat(
-                [self.q_proj.weight, self.k_proj.weight, self.v_proj.weight], dim=0
-            )
-            if self.q_proj.bias is not None:
-                biases = torch.cat(
-                    [self.q_proj.bias, self.k_proj.bias, self.v_proj.bias], dim=0
-                )
-                self.concat_qkv = torch.nn.Linear(
-                    weights.shape[0], weights.shape[1], bias=True
-                )
-                self.concat_qkv.bias = torch.nn.Parameter(biases)
-            else:
-                self.concat_qkv = torch.nn.Linear(
-                    weights.shape[0], weights.shape[1], bias=False
-                )
-            self.concat_qkv.weight = torch.nn.Parameter(weights)
-            setattr(self.concat_qkv, "_num_concats", 3)
-
-        self.out_proj = module.out_proj
-        self.rotary_dim = module.rotary_dim
-        pos_embd_dim = self.rotary_dim or self.embed_dim
-        self.embed_positions = create_sinusoidal_positions(max_positions, pos_embd_dim)
-        self.text_max_length = (
-            config.text_max_length if hasattr(config, "text_max_length") else 2048
-        )
-
-    def _split_heads(self, tensor, num_attention_heads, attn_head_size, rotary):
-        """
-        Splits hidden dim into attn_head_size and num_attention_heads
-        """
-        new_shape = tensor.size()[:-1] + (num_attention_heads, attn_head_size)
-        tensor = tensor.view(new_shape)
-        if rotary:
-            return tensor
-        if len(tensor.shape) == 5:
-            return tensor.permute(
-                0, 1, 3, 2, 4
-            )  # (batch, blocks, head, block_length, head_features)
-        elif len(tensor.shape) == 4:
-            return tensor.permute(
-                0, 2, 1, 3
-            )  # (batch, head, seq_length, head_features)
-        else:
-            raise ValueError(
-                f"Input tensor rank should be one of [4, 5], but is: {len(tensor.shape)}"
-            )
-
-    def _merge_heads(self, tensor, num_attention_heads, attn_head_size):
-        """
-        Merges attn_head_size dim and num_attn_heads dim into hidden dim
-        """
-        if len(tensor.shape) == 5:
-            tensor = tensor.permute(0, 1, 3, 2, 4).contiguous()
-        elif len(tensor.shape) == 4:
-            tensor = tensor.permute(0, 2, 1, 3).contiguous()
-        else:
-            raise ValueError(
-                f"Input tensor rank should be one of [4, 5], but is: {len(tensor.shape)}"
-            )
-        new_shape = tensor.size()[:-2] + (num_attention_heads * attn_head_size,)
-        return tensor.view(new_shape)
-
-    def _attn(
-        self,
-        query,
-        key,
-        value,
-        attention_mask=None,
-        head_mask=None,
-    ):
-        # compute causal mask from causal mask buffer
-        query_length, key_length = query.size(-2), key.size(-2)
-        causal_mask = self.bias[
-            :, :, key_length - query_length : key_length, :key_length
-        ]
-
-        attn_weights = torch.matmul(query, key.transpose(-1, -2))
-
-        mask_value = torch.finfo(attn_weights.dtype).min
-        # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.
-        # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`
-        mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(
-            attn_weights.device
-        )
-        attn_weights = torch.where(causal_mask, attn_weights, mask_value)
-
-        attn_weights = attn_weights / self.scale_attn
-
-        if attention_mask is not None:
-            # Apply the attention mask
-            attn_weights = attn_weights + attention_mask
-
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
-        attn_weights = attn_weights.to(value.dtype)
-        attn_weights = self.attn_dropout(attn_weights)
-
-        # Mask heads if we want to
-        if head_mask is not None:
-            attn_weights = attn_weights * head_mask
-
-        attn_output = torch.matmul(attn_weights, value)
-
-        return attn_output, attn_weights
-
-    def _get_embed_positions(self, position_ids):
-        embed_positions = self.embed_positions
-        if embed_positions.device != position_ids.device:
-            embed_positions = embed_positions.to(position_ids.device)
-            self.embed_positions = embed_positions
-        return embed_positions.repeat(position_ids.shape[0], 1, 1)
-
-    def forward(
-        self,
-        hidden_states: torch.FloatTensor,
-        layer_past: Optional[Tuple[torch.Tensor]] = None,
-        attention_mask: Optional[torch.FloatTensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        head_mask: Optional[torch.FloatTensor] = None,
-        use_cache: Optional[bool] = False,
-        output_attentions: Optional[bool] = False,
-    ) -> Union[
-        Tuple[torch.Tensor, Tuple[torch.Tensor]],
-        Optional[Tuple[torch.Tensor, Tuple[torch.Tensor], Tuple[torch.Tensor, ...]]],
-    ]:
-        if self.enable_concat_linear:
-            num_concats = getattr(self.concat_qkv, "_num_concats")
-            assert num_concats == 3
-            qkv_output = self.concat_qkv(hidden_states)
-            hidden_size = qkv_output.shape[-1] // num_concats
-            qkv = qkv_output.view(num_concats, -1, hidden_size)
-            expected_shape = list(hidden_states.shape)[:-1] + [hidden_size]
-            query, key, value = (
-                qkv[0].view(expected_shape),
-                qkv[1].view(expected_shape),
-                qkv[2].view(expected_shape),
-            )
-        else:
-            query = self.q_proj(hidden_states)
-            key = self.k_proj(hidden_states)
-            value = self.v_proj(hidden_states)
-
-        query = self._split_heads(query, self.num_attention_heads, self.head_dim, True)
-        key = self._split_heads(key, self.num_attention_heads, self.head_dim, True)
-
-        position_ids = position_ids.contiguous()
-        torch.ops.torch_ipex.rotary_position_embedding(
-            key,
-            self.embed_positions,
-            position_ids,
-            self.num_attention_heads,
-            self.head_dim,
-            1,  # neighbor elements
-            64,
-        )
-        torch.ops.torch_ipex.rotary_position_embedding(
-            query,
-            self.embed_positions,
-            position_ids,
-            self.num_attention_heads,
-            self.head_dim,
-            1,
-            64,
-        )
-        if use_cache:
-            value = self._split_heads(
-                value, self.num_attention_heads, self.head_dim, True
-            )
-            key_cache = layer_past[0].contiguous()
-            value_cache = layer_past[1].contiguous()
-            beam_idx = layer_past[2].contiguous()
-            decoded_tokens = layer_past[3].contiguous()[0]
-            (
-                attn_output,
-                attn_weights,
-                key_cache,
-                value_cache,
-                beam_idx,
-            ) = torch.ops.torch_ipex.masked_multihead_self_attention(
-                query,
-                key,
-                value,
-                key_cache,
-                value_cache,
-                beam_idx,
-                decoded_tokens,
-                self.scale_attn,
-                self.text_max_length,
-                head_mask,
-                attention_mask,
-            )
-            present = (
-                key_cache,
-                value_cache,
-                beam_idx,
-                torch.tensor(layer_past[3] + query.shape[1], dtype=torch.long),
-            )
-        else:
-            key = key.permute(0, 2, 1, 3)
-            query = query.permute(0, 2, 1, 3)
-            value = self._split_heads(
-                value, self.num_attention_heads, self.head_dim, False
-            )
-            present = None
-            # compute self-attention: V x Softmax(QK^T)
-            attn_output, attn_weights = self._attn(
-                query, key, value, attention_mask, head_mask
-            )
-        attn_output = self._merge_heads(
-            attn_output, self.num_attention_heads, self.head_dim
-        )
-        attn_output = self.out_proj(attn_output)
-        attn_output = self.resid_dropout(attn_output)
-
-        outputs = (attn_output, present)
-        if output_attentions:
-            outputs += (attn_weights,)
-
-        return outputs  # a, present, (attentions)
-
-
-def _LlamaRMSNorm_forward(self, hidden_states):
-    variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)
-    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
-
-    # convert into half-precision if necessary
-    if self.weight.dtype in [torch.float16, torch.bfloat16]:
-        hidden_states = hidden_states.to(self.weight.dtype)
-
-    return self.weight * hidden_states
-
-
-class _LlamaAttention(nn.Module):
-    """Multi-headed attention from 'Attention Is All You Need' paper"""
-
-    def __init__(self, module, config: LlamaConfig):
-        super().__init__()
-        self.config = config
-        self.hidden_size = module.hidden_size
-        self.num_heads = module.num_heads
-        self.head_dim = self.hidden_size // self.num_heads
-        self.max_position_embeddings = self.config.max_position_embeddings
-
-        if (self.head_dim * self.num_heads) != self.hidden_size:
-            raise ValueError(
-                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
-                f" and `num_heads`: {self.num_heads})."
-            )
-        self.q_proj = module.q_proj
-        self.k_proj = module.k_proj
-        self.v_proj = module.v_proj
-        self.o_proj = module.o_proj
-        self.rotary_emb = create_sinusoidal_positions(
-            self.max_position_embeddings, self.head_dim
-        )
-        self.text_max_length = (
-            self.config.text_max_length
-            if hasattr(self.config, "text_max_length")
-            else 2048
-        )
-
-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
-        return (
-            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)
-            .transpose(1, 2)
-            .contiguous()
-        )
-
-    def forward(
-        self,
-        hidden_states: torch.Tensor,
-        attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        output_attentions: bool = False,
-        use_cache: bool = False,
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-        bsz, q_len, _ = hidden_states.size()
-        query = self.q_proj(hidden_states).view(
-            bsz, q_len, self.num_heads, self.head_dim
-        )
-        key = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim)
-        value = self.v_proj(hidden_states).view(
-            bsz, q_len, self.num_heads, self.head_dim
-        )
-        position_ids = position_ids.contiguous()
-        key = key.contiguous()
-        query = query.contiguous()
-        torch.ops.torch_ipex.rotary_position_embedding(
-            key,
-            self.rotary_emb,
-            position_ids,
-            self.num_heads,
-            self.head_dim,
-            self.head_dim // 2,
-            self.head_dim,
-        )
-        torch.ops.torch_ipex.rotary_position_embedding(
-            query,
-            self.rotary_emb,
-            position_ids,
-            self.num_heads,
-            self.head_dim,
-            self.head_dim // 2,
-            self.head_dim,
-        )
-
-        if use_cache:
-            if past_key_value is None:
-                beam_idx_tmp = torch.zeros(
-                    (2048, int(4)), dtype=torch.long
-                ).contiguous()
-                past_key_value = tuple(
-                    [
-                        torch.zeros([1, 32, 1, 128]).contiguous(),
-                        torch.zeros([1, 32, 1, 128]).contiguous(),
-                        beam_idx_tmp,
-                        torch.zeros(1, dtype=torch.long).contiguous(),
-                    ]
-                )
-            key_cache = past_key_value[0].contiguous()
-            value_cache = past_key_value[1].contiguous()
-            beam_idx = past_key_value[2].contiguous()
-            decoded_tokens = past_key_value[3].contiguous()[0]
-            (
-                attn_output,
-                attn_weights,
-                key_cache,
-                value_cache,
-                beam_idx,
-            ) = torch.ops.torch_ipex.masked_multihead_self_attention(
-                query,
-                key,
-                value,
-                key_cache,
-                value_cache,
-                beam_idx,
-                decoded_tokens,
-                math.sqrt(self.head_dim),
-                self.text_max_length,
-                None,
-                attention_mask,
-            )
-            past_key_value = (
-                key_cache,
-                value_cache,
-                beam_idx,
-                torch.tensor(past_key_value[3] + query.shape[1], dtype=torch.long),
-            )
-        else:
-            value_states = value.transpose(1, 2)
-            query_states = query.transpose(1, 2)
-            key_states = key.transpose(1, 2)
-            kv_seq_len = key_states.shape[-2]
-
-            past_key_value = None
-
-            attn_weights = torch.matmul(
-                query_states, key_states.transpose(2, 3)
-            ) / math.sqrt(self.head_dim)
-
-            if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
-                raise ValueError(
-                    f"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is"
-                    f" {attn_weights.size()}"
-                )
-
-            if attention_mask is not None:
-                if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
-                    raise ValueError(
-                        f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
-                    )
-                attn_weights = torch.tensor(attn_weights) + torch.tensor(attention_mask)
-                attn_weights = torch.max(
-                    attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)
-                )
-
-            # upcast attention to fp32
-            attn_weights = nn.functional.softmax(
-                attn_weights, dim=-1, dtype=torch.float32
-            ).to(query_states.dtype)
-            attn_output = torch.matmul(attn_weights, value_states)
-
-            if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
-                raise ValueError(
-                    f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
-                    f" {attn_output.size()}"
-                )
-
-        attn_output = attn_output.transpose(1, 2)
-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
-
-        attn_output = self.o_proj(attn_output)
-
-        if not output_attentions:
-            attn_weights = None
-
-        return attn_output, attn_weights, past_key_value
-
-
-class _LlamaAttention_GQA(nn.Module):
-    """Multi-headed attention from 'Attention Is All You Need' paper"""
-
-    def __init__(self, module, config: LlamaConfig):
-        super().__init__()
-        self.config = config
-        self.hidden_size = module.hidden_size
-        self.num_heads = module.num_heads
-        if self.config.num_key_value_heads == self.config.num_attention_heads:
-            self.num_key_value_heads = self.num_heads
-        else:
-            if hasattr(module, "num_key_value_heads"):
-                if module.num_key_value_heads != self.config.num_key_value_heads:
-                    self.num_key_value_heads = module.num_key_value_heads
-                else:  # workaround here as deepspeed does not support llama2 GQA autoTP, will remove once it supports
-                    self.num_key_value_heads = self.config.num_key_value_heads // (
-                        self.config.num_attention_heads // module.num_heads
-                    )
-                    if self.num_key_value_heads < 1:
-                        assert (
-                            False
-                        ), "Does not support Tensor parallel in this num_key_value_heads < 1 case, please reach out deepspeed's support"
-
-            else:
-                assert (
-                    False
-                ), "Your transformers version does not support LLaMA2 GQA feature, plese upgrade"
-
-        self.head_dim = self.hidden_size // self.num_heads
-        self.max_position_embeddings = self.config.max_position_embeddings
-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
-        if (self.head_dim * self.num_heads) != self.hidden_size:
-            raise ValueError(
-                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
-                f" and `num_heads`: {self.num_heads})."
-            )
-        self.q_proj = module.q_proj
-        self.k_proj = nn.Linear(
-            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
-        )
-        self.k_proj.weight = module.k_proj.weight
-        self.v_proj = nn.Linear(
-            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
-        )
-        self.v_proj.weight = module.v_proj.weight
-        self.o_proj = module.o_proj
-        self.rotary_emb = create_sinusoidal_positions(
-            self.max_position_embeddings, self.head_dim
-        )
-        self.text_max_length = (
-            self.config.text_max_length
-            if hasattr(self.config, "text_max_length")
-            else 2048
-        )
-
-    def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
-        """
-        This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
-        num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
-        """
-        batch, num_key_value_heads, slen, head_dim = hidden_states.shape
-        if n_rep == 1:
-            return hidden_states
-        hidden_states = hidden_states[:, :, None, :, :].expand(
-            batch, num_key_value_heads, n_rep, slen, head_dim
-        )
-        return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
-
-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
-        return (
-            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)
-            .transpose(1, 2)
-            .contiguous()
-        )
-
-    def forward(
-        self,
-        hidden_states: torch.Tensor,
-        attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        output_attentions: bool = False,
-        use_cache: bool = False,
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-        bsz, q_len, _ = hidden_states.size()
-        query = self.q_proj(hidden_states).view(
-            bsz, q_len, self.num_heads, self.head_dim
-        )
-        key = self.k_proj(hidden_states).view(
-            bsz, q_len, self.num_key_value_heads, self.head_dim
-        )
-        value = self.v_proj(hidden_states).view(
-            bsz, q_len, self.num_key_value_heads, self.head_dim
-        )
-        position_ids = position_ids.contiguous()
-        key = key.contiguous()
-        query = query.contiguous()
-        torch.ops.torch_ipex.rotary_position_embedding(
-            key,
-            self.rotary_emb,
-            position_ids,
-            self.num_key_value_heads,
-            self.head_dim,
-            self.head_dim // 2,
-            self.head_dim,
-        )
-        torch.ops.torch_ipex.rotary_position_embedding(
-            query,
-            self.rotary_emb,
-            position_ids,
-            self.num_heads,
-            self.head_dim,
-            self.head_dim // 2,
-            self.head_dim,
-        )
-
-        if use_cache:
-            if past_key_value is None:
-                beam_idx_tmp = torch.zeros(
-                    (2048, int(4)), dtype=torch.long
-                ).contiguous()
-                past_key_value = tuple(
-                    [
-                        torch.zeros([1, 32, 1, 128]).contiguous(),
-                        torch.zeros([1, 32, 1, 128]).contiguous(),
-                        beam_idx_tmp,
-                        torch.zeros(1, dtype=torch.long).contiguous(),
-                    ]
-                )
-            key_cache = past_key_value[0].contiguous()
-            value_cache = past_key_value[1].contiguous()
-            beam_idx = past_key_value[2].contiguous()
-            decoded_tokens = past_key_value[3].contiguous()[0]
-            (
-                attn_output,
-                attn_weights,
-                key_cache,
-                value_cache,
-                beam_idx,
-            ) = torch.ops.torch_ipex.masked_multihead_self_attention(
-                query,
-                key,
-                value,
-                key_cache,
-                value_cache,
-                beam_idx,
-                decoded_tokens,
-                math.sqrt(self.head_dim),
-                self.text_max_length,
-                None,
-                attention_mask,
-            )
-            past_key_value = (
-                key_cache,
-                value_cache,
-                beam_idx,
-                torch.tensor(past_key_value[3] + query.shape[1], dtype=torch.long),
-            )
-        else:
-            value_states = value.transpose(1, 2)
-            query_states = query.transpose(1, 2)
-            key_states = key.transpose(1, 2)
-            kv_seq_len = key_states.shape[-2]
-
-            past_key_value = None
-            # repeat k/v heads if n_kv_heads < n_heads
-            key_states = self.repeat_kv(key_states, self.num_key_value_groups)
-            value_states = self.repeat_kv(value_states, self.num_key_value_groups)
-            attn_weights = torch.matmul(
-                query_states, key_states.transpose(2, 3)
-            ) / math.sqrt(self.head_dim)
-
-            if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
-                raise ValueError(
-                    f"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is"
-                    f" {attn_weights.size()}"
-                )
-
-            if attention_mask is not None:
-                if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
-                    raise ValueError(
-                        f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
-                    )
-                attn_weights = torch.tensor(attn_weights) + torch.tensor(attention_mask)
-                attn_weights = torch.max(
-                    attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)
-                )
-
-            # upcast attention to fp32
-            attn_weights = nn.functional.softmax(
-                attn_weights, dim=-1, dtype=torch.float32
-            ).to(query_states.dtype)
-            attn_output = torch.matmul(attn_weights, value_states)
-
-            if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
-                raise ValueError(
-                    f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
-                    f" {attn_output.size()}"
-                )
-
-        attn_output = attn_output.transpose(1, 2)
-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
-
-        attn_output = self.o_proj(attn_output)
-
-        if not output_attentions:
-            attn_weights = None
-
-        return attn_output, attn_weights, past_key_value
-
-
-class RotaryEmbedding(torch.nn.Module):
-    def __init__(self, dim, max_position_embeddings, base=10000, device=None):
-        super().__init__()
-        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))
-        self.register_buffer("inv_freq", inv_freq)
-
-        # Build here to make `torch.jit.trace` work.
-        self.max_seq_len_cached = max_position_embeddings
-        t = torch.arange(
-            self.max_seq_len_cached,
-            device=self.inv_freq.device,
-            dtype=self.inv_freq.dtype,
-        )
-        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
-        # Different from paper, but it uses a different permutation in order to obtain the same calculation
-        emb = torch.cat((freqs, freqs), dim=-1)
-        self.cos_cached = emb.cos()[None, None, :, :]
-        self.sin_cached = emb.sin()[None, None, :, :]
-        self.sin_cos = torch.cat((torch.sin(freqs), torch.cos(freqs)), dim=1)
-
-    def forward(self, x, seq_len=None):
-        # x: [bs, num_attention_heads, seq_len, head_size]
-        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.
-        if seq_len > self.max_seq_len_cached:
-            self.max_seq_len_cached = seq_len
-            t = torch.arange(
-                self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype
-            )
-            freqs = torch.einsum("i,j->ij", t, self.inv_freq)
-            # Different from paper, but it uses a different permutation in order to obtain the same calculation
-            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
-            self.cos_cached = emb.cos()[None, None, :, :]
-            self.sin_cached = emb.sin()[None, None, :, :]
-            self.sin_cos = torch.cat((torch.sin(freqs), torch.cos(freqs)), dim=1)
-        return self.sin_cos
-
-
-class _GPTNeoXAttention(nn.Module):
-    def __init__(self, module, config):
-        super().__init__()
-        self.num_attention_heads = module.num_attention_heads
-        self.hidden_size = module.hidden_size
-        self.head_size = self.hidden_size // self.num_attention_heads
-        self.rotary_ndims = int(self.head_size * config.rotary_pct)
-        max_positions = config.max_position_embeddings
-        self.register_buffer(
-            "bias",
-            torch.tril(
-                torch.ones((max_positions, max_positions), dtype=torch.bool)
-            ).view(1, 1, max_positions, max_positions),
-        )
-        self.register_buffer("masked_bias", torch.tensor(-1e9))
-        self.rotary_emb = RotaryEmbedding(
-            self.rotary_ndims,
-            config.max_position_embeddings,
-            base=config.rotary_emb_base,
-        )
-        self.norm_factor = torch.sqrt(
-            torch.tensor(self.head_size, dtype=torch.float32)
-        ).to(torch.get_default_dtype())
-        self.query_key_value = module.query_key_value
-        self.dense = module.dense
-        self.text_max_length = (
-            config.text_max_length if hasattr(config, "text_max_length") else 2048
-        )
-
-    def _split_heads(cls, tensor, num_attention_heads, attn_head_size):
-        """
-        Splits hidden dim into attn_head_size and num_attention_heads
-        """
-        # tensor: [bs, seq_len, hidden_size]
-        new_shape = tensor.size()[:-1] + (num_attention_heads, attn_head_size)
-        # -> [bs, seq_len, num_attention_heads, attn_head_size]
-        tensor = tensor.view(new_shape)
-        # -> [bs, num_attention_heads, seq_len, attn_head_size]
-        tensor = tensor.permute(0, 2, 1, 3)
-        return tensor
-
-    def _merge_heads(cls, tensor, num_attention_heads, attn_head_size):
-        """
-        Merges attn_head_size dim and num_attn_heads dim into hidden dim
-        """
-        # tensor [bs, num_attention_heads, seq_len, attn_head_size]
-        tensor = tensor.permute(0, 2, 1, 3).contiguous()
-        # -> [bs, seq_len, num_attention_heads, attn_head_size]
-        tensor = tensor.view(
-            tensor.size(0), tensor.size(1), num_attention_heads * attn_head_size
-        )
-        # -> [bs, seq_len, hidden_size]
-        return tensor
-
-    def _attn(self, query, key, value, attention_mask=None, head_mask=None):
-        # q, k, v: [bs, num_attention_heads, seq_len, attn_head_size]
-        # compute causal mask from causal mask buffer
-        batch_size, num_attention_heads, query_length, attn_head_size = query.size()
-        key_length = key.size(-2)
-
-        causal_mask = self.bias[
-            :, :, key_length - query_length : key_length, :key_length
-        ]
-
-        query = query.view(
-            batch_size * num_attention_heads, query_length, attn_head_size
-        )
-        key = key.view(batch_size * num_attention_heads, key_length, attn_head_size)
-        attn_scores = torch.zeros(
-            batch_size * num_attention_heads,
-            query_length,
-            key_length,
-            dtype=query.dtype,
-            device=key.device,
-        )
-        attn_scores = torch.baddbmm(
-            attn_scores,
-            query,
-            key.transpose(1, 2),
-            beta=1.0,
-            alpha=(
-                torch.tensor(
-                    1.0, dtype=self.norm_factor.dtype, device=self.norm_factor.device
-                )
-                / self.norm_factor
-            ),
-        )
-        attn_scores = attn_scores.view(
-            batch_size, num_attention_heads, query_length, key_length
-        )
-
-        mask_value = torch.finfo(attn_scores.dtype).min
-        # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.
-        # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`
-        mask_value = torch.tensor(mask_value, dtype=attn_scores.dtype).to(
-            attn_scores.device
-        )
-        attn_scores = torch.where(causal_mask, attn_scores, mask_value)
-
-        if attention_mask is not None:
-            # Apply the attention mask
-            attn_scores = attn_scores + attention_mask
-
-        attn_weights = nn.functional.softmax(attn_scores, dim=-1)
-        attn_weights = attn_weights.to(value.dtype)
-
-        # Mask heads if we want to
-        if head_mask is not None:
-            attn_weights = attn_weights * head_mask
-
-        attn_output = torch.matmul(attn_weights, value)
-        return attn_output, attn_weights
-
-    def forward(
-        self,
-        hidden_states: torch.FloatTensor,
-        attention_mask: torch.FloatTensor,
-        position_ids: torch.LongTensor,
-        head_mask: Optional[torch.FloatTensor] = None,
-        layer_past: Optional[Tuple[torch.Tensor]] = None,
-        use_cache: Optional[bool] = False,
-        output_attentions: Optional[bool] = False,
-    ):
-        has_layer_past = layer_past is not None
-
-        # Compute QKV
-        # Attention heads [batch, seq_len, hidden_size]
-        #   --> [batch, seq_len, (np * 3 * head_size)]
-        qkv = self.query_key_value(hidden_states)
-
-        # [batch, seq_len, (num_heads * 3 * head_size)]
-        #   --> [batch, seq_len, num_heads, 3 * head_size]
-        new_qkv_shape = qkv.size()[:-1] + (self.num_attention_heads, 3 * self.head_size)
-        qkv = qkv.view(*new_qkv_shape)
-
-        # [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]
-        query = qkv[..., : self.head_size]
-        key = qkv[..., self.head_size : 2 * self.head_size]
-        value = qkv[..., 2 * self.head_size :]
-        seq_len = key.shape[1]
-        if has_layer_past:
-            seq_len += layer_past[3].item()
-        sin_cos = self.rotary_emb(value, seq_len=seq_len)
-        position_ids = position_ids.contiguous()
-        key = key.contiguous()
-        query = query.contiguous()
-        torch.ops.torch_ipex.rotary_position_embedding(
-            key,
-            sin_cos,
-            position_ids,
-            self.num_attention_heads,
-            self.head_size,
-            self.rotary_ndims // 2,
-            self.rotary_ndims,
-        )
-        torch.ops.torch_ipex.rotary_position_embedding(
-            query,
-            sin_cos,
-            position_ids,
-            self.num_attention_heads,
-            self.head_size,
-            self.rotary_ndims // 2,
-            self.rotary_ndims,
-        )
-
-        if not use_cache:
-            value = value.permute(0, 2, 1, 3)
-            key = key.permute(0, 2, 1, 3)
-            query = query.permute(0, 2, 1, 3)
-            present = None
-
-            # Compute attention
-            attn_output, attn_weights = self._attn(
-                query, key, value, attention_mask, head_mask
-            )
-        else:
-            if layer_past is None:
-                layer_past = (
-                    torch.randn(1),
-                    torch.randn(1),
-                    torch.zeros(2048, 4, dtype=torch.long),
-                    torch.zeros(1, dtype=torch.long),
-                )
-            key_cache = layer_past[0].contiguous()
-            value_cache = layer_past[1].contiguous()
-            beam_idx = layer_past[2].contiguous()
-            decoded_tokens = layer_past[3].contiguous()[0]
-            (
-                attn_output,
-                attn_weights,
-                key_cache,
-                value_cache,
-                beam_idx,
-            ) = torch.ops.torch_ipex.masked_multihead_self_attention(
-                query,
-                key,
-                value,
-                key_cache,
-                value_cache,
-                beam_idx,
-                decoded_tokens,
-                self.norm_factor,
-                self.text_max_length,
-                head_mask,
-                attention_mask,
-            )
-            present = (
-                key_cache,
-                value_cache,
-                beam_idx,
-                torch.tensor(layer_past[3] + query.shape[1], dtype=torch.long),
-            )
-        attn_output = self._merge_heads(
-            attn_output, self.num_attention_heads, self.head_size
-        )
-        attn_output = self.dense(attn_output)
-
-        outputs = (attn_output, present)
-        if output_attentions:
-            outputs += (attn_weights,)
-
-        return outputs
-
-
-class _OPTAttention(nn.Module):
-    """Multi-headed attention from 'Attention Is All You Need' paper"""
-
-    def __init__(self, module, config):
-        super().__init__()
-        self.embed_dim = module.embed_dim
-        self.num_heads = module.num_heads
-        self.dropout = module.dropout
-        self.head_dim = self.embed_dim // self.num_heads
-
-        if (self.head_dim * self.num_heads) != self.embed_dim:
-            raise ValueError(
-                f"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}"
-                f" and `num_heads`: {self.num_heads})."
-            )
-        self.scaling = self.head_dim**-0.5
-        self.is_decoder = module.is_decoder
-
-        self.k_proj = module.k_proj
-        self.v_proj = module.v_proj
-        self.q_proj = module.q_proj
-        self.out_proj = module.out_proj
-        self.text_max_length = (
-            config.text_max_length if hasattr(config, "text_max_length") else 2048
-        )
-
-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
-        return (
-            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)
-            .transpose(1, 2)
-            .contiguous()
-        )
-
-    def forward(
-        self,
-        hidden_states: torch.Tensor,
-        key_value_states: Optional[torch.Tensor] = None,
-        past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        attention_mask: Optional[torch.Tensor] = None,
-        layer_head_mask: Optional[torch.Tensor] = None,
-        output_attentions: bool = False,
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-        """Input shape: Batch x Time x Channel"""
-
-        # if key_value_states are provided this layer is used as a cross-attention layer
-        # for the decoder
-        is_cross_attention = key_value_states is not None
-
-        bsz, tgt_len, _ = hidden_states.size()
-
-        if is_cross_attention and past_key_value is not None:
-            key = (
-                past_key_value[0]
-                .view(bsz, tgt_len, self.num_heads, self.head_dim)
-                .contiguous()
-            )
-            value = (
-                past_key_value[1]
-                .view(bsz, tgt_len, self.num_heads, self.head_dim)
-                .contiguous()
-            )
-        elif is_cross_attention:
-            key = (
-                self.k_proj(key_value_states)
-                .view(bsz, tgt_len, self.num_heads, self.head_dim)
-                .contiguous()
-            )
-            value = (
-                self.v_proj(key_value_states)
-                .view(bsz, tgt_len, self.num_heads, self.head_dim)
-                .contiguous()
-            )
-        else:
-            key = (
-                self.k_proj(hidden_states)
-                .view(bsz, tgt_len, self.num_heads, self.head_dim)
-                .contiguous()
-            )
-            value = (
-                self.v_proj(hidden_states)
-                .view(bsz, tgt_len, self.num_heads, self.head_dim)
-                .contiguous()
-            )
-        if past_key_value is None:
-            past_key_value = (
-                torch.randn(0),
-                torch.randn(0),
-                torch.zeros(2048, 4, dtype=torch.long),
-                torch.zeros(1, dtype=torch.long),
-            )
-        query = (
-            self.q_proj(hidden_states)
-            .view(bsz, tgt_len, self.num_heads, self.head_dim)
-            .contiguous()
-        )
-        key_cache = past_key_value[0].contiguous()
-        value_cache = past_key_value[1].contiguous()
-        beam_idx = past_key_value[2].contiguous()
-        decoded_tokens = past_key_value[3].contiguous()[0]
-        (
-            attn_output,
-            attn_weights,
-            key_cache,
-            value_cache,
-            beam_idx,
-        ) = torch.ops.torch_ipex.masked_multihead_self_attention(
-            query,
-            key,
-            value,
-            key_cache,
-            value_cache,
-            beam_idx,
-            decoded_tokens,
-            1 / self.scaling,
-            self.text_max_length,
-            layer_head_mask,
-            attention_mask,
-        )
-
-        if self.is_decoder:
-            past_key_value = (
-                key_cache,
-                value_cache,
-                beam_idx,
-                torch.tensor(past_key_value[3] + query.shape[1], dtype=torch.long),
-            )
-
-        if not output_attentions:
-            attn_weights_reshaped = None
-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)
-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):
-            raise ValueError(
-                f"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is"
-                f" {attn_output.size()}"
-            )
-        attn_output = attn_output.transpose(1, 2)
-
-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be
-        # partitioned aross GPUs when using tensor-parallelism.
-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)
-        return attn_output, attn_weights_reshaped, past_key_value
-
-
-def _reorder_cache(
-    self, past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor
-) -> Tuple[Tuple[torch.Tensor]]:
-    if len(past_key_values[0]) == 4:  # discrete kv_cache
-        for layer_past in past_key_values:
-            layer_past[2][layer_past[3] - 1] = beam_idx
-        return past_key_values
-    else:
-        return tuple(layer_past + (beam_idx,) for layer_past in past_key_values)
diff --git a/intel_extension_for_pytorch/cpu/transformers/generation.py b/intel_extension_for_pytorch/cpu/transformers/generation.py
deleted file mode 100644
index 665593ad..00000000
--- a/intel_extension_for_pytorch/cpu/transformers/generation.py
+++ /dev/null
@@ -1,789 +0,0 @@
-import torch
-from torch import nn
-import torch.distributed as dist
-import warnings
-from typing import Optional, Tuple, Union, List
-from transformers.generation.stopping_criteria import (
-    StoppingCriteriaList,
-    validate_stopping_criteria,
-)
-from transformers.generation.logits_process import LogitsProcessorList
-from transformers.generation.beam_search import BeamScorer
-from transformers.generation.streamers import BaseStreamer
-from transformers.utils import ModelOutput
-import time
-import inspect, re
-
-
-class GreedySearchDecoderOnlyOutput(ModelOutput):
-    sequences: torch.LongTensor = None
-    scores: Optional[Tuple[torch.FloatTensor]] = None
-    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
-    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
-
-
-class GreedySearchEncoderDecoderOutput(ModelOutput):
-    sequences: torch.LongTensor = None
-    scores: Optional[Tuple[torch.FloatTensor]] = None
-    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None
-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None
-    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
-    cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
-    decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
-
-
-class BeamSearchEncoderDecoderOutput(ModelOutput):
-    sequences: torch.LongTensor = None
-    sequences_scores: Optional[torch.FloatTensor] = None
-    scores: Optional[Tuple[torch.FloatTensor]] = None
-    beam_indices: Optional[torch.LongTensor] = None
-    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None
-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None
-    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
-    cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
-    decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
-
-
-class BeamSearchDecoderOnlyOutput(ModelOutput):
-    sequences: torch.LongTensor = None
-    sequences_scores: Optional[torch.FloatTensor] = None
-    scores: Optional[Tuple[torch.FloatTensor]] = None
-    beam_indices: Optional[torch.LongTensor] = None
-    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
-    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
-
-
-BeamSearchOutput = Union[BeamSearchEncoderDecoderOutput, BeamSearchDecoderOnlyOutput]
-GreedySearchOutput = Union[
-    GreedySearchEncoderDecoderOutput, GreedySearchDecoderOnlyOutput
-]
-
-
-def _extract_past_from_model_output(
-    self, outputs: ModelOutput, standardize_cache_format: bool = False
-):
-    past_key_values = None
-    # To use torch.jit.trace, the output is no longer a Dict. outputs[1] corresponds to "past_key_values"
-    if hasattr(self, "trace_graph"):
-        past_key_values = outputs[1]
-    if "past_key_values" in outputs:
-        past_key_values = outputs.past_key_values
-    elif "mems" in outputs:
-        past_key_values = outputs.mems
-    elif "past_buckets_states" in outputs:
-        past_key_values = outputs.past_buckets_states
-
-    # Bloom fix: standardizes the cache format when requested
-    if standardize_cache_format and hasattr(self, "_convert_to_standard_cache"):
-        batch_size = outputs.logits.shape[0]
-        past_key_values = self._convert_to_standard_cache(
-            past_key_values, batch_size=batch_size
-        )
-    return past_key_values
-
-
-def _beam_search(
-    self,
-    input_ids: torch.LongTensor,
-    beam_scorer: BeamScorer,
-    logits_processor: Optional[LogitsProcessorList] = None,
-    stopping_criteria: Optional[StoppingCriteriaList] = None,
-    max_length: Optional[int] = None,
-    pad_token_id: Optional[int] = None,
-    eos_token_id: Optional[Union[int, List[int]]] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    output_scores: Optional[bool] = None,
-    return_dict_in_generate: Optional[bool] = None,
-    synced_gpus: Optional[bool] = False,
-    **model_kwargs,
-) -> Union[BeamSearchOutput, torch.LongTensor]:
-    token_latency = (
-        self.config.token_latency if hasattr(self.config, "token_latency") else False
-    )
-
-    latency_list = []
-    logits_processor = (
-        logits_processor if logits_processor is not None else LogitsProcessorList()
-    )
-    stopping_criteria = (
-        stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()
-    )
-    if max_length is not None:
-        warnings.warn(
-            "`max_length` is deprecated in this function, use"
-            " `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.",
-            UserWarning,
-        )
-        stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)
-    if len(stopping_criteria) == 0:
-        warnings.warn(
-            "You don't have defined any stopping_criteria, this will likely loop forever",
-            UserWarning,
-        )
-    pad_token_id = (
-        pad_token_id
-        if pad_token_id is not None
-        else self.generation_config.pad_token_id
-    )
-    eos_token_id = (
-        eos_token_id
-        if eos_token_id is not None
-        else self.generation_config.eos_token_id
-    )
-    if isinstance(eos_token_id, int):
-        eos_token_id = [eos_token_id]
-    output_scores = (
-        output_scores
-        if output_scores is not None
-        else self.generation_config.output_scores
-    )
-    output_attentions = (
-        output_attentions
-        if output_attentions is not None
-        else self.generation_config.output_attentions
-    )
-    output_hidden_states = (
-        output_hidden_states
-        if output_hidden_states is not None
-        else self.generation_config.output_hidden_states
-    )
-    return_dict_in_generate = (
-        return_dict_in_generate
-        if return_dict_in_generate is not None
-        else self.generation_config.return_dict_in_generate
-    )
-
-    batch_size = len(beam_scorer._beam_hyps)
-    num_beams = beam_scorer.num_beams
-
-    batch_beam_size, cur_len = input_ids.shape
-
-    if num_beams * batch_size != batch_beam_size:
-        raise ValueError(
-            f"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}."
-        )
-
-    # init attention / hidden states / scores tuples
-    scores = () if (return_dict_in_generate and output_scores) else None
-    beam_indices = (
-        tuple(() for _ in range(batch_beam_size))
-        if (return_dict_in_generate and output_scores)
-        else None
-    )
-    decoder_attentions = () if (return_dict_in_generate and output_attentions) else None
-    cross_attentions = () if (return_dict_in_generate and output_attentions) else None
-    decoder_hidden_states = (
-        () if (return_dict_in_generate and output_hidden_states) else None
-    )
-
-    # if model is an encoder-decoder, retrieve encoder attention weights and hidden states
-    if return_dict_in_generate and self.config.is_encoder_decoder:
-        encoder_attentions = (
-            model_kwargs["encoder_outputs"].get("attentions")
-            if output_attentions
-            else None
-        )
-        encoder_hidden_states = (
-            model_kwargs["encoder_outputs"].get("hidden_states")
-            if output_hidden_states
-            else None
-        )
-
-    # initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens
-    # of the first beam are considered to avoid sampling the exact same tokens across all beams.
-    beam_scores = torch.zeros(
-        (batch_size, num_beams), dtype=torch.float, device=input_ids.device
-    )
-    beam_scores[:, 1:] = -1e9
-    beam_scores = beam_scores.view((batch_size * num_beams,))
-    this_peer_finished = False  # used by synced_gpus only
-    while True:
-        tic = time.time()
-        if synced_gpus:
-            # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.
-            # The following logic allows an early break if all peers finished generating their sequence
-            this_peer_finished_flag = torch.tensor(
-                0.0 if this_peer_finished else 1.0
-            ).to(input_ids.device)
-            # send 0.0 if we finished, 1.0 otherwise
-            dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)
-            # did all peers finish? the reduced sum will be 0.0 then
-            if this_peer_finished_flag.item() == 0.0:
-                break
-
-        model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
-        if (
-            re.search("GPTJ", self.config.architectures[0])
-            or re.search("llama", self.config.architectures[0], re.IGNORECASE)
-            or re.search("gptneox", self.config.architectures[0], re.IGNORECASE)
-            or re.search("OPT", self.config.architectures[0], re.IGNORECASE)
-        ):
-            first_token = False
-            input_bs = input_ids.size()[0]
-            has_position_id = True
-            if model_inputs["past_key_values"] is None:
-                first_token = True
-            if first_token:
-                if re.search("GPTJ", self.config.architectures[0]):
-                    beam_idx_tmp = torch.zeros(
-                        (2048, int(batch_size * num_beams)), dtype=torch.long
-                    ).contiguous()
-                    model_inputs["past_key_values"] = tuple(
-                        [
-                            (
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                beam_idx_tmp,
-                                torch.zeros(1, dtype=torch.long).contiguous(),
-                            )
-                            for i in range(self.config.n_layer)
-                        ]
-                    )
-                elif re.search("llama", self.config.architectures[0], re.IGNORECASE):
-                    beam_idx_tmp = torch.zeros(
-                        (2048, int(batch_size * num_beams)), dtype=torch.long
-                    ).contiguous()
-                    model_inputs["past_key_values"] = tuple(
-                        [
-                            (
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                beam_idx_tmp,
-                                torch.zeros(1, dtype=torch.long).contiguous(),
-                            )
-                            for i in range(self.config.num_hidden_layers)
-                        ]
-                    )
-                elif re.search("gptneox", self.config.architectures[0], re.IGNORECASE):
-                    beam_idx_tmp = torch.zeros(
-                        (2048, int(batch_size * num_beams)), dtype=torch.long
-                    ).contiguous()
-                    model_inputs["past_key_values"] = tuple(
-                        [
-                            (
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                beam_idx_tmp,
-                                torch.zeros(1, dtype=torch.long).contiguous(),
-                            )
-                            for i in range(self.config.num_hidden_layers)
-                        ]
-                    )
-                elif re.search("OPT", self.config.architectures[0], re.IGNORECASE):
-                    beam_idx_tmp = torch.zeros(
-                        (2048, int(batch_size * num_beams)), dtype=torch.long
-                    ).contiguous()
-                    model_inputs["past_key_values"] = tuple(
-                        [
-                            (
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                beam_idx_tmp,
-                                torch.zeros(1, dtype=torch.long).contiguous(),
-                            )
-                            for i in range(self.config.num_hidden_layers)
-                        ]
-                    )
-                    has_position_id = False
-
-            if hasattr(self, "trace_graph"):
-                if first_token:
-                    new_attention_mask = model_inputs["attention_mask"][
-                        :batch_size
-                    ].clone()
-                    new_input_ids = model_inputs["input_ids"][:batch_size].clone()
-                    if has_position_id:
-                        new_position_ids = model_inputs["position_ids"][:batch_size].clone()
-                    for i in range(batch_size):
-                        new_attention_mask[i] = model_inputs["attention_mask"][
-                            i * num_beams
-                        ]
-                        new_input_ids[i] = model_inputs["input_ids"][i * num_beams]
-                        if has_position_id:
-                            new_position_ids[i] = model_inputs["position_ids"][
-                                i * num_beams
-                            ]
-                    model_inputs["attention_mask"] = new_attention_mask
-                    model_inputs["input_ids"] = new_input_ids
-                    if has_position_id:
-                        model_inputs["position_ids"] = new_position_ids
-                model_inputs.pop("use_cache", None)
-                model_inputs.pop("token_type_ids", None)
-                if first_token and hasattr(self, "trace_graph_first"):
-                    outputs = self.trace_graph_first(**model_inputs)
-                else:
-                    outputs = self.trace_graph(**model_inputs)
-                if first_token and len(model_inputs["past_key_values"][0]) == 4:
-                    outputs = list(outputs)
-                    outputs[0] = outputs[0].repeat_interleave(num_beams, dim=0)
-                    outputs = tuple(outputs)
-                if synced_gpus and this_peer_finished:
-                    cur_len = cur_len + 1
-                    continue  # don't waste resources running the code we don't need
-                next_token_logits = outputs[0][:, -1, :]
-            else:
-                outputs = self(
-                    **model_inputs,
-                    return_dict=True,
-                    output_attentions=output_attentions,
-                    output_hidden_states=output_hidden_states,
-                )
-                if synced_gpus and this_peer_finished:
-                    cur_len = cur_len + 1
-                    continue  # don't waste resources running the code we don't need
-                next_token_logits = outputs.logits[:, -1, :]
-        else:
-            outputs = self(
-                **model_inputs,
-                return_dict=True,
-                output_attentions=output_attentions,
-                output_hidden_states=output_hidden_states,
-            )
-            if synced_gpus and this_peer_finished:
-                cur_len = cur_len + 1
-                continue  # don't waste resources running the code we don't need
-            next_token_logits = outputs.logits[:, -1, :]
-
-        # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`
-        # cannot be generated both before and after the `nn.functional.log_softmax` operation.
-        next_token_logits = self.adjust_logits_during_generation(
-            next_token_logits, cur_len=cur_len
-        )
-        next_token_scores = nn.functional.log_softmax(
-            next_token_logits, dim=-1
-        )  # (batch_size * num_beams, vocab_size)
-
-        next_token_scores_processed = logits_processor(input_ids, next_token_scores)
-        next_token_scores = next_token_scores_processed + beam_scores[
-            :, None
-        ].expand_as(next_token_scores)
-
-        # Store scores, attentions and hidden_states when required
-        if return_dict_in_generate:
-            if output_scores:
-                scores += (next_token_scores_processed,)
-            if output_attentions:
-                decoder_attentions += (
-                    (outputs.decoder_attentions,)
-                    if self.config.is_encoder_decoder
-                    else (outputs.attentions,)
-                )
-                if self.config.is_encoder_decoder:
-                    cross_attentions += (outputs.cross_attentions,)
-
-            if output_hidden_states:
-                decoder_hidden_states += (
-                    (outputs.decoder_hidden_states,)
-                    if self.config.is_encoder_decoder
-                    else (outputs.hidden_states,)
-                )
-
-        # reshape for beam search
-        vocab_size = next_token_scores.shape[-1]
-        next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)
-
-        # Sample 2 next tokens for each beam (so we have some spare tokens and match output of beam search)
-        next_token_scores, next_tokens = torch.topk(
-            next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True
-        )
-
-        next_indices = torch.div(next_tokens, vocab_size, rounding_mode="floor")
-        next_tokens = next_tokens % vocab_size
-
-        # stateless
-        beam_outputs = beam_scorer.process(
-            input_ids,
-            next_token_scores,
-            next_tokens,
-            next_indices,
-            pad_token_id=pad_token_id,
-            eos_token_id=eos_token_id,
-            beam_indices=beam_indices,
-        )
-
-        beam_scores = beam_outputs["next_beam_scores"]
-        beam_next_tokens = beam_outputs["next_beam_tokens"]
-        beam_idx = beam_outputs["next_beam_indices"]
-
-        input_ids = torch.cat(
-            [input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1
-        )
-
-        model_kwargs = self._update_model_kwargs_for_generation(
-            outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder
-        )
-        if model_kwargs["past_key_values"] is not None:
-            model_kwargs["past_key_values"] = self._reorder_cache(
-                model_kwargs["past_key_values"], beam_idx
-            )
-
-        if return_dict_in_generate and output_scores:
-            beam_indices = tuple(
-                (
-                    beam_indices[beam_idx[i]] + (beam_idx[i],)
-                    for i in range(len(beam_indices))
-                )
-            )
-
-        # increase cur_len
-        cur_len = cur_len + 1
-        latency_list.append(time.time() - tic)
-
-        if beam_scorer.is_done or stopping_criteria(input_ids, scores):
-            if not synced_gpus:
-                break
-            else:
-                this_peer_finished = True
-
-    sequence_outputs = beam_scorer.finalize(
-        input_ids,
-        beam_scores,
-        next_tokens,
-        next_indices,
-        pad_token_id=pad_token_id,
-        eos_token_id=eos_token_id,
-        max_length=stopping_criteria.max_length,
-        beam_indices=beam_indices,
-    )
-
-    if return_dict_in_generate:
-        if not output_scores:
-            sequence_outputs["sequence_scores"] = None
-
-        if self.config.is_encoder_decoder:
-            output_result = BeamSearchEncoderDecoderOutput(
-                sequences=sequence_outputs["sequences"],
-                sequences_scores=sequence_outputs["sequence_scores"],
-                scores=scores,
-                beam_indices=sequence_outputs["beam_indices"],
-                encoder_attentions=encoder_attentions,
-                encoder_hidden_states=encoder_hidden_states,
-                decoder_attentions=decoder_attentions,
-                cross_attentions=cross_attentions,
-                decoder_hidden_states=decoder_hidden_states,
-            )
-        else:
-            output_result = BeamSearchDecoderOnlyOutput(
-                sequences=sequence_outputs["sequences"],
-                sequences_scores=sequence_outputs["sequence_scores"],
-                scores=scores,
-                beam_indices=sequence_outputs["beam_indices"],
-                attentions=decoder_attentions,
-                hidden_states=decoder_hidden_states,
-            )
-    else:
-        output_result = sequence_outputs["sequences"]
-    # result
-    if token_latency:
-        return (output_result, latency_list)
-    else:
-        return output_result
-
-
-def _greedy_search(
-    self,
-    input_ids: torch.LongTensor,
-    logits_processor: Optional[LogitsProcessorList] = None,
-    stopping_criteria: Optional[StoppingCriteriaList] = None,
-    max_length: Optional[int] = None,
-    pad_token_id: Optional[int] = None,
-    eos_token_id: Optional[Union[int, List[int]]] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    output_scores: Optional[bool] = None,
-    return_dict_in_generate: Optional[bool] = None,
-    synced_gpus: Optional[bool] = False,
-    streamer: Optional["BaseStreamer"] = None,
-    **model_kwargs,
-) -> Union[GreedySearchOutput, torch.LongTensor]:
-    token_latency = (
-        self.config.token_latency if hasattr(self.config, "token_latency") else False
-    )
-
-    latency_list = []
-    logits_processor = (
-        logits_processor if logits_processor is not None else LogitsProcessorList()
-    )
-    stopping_criteria = (
-        stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()
-    )
-    if max_length is not None:
-        warnings.warn(
-            "`max_length` is deprecated in this function, use"
-            " `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.",
-            UserWarning,
-        )
-        stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)
-    pad_token_id = (
-        pad_token_id
-        if pad_token_id is not None
-        else self.generation_config.pad_token_id
-    )
-    eos_token_id = (
-        eos_token_id
-        if eos_token_id is not None
-        else self.generation_config.eos_token_id
-    )
-    if isinstance(eos_token_id, int):
-        eos_token_id = [eos_token_id]
-    eos_token_id_tensor = (
-        torch.tensor(eos_token_id).to(input_ids.device)
-        if eos_token_id is not None
-        else None
-    )
-    output_scores = (
-        output_scores
-        if output_scores is not None
-        else self.generation_config.output_scores
-    )
-    output_attentions = (
-        output_attentions
-        if output_attentions is not None
-        else self.generation_config.output_attentions
-    )
-    output_hidden_states = (
-        output_hidden_states
-        if output_hidden_states is not None
-        else self.generation_config.output_hidden_states
-    )
-    return_dict_in_generate = (
-        return_dict_in_generate
-        if return_dict_in_generate is not None
-        else self.generation_config.return_dict_in_generate
-    )
-
-    # init attention / hidden states / scores tuples
-    scores = () if (return_dict_in_generate and output_scores) else None
-    decoder_attentions = () if (return_dict_in_generate and output_attentions) else None
-    cross_attentions = () if (return_dict_in_generate and output_attentions) else None
-    decoder_hidden_states = (
-        () if (return_dict_in_generate and output_hidden_states) else None
-    )
-
-    # if model is an encoder-decoder, retrieve encoder attention weights and hidden states
-    if return_dict_in_generate and self.config.is_encoder_decoder:
-        encoder_attentions = (
-            model_kwargs["encoder_outputs"].get("attentions")
-            if output_attentions
-            else None
-        )
-        encoder_hidden_states = (
-            model_kwargs["encoder_outputs"].get("hidden_states")
-            if output_hidden_states
-            else None
-        )
-
-    # keep track of which sequences are already finished
-    unfinished_sequences = torch.ones(
-        input_ids.shape[0], dtype=torch.long, device=input_ids.device
-    )
-
-    this_peer_finished = False  # used by synced_gpus only
-    while True:
-        tic = time.time()
-        if synced_gpus:
-            # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.
-            # The following logic allows an early break if all peers finished generating their sequence
-            this_peer_finished_flag = torch.tensor(
-                0.0 if this_peer_finished else 1.0
-            ).to(input_ids.device)
-            # send 0.0 if we finished, 1.0 otherwise
-            dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)
-            # did all peers finish? the reduced sum will be 0.0 then
-            if this_peer_finished_flag.item() == 0.0:
-                break
-
-        # prepare model inputs
-        model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
-        if (
-            re.search("GPTJ", self.config.architectures[0])
-            or re.search("llama", self.config.architectures[0], re.IGNORECASE)
-            or re.search("gptneox", self.config.architectures[0], re.IGNORECASE)
-            or re.search("OPT", self.config.architectures[0], re.IGNORECASE)
-        ):
-            first_token = False
-            input_bs = input_ids.size()[0]
-            if model_inputs["past_key_values"] is None:
-                first_token = True
-            if first_token:
-                if re.search("GPTJ", self.config.architectures[0]):
-                    beam_idx_tmp = torch.zeros(
-                        (2048, int(input_bs)), dtype=torch.long
-                    ).contiguous()
-                    model_inputs["past_key_values"] = tuple(
-                        [
-                            (
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                beam_idx_tmp,
-                                torch.zeros(1, dtype=torch.long).contiguous(),
-                            )
-                            for i in range(self.config.n_layer)
-                        ]
-                    )
-                elif re.search("llama", self.config.architectures[0], re.IGNORECASE):
-                    beam_idx_tmp = torch.zeros(
-                        (2048, int(input_bs)), dtype=torch.long
-                    ).contiguous()
-                    model_inputs["past_key_values"] = tuple(
-                        [
-                            (
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                beam_idx_tmp,
-                                torch.zeros(1, dtype=torch.long).contiguous(),
-                            )
-                            for i in range(self.config.num_hidden_layers)
-                        ]
-                    )
-                elif re.search("gptneox", self.config.architectures[0], re.IGNORECASE):
-                    beam_idx_tmp = torch.zeros(
-                        (2048, int(input_bs)), dtype=torch.long
-                    ).contiguous()
-                    model_inputs["past_key_values"] = tuple(
-                        [
-                            (
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                beam_idx_tmp,
-                                torch.zeros(1, dtype=torch.long).contiguous(),
-                            )
-                            for i in range(self.config.num_hidden_layers)
-                        ]
-                    )
-                elif re.search("OPT", self.config.architectures[0], re.IGNORECASE):
-                    beam_idx_tmp = torch.zeros(
-                        (2048, int(input_bs)), dtype=torch.long
-                    ).contiguous()
-                    model_inputs["past_key_values"] = tuple(
-                        [
-                            (
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                torch.zeros([1, 1, 1, 1]).contiguous(),
-                                beam_idx_tmp,
-                                torch.zeros(1, dtype=torch.long).contiguous(),
-                            )
-                            for i in range(self.config.num_hidden_layers)
-                        ]
-                    )
-
-            if hasattr(self, "trace_graph"):
-                model_inputs.pop("use_cache", None)
-                model_inputs.pop("token_type_ids", None)
-                outputs = self.trace_graph(**model_inputs)
-                if synced_gpus and this_peer_finished:
-                    cur_len = cur_len + 1
-                    continue  # don't waste resources running the code we don't need
-                next_token_logits = outputs[0][:, -1, :]
-            else:
-                outputs = self(
-                    **model_inputs,
-                    return_dict=True,
-                    output_attentions=output_attentions,
-                    output_hidden_states=output_hidden_states,
-                )
-                if synced_gpus and this_peer_finished:
-                    cur_len = cur_len + 1
-                    continue  # don't waste resources running the code we don't need
-                next_token_logits = outputs.logits[:, -1, :]
-        else:
-            outputs = self(
-                **model_inputs,
-                return_dict=True,
-                output_attentions=output_attentions,
-                output_hidden_states=output_hidden_states,
-            )
-            if synced_gpus and this_peer_finished:
-                cur_len = cur_len + 1
-                continue  # don't waste resources running the code we don't need
-            next_token_logits = outputs.logits[:, -1, :]
-
-        # pre-process distribution
-        next_tokens_scores = logits_processor(input_ids, next_token_logits)
-
-        # Store scores, attentions and hidden_states when required
-        if return_dict_in_generate:
-            if output_scores:
-                scores += (next_tokens_scores,)
-            if output_attentions:
-                decoder_attentions += (
-                    (outputs.decoder_attentions,)
-                    if self.config.is_encoder_decoder
-                    else (outputs.attentions,)
-                )
-                if self.config.is_encoder_decoder:
-                    cross_attentions += (outputs.cross_attentions,)
-
-            if output_hidden_states:
-                decoder_hidden_states += (
-                    (outputs.decoder_hidden_states,)
-                    if self.config.is_encoder_decoder
-                    else (outputs.hidden_states,)
-                )
-
-        # argmax
-        next_tokens = torch.argmax(next_tokens_scores, dim=-1)
-
-        # finished sentences should have their next token be a padding token
-        if eos_token_id is not None:
-            if pad_token_id is None:
-                raise ValueError(
-                    "If `eos_token_id` is defined, make sure that `pad_token_id` is defined."
-                )
-            next_tokens = next_tokens * unfinished_sequences + pad_token_id * (
-                1 - unfinished_sequences
-            )
-
-        # update generated ids, model inputs, and length for next step
-        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
-        if streamer is not None:
-            streamer.put(next_tokens.cpu())
-        model_kwargs = self._update_model_kwargs_for_generation(
-            outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder
-        )
-
-        # if eos_token was found in one sentence, set sentence to finished
-        if eos_token_id_tensor is not None:
-            unfinished_sequences = unfinished_sequences.mul(
-                next_tokens.tile(eos_token_id_tensor.shape[0], 1)
-                .ne(eos_token_id_tensor.unsqueeze(1))
-                .prod(dim=0)
-            )
-
-        # stop when each sentence is finished, or if we exceed the maximum length
-        latency_list.append(time.time() - tic)
-        if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):
-            if not synced_gpus:
-                break
-            else:
-                this_peer_finished = True
-
-    if streamer is not None:
-        streamer.end()
-
-    if return_dict_in_generate:
-        if self.config.is_encoder_decoder:
-            output_result = GreedySearchEncoderDecoderOutput(
-                sequences=input_ids,
-                scores=scores,
-                encoder_attentions=encoder_attentions,
-                encoder_hidden_states=encoder_hidden_states,
-                decoder_attentions=decoder_attentions,
-                cross_attentions=cross_attentions,
-                decoder_hidden_states=decoder_hidden_states,
-            )
-        else:
-            output_result = GreedySearchDecoderOnlyOutput(
-                sequences=input_ids,
-                scores=scores,
-                attentions=decoder_attentions,
-                hidden_states=decoder_hidden_states,
-            )
-    else:
-        output_result = input_ids
-
-    if token_latency:
-        return (output_result, latency_list)
-    else:
-        return output_result
diff --git a/intel_extension_for_pytorch/cpu/transformers/models.py b/intel_extension_for_pytorch/cpu/transformers/models.py
deleted file mode 100644
index 2fab54d2..00000000
--- a/intel_extension_for_pytorch/cpu/transformers/models.py
+++ /dev/null
@@ -1,919 +0,0 @@
-import torch
-from torch import nn
-from torch.nn import CrossEntropyLoss
-from typing import Optional, Tuple, Union, List
-from transformers.modeling_outputs import (
-    BaseModelOutputWithPast,
-    CausalLMOutputWithPast,
-)
-from transformers.utils import logging
-import random
-
-logger = logging.get_logger(__name__)
-
-
-def GPTJModel_forward(
-    self,
-    input_ids: Optional[torch.LongTensor] = None,
-    attention_mask: Optional[torch.FloatTensor] = None,
-    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    token_type_ids: Optional[torch.LongTensor] = None,
-    head_mask: Optional[torch.FloatTensor] = None,
-    inputs_embeds: Optional[torch.FloatTensor] = None,
-    use_cache: Optional[bool] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    return_dict: Optional[bool] = None,
-) -> Union[Tuple, BaseModelOutputWithPast]:
-    output_attentions = (
-        output_attentions
-        if output_attentions is not None
-        else self.config.output_attentions
-    )
-    output_hidden_states = (
-        output_hidden_states
-        if output_hidden_states is not None
-        else self.config.output_hidden_states
-    )
-    use_cache = use_cache if use_cache is not None else self.config.use_cache
-    return_dict = (
-        return_dict if return_dict is not None else self.config.use_return_dict
-    )
-
-    if input_ids is not None and inputs_embeds is not None:
-        raise ValueError(
-            "You cannot specify both input_ids and inputs_embeds at the same time"
-        )
-    elif input_ids is not None:
-        input_shape = input_ids.size()
-        input_ids = input_ids.view(-1, input_shape[-1])
-        batch_size = input_ids.shape[0]
-    elif inputs_embeds is not None:
-        input_shape = inputs_embeds.size()[:-1]
-        batch_size = inputs_embeds.shape[0]
-    else:
-        raise ValueError("You have to specify either input_ids or inputs_embeds")
-
-    device = input_ids.device if input_ids is not None else inputs_embeds.device
-
-    if token_type_ids is not None:
-        token_type_ids = token_type_ids.view(-1, input_shape[-1])
-
-    if position_ids is not None:
-        position_ids = position_ids.view(-1, input_shape[-1]).long()
-
-    if past_key_values is None:
-        past_length = 0
-        past_key_values = tuple([None] * len(self.h))
-    else:
-        past_length = past_key_values[0][0].size(-2)
-        if len(past_key_values[0]) == 4:
-            past_length = past_key_values[0][3]
-
-    if position_ids is None:
-        position_ids = torch.arange(
-            past_length, input_shape[-1] + past_length, dtype=torch.long, device=device
-        )
-        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
-
-    # Attention mask.
-    if attention_mask is not None:
-        if batch_size <= 0:
-            raise ValueError("batch_size has to be defined and > 0")
-        attention_mask = attention_mask.view(batch_size, -1)
-        # We create a 3D attention mask from a 2D tensor mask.
-        # Sizes are [batch_size, 1, 1, to_seq_length]
-        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
-        # this attention mask is more simple than the triangular masking of causal attention
-        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
-        attention_mask = attention_mask[:, None, None, :]
-
-        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
-        # masked positions, this operation will create a tensor which is 0.0 for
-        # positions we want to attend and the dtype's smallest value for masked positions.
-        # Since we are adding it to the raw scores before the softmax, this is
-        # effectively the same as removing these entirely.
-        attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility
-        attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min
-
-    # Prepare head mask if needed
-    # 1.0 in head_mask indicate we keep the head
-    # attention_probs has shape bsz x num_attention_heads x N x N
-    # head_mask has shape n_layer x batch x num_attention_heads x N x N
-    head_mask = self.get_head_mask(head_mask, self.config.n_layer)
-
-    if inputs_embeds is None:
-        inputs_embeds = self.wte(input_ids)
-
-    hidden_states = inputs_embeds
-
-    if token_type_ids is not None:
-        token_type_embeds = self.wte(token_type_ids)
-        hidden_states = hidden_states + token_type_embeds
-
-    hidden_states = self.drop(hidden_states)
-
-    output_shape = input_shape + (hidden_states.size(-1),)
-
-    if self.gradient_checkpointing and self.training:
-        if use_cache:
-            logger.warning_once(
-                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
-            )
-            use_cache = False
-
-    presents = () if use_cache else None
-    all_self_attentions = () if output_attentions else None
-    all_hidden_states = () if output_hidden_states else None
-    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
-        # Model parallel
-        if self.model_parallel:
-            torch.cuda.set_device(hidden_states.device)
-            # Ensure layer_past is on same device as hidden_states (might not be correct)
-            if layer_past is not None:
-                layer_past = tuple(
-                    past_state.to(hidden_states.device) for past_state in layer_past
-                )
-            # Ensure that attention_mask is always on the same device as hidden_states
-            if attention_mask is not None:
-                attention_mask = attention_mask.to(hidden_states.device)
-            if isinstance(head_mask, torch.Tensor):
-                head_mask = head_mask.to(hidden_states.device)
-        if output_hidden_states:
-            all_hidden_states = all_hidden_states + (hidden_states,)
-
-        if self.gradient_checkpointing and self.training:
-
-            def create_custom_forward(module):
-                def custom_forward(*inputs):
-                    # None for past_key_value
-                    return module(*inputs, use_cache, output_attentions)
-
-                return custom_forward
-
-            outputs = torch.utils.checkpoint.checkpoint(
-                create_custom_forward(block),
-                hidden_states,
-                None,
-                attention_mask,
-                position_ids,
-                head_mask[i],
-            )
-        else:
-            outputs = block(
-                hidden_states=hidden_states,
-                layer_past=layer_past,
-                attention_mask=attention_mask,
-                position_ids=position_ids,
-                head_mask=head_mask[i],
-                use_cache=use_cache,
-                output_attentions=output_attentions,
-            )
-
-        hidden_states = outputs[0]
-        if use_cache is True:
-            presents = presents + (outputs[1],)
-
-        if output_attentions:
-            all_self_attentions = all_self_attentions + (
-                outputs[2 if use_cache else 1],
-            )
-
-        # Model Parallel: If it's the last layer for that device, put things on the next device
-        if self.model_parallel:
-            for k, v in self.device_map.items():
-                if i == v[-1] and "cuda:" + str(k) != self.last_device:
-                    hidden_states = hidden_states.to("cuda:" + str(k + 1))
-
-    hidden_states = self.ln_f(hidden_states)
-
-    hidden_states = hidden_states.view(output_shape)
-    # Add last hidden state
-    if output_hidden_states:
-        all_hidden_states = all_hidden_states + (hidden_states,)
-
-    if not return_dict:
-        return tuple(
-            v
-            for v in [hidden_states, presents, all_hidden_states, all_self_attentions]
-            if v is not None
-        )
-
-    return BaseModelOutputWithPast(
-        last_hidden_state=hidden_states,
-        past_key_values=presents,
-        hidden_states=all_hidden_states,
-        attentions=all_self_attentions,
-    )
-
-
-def LlamaModel_forward(
-    self,
-    input_ids: torch.LongTensor = None,
-    attention_mask: Optional[torch.Tensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    past_key_values: Optional[List[torch.FloatTensor]] = None,
-    inputs_embeds: Optional[torch.FloatTensor] = None,
-    use_cache: Optional[bool] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    return_dict: Optional[bool] = None,
-) -> Union[Tuple, BaseModelOutputWithPast]:
-    output_attentions = (
-        output_attentions
-        if output_attentions is not None
-        else self.config.output_attentions
-    )
-    output_hidden_states = (
-        output_hidden_states
-        if output_hidden_states is not None
-        else self.config.output_hidden_states
-    )
-    use_cache = use_cache if use_cache is not None else self.config.use_cache
-
-    return_dict = (
-        return_dict if return_dict is not None else self.config.use_return_dict
-    )
-
-    # retrieve input_ids and inputs_embeds
-    if input_ids is not None and inputs_embeds is not None:
-        raise ValueError(
-            "You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time"
-        )
-    elif input_ids is not None:
-        batch_size, seq_length = input_ids.shape
-    elif inputs_embeds is not None:
-        batch_size, seq_length, _ = inputs_embeds.shape
-    else:
-        raise ValueError(
-            "You have to specify either decoder_input_ids or decoder_inputs_embeds"
-        )
-    seq_length_with_past = seq_length
-    past_key_values_length = 0
-
-    if past_key_values is not None:
-        if len(past_key_values[0]) == 4:  # not discrete kv cache
-            past_key_values_length = past_key_values[0][3].item()
-            seq_length_with_past = seq_length_with_past + past_key_values_length
-        else:
-            past_key_values_length = past_key_values[0][0].shape[2]
-            seq_length_with_past = seq_length_with_past + past_key_values_length
-
-    if position_ids is None:
-        device = input_ids.device if input_ids is not None else inputs_embeds.device
-        position_ids = torch.arange(
-            past_key_values_length,
-            seq_length + past_key_values_length,
-            dtype=torch.long,
-            device=device,
-        )
-        position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
-    else:
-        position_ids = position_ids.view(-1, seq_length).long()
-
-    if inputs_embeds is None:
-        inputs_embeds = self.embed_tokens(input_ids)
-    # embed positions
-    if attention_mask is None:
-        attention_mask = torch.ones(
-            (batch_size, seq_length_with_past),
-            dtype=torch.bool,
-            device=inputs_embeds.device,
-        )
-    attention_mask = self._prepare_decoder_attention_mask(
-        attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
-    )
-
-    hidden_states = inputs_embeds
-
-    if self.gradient_checkpointing and self.training:
-        if use_cache:
-            logger.warning_once(
-                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
-            )
-            use_cache = False
-
-    # decoder layers
-    all_hidden_states = () if output_hidden_states else None
-    all_self_attns = () if output_attentions else None
-    next_decoder_cache = () if use_cache else None
-
-    for idx, decoder_layer in enumerate(self.layers):
-        if output_hidden_states:
-            all_hidden_states += (hidden_states,)
-
-        past_key_value = past_key_values[idx] if past_key_values is not None else None
-
-        if self.gradient_checkpointing and self.training:
-
-            def create_custom_forward(module):
-                def custom_forward(*inputs):
-                    # None for past_key_value
-                    return module(*inputs, output_attentions, None)
-
-                return custom_forward
-
-            layer_outputs = torch.utils.checkpoint.checkpoint(
-                create_custom_forward(decoder_layer),
-                hidden_states,
-                attention_mask,
-                position_ids,
-                None,
-            )
-        else:
-            layer_outputs = decoder_layer(
-                hidden_states,
-                attention_mask=attention_mask,
-                position_ids=position_ids,
-                past_key_value=past_key_value,
-                output_attentions=output_attentions,
-                use_cache=use_cache,
-            )
-
-        hidden_states = layer_outputs[0]
-
-        if use_cache:
-            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
-
-        if output_attentions:
-            all_self_attns += (layer_outputs[1],)
-
-    hidden_states = self.norm(hidden_states)
-
-    # add hidden states from the last decoder layer
-    if output_hidden_states:
-        all_hidden_states += (hidden_states,)
-
-    next_cache = next_decoder_cache if use_cache else None
-    if not return_dict:
-        return tuple(
-            v
-            for v in [hidden_states, next_cache, all_hidden_states, all_self_attns]
-            if v is not None
-        )
-    return BaseModelOutputWithPast(
-        last_hidden_state=hidden_states,
-        past_key_values=next_cache,
-        hidden_states=all_hidden_states,
-        attentions=all_self_attns,
-    )
-
-
-def GPTNeoXModel_forward(
-    self,
-    input_ids: Optional[torch.LongTensor] = None,
-    attention_mask: Optional[torch.FloatTensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    head_mask: Optional[torch.FloatTensor] = None,
-    inputs_embeds: Optional[torch.FloatTensor] = None,
-    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
-    use_cache: Optional[bool] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    return_dict: Optional[bool] = None,
-) -> Union[Tuple, BaseModelOutputWithPast]:
-    r"""
-    past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):
-        Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.
-        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
-        don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
-        `decoder_input_ids` of shape `(batch_size, sequence_length)`.
-    use_cache (`bool`, *optional*):
-        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
-        `past_key_values`).
-    """
-    output_attentions = (
-        output_attentions
-        if output_attentions is not None
-        else self.config.output_attentions
-    )
-    output_hidden_states = (
-        output_hidden_states
-        if output_hidden_states is not None
-        else self.config.output_hidden_states
-    )
-    return_dict = (
-        return_dict if return_dict is not None else self.config.use_return_dict
-    )
-    use_cache = use_cache if use_cache is not None else self.config.use_cache
-
-    if input_ids is not None and inputs_embeds is not None:
-        raise ValueError(
-            "You cannot specify both input_ids and inputs_embeds at the same time"
-        )
-    elif input_ids is not None:
-        input_shape = input_ids.size()
-    elif inputs_embeds is not None:
-        input_shape = inputs_embeds.size()[:-1]
-    else:
-        raise ValueError("You have to specify either input_ids or inputs_embeds")
-
-    batch_size, seq_length = input_shape
-    if past_key_values is None:
-        past_length = 0
-        past_key_values = tuple([None] * self.config.num_hidden_layers)
-    else:
-        past_length = past_key_values[0][0].size(-2)
-        if len(past_key_values[0]) == 4:
-            past_length = past_key_values[0][3]
-
-    if position_ids is None:
-        device = input_ids.device if input_ids is not None else inputs_embeds.device
-        position_ids = torch.arange(
-            past_length, seq_length + past_length, dtype=torch.long, device=device
-        )
-        position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
-    else:
-        position_ids = position_ids.view(-1, seq_length).long()
-
-    # Attention mask.
-    if attention_mask is not None:
-        assert batch_size > 0, "batch_size has to be defined and > 0"
-        attention_mask = attention_mask.view(batch_size, -1)
-        # We create a 3D attention mask from a 2D tensor mask.
-        # Sizes are [batch_size, 1, 1, to_seq_length]
-        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
-        # this attention mask is more simple than the triangular masking of causal attention
-        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
-        attention_mask = attention_mask[:, None, None, :]
-
-        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
-        # masked positions, this operation will create a tensor which is 0.0 for
-        # positions we want to attend and the dtype's smallest value for masked positions.
-        # Since we are adding it to the raw scores before the softmax, this is
-        # effectively the same as removing these entirely.
-        attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility
-        attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min
-
-    # Prepare head mask if needed
-    # 1.0 in head_mask indicate we keep the head
-    # attention_probs has shape bsz x n_heads x N x N
-    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]
-    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]
-    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)
-
-    if inputs_embeds is None:
-        inputs_embeds = self.embed_in(input_ids)
-
-    hidden_states = inputs_embeds
-
-    if self.gradient_checkpointing and self.training:
-        if use_cache:
-            logger.warning(
-                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
-            )
-            use_cache = False
-
-    presents = () if use_cache else None
-    all_attentions = () if output_attentions else None
-    all_hidden_states = () if output_hidden_states else None
-    for i, (layer, layer_past) in enumerate(zip(self.layers, past_key_values)):
-        if output_hidden_states:
-            all_hidden_states = all_hidden_states + (hidden_states,)
-
-        if self.gradient_checkpointing and self.training:
-
-            def create_custom_forward(module):
-                def custom_forward(*inputs):
-                    # None for layer_past
-                    return module(*inputs, use_cache, None, output_attentions)
-
-                return custom_forward
-
-            outputs = torch.utils.checkpoint.checkpoint(
-                create_custom_forward(layer),
-                hidden_states,
-                attention_mask,
-                position_ids,
-                head_mask[i],
-            )
-        else:
-            outputs = layer(
-                hidden_states,
-                attention_mask=attention_mask,
-                position_ids=position_ids,
-                head_mask=head_mask[i],
-                layer_past=layer_past,
-                use_cache=use_cache,
-                output_attentions=output_attentions,
-            )
-        hidden_states = outputs[0]
-        if use_cache is True:
-            presents = presents + (outputs[1],)
-        if output_attentions:
-            all_attentions = all_attentions + (outputs[2 if use_cache else 1],)
-
-    hidden_states = self.final_layer_norm(hidden_states)
-    # Add last hidden state
-    if output_hidden_states:
-        all_hidden_states = all_hidden_states + (hidden_states,)
-
-    if not return_dict:
-        return tuple(
-            v
-            for v in [hidden_states, presents, all_hidden_states, all_attentions]
-            if v is not None
-        )
-
-    return BaseModelOutputWithPast(
-        last_hidden_state=hidden_states,
-        past_key_values=presents,
-        hidden_states=all_hidden_states,
-        attentions=all_attentions,
-    )
-
-
-def OPTDecoder_forward(
-    self,
-    input_ids: torch.LongTensor = None,
-    attention_mask: Optional[torch.Tensor] = None,
-    head_mask: Optional[torch.Tensor] = None,
-    past_key_values: Optional[List[torch.FloatTensor]] = None,
-    inputs_embeds: Optional[torch.FloatTensor] = None,
-    use_cache: Optional[bool] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    return_dict: Optional[bool] = None,
-) -> Union[Tuple, BaseModelOutputWithPast]:
-    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-    output_hidden_states = (
-        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-    )
-    use_cache = use_cache if use_cache is not None else self.config.use_cache
-
-    return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-    # retrieve input_ids and inputs_embeds
-    if input_ids is not None and inputs_embeds is not None:
-        raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
-    elif input_ids is not None:
-        input_shape = input_ids.size()
-        input_ids = input_ids.view(-1, input_shape[-1])
-    elif inputs_embeds is not None:
-        input_shape = inputs_embeds.size()[:-1]
-    else:
-        raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
-
-    if inputs_embeds is None:
-        inputs_embeds = self.embed_tokens(input_ids)
-
-    batch_size, seq_length = input_shape
-    past_key_values_length = 0
-    if past_key_values is not None and len(past_key_values[0]) !=4: #not discrete kv cache
-        past_key_values_length = past_key_values[0][0].shape[2]
-    elif past_key_values is not None and len(past_key_values[0]) ==4: #discrete kv cache
-        past_key_values_length = past_key_values[0][3]
-    # required mask seq length can be calculated via length of past
-    mask_seq_length = past_key_values_length + seq_length
-
-    # embed positions
-    if attention_mask is None:
-        attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)
-    causal_attention_mask = self._prepare_decoder_attention_mask(
-        attention_mask, input_shape, inputs_embeds, past_key_values_length
-    )
-    pos_embeds = self.embed_positions(attention_mask, past_key_values_length)
-
-    if self.project_in is not None:
-        inputs_embeds = self.project_in(inputs_embeds)
-
-    hidden_states = inputs_embeds + pos_embeds
-
-    if self.gradient_checkpointing and self.training:
-        if use_cache:
-            logger.warning_once(
-                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
-            )
-            use_cache = False
-
-    # decoder layers
-    all_hidden_states = () if output_hidden_states else None
-    all_self_attns = () if output_attentions else None
-    next_decoder_cache = () if use_cache else None
-
-    # check if head_mask has a correct number of layers specified if desired
-    for attn_mask, mask_name in zip([head_mask], ["head_mask"]):
-        if attn_mask is not None:
-            if attn_mask.size()[0] != (len(self.layers)):
-                raise ValueError(
-                    f"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for"
-                    f" {head_mask.size()[0]}."
-                )
-
-    for idx, decoder_layer in enumerate(self.layers):
-        # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
-        if output_hidden_states:
-            all_hidden_states += (hidden_states,)
-
-        dropout_probability = random.uniform(0, 1)
-        if self.training and (dropout_probability < self.layerdrop):
-            continue
-
-        past_key_value = past_key_values[idx] if past_key_values is not None else None
-
-        if self.gradient_checkpointing and self.training:
-
-            def create_custom_forward(module):
-                def custom_forward(*inputs):
-                    # None for past_key_value
-                    return module(*inputs, output_attentions, None)
-
-                return custom_forward
-
-            layer_outputs = torch.utils.checkpoint.checkpoint(
-                create_custom_forward(decoder_layer),
-                hidden_states,
-                causal_attention_mask,
-                head_mask[idx] if head_mask is not None else None,
-                None,
-            )
-        else:
-            layer_outputs = decoder_layer(
-                hidden_states,
-                attention_mask=causal_attention_mask,
-                layer_head_mask=(head_mask[idx] if head_mask is not None else None),
-                past_key_value=past_key_value,
-                output_attentions=output_attentions,
-                use_cache=use_cache,
-            )
-
-        hidden_states = layer_outputs[0]
-
-        if use_cache:
-            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
-
-        if output_attentions:
-            all_self_attns += (layer_outputs[1],)
-
-    if self.final_layer_norm is not None:
-        hidden_states = self.final_layer_norm(hidden_states)
-
-    if self.project_out is not None:
-        hidden_states = self.project_out(hidden_states)
-
-    # add hidden states from the last decoder layer
-    if output_hidden_states:
-        all_hidden_states += (hidden_states,)
-
-    next_cache = next_decoder_cache if use_cache else None
-    if not return_dict:
-        return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
-    return BaseModelOutputWithPast(
-        last_hidden_state=hidden_states,
-        past_key_values=next_cache,
-        hidden_states=all_hidden_states,
-        attentions=all_self_attns,
-    )
-
-
-def GPTJForCausalLM_forward(
-    self,
-    input_ids: Optional[torch.LongTensor] = None,
-    attention_mask: Optional[torch.FloatTensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
-    token_type_ids: Optional[torch.LongTensor] = None,
-    head_mask: Optional[torch.FloatTensor] = None,
-    inputs_embeds: Optional[torch.FloatTensor] = None,
-    labels: Optional[torch.LongTensor] = None,
-    use_cache: Optional[bool] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    return_dict: Optional[bool] = None,
-) -> Union[Tuple, CausalLMOutputWithPast]:
-    return_dict = (
-        return_dict if return_dict is not None else self.config.use_return_dict
-    )
-
-    transformer_outputs = self.transformer(
-        input_ids,
-        past_key_values=past_key_values,
-        attention_mask=attention_mask,
-        token_type_ids=token_type_ids,
-        position_ids=position_ids,
-        head_mask=head_mask,
-        inputs_embeds=inputs_embeds,
-        use_cache=use_cache,
-        output_attentions=output_attentions,
-        output_hidden_states=output_hidden_states,
-        return_dict=return_dict,
-    )
-    hidden_states = transformer_outputs[0]
-
-    # Set device for model parallelism
-    if self.model_parallel:
-        torch.cuda.set_device(self.transformer.first_device)
-        hidden_states = hidden_states.to(self.lm_head.weight.device)
-
-    # make sure sampling in fp16 works correctly and
-    # compute loss in fp32 to match with mesh-tf version
-    # https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179
-    lm_logits = self.lm_head(hidden_states).to(torch.float32)
-
-    loss = None
-    if labels is not None:
-        # Shift so that tokens < n predict n
-        shift_logits = lm_logits[..., :-1, :].contiguous()
-        shift_labels = labels[..., 1:].contiguous()
-        # Flatten the tokens
-        loss_fct = CrossEntropyLoss()
-        loss = loss_fct(
-            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)
-        )
-
-        loss = loss.to(hidden_states.dtype)
-
-    if not return_dict:
-        output = (lm_logits,) + transformer_outputs[1:]
-        return ((loss,) + output) if loss is not None else output
-
-    return CausalLMOutputWithPast(
-        loss=loss,
-        logits=lm_logits,
-        past_key_values=transformer_outputs.past_key_values,
-        hidden_states=transformer_outputs.hidden_states,
-        attentions=transformer_outputs.attentions,
-    )
-
-
-def LlamaForCausalLM_forward(
-    self,
-    input_ids: torch.LongTensor = None,
-    attention_mask: Optional[torch.Tensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    past_key_values: Optional[List[torch.FloatTensor]] = None,
-    inputs_embeds: Optional[torch.FloatTensor] = None,
-    labels: Optional[torch.LongTensor] = None,
-    use_cache: Optional[bool] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    return_dict: Optional[bool] = None,
-) -> Union[Tuple, CausalLMOutputWithPast]:
-    output_attentions = (
-        output_attentions
-        if output_attentions is not None
-        else self.config.output_attentions
-    )
-    output_hidden_states = (
-        output_hidden_states
-        if output_hidden_states is not None
-        else self.config.output_hidden_states
-    )
-    return_dict = (
-        return_dict if return_dict is not None else self.config.use_return_dict
-    )
-
-    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-    outputs = self.model(
-        input_ids=input_ids,
-        attention_mask=attention_mask,
-        position_ids=position_ids,
-        past_key_values=past_key_values,
-        inputs_embeds=inputs_embeds,
-        use_cache=use_cache,
-        output_attentions=output_attentions,
-        output_hidden_states=output_hidden_states,
-        return_dict=return_dict,
-    )
-
-    hidden_states = outputs[0]
-    logits = self.lm_head(hidden_states)
-
-    loss = None
-    if labels is not None:
-        # Shift so that tokens < n predict n
-        shift_logits = logits[..., :-1, :].contiguous()
-        shift_labels = labels[..., 1:].contiguous()
-        # Flatten the tokens
-        loss_fct = CrossEntropyLoss()
-        shift_logits = shift_logits.view(-1, self.config.vocab_size)
-        shift_labels = shift_labels.view(-1)
-        # Enable model parallelism
-        shift_labels = shift_labels.to(shift_logits.device)
-        loss = loss_fct(shift_logits, shift_labels)
-
-    if not return_dict:
-        output = (logits,) + outputs[1:]
-        return (loss,) + output if loss is not None else output
-
-    return CausalLMOutputWithPast(
-        loss=loss,
-        logits=logits,
-        past_key_values=outputs.past_key_values,
-        hidden_states=outputs.hidden_states,
-        attentions=outputs.attentions,
-    )
-
-
-def GPTNeoXForCausalLM_forward(
-    self,
-    input_ids: Optional[torch.LongTensor] = None,
-    attention_mask: Optional[torch.FloatTensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
-    inputs_embeds: Optional[torch.FloatTensor] = None,
-    head_mask: Optional[torch.FloatTensor] = None,
-    labels: Optional[torch.LongTensor] = None,
-    use_cache: Optional[bool] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    return_dict: Optional[bool] = None,
-) -> Union[Tuple, CausalLMOutputWithPast]:
-    return_dict = (
-        return_dict if return_dict is not None else self.config.use_return_dict
-    )
-    outputs = self.gpt_neox(
-        input_ids,
-        attention_mask=attention_mask,
-        position_ids=position_ids,
-        head_mask=head_mask,
-        inputs_embeds=inputs_embeds,
-        past_key_values=past_key_values,
-        use_cache=use_cache,
-        output_attentions=output_attentions,
-        output_hidden_states=output_hidden_states,
-        return_dict=return_dict,
-    )
-
-    hidden_states = outputs[0]
-    lm_logits = self.embed_out(hidden_states)
-
-    lm_loss = None
-    if labels is not None:
-        # move labels to correct device to enable model parallelism
-        labels = labels.to(lm_logits.device)
-        # we are doing next-token prediction; shift prediction scores and input ids by one
-        shift_logits = lm_logits[:, :-1, :].contiguous()
-        labels = labels[:, 1:].contiguous()
-        loss_fct = CrossEntropyLoss()
-        lm_loss = loss_fct(
-            shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1)
-        )
-
-    if not return_dict:
-        output = (lm_logits,) + outputs[1:]
-        return ((lm_loss,) + output) if lm_loss is not None else output
-
-    return CausalLMOutputWithPast(
-        loss=lm_loss,
-        logits=lm_logits,
-        past_key_values=outputs.past_key_values,
-        hidden_states=outputs.hidden_states,
-        attentions=outputs.attentions,
-    )
-
-def OPTForCausalLM_forward(
-    self,
-    input_ids: torch.LongTensor = None,
-    attention_mask: Optional[torch.Tensor] = None,
-    past_key_values: Optional[List[torch.FloatTensor]] = None,
-    head_mask: Optional[torch.Tensor] = None,
-    inputs_embeds: Optional[torch.FloatTensor] = None,
-    labels: Optional[torch.LongTensor] = None,
-    use_cache: Optional[bool] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    return_dict: Optional[bool] = None,
-) -> Union[Tuple, CausalLMOutputWithPast]:
-    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-    output_hidden_states = (
-        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-    )
-    return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-    outputs = self.model.decoder(
-        input_ids=input_ids,
-        attention_mask=attention_mask,
-        head_mask=head_mask,
-        past_key_values=past_key_values,
-        inputs_embeds=inputs_embeds,
-        use_cache=use_cache,
-        output_attentions=output_attentions,
-        output_hidden_states=output_hidden_states,
-        return_dict=return_dict,
-    )
-
-    logits = self.lm_head(outputs[0]).contiguous()
-
-    loss = None
-    if labels is not None:
-        # Shift so that tokens < n predict n
-        shift_logits = logits[..., :-1, :].contiguous()
-        shift_labels = labels[..., 1:].contiguous()
-        # Flatten the tokens
-        loss_fct = CrossEntropyLoss()
-        loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))
-
-    if not return_dict:
-        output = (logits,) + outputs[1:]
-        return (loss,) + output if loss is not None else output
-
-    return CausalLMOutputWithPast(
-        loss=loss,
-        logits=logits,
-        past_key_values=outputs.past_key_values,
-        hidden_states=outputs.hidden_states,
-        attentions=outputs.attentions,
-    )
\ No newline at end of file
diff --git a/intel_extension_for_pytorch/cpu/transformers/optimize.py b/intel_extension_for_pytorch/cpu/transformers/optimize.py
deleted file mode 100644
index 8d42a3b9..00000000
--- a/intel_extension_for_pytorch/cpu/transformers/optimize.py
+++ /dev/null
@@ -1,362 +0,0 @@
-import torch
-import torch.nn as nn
-import copy
-import re
-import warnings
-import pkg_resources
-from intel_extension_for_pytorch.cpu._auto_kernel_selection import (
-    _enable_tpp,
-    _disable_tpp,
-)
-import intel_extension_for_pytorch
-from intel_extension_for_pytorch.frontend import optimize
-
-
-def convert_class(m, target_m, new_class, config):
-    for name, sub_m in m.named_children():
-        if isinstance(sub_m, target_m):
-            new_m = new_class(sub_m, config)
-            setattr(m, name, new_m)
-        convert_class(sub_m, target_m, new_class, config)
-
-
-def convert_forward(m, target_m, new_forward):
-    for _, sub_m in m.named_children():
-        if isinstance(sub_m, target_m):
-            bound_method = new_forward.__get__(sub_m, sub_m.__class__)
-            setattr(sub_m, "forward", bound_method)
-        convert_forward(sub_m, target_m, new_forward)
-
-
-def convert_functions(m, target_m, new_function_name, new_function):
-    for _, sub_m in m.named_children():
-        if isinstance(sub_m, target_m):
-            bound_method = new_function.__get__(sub_m, sub_m.__class__)
-            setattr(sub_m, new_function_name, bound_method)
-        convert_functions(sub_m, target_m, new_function_name, new_function)
-
-
-def convert_function(m, func_name, new_function):
-    bound_method = new_function.__get__(m, m.__class__)
-    setattr(m, func_name, bound_method)
-
-
-distributed = False
-
-
-def is_distributed(m):
-    for _, sub_m in m.named_children():
-        if isinstance(
-            sub_m,
-            intel_extension_for_pytorch.nn.utils._weight_prepack._IPEXLinearAllreduce,
-        ):
-            global distributed
-            distributed = True
-            return
-        is_distributed(sub_m)
-
-
-def _set_optimized_model_for_generation(
-    model,
-    optimized_model,
-    first_token_optimized_model=None,
-):
-    if first_token_optimized_model is not None:
-        setattr(model, "trace_graph_first", first_token_optimized_model)
-
-    setattr(model, "trace_graph", optimized_model)
-
-
-def _optimize_transformers(
-    model,
-    dtype=torch.float,
-    inplace=False,
-    device="cpu",
-):
-    r"""
-    Apply optimizations at Python frontend to the given transformers model (nn.Module) for inference only.
-    This API focus on transformers models, especially for generation tasks inference.
-    Well supported model list: Llama, GPT-J, GPT-Neox, OPT.
-
-    Args:
-        model (torch.nn.Module): User model to apply optimizations on.
-        dtype (torch.dtype): Only works for ``torch.bfloat16`` and ``torch.int8`` and ``torch.float``.
-        inplace (bool): Whether to perform inplace optimization. Default value is ``False``.
-        device (str): Perform optimization on which device. Curentlty only support cpu. Default value is ``cpu``.
-
-    Returns:
-        optimized Model
-
-    .. warning::
-        Please invoke ``_optimize_transformers`` function AFTER invoking DeepSpeed in Tensor Parallel
-        inference scenario.
-
-    Examples:
-
-        >>> # bfloat16 inference case.
-        >>> model = ...
-        >>> model.load_state_dict(torch.load(PATH))
-        >>> model.eval()
-        >>> optimized_model = ipex._optimize_transformers(model, dtype=torch.bfloat16)
-        >>> # running evaluation step.
-
-
-    """
-    if isinstance(model, torch.jit.ScriptModule):
-        return model
-    if model.training:
-        return model
-
-    if device == "cpu":
-        try:
-            # tpp rope optimization has transformers version requirements
-            installed_pkg = {pkg.key for pkg in pkg_resources.working_set}
-            min_version = "4.28.0"
-            if "transformers" not in installed_pkg:
-                raise RuntimeError(
-                    "optimize_transformers optimization requires transformers package and its version at least {} , fallback due to not meet".format(
-                        min_version
-                    )
-                )
-
-            import transformers
-            from packaging import version
-
-            trans_version = transformers.__version__
-            if version.parse(trans_version) < version.parse(min_version):
-                raise RuntimeError(
-                    "optimize_transformers optimization requires the transformers with version: at least {} while now transformers== {}, fallback due to not meet".format(
-                        min_version, trans_version
-                    )
-                )
-
-            from .generation import (
-                _beam_search,
-                _greedy_search,
-                _extract_past_from_model_output,
-            )
-            from .attentions import (
-                _prepare_decoder_attention_mask,
-                _LlamaAttention,
-                _LlamaAttention_GQA,
-                _LlamaRMSNorm_forward,
-                _GPTJAttention,
-                _GPTNeoXAttention,
-                _OPTAttention,
-                _reorder_cache,
-            )
-            from intel_extension_for_pytorch.cpu.tpp.fused_llm import (
-                GPTJBlock_forward,
-                GPTJMLP_forward,
-                GPTJMLP_forward_distributed,
-                GPTNeoXMLP_forward,
-                GPTNeoXLayer_forward,
-                LlamaMLP_forward,
-                LlamaMLP_forward_distributed,
-                LlamaDecoderLayer_forward,
-                OPTDecoderLayer_forward,
-                OPTDecoderLayer_forward_distributed,
-            )
-            from intel_extension_for_pytorch.cpu.woq.fused_llm import (
-                GPTJMLP_woq_forward,
-                GPTJBlock_woq_forward,
-            )
-            from .models import (
-                GPTJModel_forward,
-                LlamaModel_forward,
-                GPTNeoXModel_forward,
-                OPTDecoder_forward,
-                GPTJForCausalLM_forward,
-                LlamaForCausalLM_forward,
-                GPTNeoXForCausalLM_forward,
-                OPTForCausalLM_forward,
-            )
-
-            well_supported_model = (
-                re.search("GPTJ", model.config.architectures[0], re.IGNORECASE)
-                or re.search("llama", model.config.architectures[0], re.IGNORECASE)
-                or re.search("gptneox", model.config.architectures[0], re.IGNORECASE)
-                or re.search("OPT", model.config.architectures[0], re.IGNORECASE)
-            )
-            if not well_supported_model:
-                warnings.warn(
-                    "optimize_transformers currently well supports Llama, GPT-J, GPT-Neox, OPT"
-                )
-
-            if not inplace:
-                _model = copy.deepcopy(model)
-            else:
-                _model = model
-
-            if dtype == torch.bfloat16 or dtype == torch.float or dtype == torch.int8:
-                # generation-wise optimizations
-                convert_function(_model, "_reorder_cache", _reorder_cache)
-                convert_function(_model, "beam_search", _beam_search)
-                convert_function(_model, "greedy_search", _greedy_search)
-                convert_function(
-                    _model,
-                    "_extract_past_from_model_output",
-                    _extract_past_from_model_output,
-                )
-
-                # model-wise optimizations
-                if re.search("GPTJ", model.config.architectures[0], re.IGNORECASE):
-                    convert_function(
-                        _model,
-                        "forward",
-                        GPTJForCausalLM_forward,
-                    )
-                elif re.search("llama", model.config.architectures[0], re.IGNORECASE):
-                    convert_function(
-                        _model,
-                        "forward",
-                        LlamaForCausalLM_forward,
-                    )
-                elif re.search("gptneox", model.config.architectures[0], re.IGNORECASE):
-                    convert_function(
-                        _model,
-                        "forward",
-                        GPTNeoXForCausalLM_forward,
-                    )
-                elif re.search("OPT", model.config.architectures[0], re.IGNORECASE):
-                    convert_function(
-                        _model,
-                        "forward",
-                        OPTForCausalLM_forward,
-                    )
-
-                convert_forward(
-                    _model,
-                    transformers.models.gptj.modeling_gptj.GPTJModel,
-                    GPTJModel_forward,
-                )
-                convert_forward(
-                    _model,
-                    transformers.models.llama.modeling_llama.LlamaModel,
-                    LlamaModel_forward,
-                )
-                convert_forward(
-                    _model,
-                    transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel,
-                    GPTNeoXModel_forward,
-                )
-                convert_forward(
-                    _model,
-                    transformers.models.opt.modeling_opt.OPTDecoder,
-                    OPTDecoder_forward,
-                )
-                convert_class(
-                    _model,
-                    transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention,
-                    _GPTNeoXAttention,
-                    _model.config,
-                )
-                if hasattr(_model.config, "num_key_value_heads"):
-                    convert_class(
-                        _model,
-                        transformers.models.llama.modeling_llama.LlamaAttention,
-                        _LlamaAttention_GQA,
-                        _model.config,
-                    )
-                else:
-                    convert_class(
-                        _model,
-                        transformers.models.llama.modeling_llama.LlamaAttention,
-                        _LlamaAttention,
-                        _model.config,
-                    )
-                convert_class(
-                    _model,
-                    transformers.models.gptj.modeling_gptj.GPTJAttention,
-                    _GPTJAttention,
-                    _model.config,
-                )
-                convert_class(
-                    _model,
-                    transformers.models.opt.modeling_opt.OPTAttention,
-                    _OPTAttention,
-                    _model.config,
-                )
-
-                if dtype == torch.int8:
-                    convert_functions(
-                        _model,
-                        transformers.models.llama.modeling_llama.LlamaModel,
-                        "_prepare_decoder_attention_mask",
-                        _prepare_decoder_attention_mask,
-                    )
-                    convert_forward(
-                        _model,
-                        transformers.models.llama.modeling_llama.LlamaRMSNorm,
-                        _LlamaRMSNorm_forward,
-                    )
-                    if getattr(_model.config, "weight_only_quantization", False):
-                        convert_forward(
-                            _model,
-                            transformers.models.gptj.modeling_gptj.GPTJBlock,
-                            GPTJBlock_woq_forward,
-                        )
-                        convert_forward(
-                            _model,
-                            transformers.models.gptj.modeling_gptj.GPTJMLP,
-                            GPTJMLP_woq_forward,
-                        )
-                else:
-                    # linear-wise optimizations
-                    _enable_tpp()
-                    _model = optimize(_model.eval(), dtype=dtype, inplace=True)
-                    # linear-postop-wise optimizations
-                    is_distributed(_model)
-                    if not distributed:
-                        convert_forward(
-                            _model,
-                            transformers.models.gptj.modeling_gptj.GPTJBlock,
-                            GPTJBlock_forward,
-                        )
-                        convert_forward(
-                            _model,
-                            transformers.models.gptj.modeling_gptj.GPTJMLP,
-                            GPTJMLP_forward,
-                        )
-                        convert_forward(
-                            _model,
-                            transformers.models.llama.modeling_llama.LlamaDecoderLayer,
-                            LlamaDecoderLayer_forward,
-                        )
-                        convert_forward(
-                            _model,
-                            transformers.models.llama.modeling_llama.LlamaMLP,
-                            LlamaMLP_forward,
-                        )
-                        convert_forward(
-                            _model,
-                            transformers.models.opt.modeling_opt.OPTDecoderLayer,
-                            OPTDecoderLayer_forward,
-                        )
-                    else:
-                        convert_forward(
-                            _model,
-                            transformers.models.llama.modeling_llama.LlamaMLP,
-                            LlamaMLP_forward_distributed,
-                        )
-                        convert_forward(
-                            _model,
-                            transformers.models.gptj.modeling_gptj.GPTJMLP,
-                            GPTJMLP_forward_distributed,
-                        )
-                        convert_forward(
-                            _model,
-                            transformers.models.opt.modeling_opt.OPTDecoderLayer,
-                            OPTDecoderLayer_forward_distributed,
-                        )
-            else:
-                raise RuntimeError(
-                    "optimize_transformers optimization currently supports dtype: torch.float, torch.bfloat16, torch.int8, will cover more soon."
-                )
-
-            return _model
-
-        except RuntimeError:
-            return model
-
-    return model
diff --git a/intel_extension_for_pytorch/cpu/woq/__init__.py b/intel_extension_for_pytorch/cpu/woq/__init__.py
deleted file mode 100644
index 1d3a30ae..00000000
--- a/intel_extension_for_pytorch/cpu/woq/__init__.py
+++ /dev/null
@@ -1,2 +0,0 @@
-import pkg_resources
-import warnings
\ No newline at end of file
diff --git a/intel_extension_for_pytorch/cpu/woq/fused_llm.py b/intel_extension_for_pytorch/cpu/woq/fused_llm.py
deleted file mode 100644
index a1a64866..00000000
--- a/intel_extension_for_pytorch/cpu/woq/fused_llm.py
+++ /dev/null
@@ -1,57 +0,0 @@
-import torch
-from torch import nn
-from typing import Optional, Tuple, Union
-
-def GPTJMLP_woq_forward(
-    self, hidden_states: Optional[torch.FloatTensor]
-) -> torch.FloatTensor:
-    if hasattr(self.fc_in, '_op_context') and self.fc_in._op_context is not None:
-        hidden_states = torch.ops.torch_ipex.woq_linear_gelu(
-            hidden_states, self.fc_in._op_context.get_data_handle()
-        )
-    else:
-        hidden_states = self.fc_in(hidden_states)
-    return hidden_states
-
-def GPTJBlock_woq_forward(
-    self,
-    hidden_states: Optional[torch.FloatTensor],
-    layer_past: Optional[Tuple[torch.Tensor]] = None,
-    attention_mask: Optional[torch.FloatTensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    head_mask: Optional[torch.FloatTensor] = None,
-    use_cache: Optional[bool] = False,
-    output_attentions: Optional[bool] = False,
-) -> Union[
-    Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]
-]:
-    residual = hidden_states
-    hidden_states = self.ln_1(hidden_states)
-    attn_outputs = self.attn(
-        hidden_states=hidden_states,
-        layer_past=layer_past,
-        attention_mask=attention_mask,
-        position_ids=position_ids,
-        head_mask=head_mask,
-        use_cache=use_cache,
-        output_attentions=output_attentions,
-    )
-    attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)
-    outputs = attn_outputs[1:]
-    feed_forward_hidden_states = self.mlp(hidden_states)
-    others = [attn_output, residual]
-    if hasattr(self.mlp.fc_out, '_op_context') and self.mlp.fc_out._op_context is not None:
-        hidden_states = torch.ops.torch_ipex.woq_linear_add_add(
-            feed_forward_hidden_states,
-            self.mlp.fc_out._op_context.get_data_handle(),
-            others
-        )
-    else:
-        hidden_states = self.mlp.fc_out(feed_forward_hidden_states)
-
-    if use_cache:
-        outputs = (hidden_states,) + outputs
-    else:
-        outputs = (hidden_states,) + outputs[1:]
-
-    return outputs  # hidden_states, present, (attentions)
diff --git a/intel_extension_for_pytorch/frontend.py b/intel_extension_for_pytorch/frontend.py
index 8d12258b..5d0850c8 100644
--- a/intel_extension_for_pytorch/frontend.py
+++ b/intel_extension_for_pytorch/frontend.py
@@ -6,6 +6,7 @@ import torch
 import torch._dynamo
 import torch.fx.experimental.optimization as optimization
 from enum import IntFlag, IntEnum
+
 from .nn import utils
 from .optim._optimizer_utils import (
     optimizer_fusion,
@@ -17,15 +18,13 @@ from .cpu.utils.linear_bn_folding import linear_bn_fuse
 from .cpu.graph_capture import GraphCapture
 from .nn.utils._lstm_convert import _LSTM, replace_lstm_with_ipex_lstm
 from .nn.utils._weight_prepack import _IPEXConv2d, _IPEXConvTranspose2d, _IPEXLinear
-from .nn.utils._weight_prepack import (
-    weight_prepack_with_ipex,
-    record_input_shape_for_prepack,
-)
+from .nn.utils._weight_prepack import weight_prepack_with_ipex, record_input_shape_for_prepack
 from .cpu._auto_kernel_selection import (
     _enable_dnnl,
     _disable_dnnl,
 )
 from .fx.concat_linear import _concat_linear
+
 import intel_extension_for_pytorch._C as core
 
 
@@ -557,6 +556,7 @@ def optimize(
         torch._dynamo.allow_in_graph(_IPEXLinear)
         torch._dynamo.allow_in_graph(_LSTM)
 
+
     if opt_properties.graph_mode:
         _old_forward = optimized_model.forward
         wrapper = GraphCapture(
diff --git a/intel_extension_for_pytorch/nn/modules/weight_only_quantization.py b/intel_extension_for_pytorch/nn/modules/weight_only_quantization.py
index 58a71891..d9a152cd 100644
--- a/intel_extension_for_pytorch/nn/modules/weight_only_quantization.py
+++ b/intel_extension_for_pytorch/nn/modules/weight_only_quantization.py
@@ -1,38 +1,9 @@
-import os
-
 import torch
 import torch.ao.nn.quantized as nnq
-from torch.ao.nn.quantized.modules.utils import _clamp_weights
+from torch.ao.nn.quantized.modules.utils import _quantize_weight
 import torch.ao.nn.intrinsic as nni
 from ...quantization._qconfig import get_weight_only_quant_qconfig_mapping
-from intel_extension_for_pytorch.nn.utils._weight_prepack import (
-    may_import_deepspeed_modules,
-    _all_reduce_and_bias_add,
-)
 
-# Port from PyTorch with a few changes
-def _quantize_weight(float_wt, observer):
-    wt_scale, wt_zp = observer.calculate_qparams()
-    dtype = observer.dtype
-    if observer.qscheme in [torch.per_tensor_symmetric, torch.per_tensor_affine]:
-        qweight = torch.quantize_per_tensor(
-            float_wt,
-            float(wt_scale), int(wt_zp), dtype)
-        qweight = _clamp_weights(qweight, observer, wt_scale, wt_zp)
-    elif observer.qscheme in [torch.per_channel_symmetric, torch.per_channel_affine]:
-        wt_axis = observer.ch_axis
-        qweight = torch.quantize_per_channel(
-            float_wt,
-            wt_scale.to(torch.double), wt_zp.to(torch.int64), wt_axis, dtype)
-        qweight = _clamp_weights(qweight, observer, wt_scale, wt_zp)
-    elif observer.qscheme in [torch.per_channel_affine_float_qparams]:
-        qweight = torch.quantize_per_channel(
-            float_wt,
-            wt_scale.to(torch.float), wt_zp.to(torch.float), observer.ch_axis, dtype)
-        qweight = _clamp_weights(qweight, observer, wt_scale, wt_zp)
-    else:
-        raise ValueError("Unexpected qscheme " + observer.qscheme)
-    return qweight
 
 class IpexWoqLinear(nnq.Linear):
     r"""
@@ -68,20 +39,27 @@ class IpexWoqLinear(nnq.Linear):
         # This dtype is used for weight prepacking and we do not rely on the prepacking
         # of nnq.Linear. So, it won't affect our implementation here.
         super().__init__(in_features, out_features, bias_, dtype=torch.qint8)
-        self._op_context = None
-        self._weight_qscheme = self.weight().qscheme()
-        self._lowp_mode = 0
-        self._num_concats = 1
-
-    def post_ipex_gemm(self, output):
-        return output
-
-    def forward(self, x):
-        Y = torch.ops.torch_ipex.ipex_woq_linear(
-            x, self._op_context.get_data_handle()
+        weight = torch.rand(out_features, in_features)
+        qweight = torch.quantize_per_channel(
+            weight, torch.ones(out_features), torch.zeros(out_features), 0, dtype
         )
+        bias = torch.rand(out_features)
+        self._op_context = torch.ops.ipex_prepack.weight_only_qlinear_prepack(
+            qweight, bias, None
+        )
+        self.weight_qscheme = self.weight().qscheme()
+        del weight
+        del qweight
 
-        return self.post_ipex_gemm(Y)
+    def forward(self, x):
+        # Note that we can handle self.bias == None case.
+        if self._packed_params.dtype in [torch.qint8, torch.quint4x2]:
+            Y = torch.ops.torch_ipex.ipex_woq_linear(
+                x, self._op_context.get_data_handle()
+            )
+        else:
+            raise RuntimeError("Unsupported dtype of wegiht only quantized linear!")
+        return Y.to(x.dtype)
 
     def _get_name(self):
         return "IpexWeightOnlyQuantizedLinear"
@@ -90,15 +68,14 @@ class IpexWoqLinear(nnq.Linear):
         extra_repr_str = "in_features={}, out_features={}, dtype={}".format(
             self.in_features, self.out_features, self._packed_params.dtype
         )
-        if self._packed_params.dtype in [torch.quint8, torch.qint8, torch.quint4x2]:
-            extra_repr_str += ", qscheme={}".format(self._weight_qscheme)
-        extra_repr_str += ", lowp_mode={}".format(self._lowp_mode)
+        if self._packed_params.dtype in [torch.qint8, torch.quint4x2]:
+            extra_repr_str += ", qscheme={}".format(self.weight_qscheme)
         return extra_repr_str
 
     def _save_to_state_dict(self, destination, prefix, keep_vars):
         assert (
             not keep_vars
-        ), "can not using keep_vars true when to save IpexWoqLinear's parameters"
+        ), "can not using keep_vars true when to save _IPEXConvNd's parameters"
         if self.bias is not None:
             bias = self.bias.float()
             destination[prefix + "bias"] = bias.detach()
@@ -138,9 +115,6 @@ class IpexWoqLinear(nnq.Linear):
                           utilities or provided by the user
         """
         float_modules = [torch.nn.Linear]
-        deepspeed_modules = may_import_deepspeed_modules()
-        if deepspeed_modules is not None:
-            float_modules.extend(deepspeed_modules)
 
         assert (
             type(mod) in float_modules
@@ -148,92 +122,32 @@ class IpexWoqLinear(nnq.Linear):
             [float_mod.__name__ for float_mod in float_modules]
         )
         assert hasattr(mod, "qconfig"), "Input float module must have qconfig defined"
-        lowp_mode = 0
+        if type(mod) == nni.LinearReLU:
+            mod = mod[0]
         if mod.qconfig is not None and mod.qconfig.weight is not None:
             weight_observer = mod.qconfig.weight()
-            if hasattr(mod.qconfig, 'lowp_mode'):
-                lowp_mode = mod.qconfig.lowp_mode
         else:
             weight_observer = (
                 get_weight_only_quant_qconfig_mapping().global_qconfig.weight()
             )
-        num_concats = 1
-        if hasattr(mod, '_num_concats'):
-            num_concats = mod._num_concats
         dtype = weight_observer.dtype
-        assert dtype in [torch.quint8, torch.qint8, torch.quint4x2], (
+        assert dtype in [torch.qint8, torch.quint4x2], (
             "The only supported dtypes for "
-            "weight-only quantized linear are quint8, qint8 and quint4x2 got: {}".format(dtype)
+            "weight-only quantized linear are qint8 and quint4x2 got: {}".format(dtype)
         )
         weight_observer(mod.weight)
-        qweight = _quantize_weight(mod.weight.float(), weight_observer)
-        if not hasattr(mod, "in_features"):
-            mod.in_features = mod.weight.size()[1]
-        if not hasattr(mod, "out_features"):
-            mod.out_features = mod.weight.size()[0]
-
-        qlinear = cls._init_cls(mod, dtype, qweight, lowp_mode, num_concats)
-        del qweight
-        return qlinear
-
-    @classmethod
-    def from_float_and_int4_weight(cls, mod, qweight, scales, zero_points):
-        r"""Create a weight-only quantized module from a float module and int4 weight
-
-        Args:
-            mod (Module): a float module, either produced by torch.ao.quantization
-                          utilities or provided by the user
-            qweight (Tensor): tensor in int32 dtype and contains actually int4 data
-            scales (Tensor): scales for qweight
-            zero_points (Tensor): zero points for qweight
-        """
-        float_modules = [torch.nn.Linear]
-        deepspeed_modules = may_import_deepspeed_modules()
-        if deepspeed_modules is not None:
-            float_modules.extend(deepspeed_modules)
-
-        assert (
-            type(mod) in float_modules
-        ), "IpexWoqLinear.from_float only works for one of" + str(
-            [float_mod.__name__ for float_mod in float_modules]
-        )
-        assert hasattr(mod, "qconfig"), "Input float module must have qconfig defined"
-
-        lowp_mode = 0
-        if mod.qconfig is not None and hasattr(mod.qconfig, 'lowp_mode'):
-            lowp_mode = mod.qconfig.lowp_mode
-        num_concats = 1
-        if hasattr(mod, '_num_concats'):
-            num_concats = mod._num_concats
-
-        w_dtype = qweight.dtype
-        assert w_dtype in [torch.int32, torch.quint4x2, torch.bfloat16, torch.float32], (
-            "Quantized int4 weight should have data type int32 or quint4x2, but got: {}".format(w_dtype)
-        )
-        if not hasattr(mod, "in_features"):
-            mod.in_features = mod.weight.size()[1]
-        if not hasattr(mod, "out_features"):
-            mod.out_features = mod.weight.size()[0]
-
-        qlinear = cls(mod.in_features, mod.out_features, dtype=w_dtype)
-        qlinear._op_context = torch.ops.ipex_prepack.weight_only_qlinear_prepack_int4(
-            qweight, scales, zero_points, mod.bias, None, int(lowp_mode), num_concats
-        )
-        qlinear._lowp_mode = lowp_mode
-        qlinear._num_concats = num_concats
-        qlinear._weight_qscheme = qlinear.weight().qscheme()
-        del qweight
-        return qlinear
-
-    @classmethod
-    def _init_cls(cls, mod, dtype, qweight, lowp_mode, num_concats):
+        if dtype in [torch.qint8, torch.quint4x2]:
+            qweight = _quantize_weight(mod.weight.float(), weight_observer)
+        else:
+            raise RuntimeError(
+                "Unsupported dtype specified for dynamic quantized Linear!"
+            )
         qlinear = cls(mod.in_features, mod.out_features, dtype=dtype)
         qlinear._op_context = torch.ops.ipex_prepack.weight_only_qlinear_prepack(
-            qweight, mod.bias, None, int(lowp_mode), num_concats
+            qweight, mod.bias, None
         )
-        qlinear._lowp_mode = lowp_mode
-        qlinear._num_concats = num_concats
-        qlinear._weight_qscheme = qlinear.weight().qscheme()
+        qlinear.weight_qscheme = qlinear.weight().qscheme()
+        del qweight
         return qlinear
 
     @classmethod
@@ -250,50 +164,9 @@ class IpexWoqLinear(nnq.Linear):
         )
         qweight = ref_qlinear.get_quantized_weight()
         bias = ref_qlinear.bias
+        # qlinear.set_weight_bias(qweight, bias)
         qlinear._op_context = torch.ops.ipex_prepack.weight_only_qlinear_prepack(
             qweight, bias, None
         )
         qlinear.weight_qscheme = qlinear.weight().qscheme()
         return qlinear
-
-
-class IpexWoqLinearAllreduce(IpexWoqLinear):
-    def __init__(
-        self,
-        in_features,
-        out_features,
-        mp_group,
-        bias_value,
-        bias_=True,
-        dtype=torch.qint8,
-    ):
-        # Save the original bias here
-        # For bias handling, please refer to the comment in __init__ of _IPEXLinearAllreduce
-        super().__init__(in_features, out_features, bias_, dtype=dtype)
-        self.mp_group = mp_group
-        self.original_bias = bias_value
-
-    @classmethod
-    def _init_cls(cls, mod, dtype, qweight, lowp_mode, num_concats):
-        qlinear = cls(
-            mod.in_features,
-            mod.out_features,
-            mod.mp_group,
-            mod.bias,  # save the original bias value
-            dtype=dtype,
-        )
-        # For bias handling, please refer to the comment in __init__ of _IPEXLinearAllreduce
-        qlinear.set_weight_bias(qweight, None)
-
-        qlinear._op_context = torch.ops.ipex_prepack.weight_only_qlinear_prepack(
-            qweight,
-            None,  # Set bias to None when prepacking. Please refer to the comment in __init__ of _IPEXLinearAllreduce
-            None,  # batch_size
-            lowp_mode,
-            num_concats
-        )
-
-        return qlinear
-
-    def post_ipex_gemm(self, output):
-        return _all_reduce_and_bias_add(self.mp_group, self.original_bias, output)
diff --git a/intel_extension_for_pytorch/nn/utils/__init__.py b/intel_extension_for_pytorch/nn/utils/__init__.py
index 5f16ee20..298c89e1 100644
--- a/intel_extension_for_pytorch/nn/utils/__init__.py
+++ b/intel_extension_for_pytorch/nn/utils/__init__.py
@@ -1,4 +1,3 @@
 from intel_extension_for_pytorch.nn.utils import _weight_prepack
 from intel_extension_for_pytorch.nn.utils import _lstm_convert
 from . import _model_convert, _weight_cast
-from ._weight_prepack import Apply_TPPLinear_weight_prepack
diff --git a/intel_extension_for_pytorch/nn/utils/_parameter_wrapper.py b/intel_extension_for_pytorch/nn/utils/_parameter_wrapper.py
index 8680dcda..f729a74d 100644
--- a/intel_extension_for_pytorch/nn/utils/_parameter_wrapper.py
+++ b/intel_extension_for_pytorch/nn/utils/_parameter_wrapper.py
@@ -4,10 +4,7 @@ import functools
 import contextlib
 import types
 import warnings
-from intel_extension_for_pytorch.cpu._auto_kernel_selection import (
-    _using_dnnl,
-    _using_tpp,
-)
+from intel_extension_for_pytorch.cpu._auto_kernel_selection import _using_dnnl
 from intel_extension_for_pytorch import frontend
 from intel_extension_for_pytorch.nn.utils._weight_prepack import (
     _IPEXLinear,
@@ -17,7 +14,6 @@ from intel_extension_for_pytorch.nn.utils._weight_prepack import (
     _IPEXConvTranspose2d,
     _IPEXConvTranspose3d,
     _IPEXLinearAllreduce,
-    _IPEXLmHeadLinearAllreduce,
     may_import_deepspeed_modules,
 )
 
@@ -35,17 +31,12 @@ def IPEX_WEIGHT_PREPACK_MODULE_CPU():
 
     deepspeed_modules = may_import_deepspeed_modules()
     if deepspeed_modules is not None:
-        LinearAllreduce, LinearLayer = deepspeed_modules[:2]
-        deepspeed_modules_mapping = {
+        LinearAllreduce, LinearLayer = deepspeed_modules
+        deepspeed_modules = {
             LinearLayer: _IPEXLinear,
             LinearAllreduce: _IPEXLinearAllreduce,
         }
-        if len(deepspeed_modules) > 2:
-            LmHeadLinearAllreduce = deepspeed_modules[2]
-            deepspeed_modules_mapping.update(
-                {LmHeadLinearAllreduce: _IPEXLmHeadLinearAllreduce}
-            )
-        torch_modules.update(deepspeed_modules_mapping)
+        torch_modules.update(deepspeed_modules)
 
     return torch_modules
 
@@ -116,7 +107,6 @@ def _should_prepack(module, is_training, is_xpu=False):
         isinstance(module, torch.nn.Linear)
         and not _using_dnnl()
         and is_training
-        and _using_tpp()
         and module.weight.dtype is torch.float
     ):
         return False
@@ -442,7 +432,6 @@ class ParameterWrapper(object):
             assert target_module in (
                 _IPEXLinear,
                 _IPEXLinearAllreduce,
-                _IPEXLmHeadLinearAllreduce,
             )
             self.linear_prepack(module, is_training)
 
@@ -533,36 +522,26 @@ class ParameterWrapper(object):
                     torch.float32,
                     torch.bfloat16,
                 ], "Only float, bf16 and fp16 are supported"
-                use_dnnl = True if not _using_tpp() else False
-        module.use_tpp = _using_tpp()
+                use_dnnl = True
         module.use_dnnl = use_dnnl
-        if not module.use_tpp:
-            if not hasattr(module, "out_features"):
-                setattr(module, "out_features", module.weight.shape[0])  # noqa: B010
-            # prepare batch size
-            module.batch_size_collapsed = None
-            if hasattr(module, "input_shape"):
-                module.batch_size_collapsed = 1
-                for i in range(len(module.input_shape) - 1):
-                    module.batch_size_collapsed *= module.input_shape[i]
-            # create linear op context
-            if module.use_dnnl:
-                self.op_ctx = torch.ops.ipex_prepack.linear_prepack(
-                    module.weight, module.bias, module.batch_size_collapsed
-                )
-            else:
-                self.op_ctx = torch.ops.ipex_prepack.mkl_sgemm_prepack(
-                    module.weight, module.bias, module.batch_size_collapsed
-                )
-            self.pack_weight(use_dnnl)
+        if not hasattr(module, "out_features"):
+            setattr(module, "out_features", module.weight.shape[0])  # noqa: B010
+        # prepare batch size
+        module.batch_size_collapsed = None
+        if hasattr(module, "input_shape"):
+            module.batch_size_collapsed = 1
+            for i in range(len(module.input_shape) - 1):
+                module.batch_size_collapsed *= module.input_shape[i]
+        # create linear op context
+        if module.use_dnnl:
+            self.op_ctx = torch.ops.ipex_prepack.linear_prepack(
+                module.weight, module.bias, module.batch_size_collapsed
+            )
         else:
-            from intel_extension_for_pytorch.nn.utils import (
-                Apply_TPPLinear_weight_prepack,
+            self.op_ctx = torch.ops.ipex_prepack.mkl_sgemm_prepack(
+                module.weight, module.bias, module.batch_size_collapsed
             )
-
-            Apply_TPPLinear_weight_prepack(module, dtype=module.weight.dtype)
-            self.parameter.data = module.weight.data
-            self.parameter = module.weight
+        self.pack_weight(use_dnnl)
 
     def load_cast_and_prepack(self, module, param):
         # load from state dict
diff --git a/intel_extension_for_pytorch/nn/utils/_weight_prepack.py b/intel_extension_for_pytorch/nn/utils/_weight_prepack.py
index 34deceef..e4715258 100644
--- a/intel_extension_for_pytorch/nn/utils/_weight_prepack.py
+++ b/intel_extension_for_pytorch/nn/utils/_weight_prepack.py
@@ -1,75 +1,13 @@
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
-import copy
 import logging
 import os
 import pkg_resources
 from intel_extension_for_pytorch import optim
-from intel_extension_for_pytorch.cpu.tpp.utils.blocked_layout import (
-    BlockedParameter,
-    get_vnni_blocking,
-)
-
-from intel_extension_for_pytorch.cpu._auto_kernel_selection import _using_tpp
 
 logger = logging.getLogger(__name__)
 
-USE_LOW_PREC_PARAMS = True
-
-
-def TPPLinear_weight_prepack(m, bk=None, bc=None, layer_dtype=torch.float32):
-    m.__class__ = _IPEXLinear
-    m.weight = BlockedParameter(m.weight.data)
-    m.weight.set_blocking_param(
-        (
-            [bk, bc],
-            [0, 2, 3, 1],
-        )
-    )
-    layer_use_low_prec = layer_dtype != torch.float32
-    if layer_use_low_prec == True and USE_LOW_PREC_PARAMS:
-        low_prec_vnni_blocking = get_vnni_blocking(layer_dtype)
-        m.weight.set_blocking_param(
-            (
-                [
-                    bk,
-                    [
-                        bc // low_prec_vnni_blocking,
-                        low_prec_vnni_blocking,
-                    ],
-                ],
-                [0, 2, 3, 1, 4],
-                layer_dtype,
-            )
-        )
-
-    if m.bias is not None:
-        m.bias = BlockedParameter(m.bias.data)
-        m.bias.set_blocking_param((None, None, layer_dtype))
-    return m
-
-
-def Apply_TPPLinear_weight_prepack(m, dtype, device="cpu"):
-    if (m.weight.size()[0] == 50400 or m.weight.size()[0] == 32000) and m.weight.size()[
-        1
-    ] % 64 == 0:
-        m = TPPLinear_weight_prepack(m, 100, 64, dtype)
-    elif m.weight.size()[0] % 16 == 0 and m.weight.size()[1] % 64 == 0:
-        m = TPPLinear_weight_prepack(m, 16, 64, dtype)
-    else:
-        setattr(m, "tpp_fallback", True)
-        return
-    setattr(m, "tpp_fallback", False)
-
-    block(m)
-
-
-def block(model):
-    for m in model.modules():
-        if hasattr(m, "maybe_block_params"):
-            m.maybe_block_params()
-
 
 def may_import_deepspeed_modules():
     try:
@@ -77,16 +15,7 @@ def may_import_deepspeed_modules():
         # intel-extension-for-deepspeed imports both IPEX and deepspeed
         from deepspeed.module_inject.layers import LinearAllreduce, LinearLayer
 
-        ds_layers = [LinearAllreduce, LinearLayer]
-
-        # TODO: remove this logic once deepspeed LmHeadLinearAllreduce change has been upstream-ed.
-        try:
-            from deepspeed.module_inject.layers import LmHeadLinearAllreduce
-
-            ds_layers.append(LmHeadLinearAllreduce)
-            return ds_layers
-        except ImportError:
-            return ds_layers
+        return LinearAllreduce, LinearLayer
     except ImportError:
         return None
 
@@ -95,13 +24,8 @@ installed_pkg = {pkg.key for pkg in pkg_resources.working_set}
 if "deepspeed" in installed_pkg:
     from deepspeed import comm
 
-    DS_SHM_ALLREDUCE = os.getenv("DS_SHM_ALLREDUCE")
-
     def _all_reduce(self, reduceOp, tag, ranks, group_size):
-        if DS_SHM_ALLREDUCE == "1":
-            comm.all_reduce_low_latency(self, async_op=False)
-        else:
-            comm.all_reduce(self, async_op=False)
+        comm.all_reduce(self, async_op=False)
         return self
 
     ds_comm = torch.library.Library("deepspeed_comm", "DEF")
@@ -112,22 +36,6 @@ if "deepspeed" in installed_pkg:
     ds_comm_lib_cpu.impl("all_reduce", _all_reduce)
 
 
-def _all_reduce_and_bias_add(mp_group, original_bias, output):
-    if mp_group is not None:
-        torch.ops.deepspeed_comm.all_reduce(
-            output,
-            "sum",
-            "",
-            list(torch.arange(int(os.environ["WORLD_SIZE"]))),
-            int(os.environ["WORLD_SIZE"]),
-        )
-
-    if original_bias is not None:
-        output += original_bias
-
-    return output
-
-
 def _ipex_module_load_from_state_dict_(self, state_dict, prefix):
     w_name = prefix + "weight"
     b_name = prefix + "bias"
@@ -238,20 +146,10 @@ class _IPEXLinear(_IPEXPrepackModule):
     def __init__(self):
         super(_IPEXLinear, self).__init__()
 
-    def maybe_block_params(self):
-        self.weight.block()
-        if self.bias is not None:
-            self.bias.block()
-
-    def pre_ipex_gemm(self, input):
-        return input
-
     def post_ipex_gemm(self, output):
         return output
 
     def forward(self, x):
-        x = self.pre_ipex_gemm(x)
-
         if self.use_dnnl:
             output = torch.ops.torch_ipex.ipex_linear(
                 x,
@@ -260,17 +158,6 @@ class _IPEXLinear(_IPEXPrepackModule):
                 self.ctx.get_data_handle(),
                 self.out_features,
             )
-        elif self.use_tpp:
-            if self.tpp_fallback:
-                output = torch.nn.functional.linear(x, self.weight, self.bias)
-            else:
-                x = x.to(self.weight.dtype).contiguous()
-                if self.bias is not None:
-                    output = torch.ops.torch_ipex.tpp_linear_bias(
-                        x, self.weight, self.bias
-                    )
-                else:
-                    output = torch.ops.torch_ipex.tpp_linear(x, self.weight)
         else:
             output = torch.ops.torch_ipex.ipex_MKLSGEMM(
                 x,
@@ -283,20 +170,10 @@ class _IPEXLinear(_IPEXPrepackModule):
         return self.post_ipex_gemm(output)
 
     def _save_to_state_dict(self, destination, prefix, keep_vars):
-        if self.use_tpp:
-            blocked_params = []
-            for p in self.parameters(recurse=False):
-                if isinstance(p, BlockedParameter) and p.is_blocked():
-                    p.unblock()
-                    blocked_params.append(p)
-            super(_IPEXLinear, self)._save_to_state_dict(destination, prefix, keep_vars)
-            for p in blocked_params:
-                p.block()
-        else:
-            assert (
-                not keep_vars
-            ), "can not using keep_vars true when to save _IPEXLinear's parameters"
-            super(_IPEXLinear, self)._save_to_state_dict(destination, prefix, keep_vars)
+        assert (
+            not keep_vars
+        ), "can not using keep_vars true when to save _IPEXLinear's parameters"
+        super(_IPEXLinear, self)._save_to_state_dict(destination, prefix, keep_vars)
 
     def _load_from_state_dict(
         self,
@@ -308,26 +185,8 @@ class _IPEXLinear(_IPEXPrepackModule):
         unexpected_keys,
         error_msgs,
     ):
-        if self.use_tpp:
-            blocked_params = []
-            for p in self.parameters(recurse=False):
-                if isinstance(p, BlockedParameter) and p.is_blocked():
-                    p.unblock()
-                    blocked_params.append(p)
-            super(_IPEXLinear, self)._load_from_state_dict(
-                state_dict,
-                prefix,
-                local_metadata,
-                strict,
-                missing_keys,
-                unexpected_keys,
-                error_msgs,
-            )
-            for p in blocked_params:
-                p.block()
-        else:
-            with torch.no_grad():
-                _ipex_module_load_from_state_dict_(self, state_dict, prefix)
+        with torch.no_grad():
+            _ipex_module_load_from_state_dict_(self, state_dict, prefix)
 
 
 class _IPEXLinearAllreduce(_IPEXLinear):
@@ -335,22 +194,17 @@ class _IPEXLinearAllreduce(_IPEXLinear):
         super(_IPEXLinearAllreduce, self).__init__()
 
     def post_ipex_gemm(self, output):
-        return _all_reduce_and_bias_add(self.mp_group, self.original_bias, output)
-
-
-class _IPEXLmHeadLinearAllreduce(_IPEXLinear):
-    def __init__(self):
-        super(_IPEXLmHeadLinearAllreduce, self).__init__()
-
-    def pre_ipex_gemm(self, input):
-        assert (
-            input.shape[-1] % self.world_size == 0
-        ), "please ensure input.shape[-1] % self.world_size == 0"
-        input_shard = input.shape[-1] // self.world_size
-        return input[:, :, self.rank * input_shard : (self.rank + 1) * input_shard]
-
-    def post_ipex_gemm(self, output):
-        return _all_reduce_and_bias_add(self.mp_group, self.original_bias, output)
+        if self.mp_group is not None:
+            torch.ops.deepspeed_comm.all_reduce(
+                output,
+                "sum",
+                "",
+                list(torch.arange(int(os.environ["WORLD_SIZE"]))),
+                int(os.environ["WORLD_SIZE"]),
+            )
+        if self.module_bias is not None:
+            output += self.module_bias
+        return output
 
 
 class _IPEXConvTransposeNd(_IPEXPrepackModule):
@@ -485,22 +339,12 @@ def weight_prepack_with_ipex(model, optimizer, params_attr, device_type="cpu"):
         if param_wrapper.can_prepack(m, is_training):
             new_m = IPEX_WEIGHT_PREPACK_MODULE_CPU()[m.__class__]()
             all_reduce_bias = m.bias
-            if isinstance(new_m, (_IPEXLinearAllreduce, _IPEXLmHeadLinearAllreduce)):
+            if isinstance(new_m, _IPEXLinearAllreduce):
                 m.bias = None
-            if _using_tpp():
-                weight_key = m.weight
-                param_wrapper.prepack(m, is_training)
-                if m.tpp_fallback:
-                    setattr(new_m, "tpp_fallback", True)
-                params_attr[m.weight] = params_attr.pop(weight_key)
-                del weight_key
-
-            else:
-                param_wrapper.prepack(m, is_training)
-
+            param_wrapper.prepack(m, is_training)
             new_m.__dict__ = m.__dict__
-            if isinstance(new_m, (_IPEXLinearAllreduce, _IPEXLmHeadLinearAllreduce)):
-                new_m.original_bias = all_reduce_bias
+            if isinstance(new_m, _IPEXLinearAllreduce):
+                new_m.module_bias = all_reduce_bias
             new_m.ctx = param_wrapper.op_ctx
             setattr(new_m, "weight_wrapper", param_wrapper)  # noqa: B010
             setattr(new_m, "bias_wrapper", bias_wrapper)  # noqa: B010
@@ -511,18 +355,21 @@ def weight_prepack_with_ipex(model, optimizer, params_attr, device_type="cpu"):
                 optimizer, optimizer_para, param_wrapper
             )
             new_m.training = is_training
-            if not is_training:
-                # _ipex_module_empty_weight_tensor and _ipex_module_empty_bias_tensor
-                # have to be a Parameter so that dynamo could convert it into FakeTensor
-                new_m._ipex_module_empty_weight_tensor = torch.nn.Parameter(
-                    torch.Tensor().to(dtype=new_m.weight.dtype)
+            # _ipex_module_empty_weight_tensor and _ipex_module_empty_bias_tensor
+            # have to be a Parameter so that dynamo could convert it into FakeTensor
+            # These empty tensors will only be used during inference but we'll set
+            # it in both training and eval mode to supprt the use case of the below
+            # workflow:
+            # model.train() -> ipex.optimize(model) -> model.eval()
+            new_m._ipex_module_empty_weight_tensor = torch.nn.Parameter(
+                torch.Tensor().to(dtype=new_m.weight.dtype)
+            )
+            if new_m.bias is None:
+                new_m.register_parameter("_ipex_module_empty_bias_tensor", None)
+            else:
+                new_m._ipex_module_empty_bias_tensor = torch.nn.Parameter(
+                    torch.Tensor().to(dtype=new_m.bias.dtype)
                 )
-                if new_m.bias is None:
-                    new_m.register_parameter("_ipex_module_empty_bias_tensor", None)
-                else:
-                    new_m._ipex_module_empty_bias_tensor = torch.nn.Parameter(
-                        torch.Tensor().to(dtype=new_m.bias.dtype)
-                    )
             return new_m
         else:
             return m
@@ -575,7 +422,10 @@ def record_input_shape_for_prepack(module, sample_input):
         for child in module.children():
             register_hook_function_rec(child)
 
-    origin_state_dict = copy.deepcopy(module.state_dict())
+    module_is_train = module.training
+    module.eval()
     register_hook_function_rec(module)
     module(*sample_input)
-    module.load_state_dict(origin_state_dict)
+    if module_is_train:
+        module.train()
+
diff --git a/intel_extension_for_pytorch/quantization/__init__.py b/intel_extension_for_pytorch/quantization/__init__.py
index 6b882964..2e058a46 100644
--- a/intel_extension_for_pytorch/quantization/__init__.py
+++ b/intel_extension_for_pytorch/quantization/__init__.py
@@ -6,6 +6,5 @@ from ._qconfig import (
     default_dynamic_qconfig_mapping,
     get_smooth_quant_qconfig_mapping,
     get_weight_only_quant_qconfig_mapping,
-    WoqLowpMode,
 )
 from ._autotune import autotune
diff --git a/intel_extension_for_pytorch/quantization/_qconfig.py b/intel_extension_for_pytorch/quantization/_qconfig.py
index aaf63dc3..03ad4295 100644
--- a/intel_extension_for_pytorch/quantization/_qconfig.py
+++ b/intel_extension_for_pytorch/quantization/_qconfig.py
@@ -1,5 +1,3 @@
-from collections import namedtuple
-from enum import IntEnum
 import torch
 from torch.ao.quantization import (
     PlaceholderObserver,
@@ -77,30 +75,18 @@ def get_smooth_quant_qconfig_mapping(
 
 
 # For weight-only quantization
-class WoqLowpMode(IntEnum):
-    NONE = 0
-    FP16 = 1
-    BF16 = 2
-    INT8 = 3
-
-QConfigWoq = namedtuple('QConfigWoq', [*QConfig._fields, 'lowp_mode'])
-def get_weight_only_quant_qconfig_mapping(
-        *,
-        weight_dtype: torch.dtype = torch.qint8,
-        lowp_mode: int = WoqLowpMode.NONE):
+def get_weight_only_quant_qconfig_mapping(weight_dtype: torch.dtype = torch.qint8):
     dtype_to_qscheme = {
         torch.qint8: torch.per_channel_affine,
-        torch.quint8: torch.per_channel_affine,
         # It is required to use per_channel_affine_float_qparams for quint4x2 by PyTorch
         torch.quint4x2: torch.per_channel_affine_float_qparams,
     }
     weight_qscheme = dtype_to_qscheme[weight_dtype]
-    _weight_only_quant_qconfig = QConfigWoq(
+    _weight_only_quant_qconfig = QConfig(
         activation=PlaceholderObserver.with_args(dtype=torch.float, is_dynamic=False),
         weight=PerChannelMinMaxObserver.with_args(
             dtype=weight_dtype, qscheme=weight_qscheme
         ),
-        lowp_mode=lowp_mode,
     )
     weight_only_quant_qconfig_mapping = QConfigMapping().set_global(
         _weight_only_quant_qconfig
diff --git a/intel_extension_for_pytorch/quantization/_quantization_state_utils.py b/intel_extension_for_pytorch/quantization/_quantization_state_utils.py
index 0a9f00dd..5d881100 100644
--- a/intel_extension_for_pytorch/quantization/_quantization_state_utils.py
+++ b/intel_extension_for_pytorch/quantization/_quantization_state_utils.py
@@ -50,6 +50,7 @@ functions_supported_by_quantization_ipex = set(
     [
         interaction,
         torch.ops.torch_ipex.interaction_forward,
+        torch.ops.torch_ipex.merged_emb_with_cat,
     ]
 )
 
diff --git a/intel_extension_for_pytorch/quantization/_quantize.py b/intel_extension_for_pytorch/quantization/_quantize.py
index 46258be5..a61063d9 100644
--- a/intel_extension_for_pytorch/quantization/_quantize.py
+++ b/intel_extension_for_pytorch/quantization/_quantize.py
@@ -15,7 +15,6 @@ import intel_extension_for_pytorch._C as core
 from intel_extension_for_pytorch.cpu.utils.linear_bn_folding import linear_bn_fuse
 from intel_extension_for_pytorch.nn.utils._weight_prepack import (
     may_import_deepspeed_modules,
-    _all_reduce_and_bias_add,
 )
 from ._quantize_utils import auto_prepare, auto_convert, copy_prepared_model
 from .. import nn
@@ -110,20 +109,6 @@ def prepare(
     return auto_prepare(prepare_model, configure, example_inputs, example_kwarg_inputs)
 
 
-def _may_insert_deepspeed_modules(
-    torch_modules, q_linear_layer_module, q_linear_all_reduce_module
-):
-    deepspeed_modules = may_import_deepspeed_modules()
-    if deepspeed_modules is not None:
-        LinearAllreduce, LinearLayer = deepspeed_modules[:2]
-        deepspeed_modules = {
-            LinearLayer: q_linear_layer_module,
-            LinearAllreduce: q_linear_all_reduce_module,
-        }
-        torch_modules.update(deepspeed_modules)
-    return torch_modules
-
-
 @functools.lru_cache(None)
 def IPEX_DYNAMIC_QUANTIZATION_MODULE_CPU():
     # TODO: have to override Linear here for GPT-J performance.
@@ -148,20 +133,15 @@ def IPEX_DYNAMIC_QUANTIZATION_MODULE_CPU():
     torch_modules = {
         torch.nn.Linear: DynamicQuantizedLinearLayer,
     }
-    torch_modules = _may_insert_deepspeed_modules(
-        torch_modules, DynamicQuantizedLinearLayer, DynamicQuantizedLinearAllreduce
-    )
-    return torch_modules
 
-
-@functools.lru_cache(None)
-def IPEX_WEIGHT_ONLY_QUANTIZATION_MODULE_CPU():
-    torch_modules = {}
-    torch_modules = _may_insert_deepspeed_modules(
-        torch_modules,
-        nn.modules.weight_only_quantization.IpexWoqLinear,
-        nn.modules.weight_only_quantization.IpexWoqLinearAllreduce,
-    )
+    deepspeed_modules = may_import_deepspeed_modules()
+    if deepspeed_modules is not None:
+        LinearAllreduce, LinearLayer = deepspeed_modules
+        deepspeed_modules = {
+            LinearLayer: DynamicQuantizedLinearLayer,
+            LinearAllreduce: DynamicQuantizedLinearAllreduce,
+        }
+        torch_modules.update(deepspeed_modules)
     return torch_modules
 
 
@@ -207,7 +187,7 @@ class DynamicQuantizedLinearLayer(_IPEXDynamicQuantizedLinear):
         _FLOAT_MODULE = [torch.nn.Linear]
         deepspeed_modules = may_import_deepspeed_modules()
         if deepspeed_modules is not None:
-            LinearLayer = deepspeed_modules[1]
+            _, LinearLayer = deepspeed_modules
             _FLOAT_MODULE.extend([LinearLayer])
         return _FLOAT_MODULE
 
@@ -236,7 +216,7 @@ class DynamicQuantizedLinearAllreduce(_IPEXDynamicQuantizedLinear):
         assert (
             deepspeed_modules is not None
         ), "DynamicQuantizedLinearAllreduce requires deepspeed to be installed"
-        LinearAllreduce = deepspeed_modules[0]
+        LinearAllreduce, _ = deepspeed_modules
         _FLOAT_MODULE = [LinearAllreduce]
         return _FLOAT_MODULE
 
@@ -273,27 +253,23 @@ class DynamicQuantizedLinearAllreduce(_IPEXDynamicQuantizedLinear):
             raise RuntimeError("Unsupported dtype on dynamic quantized linear!")
         output = Y.to(x.dtype)
 
-        return _all_reduce_and_bias_add(self.mp_group, self.original_bias, output)
+        if self.mp_group is not None:
+            torch.ops.deepspeed_comm.all_reduce(
+                output,
+                "sum",
+                "",
+                list(torch.arange(int(os.environ["WORLD_SIZE"]))),
+                int(os.environ["WORLD_SIZE"]),
+            )
+
+        if self.original_bias is not None:
+            output += self.original_bias
+        return output
 
     def __repr__(self):
         return "DynamicQuantizedLinearAllreduce()"
 
 
-def may_quantize_deepspeed_modules(
-    IPEX_QUANTIZATION_MODULE, q_config, module_mappings, qconfig_spec
-):
-    deepspeed_modules = may_import_deepspeed_modules()
-    if deepspeed_modules is not None:
-        LinearAllreduce, LinearLayer = deepspeed_modules[:2]
-        module_mappings.update(IPEX_QUANTIZATION_MODULE)
-        deepspeed_qconfig_spec = {
-            LinearLayer: q_config,
-            LinearAllreduce: q_config,
-        }
-        qconfig_spec.update(deepspeed_qconfig_spec)
-    return module_mappings, qconfig_spec
-
-
 def convert(model, inplace=False):
     r"""
     Convert an FP32 prepared model to a model which will automatically insert fake quant
@@ -340,20 +316,12 @@ def convert(model, inplace=False):
         module_mappings[
             torch.nn.Linear
         ] = nn.modules.weight_only_quantization.IpexWoqLinear
-
-        module_mappings, qconfig_spec = may_quantize_deepspeed_modules(
-            IPEX_WEIGHT_ONLY_QUANTIZATION_MODULE_CPU(),
-            convert_model.q_config,
-            module_mappings,
-            qconfig_spec,
-        )
-
         converted_model = torch.quantization.quantize_dynamic(
             convert_model,
             qconfig_spec=qconfig_spec,
             dtype=torch.qint8,
             mapping=module_mappings,
-            inplace=inplace,
+            inplace=False,
         )
         return converted_model
 
@@ -370,12 +338,15 @@ def convert(model, inplace=False):
             torch.nn.GRUCell: convert_model.q_config,
         }
 
-        module_mappings, qconfig_spec = may_quantize_deepspeed_modules(
-            IPEX_DYNAMIC_QUANTIZATION_MODULE_CPU(),
-            convert_model.q_config,
-            module_mappings,
-            qconfig_spec,
-        )
+        deepspeed_modules = may_import_deepspeed_modules()
+        if deepspeed_modules is not None:
+            LinearAllreduce, LinearLayer = deepspeed_modules
+            module_mappings.update(IPEX_DYNAMIC_QUANTIZATION_MODULE_CPU())
+            deepspeed_qconfig_spec = {
+                LinearLayer: convert_model.q_config,
+                LinearAllreduce: convert_model.q_config,
+            }
+            qconfig_spec.update(deepspeed_qconfig_spec)
 
         return torch.quantization.quantize_dynamic(
             convert_model,
diff --git a/intel_extension_for_pytorch/quantization/_recipe.py b/intel_extension_for_pytorch/quantization/_recipe.py
index 1ebda25a..139995ce 100644
--- a/intel_extension_for_pytorch/quantization/_recipe.py
+++ b/intel_extension_for_pytorch/quantization/_recipe.py
@@ -1,552 +1,556 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from intel_extension_for_pytorch.nn.functional import interaction
-
-from ._utils import ParentNode, set_node_output_quantized
-
-add_inplace_ops = [str(torch.Tensor.add_)]
-add_ops = [str(torch.add), str(torch.Tensor.add)]
-elt_wise_q_ops = [str(torch.Tensor.relu), str(torch.relu), str(F.relu), str(nn.ReLU)]
-elt_wise_noq_ops = [
-    str(torch.relu_),
-    str(torch.sigmoid_),
-    str(nn.ReLU),
-    str(torch.Tensor.relu_),
-    str(torch.Tensor.sigmoid_),
-    str(torch.nn.Hardtanh),
-    str(F.hardtanh),
-    str(F.hardtanh_),
-    str(torch.nn.ELU),
-    str(F.elu),
-    str(F.elu_),
-    str(nn.SiLU),
-    str(F.silu),
-    str(torch.Tensor.sigmoid),
-    str(torch.sigmoid),
-    str(F.sigmoid),
-    str(nn.Sigmoid),
-    str(F.gelu),
-    str(nn.GELU),
-]
-conv_gemm_ops = [
-    str(F.conv2d),
-    str(nn.Conv2d),
-    str(F.conv3d),
-    str(nn.Conv3d),
-    str(torch.conv2d),
-    str(torch.conv3d),
-    str(F.conv_transpose2d),
-    str(torch.nn.ConvTranspose2d),
-    str(F.conv_transpose3d),
-    str(torch.nn.ConvTranspose3d),
-    str(torch.conv_transpose2d),
-    str(torch.conv_transpose2d),
-    str(F.linear),
-    str(nn.Linear),
-    str(torch.matmul),
-    str(torch.Tensor.matmul),
-    str(torch.bmm),
-    str(torch.Tensor.bmm),
-]
-conv_ops = [
-    str(F.conv2d),
-    str(nn.Conv2d),
-    str(F.conv3d),
-    str(nn.Conv3d),
-    str(torch.conv2d),
-    str(torch.conv3d),
-    str(F.conv_transpose2d),
-    str(torch.nn.ConvTranspose2d),
-    str(F.conv_transpose3d),
-    str(torch.nn.ConvTranspose3d),
-    str(torch.conv_transpose2d),
-    str(torch.conv_transpose2d),
-]
-rnn_ops = [str(torch.nn.LSTM)]
-
-# Those ops only support s8->s8 path, and also require the qscheme is per_tensor_symmetric.
-s8_s8_symmetric_ops = [
-    str(interaction),
-    str(torch.ops.torch_ipex.interaction_forward),
-    str(torch.embedding_bag),
-    str(F.embedding_bag),
-    str(torch.nn.EmbeddingBag),
-]
-conv_gemm_fs = [
-    str(F.conv2d),
-    str(F.conv3d),
-    str(F.conv_transpose2d),
-    str(F.conv_transpose3d),
-    str(torch.conv2d),
-    str(torch.conv3d),
-    str(torch.conv_transpose2d),
-    str(torch.conv_transpose2d),
-    str(F.linear),
-    str(torch._C._nn.linear),
-]
-
-
-def _default_recipe_init(nodes):
-    r"""
-    This function is about init default recipe: setting the quantizable op's inf dtype to qint8 or quint8 according the qconfig,
-    there have some special cases, for some ops(interaction, EmbeddingBag), we only support some special \
-    quantization path, so if the related qconfig
-    doesn't meet the requirements, we will not set their inf dtype.
-    """
-    for node in nodes:
-        if isinstance(node, ParentNode):
-            continue
-        if node.qconfig is not None:
-            # Add q+dq before the quantizable op firstly.
-            for idx, tensor_info in enumerate(node.input_tensor_infos):
-                # only support fp32 tensor->int8 tensor
-                if (
-                    tensor_info is not None
-                    and (tensor_info.orig_dtype == torch.float32)
-                    and tensor_info.id in node.input_scale_zero
-                ):
-                    # gemm's weight
-                    if node.type in conv_gemm_fs and idx == 1:
-                        tensor_info.inf_dtype = node.qconfig.weight().dtype
-                    else:
-                        tensor_info.inf_dtype = node.qconfig.activation().dtype
-                    node.input_tensor_force_inf_dtype[idx] = tensor_info.inf_dtype
-            # For EmbeddingBag and interaction, we need to check the qconfig's setting, if not meet the requirements, \
-            # reset the inputs'(or weight) inf dtype
-            for tensor_info in node.weight_tensor_infos:
-                # nn.EmbeddingBag use activation observer and only support torch.qint8 and torch.per_tensor_symmetric
-                if (
-                    tensor_info is not None
-                    and (tensor_info.orig_dtype == torch.float32)
-                    and (
-                        str(node.idx) + "_" + str(tensor_info.id)
-                        in node.weight_scale_zero
-                    )
-                ):
-                    if (
-                        node.type == str(torch.nn.EmbeddingBag)
-                        and node.qconfig.activation().dtype == torch.qint8
-                        and node.qconfig.activation().qscheme
-                        == torch.per_tensor_symmetric
-                    ) or node.type != str(torch.nn.EmbeddingBag):
-                        tensor_info.inf_dtype = node.qconfig.weight().dtype
-            # interaction only supports qint8 and torch.per_tensor_symmetric, if not meet the requirement,
-            # reset the input's inf dtype.
-            if node.type in s8_s8_symmetric_ops:
-                if not (
-                    node.qconfig.activation().dtype == torch.qint8
-                    and node.qconfig.activation().qscheme == torch.per_tensor_symmetric
-                ):
-                    for idx, tensor_info in enumerate(node.input_tensor_infos):
-                        if tensor_info is not None:
-                            tensor_info.inf_dtype = tensor_info.orig_dtype
-                            node.input_tensor_force_inf_dtype[
-                                idx
-                            ] = tensor_info.inf_dtype
-
-            # For LSTM, if it's input is a PackedSequence, we don't support ot now.
-            # TODO: support PackedSequence input for quantization LSTM.
-            if (
-                node.type in rnn_ops
-                and len(node.input_tensor_infos) > 2
-                and node.input_tensor_infos[1].orig_dtype == torch.int64
-            ):
-                for idx, tensor_info in enumerate(node.input_tensor_infos):
-                    if tensor_info is not None:
-                        tensor_info.inf_dtype = tensor_info.orig_dtype
-                        node.input_tensor_force_inf_dtype[idx] = tensor_info.inf_dtype
-                for idx, tensor_info in enumerate(node.weight_tensor_infos):
-                    if tensor_info is not None:
-                        tensor_info.inf_dtype = tensor_info.orig_dtype
-
-
-# TODO: making fusion pattern check more general.
-def _find_fused_node_with_cur_elt_wise(node, ops):
-    r"""
-    Find a node before cur elt_wise which can be fused with cur elt_wise, which used by check
-    whether has a op can be fused with elt_wise.
-    """
-    if len(node.pre_nodes) == 0:
-        return None
-    pre_node = node.pre_nodes[0]
-    if pre_node is not None:
-        if pre_node.type in ops:
-            if len(pre_node.post_nodes) == 1:
-                return pre_node
-            elif len(node.post_nodes) == 1 and _find_conv_or_gemm_swish_fusion_node(
-                node.post_nodes[0]
-            ):
-                # conv+sigmoid+mul
-                return pre_node
-            else:
-                return None
-        elif (
-            pre_node.type in ([str(nn.Identity)] + elt_wise_q_ops + elt_wise_noq_ops)
-            and len(pre_node.post_nodes) == 1
-            and len(pre_node.pre_nodes) > 0
-        ):
-            return _find_fused_node_with_cur_elt_wise(pre_node.pre_nodes[0], ops)
-        else:
-            return None
-    else:
-        return None
-
-
-def _find_fused_node_with_cur_add(node, ops):
-    r"""
-    Find a node before the cur node which can be fused with cur add node, which used to check
-    whether has a node can be fused with add.
-    """
-    if len(node.pre_nodes) == 0:
-        return None
-    if len(node.pre_nodes) > 0:
-        if (
-            node.pre_nodes[0].type in ops
-            and len(node.pre_nodes[0].post_nodes) == 1
-            and node.pre_nodes[0].qconfig is not None
-        ):
-            return node.pre_nodes[0]
-        elif (
-            node.pre_nodes[0].type == str(nn.Identity)
-            and len(node.pre_nodes[0].post_nodes) == 1
-            and len(node.pre_nodes[0].pre_nodes) > 0
-        ):
-            fused_node = _find_fused_node_with_cur_add(node.pre_nodes[0], ops)
-            if fused_node is not None:
-                return node.pre_nodes[0]
-            else:
-                return None
-
-        if len(node.pre_nodes) == 2:
-            if (
-                node.pre_nodes[1].type in ops
-                and len(node.pre_nodes[1].post_nodes) == 1
-                and node.pre_nodes[1].qconfig is not None
-            ):
-                return node.pre_nodes[1]
-            elif (
-                node.pre_nodes[1].type == str(nn.Identity)
-                and len(node.pre_nodes[1].post_nodes) == 1
-                and len(node.pre_nodes[1].pre_nodes) > 0
-            ):
-                fused_node = _find_fused_node_with_cur_add(node.pre_nodes[1], ops)
-                if fused_node is not None:
-                    return node.pre_nodes[1]
-                else:
-                    return None
-        return None
-
-
-def _find_conv_or_gemm_swish_fusion_node(node):
-    r"""
-    Check whether has conv/gemm_sigmoid_mul fusion before cur node(including).
-        conv/gemm
-          /  \
-         /  sigmoid
-         \     /
-           mul(_)
-    """
-    mul_ops = [str(torch.mul), str(torch.Tensor.mul), str(torch.Tensor.mul_)]
-    sigmoid_ops = [
-        str(torch.Tensor.sigmoid),
-        str(torch.Tensor.sigmoid_),
-        str(torch.sigmoid),
-        str(torch.sigmoid_),
-        str(F.sigmoid),
-        str(torch.nn.Sigmoid),
-    ]
-    if node.type in mul_ops and len(node.pre_nodes) == 2:
-        if (
-            node.pre_nodes[0].type in conv_gemm_ops
-            and node.pre_nodes[1].type in sigmoid_ops
-        ):
-            if (
-                len(node.pre_nodes[0].post_nodes) == 2
-                and len(node.pre_nodes[1].post_nodes) == 1
-                and node.pre_nodes[1] in node.pre_nodes[0].post_nodes
-            ):
-                return node.pre_nodes[0]
-        elif (
-            node.pre_nodes[1].type in conv_gemm_ops
-            and node.pre_nodes[0].type in sigmoid_ops
-        ):
-            if (
-                len(node.pre_node[1].post_nodes) == 2
-                and len(node.pre_nodes[0].post_nodes) == 1
-                and node.pre_nodes[0] in node.pre_node[1].post_nodes
-            ):
-                return node.pre_nodes[1]
-    return None
-
-
-def _check_has_quantizable_node_before_node(node):
-    r"""
-    This function is about check whether has a quantizable node before(including) the given node,
-    which is used to check whether insert fake quant before one quantizable node or not. For example,
-    given_node->quantizable_node, if the given node is a none-quantizable node(also not a fusion groups nodes),
-    we can avoid inserting fake quant before this quantizable node.
-    """
-    if node.type == str(nn.Identity):
-        if len(node.pre_nodes) > 0:
-            return _check_has_quantizable_node_before_node(node.pre_nodes[0])
-        else:
-            return False
-    else:
-        # check whether has a qconfig
-        if node.qconfig is None:
-            if len(node.pre_nodes) == 0:
-                return False
-            # conv/gemm+add(_)+elt_wise
-            if node.type in elt_wise_noq_ops:
-                fused_elt_wise_node = _find_fused_node_with_cur_elt_wise(
-                    node, conv_gemm_ops + add_ops + add_inplace_ops
-                )
-                if fused_elt_wise_node is not None:
-                    # if fused_elt_wise_node is add_inplace_op, make sure it can also fused with conv/gemm.
-                    if fused_elt_wise_node.type in add_inplace_ops:
-                        fused_add_node = _find_fused_node_with_cur_add(
-                            node, conv_gemm_ops
-                        )
-                        if (
-                            fused_add_node is not None
-                            and fused_add_node.qconfig is not None
-                        ):
-                            return True
-                        else:
-                            return False
-                    else:
-                        if fused_elt_wise_node.qconfig is not None:
-                            return True
-                        else:
-                            return False
-            elif node.type in add_inplace_ops:  # check gemm+add_
-                fused_add_wise_node = _find_fused_node_with_cur_add(node, conv_gemm_ops)
-                if (
-                    fused_add_wise_node is not None
-                    and fused_add_wise_node.qconfig is not None
-                ):
-                    return True
-            # conv+sigmoid+mul(_)
-            fused_conv_or_gemm_swish_node = _find_conv_or_gemm_swish_fusion_node(node)
-            if (
-                fused_conv_or_gemm_swish_node is not None
-                and fused_conv_or_gemm_swish_node.qconfig is not None
-            ):
-                return True
-            return False
-        else:
-            if node.type in s8_s8_symmetric_ops:
-                if node.type in [
-                    str(interaction),
-                    str(torch.ops.torch_ipex.interaction_forward),
-                ]:
-                    for force_inf_dtype in node.input_tensor_force_inf_dtype:
-                        if force_inf_dtype.inf_dtype == torch.qint8:
-                            return True
-                    return False
-                else:
-                    # EmbeddingBag
-                    if node.weight_tensor_infos[0].inf_dtype == torch.qint8:
-                        return True
-                    else:
-                        return False
-            else:
-                # for none ipex customer op, if have a qconfig, we can say it is a quantizable op.
-                return True
-
-
-def _check_has_quantizable_node_after_node(node):
-    r"""
-    This function is about check whether all quantizable nodes after the given node,
-    which is used to check whether insert fake quant before one quantizable node or not.
-    """
-    if len(node.post_nodes) > 0:
-        output = True
-        for i in range(len(node.post_nodes)):
-            if node.post_nodes[i].qconfig is None:
-                output = False
-        return output
-    else:
-        return False
-
-
-def _add_recipe(node):
-    r"""
-    Case1: add has pre gemm node.
-    Given  gemm     op             gemm         op                gemm       op
-             \     /                 \         /                   \       /
-              \   /       ==>    fake_quant (fake_quant?)     ==>   \   (fake_quant?)
-               \ /                     \    /                        \   /
-               add                       add                          add
-
-          gemm     fp32_op          gemm     quantizable_op
-    ==>    \        /                \         /
-            \      /          or      \     fake_quant
-             \    /                    \    /
-              add                       add
-
-    Case2: add doesn't have pre conv/gemm node.
-    For this case, if one add input has one none-quantizable op, we will don't insert fake quant before it.
-    """
-
-    def reset_input_inf_dtype_to_orig_dtype(node, input_idx):
-        if node.input_tensor_infos[input_idx] is not None:
-            if (
-                node.input_tensor_infos[input_idx]
-                in node.pre_nodes[0].output_tensor_infos
-            ):
-                pre_node = node.pre_nodes[input_idx]
-            elif (
-                len(node.pre_nodes) == 2
-                and node.input_tensor_infos[input_idx]
-                in node.pre_nodes[1].output_tensor_infos
-            ):
-                pre_node = node.pre_nodes[1]
-            else:
-                pre_node = None
-            if pre_node is not None:
-                add_quantize_add_input_idx = _check_has_quantizable_node_before_node(
-                    pre_node
-                )
-            else:
-                add_quantize_add_input_idx = False
-            if not add_quantize_add_input_idx:
-                node.input_tensor_infos[input_idx].inf_dtype = node.input_tensor_infos[
-                    input_idx
-                ].orig_dtype
-                node.input_tensor_force_inf_dtype[input_idx] = node.input_tensor_infos[
-                    input_idx
-                ].inf_dtype
-
-    conv_gemm_node = _find_fused_node_with_cur_add(node, conv_gemm_ops)
-    conv_node = _find_fused_node_with_cur_add(node, conv_ops)
-    if conv_gemm_node is None:
-        #  If pre_nodes don't have gemm node, need to check whether have quantizable node before it,
-        #  if does't have quantizable node before it, we will not insert fake quant before add.
-        # hoping all input nodes are quantizable node.
-        if len(node.pre_nodes) > 0:
-            add_1_has_pre_quantizable_op = _check_has_quantizable_node_before_node(
-                node.pre_nodes[0]
-            )
-            add_2_has_pre_quantizable_op = False
-            if len(node.pre_nodes) == 2:
-                add_2_has_pre_quantizable_op = _check_has_quantizable_node_before_node(
-                    node.pre_nodes[1]
-                )
-            if not (add_1_has_pre_quantizable_op and add_2_has_pre_quantizable_op):
-                for idx, tensor_info in enumerate(node.input_tensor_infos):
-                    tensor_info.inf_dtype = tensor_info.orig_dtype
-                    node.input_tensor_force_inf_dtype[idx] = tensor_info.inf_dtype
-        else:
-            for idx, tensor_info in enumerate(node.input_tensor_infos):
-                tensor_info.inf_dtype = tensor_info.orig_dtype
-                node.input_tensor_force_inf_dtype[idx] = tensor_info.inf_dtype
-    else:
-        # add can fused with gemm.
-        if (
-            node.input_tensor_infos[0] is not None
-            and node.input_tensor_infos[0] in conv_gemm_node.output_tensor_infos
-        ):
-            node.input_tensor_infos[0].inf_dtype = node.input_tensor_infos[0].orig_dtype
-            node.input_tensor_force_inf_dtype[0] = node.input_tensor_infos[0].inf_dtype
-            # TODO: set another input's dtype for conv nodes when oneDNN is ready.
-            if conv_node is None or not _check_has_quantizable_node_after_node(node):
-                # set another input's dtype, if another's input is from non-quantizable op, we can remove the fake quant.
-                reset_input_inf_dtype_to_orig_dtype(node, 1)
-        elif (
-            node.input_tensor_infos[1] is not None
-            and node.input_tensor_infos[1] in conv_gemm_node.output_tensor_infos
-        ):
-            node.input_tensor_infos[1].inf_dtype = node.input_tensor_infos[1].orig_dtype
-            node.input_tensor_force_inf_dtype[1] = node.input_tensor_infos[1].inf_dtype
-            # TODO: set another input's dtype for conv nodes when oneDNN is ready.
-            if conv_node is None or not _check_has_quantizable_node_after_node(node):
-                # set another input's dtype, if another's input is from non-quantizable op, we can remove the fake quant.
-                reset_input_inf_dtype_to_orig_dtype(node, 0)
-
-
-# get a default recipe
-def get_default_recipe(nodes):
-    r"""
-    This function is about get default recipe which set where fake quant is inserted for the quantizable ops.
-    """
-    # step1: Quantization state init. Quantize inputs before quantizable node by setting their input's inf_dtype to
-    # qconfig.activation().dtype, and also setting the weight's inf_dtype to
-    # qconfig.weight().dtype if a module has a weight.
-    _default_recipe_init(nodes)
-    # step2: Optimization
-    # 1. For conv, gemm, and LSTM,  we always quantize its' inputs and weight, so we keep them state.
-    #    and for embedding_bag, which only has a weight, we always quantize it's weight to
-    #    save memory space and bandwidth, we also keep it's state.
-    # 2. For remaining quantizable ops (pooling, elt-wise op and add) which meet the following requirements, we will
-    # update them inputs' quantization state.
-    #   1. If it is a part of a quantized fusion pattern, don't need to quantize any inputs from inside the pattern.
-    #   2. If any of its inputs outside the fusion pattern are from non-quantized op, don't quantize all inputs outside the pattern.
-    #   3. If it is not part of a quantized fusion pattern, don't quantize all inputs if its one input from non-quantized op.
-    # 3. For quantizable ops (pooling, relu, flatten, interation and embedding) forcing quantized output, need to \
-    #    quantize its output if it is quantized.
-    # 4. For interation and embedding, we only support s8->s8 symmetric quantization, so if doesn't meet the \
-    #    requiresments, don't need to quantize its inputs.
-    # Note: the fusion pattern we are supported is conv/gemm/add + elt-wise, conv/gemm + add, conv/gemm + add + elt-wise.
-    # which means some ops can be combined with a single op to compute, but they are mathematically equivalent.
-    embedding_bag_ops = [
-        str(torch.embedding_bag),
-        str(F.embedding_bag),
-        str(torch.nn.EmbeddingBag),
-    ]
-    for node in nodes:
-        if isinstance(node, ParentNode):
-            continue
-        if node.qconfig is not None and node.type not in (
-            conv_gemm_ops + rnn_ops + embedding_bag_ops
-        ):
-            if node.type in add_ops:
-                # gemm+add fusion
-                _add_recipe(node)
-            elif node.type in elt_wise_q_ops:
-                # don't have a pre_node, we can say it doesn't have a pre quantizable node.
-                has_pre_quantized_node = True
-                # If Has gemm(add) pre_op can be fused, not insert fake quant.
-                if len(node.pre_nodes) > 0:
-                    if (
-                        _find_fused_node_with_cur_elt_wise(
-                            node, conv_gemm_ops + add_ops + add_inplace_ops
-                        )
-                        is not None
-                    ):
-                        has_pre_quantized_node = False
-                    else:
-                        has_pre_quantized_node = (
-                            _check_has_quantizable_node_before_node(node.pre_nodes[0])
-                        )
-                else:
-                    has_pre_quantized_node = False
-                if not has_pre_quantized_node:
-                    node.input_tensor_infos[0].inf_dtype = node.input_tensor_infos[
-                        0
-                    ].orig_dtype
-                    node.input_tensor_force_inf_dtype[0] = node.input_tensor_infos[
-                        0
-                    ].inf_dtype
-            else:
-                # For other quantizable node, we don't need add fake quant before it if it's pre node is one none-quantizable op.
-                # Now all other quantizable node only have one input info, so we can check the one pre input node info to check
-                # whether has a pre quantizable node.
-                has_pre_quantized_node = True
-                if len(node.pre_nodes) == 1:
-                    has_pre_quantized_node = _check_has_quantizable_node_before_node(
-                        node.pre_nodes[0]
-                    )
-                elif len(node.pre_nodes) == 0:
-                    has_pre_quantized_node = False
-                # the node's pre node doesn't support int8 output.
-                if not has_pre_quantized_node:
-                    node.input_tensor_infos[0].inf_dtype = node.input_tensor_infos[
-                        0
-                    ].orig_dtype
-                    node.input_tensor_force_inf_dtype[0] = node.input_tensor_infos[
-                        0
-                    ].inf_dtype
-
-    set_node_output_quantized(nodes)
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from intel_extension_for_pytorch.nn.functional import interaction
+
+from ._utils import ParentNode, set_node_output_quantized
+
+add_inplace_ops = [str(torch.Tensor.add_)]
+add_ops = [str(torch.add), str(torch.Tensor.add)]
+elt_wise_q_ops = [str(torch.Tensor.relu), str(torch.relu), str(F.relu), str(nn.ReLU)]
+elt_wise_noq_ops = [
+    str(torch.relu_),
+    str(torch.sigmoid_),
+    str(nn.ReLU),
+    str(torch.Tensor.relu_),
+    str(torch.Tensor.sigmoid_),
+    str(torch.nn.Hardtanh),
+    str(F.hardtanh),
+    str(F.hardtanh_),
+    str(torch.nn.ELU),
+    str(F.elu),
+    str(F.elu_),
+    str(nn.SiLU),
+    str(F.silu),
+    str(torch.Tensor.sigmoid),
+    str(torch.sigmoid),
+    str(F.sigmoid),
+    str(nn.Sigmoid),
+    str(F.gelu),
+    str(nn.GELU),
+]
+conv_gemm_ops = [
+    str(F.conv2d),
+    str(nn.Conv2d),
+    str(F.conv3d),
+    str(nn.Conv3d),
+    str(torch.conv2d),
+    str(torch.conv3d),
+    str(F.conv_transpose2d),
+    str(torch.nn.ConvTranspose2d),
+    str(F.conv_transpose3d),
+    str(torch.nn.ConvTranspose3d),
+    str(torch.conv_transpose2d),
+    str(torch.conv_transpose2d),
+    str(F.linear),
+    str(nn.Linear),
+    str(torch.matmul),
+    str(torch.Tensor.matmul),
+    str(torch.bmm),
+    str(torch.Tensor.bmm),
+]
+conv_ops = [
+    str(F.conv2d),
+    str(nn.Conv2d),
+    str(F.conv3d),
+    str(nn.Conv3d),
+    str(torch.conv2d),
+    str(torch.conv3d),
+    str(F.conv_transpose2d),
+    str(torch.nn.ConvTranspose2d),
+    str(F.conv_transpose3d),
+    str(torch.nn.ConvTranspose3d),
+    str(torch.conv_transpose2d),
+    str(torch.conv_transpose2d),
+]
+rnn_ops = [str(torch.nn.LSTM)]
+
+# Those ops only support s8->s8 path, and also require the qscheme is per_tensor_symmetric.
+s8_s8_symmetric_ops = [
+    str(interaction),
+    str(torch.ops.torch_ipex.interaction_forward),
+    str(torch.ops.torch_ipex.merged_emb_with_cat),
+    str(torch.embedding_bag),
+    str(F.embedding_bag),
+    str(torch.nn.EmbeddingBag),
+]
+conv_gemm_fs = [
+    str(F.conv2d),
+    str(F.conv3d),
+    str(F.conv_transpose2d),
+    str(F.conv_transpose3d),
+    str(torch.conv2d),
+    str(torch.conv3d),
+    str(torch.conv_transpose2d),
+    str(torch.conv_transpose2d),
+    str(F.linear),
+    str(torch._C._nn.linear),
+]
+
+
+def _default_recipe_init(nodes):
+    r"""
+    This function is about init default recipe: setting the quantizable op's inf dtype to qint8 or quint8 according the qconfig,
+    there have some special cases, for some ops(interaction, EmbeddingBag), we only support some special \
+    quantization path, so if the related qconfig
+    doesn't meet the requirements, we will not set their inf dtype.
+    """
+    for node in nodes:
+        if isinstance(node, ParentNode):
+            continue
+        if node.qconfig is not None:
+            # Add q+dq before the quantizable op firstly.
+            for idx, tensor_info in enumerate(node.input_tensor_infos):
+                # only support fp32 tensor->int8 tensor
+                if (
+                    tensor_info is not None
+                    and (tensor_info.orig_dtype == torch.float32)
+                    and tensor_info.id in node.input_scale_zero
+                ):
+                    # gemm's weight
+                    if node.type in conv_gemm_fs and idx == 1:
+                        tensor_info.inf_dtype = node.qconfig.weight().dtype
+                    else:
+                        tensor_info.inf_dtype = node.qconfig.activation().dtype
+                    node.input_tensor_force_inf_dtype[idx] = tensor_info.inf_dtype
+            # For EmbeddingBag and interaction, we need to check the qconfig's setting, if not meet the requirements, \
+            # reset the inputs'(or weight) inf dtype
+            for tensor_info in node.weight_tensor_infos:
+                # nn.EmbeddingBag use activation observer and only support torch.qint8 and torch.per_tensor_symmetric
+                if (
+                    tensor_info is not None
+                    and (tensor_info.orig_dtype == torch.float32)
+                    and (
+                        str(node.idx) + "_" + str(tensor_info.id)
+                        in node.weight_scale_zero
+                    )
+                ):
+                    if (
+                        node.type == str(torch.nn.EmbeddingBag)
+                        and node.qconfig.activation().dtype == torch.qint8
+                        and node.qconfig.activation().qscheme
+                        == torch.per_tensor_symmetric
+                    ) or node.type != str(torch.nn.EmbeddingBag):
+                        tensor_info.inf_dtype = node.qconfig.weight().dtype
+            # interaction only supports qint8 and torch.per_tensor_symmetric, if not meet the requirement,
+            # reset the input's inf dtype.
+            if node.type in s8_s8_symmetric_ops:
+                if not (
+                    node.qconfig.activation().dtype == torch.qint8
+                    and node.qconfig.activation().qscheme == torch.per_tensor_symmetric
+                ):
+                    for idx, tensor_info in enumerate(node.input_tensor_infos):
+                        if tensor_info is not None:
+                            tensor_info.inf_dtype = tensor_info.orig_dtype
+                            node.input_tensor_force_inf_dtype[
+                                idx
+                            ] = tensor_info.inf_dtype
+
+            # For LSTM, if it's input is a PackedSequence, we don't support ot now.
+            # TODO: support PackedSequence input for quantization LSTM.
+            if (
+                node.type in rnn_ops
+                and len(node.input_tensor_infos) > 2
+                and node.input_tensor_infos[1].orig_dtype == torch.int64
+            ):
+                for idx, tensor_info in enumerate(node.input_tensor_infos):
+                    if tensor_info is not None:
+                        tensor_info.inf_dtype = tensor_info.orig_dtype
+                        node.input_tensor_force_inf_dtype[idx] = tensor_info.inf_dtype
+                for idx, tensor_info in enumerate(node.weight_tensor_infos):
+                    if tensor_info is not None:
+                        tensor_info.inf_dtype = tensor_info.orig_dtype
+
+
+# TODO: making fusion pattern check more general.
+def _find_fused_node_with_cur_elt_wise(node, ops):
+    r"""
+    Find a node before cur elt_wise which can be fused with cur elt_wise, which used by check
+    whether has a op can be fused with elt_wise.
+    """
+    if len(node.pre_nodes) == 0:
+        return None
+    pre_node = node.pre_nodes[0]
+    if pre_node is not None:
+        if pre_node.type in ops:
+            if len(pre_node.post_nodes) == 1:
+                return pre_node
+            elif len(node.post_nodes) == 1 and _find_conv_or_gemm_swish_fusion_node(
+                node.post_nodes[0]
+            ):
+                # conv+sigmoid+mul
+                return pre_node
+            else:
+                return None
+        elif (
+            pre_node.type in ([str(nn.Identity)] + elt_wise_q_ops + elt_wise_noq_ops)
+            and len(pre_node.post_nodes) == 1
+            and len(pre_node.pre_nodes) > 0
+        ):
+            return _find_fused_node_with_cur_elt_wise(pre_node.pre_nodes[0], ops)
+        else:
+            return None
+    else:
+        return None
+
+
+def _find_fused_node_with_cur_add(node, ops):
+    r"""
+    Find a node before the cur node which can be fused with cur add node, which used to check
+    whether has a node can be fused with add.
+    """
+    if len(node.pre_nodes) == 0:
+        return None
+    if len(node.pre_nodes) > 0:
+        if (
+            node.pre_nodes[0].type in ops
+            and len(node.pre_nodes[0].post_nodes) == 1
+            and node.pre_nodes[0].qconfig is not None
+        ):
+            return node.pre_nodes[0]
+        elif (
+            node.pre_nodes[0].type == str(nn.Identity)
+            and len(node.pre_nodes[0].post_nodes) == 1
+            and len(node.pre_nodes[0].pre_nodes) > 0
+        ):
+            fused_node = _find_fused_node_with_cur_add(node.pre_nodes[0], ops)
+            if fused_node is not None:
+                return node.pre_nodes[0]
+            else:
+                return None
+
+        if len(node.pre_nodes) == 2:
+            if (
+                node.pre_nodes[1].type in ops
+                and len(node.pre_nodes[1].post_nodes) == 1
+                and node.pre_nodes[1].qconfig is not None
+            ):
+                return node.pre_nodes[1]
+            elif (
+                node.pre_nodes[1].type == str(nn.Identity)
+                and len(node.pre_nodes[1].post_nodes) == 1
+                and len(node.pre_nodes[1].pre_nodes) > 0
+            ):
+                fused_node = _find_fused_node_with_cur_add(node.pre_nodes[1], ops)
+                if fused_node is not None:
+                    return node.pre_nodes[1]
+                else:
+                    return None
+        return None
+
+
+def _find_conv_or_gemm_swish_fusion_node(node):
+    r"""
+    Check whether has conv/gemm_sigmoid_mul fusion before cur node(including).
+        conv/gemm
+          /  \
+         /  sigmoid
+         \     /
+           mul(_)
+    """
+    mul_ops = [str(torch.mul), str(torch.Tensor.mul), str(torch.Tensor.mul_)]
+    sigmoid_ops = [
+        str(torch.Tensor.sigmoid),
+        str(torch.Tensor.sigmoid_),
+        str(torch.sigmoid),
+        str(torch.sigmoid_),
+        str(F.sigmoid),
+        str(torch.nn.Sigmoid),
+    ]
+    if node.type in mul_ops and len(node.pre_nodes) == 2:
+        if (
+            node.pre_nodes[0].type in conv_gemm_ops
+            and node.pre_nodes[1].type in sigmoid_ops
+        ):
+            if (
+                len(node.pre_nodes[0].post_nodes) == 2
+                and len(node.pre_nodes[1].post_nodes) == 1
+                and node.pre_nodes[1] in node.pre_nodes[0].post_nodes
+            ):
+                return node.pre_nodes[0]
+        elif (
+            node.pre_nodes[1].type in conv_gemm_ops
+            and node.pre_nodes[0].type in sigmoid_ops
+        ):
+            if (
+                len(node.pre_node[1].post_nodes) == 2
+                and len(node.pre_nodes[0].post_nodes) == 1
+                and node.pre_nodes[0] in node.pre_node[1].post_nodes
+            ):
+                return node.pre_nodes[1]
+    return None
+
+
+def _check_has_quantizable_node_before_node(node):
+    r"""
+    This function is about check whether has a quantizable node before(including) the given node,
+    which is used to check whether insert fake quant before one quantizable node or not. For example,
+    given_node->quantizable_node, if the given node is a none-quantizable node(also not a fusion groups nodes),
+    we can avoid inserting fake quant before this quantizable node.
+    """
+    if node.type == str(nn.Identity):
+        if len(node.pre_nodes) > 0:
+            return _check_has_quantizable_node_before_node(node.pre_nodes[0])
+        else:
+            return False
+    else:
+        # check whether has a qconfig
+        if node.qconfig is None:
+            if len(node.pre_nodes) == 0:
+                return False
+            # conv/gemm+add(_)+elt_wise
+            if node.type in elt_wise_noq_ops:
+                fused_elt_wise_node = _find_fused_node_with_cur_elt_wise(
+                    node, conv_gemm_ops + add_ops + add_inplace_ops
+                )
+                if fused_elt_wise_node is not None:
+                    # if fused_elt_wise_node is add_inplace_op, make sure it can also fused with conv/gemm.
+                    if fused_elt_wise_node.type in add_inplace_ops:
+                        fused_add_node = _find_fused_node_with_cur_add(
+                            node, conv_gemm_ops
+                        )
+                        if (
+                            fused_add_node is not None
+                            and fused_add_node.qconfig is not None
+                        ):
+                            return True
+                        else:
+                            return False
+                    else:
+                        if fused_elt_wise_node.qconfig is not None:
+                            return True
+                        else:
+                            return False
+            elif node.type in add_inplace_ops:  # check gemm+add_
+                fused_add_wise_node = _find_fused_node_with_cur_add(node, conv_gemm_ops)
+                if (
+                    fused_add_wise_node is not None
+                    and fused_add_wise_node.qconfig is not None
+                ):
+                    return True
+            # conv+sigmoid+mul(_)
+            fused_conv_or_gemm_swish_node = _find_conv_or_gemm_swish_fusion_node(node)
+            if (
+                fused_conv_or_gemm_swish_node is not None
+                and fused_conv_or_gemm_swish_node.qconfig is not None
+            ):
+                return True
+            return False
+        else:
+            if node.type in s8_s8_symmetric_ops:
+                if node.type in [
+                        str(interaction),
+                        str(torch.ops.torch_ipex.interaction_forward),
+                        str(torch.ops.torch_ipex.merged_emb_with_cat)
+                ]:
+                    for force_inf_dtype in node.input_tensor_force_inf_dtype:
+                        if force_inf_dtype == torch.qint8:
+                            return True
+                    return False
+                else:
+                    # EmbeddingBag
+                    if node.weight_tensor_infos[0].inf_dtype == torch.qint8:
+                        return True
+                    else:
+                        return False
+            else:
+                # for none ipex customer op, if have a qconfig, we can say it is a quantizable op.
+                return True
+
+
+def _check_has_quantizable_node_after_node(node):
+    r"""
+    This function is about check whether all quantizable nodes after the given node,
+    which is used to check whether insert fake quant before one quantizable node or not.
+    """
+    if len(node.post_nodes) > 0:
+        output = True
+        for i in range(len(node.post_nodes)):
+            if node.post_nodes[i].qconfig is None:
+                output = False
+        return output
+    else:
+        return False
+
+
+def _add_recipe(node):
+    r"""
+    Case1: add has pre gemm node.
+    Given  gemm     op             gemm         op                gemm       op
+             \     /                 \         /                   \       /
+              \   /       ==>    fake_quant (fake_quant?)     ==>   \   (fake_quant?)
+               \ /                     \    /                        \   /
+               add                       add                          add
+
+          gemm     fp32_op          gemm     quantizable_op
+    ==>    \        /                \         /
+            \      /          or      \     fake_quant
+             \    /                    \    /
+              add                       add
+
+    Case2: add doesn't have pre conv/gemm node.
+    For this case, if one add input has one none-quantizable op, we will don't insert fake quant before it.
+    """
+
+    def reset_input_inf_dtype_to_orig_dtype(node, input_idx):
+        if node.input_tensor_infos[input_idx] is not None:
+            if (
+                node.input_tensor_infos[input_idx]
+                in node.pre_nodes[0].output_tensor_infos
+            ):
+                pre_node = node.pre_nodes[input_idx]
+            elif (
+                len(node.pre_nodes) == 2
+                and node.input_tensor_infos[input_idx]
+                in node.pre_nodes[1].output_tensor_infos
+            ):
+                pre_node = node.pre_nodes[1]
+            else:
+                pre_node = None
+            if pre_node is not None:
+                add_quantize_add_input_idx = _check_has_quantizable_node_before_node(
+                    pre_node
+                )
+            else:
+                add_quantize_add_input_idx = False
+            if not add_quantize_add_input_idx:
+                node.input_tensor_infos[input_idx].inf_dtype = node.input_tensor_infos[
+                    input_idx
+                ].orig_dtype
+                node.input_tensor_force_inf_dtype[input_idx] = node.input_tensor_infos[
+                    input_idx
+                ].inf_dtype
+
+    conv_gemm_node = _find_fused_node_with_cur_add(node, conv_gemm_ops)
+    conv_node = _find_fused_node_with_cur_add(node, conv_ops)
+    if conv_gemm_node is None:
+        #  If pre_nodes don't have gemm node, need to check whether have quantizable node before it,
+        #  if does't have quantizable node before it, we will not insert fake quant before add.
+        # hoping all input nodes are quantizable node.
+        if len(node.pre_nodes) > 0:
+            add_1_has_pre_quantizable_op = _check_has_quantizable_node_before_node(
+                node.pre_nodes[0]
+            )
+            add_2_has_pre_quantizable_op = False
+            if len(node.pre_nodes) == 2:
+                add_2_has_pre_quantizable_op = _check_has_quantizable_node_before_node(
+                    node.pre_nodes[1]
+                )
+            if not (add_1_has_pre_quantizable_op and add_2_has_pre_quantizable_op):
+                for idx, tensor_info in enumerate(node.input_tensor_infos):
+                    tensor_info.inf_dtype = tensor_info.orig_dtype
+                    node.input_tensor_force_inf_dtype[idx] = tensor_info.inf_dtype
+        else:
+            for idx, tensor_info in enumerate(node.input_tensor_infos):
+                tensor_info.inf_dtype = tensor_info.orig_dtype
+                node.input_tensor_force_inf_dtype[idx] = tensor_info.inf_dtype
+    else:
+        # add can fused with gemm.
+        if (
+            node.input_tensor_infos[0] is not None
+            and node.input_tensor_infos[0] in conv_gemm_node.output_tensor_infos
+        ):
+            node.input_tensor_infos[0].inf_dtype = node.input_tensor_infos[0].orig_dtype
+            node.input_tensor_force_inf_dtype[0] = node.input_tensor_infos[0].inf_dtype
+            # TODO: set another input's dtype for conv nodes when oneDNN is ready.
+            if conv_node is None or not _check_has_quantizable_node_after_node(node):
+                # set another input's dtype, if another's input is from non-quantizable op, we can remove the fake quant.
+                reset_input_inf_dtype_to_orig_dtype(node, 1)
+        elif (
+            node.input_tensor_infos[1] is not None
+            and node.input_tensor_infos[1] in conv_gemm_node.output_tensor_infos
+        ):
+            node.input_tensor_infos[1].inf_dtype = node.input_tensor_infos[1].orig_dtype
+            node.input_tensor_force_inf_dtype[1] = node.input_tensor_infos[1].inf_dtype
+            # TODO: set another input's dtype for conv nodes when oneDNN is ready.
+            if conv_node is None or not _check_has_quantizable_node_after_node(node):
+                # set another input's dtype, if another's input is from non-quantizable op, we can remove the fake quant.
+                reset_input_inf_dtype_to_orig_dtype(node, 0)
+
+
+# get a default recipe
+def get_default_recipe(nodes):
+    r"""
+    This function is about get default recipe which set where fake quant is inserted for the quantizable ops.
+    """
+    # step1: Quantization state init. Quantize inputs before quantizable node by setting their input's inf_dtype to
+    # qconfig.activation().dtype, and also setting the weight's inf_dtype to
+    # qconfig.weight().dtype if a module has a weight.
+    _default_recipe_init(nodes)
+    # step2: Optimization
+    # 1. For conv, gemm, and LSTM,  we always quantize its' inputs and weight, so we keep them state.
+    #    and for embedding_bag, which only has a weight, we always quantize it's weight to
+    #    save memory space and bandwidth, we also keep it's state.
+    # 2. For remaining quantizable ops (pooling, elt-wise op and add) which meet the following requirements, we will
+    # update them inputs' quantization state.
+    #   1. If it is a part of a quantized fusion pattern, don't need to quantize any inputs from inside the pattern.
+    #   2. If any of its inputs outside the fusion pattern are from non-quantized op, don't quantize all inputs outside the pattern.
+    #   3. If it is not part of a quantized fusion pattern, don't quantize all inputs if its one input from non-quantized op.
+    # 3. For quantizable ops (pooling, relu, flatten, interation and embedding) forcing quantized output, need to \
+    #    quantize its output if it is quantized.
+    # 4. For interation and embedding, we only support s8->s8 symmetric quantization, so if doesn't meet the \
+    #    requiresments, don't need to quantize its inputs.
+    # Note: the fusion pattern we are supported is conv/gemm/add + elt-wise, conv/gemm + add, conv/gemm + add + elt-wise.
+    # which means some ops can be combined with a single op to compute, but they are mathematically equivalent.
+    embedding_bag_ops = [
+        str(torch.embedding_bag),
+        str(F.embedding_bag),
+        str(torch.nn.EmbeddingBag),
+    ]
+    for node in nodes:
+        if isinstance(node, ParentNode):
+            continue
+        if node.type == 'torch_ipex.merged_emb_with_cat':
+            continue
+        if node.qconfig is not None and node.type not in (
+            conv_gemm_ops + rnn_ops + embedding_bag_ops
+        ):
+            if node.type in add_ops:
+                # gemm+add fusion
+                _add_recipe(node)
+            elif node.type in elt_wise_q_ops:
+                # don't have a pre_node, we can say it doesn't have a pre quantizable node.
+                has_pre_quantized_node = True
+                # If Has gemm(add) pre_op can be fused, not insert fake quant.
+                if len(node.pre_nodes) > 0:
+                    if (
+                        _find_fused_node_with_cur_elt_wise(
+                            node, conv_gemm_ops + add_ops + add_inplace_ops
+                        )
+                        is not None
+                    ):
+                        has_pre_quantized_node = False
+                    else:
+                        has_pre_quantized_node = (
+                            _check_has_quantizable_node_before_node(node.pre_nodes[0])
+                        )
+                else:
+                    has_pre_quantized_node = False
+                if not has_pre_quantized_node:
+                    node.input_tensor_infos[0].inf_dtype = node.input_tensor_infos[
+                        0
+                    ].orig_dtype
+                    node.input_tensor_force_inf_dtype[0] = node.input_tensor_infos[
+                        0
+                    ].inf_dtype
+            else:
+                # For other quantizable node, we don't need add fake quant before it if it's pre node is one none-quantizable op.
+                # Now all other quantizable node only have one input info, so we can check the one pre input node info to check
+                # whether has a pre quantizable node.
+                has_pre_quantized_node = True
+                if len(node.pre_nodes) == 1:
+                    has_pre_quantized_node = _check_has_quantizable_node_before_node(
+                        node.pre_nodes[0]
+                    )
+                elif len(node.pre_nodes) == 0:
+                    has_pre_quantized_node = False
+                # the node's pre node doesn't support int8 output.
+                if not has_pre_quantized_node:
+                    node.input_tensor_infos[0].inf_dtype = node.input_tensor_infos[
+                        0
+                    ].orig_dtype
+                    node.input_tensor_force_inf_dtype[0] = node.input_tensor_infos[
+                        0
+                    ].inf_dtype
+
+    set_node_output_quantized(nodes)
diff --git a/intel_extension_for_pytorch/quantization/_utils.py b/intel_extension_for_pytorch/quantization/_utils.py
index 57fa0d4f..6e28b0ed 100644
--- a/intel_extension_for_pytorch/quantization/_utils.py
+++ b/intel_extension_for_pytorch/quantization/_utils.py
@@ -68,6 +68,7 @@ int8_int8_ops = set(
         # ipex customer op
         str(interaction),
         str(torch.ops.torch_ipex.interaction_forward),
+        str(torch.ops.torch_ipex.merged_emb_with_cat),
         str(torch.embedding_bag),
         str(F.embedding_bag),
         str(torch.nn.EmbeddingBag),
@@ -467,6 +468,7 @@ def _check_after_nodes_all_quantized_give_node(node):
                 int8_int8_symmetric_ops = [
                     str(interaction),
                     str(torch.ops.torch_ipex.interaction_forward),
+                    str(torch.ops.torch_ipex.merged_emb_with_cat),
                     str(torch.embedding_bag),
                     str(F.embedding_bag),
                     str(torch.nn.EmbeddingBag),
@@ -475,6 +477,7 @@ def _check_after_nodes_all_quantized_give_node(node):
                     if next.type in [
                         str(interaction),
                         str(torch.ops.torch_ipex.interaction_forward),
+                        str(torch.ops.torch_ipex.merged_emb_with_cat),
                     ]:
                         # node.input_tensor_infos may be set, we can use force_inf_dtype to check whether this op is quantizabled.
                         for force_inf_dtype in next.input_tensor_force_inf_dtype:
@@ -537,6 +540,7 @@ def set_node_output_quantized(nodes):
             elif node.type in [
                 str(interaction),
                 str(torch.ops.torch_ipex.interaction_forward),
+                str(torch.ops.torch_ipex.merged_emb_with_cat),
             ]:
                 if (
                     node.input_tensor_force_inf_dtype[0] == torch.qint8
diff --git a/scripts/compile_bundle.sh b/scripts/compile_bundle.sh
index 891ea4ee..ca48cdd3 100644
--- a/scripts/compile_bundle.sh
+++ b/scripts/compile_bundle.sh
@@ -2,88 +2,81 @@
 set -x
 set -e
 
-VER_IPEX="llm_feature_branch"
+VER_LLVM="llvmorg-13.0.0"
+VER_PYTORCH=""
+VER_TORCHVISION=""
+VER_TORCHAUDIO=""
+VER_IPEX="master"
 
 # Check existance of required Linux commands
-for CMD in python git nproc conda; do
+for CMD in gcc g++ python git nproc; do
     command -v ${CMD} || (echo "Error: Command \"${CMD}\" not found." ; exit 4)
 done
+echo "You are using GCC: $(gcc --version | grep gcc)"
 
 MAX_JOBS_VAR=$(nproc)
 if [ ! -z "${MAX_JOBS}" ]; then
     MAX_JOBS_VAR=${MAX_JOBS}
 fi
 
-conda install -y gcc==12.3 gxx==12.3 cxx-compiler -c conda-forge
-
 # Save current directory path
 BASEFOLDER=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
 cd ${BASEFOLDER}
 # Checkout individual components
+if [ ! -d llvm-project ]; then
+    git clone https://github.com/llvm/llvm-project.git
+fi
 if [ ! -d intel-extension-for-pytorch ]; then
     git clone https://github.com/intel/intel-extension-for-pytorch.git
 fi
 
 # Checkout required branch/commit and update submodules
-if [ ! -d cmake ]; then
-    wget https://github.com/llvm/llvm-project/releases/download/llvmorg-16.0.6/cmake-16.0.6.src.tar.xz
-    tar -xvf cmake-16.0.6.src.tar.xz
-    mv cmake-16.0.6.src cmake
+cd llvm-project
+if [ ! -z ${VER_LLVM} ]; then
+    git checkout ${VER_LLVM}
 fi
-if [ ! -d llvm ]; then
-    wget https://github.com/llvm/llvm-project/releases/download/llvmorg-16.0.6/llvm-16.0.6.src.tar.xz
-    tar -xvf llvm-16.0.6.src.tar.xz
-    mv llvm-16.0.6.src llvm
-fi
-cd intel-extension-for-pytorch
+git submodule sync
+git submodule update --init --recursive
+cd ../intel-extension-for-pytorch
 if [ ! -z ${VER_IPEX} ]; then
     git checkout ${VER_IPEX}
 fi
 git submodule sync
 git submodule update --init --recursive
-cd ..
 
 # Install dependencies
 python -m pip install cmake
-python -m pip install torch==2.1.0.dev20230711+cpu torchvision==0.16.0.dev20230711+cpu torchaudio==2.1.0.dev20230711+cpu --index-url https://download.pytorch.org/whl/nightly/cpu
+python -m pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu
 ABI=$(python -c "import torch; print(int(torch._C._GLIBCXX_USE_CXX11_ABI))")
 
 # Compile individual component
-export CC=${CONDA_PREFIX}/bin/gcc
-export CXX=${CONDA_PREFIX}/bin/g++
-export LD_PRELOAD=${CONDA_PREFIX}/lib/libstdc++.so
-
 #  LLVM
-LLVM_ROOT="$(pwd)/release"
-if [ -d ${LLVM_ROOT} ]; then
-    rm -rf ${LLVM_ROOT}
-fi
-mkdir ${LLVM_ROOT}
+cd ../llvm-project
 if [ -d build ]; then
     rm -rf build
 fi
 mkdir build
 cd build
-cmake -G "Unix Makefiles" -DCMAKE_INSTALL_PREFIX=${LLVM_ROOT} -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS="-D_GLIBCXX_USE_CXX11_ABI=${ABI}" -DLLVM_TARGETS_TO_BUILD=X86 -DLLVM_ENABLE_TERMINFO=OFF -DLLVM_INCLUDE_TESTS=OFF -DLLVM_INCLUDE_EXAMPLES=OFF -DLLVM_INCLUDE_BENCHMARKS=OFF ../llvm/
-make install -j $MAX_JOBS
+cmake -G "Unix Makefiles" -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS="-D_GLIBCXX_USE_CXX11_ABI=${ABI}" -DLLVM_TARGETS_TO_BUILD=X86 -DLLVM_ENABLE_TERMINFO=OFF -DLLVM_INCLUDE_TESTS=OFF -DLLVM_INCLUDE_EXAMPLES=OFF ../llvm/
+cmake --build . -j ${MAX_JOBS_VAR}
+LLVM_ROOT="$(pwd)/../release"
+if [ -d ${LLVM_ROOT} ]; then
+    rm -rf ${LLVM_ROOT}
+fi
+cmake -DCMAKE_INSTALL_PREFIX=${LLVM_ROOT}/../release/ -P cmake_install.cmake
+#xargs rm -rf < install_manifest.txt
 ln -s ${LLVM_ROOT}/bin/llvm-config ${LLVM_ROOT}/bin/llvm-config-13
 export PATH=${LLVM_ROOT}/bin:$PATH
 export LD_LIBRARY_PATH=${LLVM_ROOT}/lib:$LD_LIBRARY_PATH
+cd ..
 #  Intel® Extension for PyTorch*
 cd ../intel-extension-for-pytorch
 python -m pip install -r requirements.txt
 export USE_LLVM=${LLVM_ROOT}
 export LLVM_DIR=${USE_LLVM}/lib/cmake/llvm
 export DNNL_GRAPH_BUILD_COMPILER_BACKEND=1
-export IPEX_VERSION=2.1.0.dev0+cpu.llm
-export IPEX_VERSIONED_BUILD=0
-CXXFLAGS_BK=${CXXFLAGS}
-export CXXFLAGS="${CXXFLAGS} -D__STDC_FORMAT_MACROS"
 python setup.py clean
 python setup.py bdist_wheel 2>&1 | tee build.log
-export CXXFLAGS=${CXXFLAGS_BK}
-unset IPEX_VERSIONED_BUILD
-unset IPEX_VERSION
 unset DNNL_GRAPH_BUILD_COMPILER_BACKEND
 unset LLVM_DIR
 unset USE_LLVM
diff --git a/setup.py b/setup.py
index d6220c79..c3638213 100644
--- a/setup.py
+++ b/setup.py
@@ -314,12 +314,15 @@ def get_submodule_commit(base_dir, submodule_dir):
 
 
 def get_build_version(ipex_git_sha):
+    pkg_type = "xpu" if _check_env_flag("BUILD_WITH_XPU") else "cpu"
     ipex_version = os.getenv("IPEX_VERSION", get_version_num())
     if _check_env_flag("IPEX_VERSIONED_BUILD", default="1"):
         try:
             ipex_version += "+git" + ipex_git_sha[:7]
         except Exception:
             pass
+    else:
+        ipex_version += "+" + pkg_type
     return ipex_version
 
 
@@ -713,23 +716,23 @@ class IPEXCPPLibBuild(build_clib, object):
                 cmake_exec, project_root_dir, cmake_args_cpu, ipex_cpu_build_dir, my_env
             )
 
-            ## Generate cmake for the CPP UT
-            #build_option_cpp_test = {
-            #    **build_option_common,
-            #    "PROJECT_DIR": project_root_dir,
-            #    "PYTORCH_INSTALL_DIR": pytorch_install_dir,
-            #    "CPP_TEST_BUILD_DIR": get_cpp_test_build_dir(),
-            #}
-
-            #cmake_args_cpp_test = []
-            #define_build_options(cmake_args_cpp_test, **build_option_cpp_test)
-            #_gen_build_cfg_from_cmake(
-            #    cmake_exec,
-            #    get_cpp_test_dir(),
-            #    cmake_args_cpp_test,
-            #    get_cpp_test_build_dir(),
-            #    my_env,
-            #)
+            # Generate cmake for the CPP UT
+            build_option_cpp_test = {
+                **build_option_common,
+                "PROJECT_DIR": project_root_dir,
+                "PYTORCH_INSTALL_DIR": pytorch_install_dir,
+                "CPP_TEST_BUILD_DIR": get_cpp_test_build_dir(),
+            }
+
+            cmake_args_cpp_test = []
+            define_build_options(cmake_args_cpp_test, **build_option_cpp_test)
+            _gen_build_cfg_from_cmake(
+                cmake_exec,
+                get_cpp_test_dir(),
+                cmake_args_cpp_test,
+                get_cpp_test_build_dir(),
+                my_env,
+            )
 
         if _get_build_target() in ["develop", "python"]:
             # Generate cmake for common python module:
@@ -777,8 +780,8 @@ class IPEXCPPLibBuild(build_clib, object):
             # Build CPU module:
             _build_project(build_args, ipex_cpu_build_dir, my_env, use_ninja)
 
-            ## Build the CPP UT
-            #_build_project(build_args, get_cpp_test_build_dir(), my_env, use_ninja)
+            # Build the CPP UT
+            _build_project(build_args, get_cpp_test_build_dir(), my_env, use_ninja)
 
         if _get_build_target() in ["develop", "python"]:
             # Build common python module:
diff --git a/tests/cpu/autocast_test_lists.py b/tests/cpu/autocast_test_lists.py
index 937e8dda..1be08532 100644
--- a/tests/cpu/autocast_test_lists.py
+++ b/tests/cpu/autocast_test_lists.py
@@ -425,14 +425,6 @@ class AutocastCPUTestLists(object):
                     torch.randn((1, 1, 768), device=dev, dtype=torch.float16),
                 ),
             ),
-            (
-                "_scaled_dot_product_attention",
-                (
-                    torch.randn((1, 1, 768), device=dev, dtype=torch.float16),
-                    torch.randn((1, 1, 768), device=dev, dtype=torch.float16),
-                    torch.randn((1, 1, 768), device=dev, dtype=torch.float16),
-                ),
-            ),
             ("adaptive_avg_pool2d", dummy_fp16[2], {"output_size": (4, 4)}),
             ("adaptive_avg_pool3d", dummy_fp16[3], {"output_size": (4, 4, 4)}),
             ("upsample_nearest1d", dummy_fp16[2], {"output_size": (n)}),
@@ -486,14 +478,6 @@ class AutocastCPUTestLists(object):
                     torch.randn((1, 1, 768), device=dev, dtype=torch.bfloat16),
                 ),
             ),
-            (
-                "_scaled_dot_product_attention",
-                (
-                    torch.randn((1, 1, 768), device=dev, dtype=torch.bfloat16),
-                    torch.randn((1, 1, 768), device=dev, dtype=torch.bfloat16),
-                    torch.randn((1, 1, 768), device=dev, dtype=torch.bfloat16),
-                ),
-            ),
             ("upsample_nearest1d", dummy_bf16[2], {"output_size": (n)}),
             ("upsample_nearest2d", dummy_bf16[3], {"output_size": (n, n)}),
             ("upsample_nearest3d", dummy_bf16[4], {"output_size": (n, n, n)}),
diff --git a/tests/cpu/common_utils.py b/tests/cpu/common_utils.py
index b6a8376f..f130f8f4 100644
--- a/tests/cpu/common_utils.py
+++ b/tests/cpu/common_utils.py
@@ -59,6 +59,7 @@ import gc
 import types
 import inspect
 import argparse
+import itertools
 import unittest
 import warnings
 import random
@@ -1755,3 +1756,14 @@ class TestModule(torch.nn.Module):
         for idx in range(len(weight.storage())):
             if weight.storage()[idx] == 0:
                 weight.grad.storage()[idx] = 0
+
+
+def _empty_weight_bias_parameter_names(prefixes):
+    param_names = [
+        "_ipex_module_empty_weight_tensor",
+        "_ipex_module_empty_bias_tensor",
+    ]
+    return [
+        f"{prefix}.{param_name}"
+        for prefix, param_name in itertools.product(prefixes, param_names)
+    ]
diff --git a/tests/cpu/cpp/CMakeLists.txt b/tests/cpu/cpp/CMakeLists.txt
index 70e05e56..985d9ff6 100644
--- a/tests/cpu/cpp/CMakeLists.txt
+++ b/tests/cpu/cpp/CMakeLists.txt
@@ -46,7 +46,6 @@ add_subdirectory(${THIRD_PARTY_ROOT}/googletest ${CPP_TEST_BUILD_DIR}/third_part
 # Add the Test Files
 set(IPEX_CPP_TEST_SOURCES test_runtime_api.cpp test_dyndisp_and_isa_api.cpp)
 
-
 add_executable(${CPU_CPP_TEST_NAME} ${IPEX_CPP_TEST_SOURCES})
 
 set(BUILD_STATIC_ONEMKL ON)
diff --git a/tests/cpu/iakv_test.py b/tests/cpu/iakv_test.py
deleted file mode 100644
index ce25d421..00000000
--- a/tests/cpu/iakv_test.py
+++ /dev/null
@@ -1,88 +0,0 @@
-import torch
-import torch.nn as nn
-import intel_extension_for_pytorch as ipex
-from common_utils import TestCase
-import unittest
-from typing import Optional, Tuple, Union
-from torch.nn import functional as F
-import time 
-
-class MaskedMHA(torch.nn.Module):
-    def __init__(self, hidden_size=4096, n_head=16, n_head_kv=16, head_dim = 256):
-        super().__init__()
-        self.num_heads = n_head
-        self.num_kv = n_head_kv
-        self.head_dim = head_dim
-        self.query_key_value = nn.Linear(hidden_size, (n_head_kv * 2 + n_head) * head_dim)
-        
-    def _split_heads(self, fused_qkv: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
-        """
-        Split the last dimension into (num_heads, head_dim), results share same memory
-        storage as `fused_qkv`
-
-        Args:
-            fused_qkv (`torch.tensor`, *required*): [batch_size, seq_length, (num_heads + kv_num * 2) * head_dim]
-
-        Returns:
-            query: [batch_size, seq_length, num_heads, head_dim]
-            key: [batch_size, seq_length, kv_num, head_dim]
-            value: [batch_size, seq_length, kv_num, head_dim]
-        """
-        bs = fused_qkv.shape[0]
-        query_layer = fused_qkv[:, :, : self.num_heads * self.head_dim]
-        query_layer = query_layer.view(bs, -1, self.num_heads, self.head_dim)
-        key_layer = fused_qkv[:, :, self.num_heads * self.head_dim : (self.num_heads + self.num_kv) * self.head_dim]
-        key_layer = key_layer.view(bs, -1, self.num_kv, self.head_dim)
-        value_layer = fused_qkv[:, :, (self.num_heads + self.num_kv) * self.head_dim :]
-        value_layer = value_layer.view(bs, -1, self.num_kv, self.head_dim)
-        return query_layer, key_layer, value_layer
-    
-    def _repeat_kv(self, x: torch.Tensor, n_rep: int) -> torch.Tensor:
-        "torch.repeat_interleave(x, dim=2, repeats=n_rep)"
-        bs, slen, n_kv_heads, head_dim = x.shape
-        if n_rep == 1:
-            return x
-        return(
-            x[:,:,:,None,:]
-            .expand(bs, slen, n_kv_heads, n_rep, head_dim)
-            .reshape(bs, slen, n_kv_heads * n_rep, head_dim)
-        )    
-        
-    def forward(self, input_t, key_cache, value_cache, max_position, attention_mask, beam_idx, indirect_access_kv_cache=True, offset=0):
-        head_size= self.head_dim
-        #self.query_key_value(input_t)
-        #linear_res=  torch.randn(input_t.shape[0], input_t.shape[1], (self.num_heads + self.num_kv * 2) * head_size, dtype=torch.bfloat16)
-        query = torch.randn(input_t.shape[0], input_t.shape[1], self.num_heads, self.head_dim, dtype=torch.bfloat16)
-        key = query.clone()
-        value = key.clone()
-        return torch.ops.torch_ipex.masked_multihead_self_attention(query, key, value, key_cache, value_cache, beam_idx, offset, head_size**0.5, max_position, None, attention_mask)
-
-mha = MaskedMHA()
-max_seq_len=2048
-head_num=16
-beam_size=4
-head_size=256
-batch_size=1
-input_t = torch.randn(batch_size*beam_size, 1, head_num * head_size, dtype=torch.bfloat16)
-key_cache_iakv = torch.randn(max_seq_len, beam_size*batch_size, head_num, head_size, dtype=torch.bfloat16) 
-value_cache_iakv = torch.randn(max_seq_len, beam_size*batch_size, head_num, head_size, dtype=torch.bfloat16)
-beam_idx = torch.zeros(max_seq_len, beam_size*batch_size, dtype=torch.int64)   
-offset = 2016 
-attention_mask = torch.zeros(batch_size*beam_size, 1, 1, offset+1, dtype=torch.bfloat16)  
-count =10000
-total_time = 0
-for i in range(count):
-    start =time.time()
-    with torch.inference_mode(), torch.no_grad(), torch.autocast(
-                            device_type="cpu",
-                            enabled=True,
-                            dtype=torch.bfloat16,
-                        ):
-        indirect_access_kv_cache_output, _, key_cache_iakv, value_cache_iakv, beam_idx = mha(input_t, key_cache_iakv, value_cache_iakv, max_seq_len, attention_mask, beam_idx, True, torch.tensor(offset))
-    end = time.time()
-    if i>=5:
-        total_time += end-start
-print("iakv time: ", total_time/(count-5))
-        
-        
-    
\ No newline at end of file
diff --git a/tests/cpu/test_ao_jit_ipex_quantization.py b/tests/cpu/test_ao_jit_ipex_quantization.py
index 3d6aae98..de0f1abb 100644
--- a/tests/cpu/test_ao_jit_ipex_quantization.py
+++ b/tests/cpu/test_ao_jit_ipex_quantization.py
@@ -179,30 +179,67 @@ class TestIpexOps(JitLlgaTestCase):
         class M(nn.Module):
             def __init__(self):
                 super(M, self).__init__()
-                self.m = nn.EmbeddingBag(10, 3, mode="sum", sparse=True)
+                self.m = nn.EmbeddingBag(10, 110, mode="sum", sparse=True)
 
             def forward(self, input, offset):
                 x = self.m(input, offset)
                 return x
 
-        # This will call in F.embeddingbag
-        m = nn.EmbeddingBag(10, 3, mode="sum", sparse=True)
-        input = torch.LongTensor([1, 2, 4, 5, 4, 3, 2, 9])
-        offsets = torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7])
-
-        graph = self.checkQuantizeTrace(
-            m, [input, offsets], atol=1e-2, qconfig=static_qconfig[1]
-        )
-        self.assertGraphContainsExactly(graph, "ipex::qembedding_bag", 1)
-        # test nn.EmbeddingBag
-        m = M().eval()
-        input = torch.LongTensor([1, 2, 4, 5, 4, 3, 2, 9])
-        offsets = torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7])
+        def get_input(bag_size_1):
+            if bag_size_1:
+                return torch.LongTensor(
+                    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
+                ), torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
+            else:
+                return torch.LongTensor(
+                    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
+                ), torch.LongTensor([0])
+
+        def fake_quant(tensor, scale, zp):
+            qtensor = torch.quantize_per_tensor(tensor, scale, zp, torch.qint8)
+            return qtensor.dequantize()
+
+        def get_expect(module, input, offsets):
+            def _calculate_scale(max_val, min_val):
+                min_val_neg = torch.min(min_val, torch.zeros_like(min_val))
+                max_val_pos = torch.max(max_val, torch.zeros_like(max_val))
+                max_val_pos = torch.max(-min_val_neg, max_val_pos)
+                scale = max_val_pos / 127.5
+                scale = max(scale.item(), torch.finfo(torch.float32).eps)
+                return scale
+
+            _module = copy.deepcopy(module)
+            y = _module(input, offsets)
+            o_scale = _calculate_scale(y.max(), y.min())
+            if isinstance(_module, nn.EmbeddingBag):
+                w_scale = _calculate_scale(_module.weight.max(), _module.weight.min())
+                _module.weight.data = fake_quant(_module.weight, w_scale, 0)
+            else:
+                w_scale = _calculate_scale(
+                    _module.m.weight.max(), _module.m.weight.min()
+                )
+                _module.m.weight.data = fake_quant(_module.m.weight, w_scale, 0)
+            expect = _module(input, offsets)
+            return fake_quant(expect, o_scale, 0)
 
-        graph = self.checkQuantizeTrace(
-            m, [input, offsets], atol=1e-2, qconfig=static_qconfig[1]
-        )
-        self.assertGraphContainsExactly(graph, "ipex::qembedding_bag", 1)
+        # This will call in F.embeddingbag
+        with torch.no_grad():
+            for bag_size_1 in [True, False]:
+                input, offsets = get_input(bag_size_1)
+                m = nn.EmbeddingBag(10, 110, mode="sum", sparse=True)
+                y = get_expect(m, input, offsets)
+                tol = 1e-2 if bag_size_1 else 5e-2
+                graph = self.checkQuantizeTrace(
+                    m, [input, offsets], qconfig=static_qconfig[1], expect_result=y
+                )
+                self.assertGraphContainsExactly(graph, "ipex::qembedding_bag", 1)
+                # test nn.EmbeddingBag
+                m = M().eval()
+                y = get_expect(m, input, offsets)
+                graph = self.checkQuantizeTrace(
+                    m, [input, offsets], qconfig=static_qconfig[1], expect_result=y
+                )
+                self.assertGraphContainsExactly(graph, "ipex::qembedding_bag", 1)
 
     def test_interaction_int8(self):
         class M(nn.Module):
diff --git a/tests/cpu/test_ao_jit_llga_utils.py b/tests/cpu/test_ao_jit_llga_utils.py
index 38cb81ee..6dc19dc7 100644
--- a/tests/cpu/test_ao_jit_llga_utils.py
+++ b/tests/cpu/test_ao_jit_llga_utils.py
@@ -153,6 +153,7 @@ class JitLlgaTestCase(JitTestCase):
         int8_bf16=False,
         freeze=True,
         x_kwarg=None,
+        expect_result=None,
     ):
         if x is None and x_kwarg is None:
             raise AssertionError(
@@ -169,8 +170,9 @@ class JitLlgaTestCase(JitTestCase):
         with torch.no_grad():
             y = self.model_forward_helper(fp32_model, x, x_kwarg)
             y = y.to(torch.bfloat16) if int8_bf16 else y
+            expect = expect_result if expect_result is not None else y
             y_llga = self.model_forward_helper(traced_model, x, x_kwarg)
-            self.assertEqual(y, y_llga, atol=atol, rtol=rtol)
+            self.assertEqual(expect, y_llga, atol=atol, rtol=rtol)
 
             # test Fallback when input shape changes:
             if x_var:
diff --git a/tests/cpu/test_auto_channels_last.py b/tests/cpu/test_auto_channels_last.py
index 9ea52cab..8da0de02 100644
--- a/tests/cpu/test_auto_channels_last.py
+++ b/tests/cpu/test_auto_channels_last.py
@@ -271,10 +271,10 @@ class TestAutoChannelsLast(TestCase):
 
         # enable auto channels_last
         ipex.enable_auto_channels_last()
-        self.assertTrue(_test_conv(2).is_contiguous(memory_format=torch.channels_last))
+
+        self.assertTrue(_test_conv(2).is_contiguous(memory_format = torch.channels_last))
         # temporary disable before https://github.com/pytorch/pytorch/pull/74023 merged
         # self.assertTrue(_test_conv(3).is_contiguous(memory_format = torch.channels_last_3d))
 
-
-if __name__ == "__main__":
+if __name__ == '__main__':
     test = unittest.main()
diff --git a/tests/cpu/test_cpu_ops.py b/tests/cpu/test_cpu_ops.py
index 25a996d6..bbcf6259 100644
--- a/tests/cpu/test_cpu_ops.py
+++ b/tests/cpu/test_cpu_ops.py
@@ -868,7 +868,7 @@ class CPUOPsTester(TestCase):
             self.assertTrue(x2.grad.is_contiguous(memory_format=torch.channels_last))
             self.assertEqual(x1.grad, x2.grad)
 
-            for dtype in [torch.bfloat16, torch.double, torch.int64]:
+            for dtype in [torch.bfloat16, torch.double, torch.int64, torch.float16]:
                 x3 = x.clone().detach().to(dtype)
                 x4 = x.clone().detach().to(dtype).to(memory_format=torch.channels_last)
                 if dtype != torch.int64:
@@ -1312,7 +1312,7 @@ class CPUOPsTester(TestCase):
                 self.assertTrue(y1_5.dtype == torch.float32)
 
     def test_cat(self):
-        for datatype in [torch.float32, torch.double, torch.bfloat16]:
+        for datatype in [torch.float32, torch.double, torch.bfloat16, torch.float16]:
             for dim, size in itertools.product([0, 1], [[2, 1], [2, 2], [5, 10]]):
                 x = torch.randn(size, dtype=datatype)
                 y = torch.cat([x, x], dim)
diff --git a/tests/cpu/test_deepspeed.py b/tests/cpu/test_deepspeed.py
index ef7b1d0e..cab0a282 100644
--- a/tests/cpu/test_deepspeed.py
+++ b/tests/cpu/test_deepspeed.py
@@ -1,18 +1,15 @@
 import sys
 import os
-import tempfile
 import unittest
 
 import torch
 import torch.nn as nn
-from torch.testing._internal.jit_utils import JitTestCase
-from torch.testing import FileCheck
+from torch.testing._internal.common_utils import TestCase
 import intel_extension_for_pytorch as ipex
 from intel_extension_for_pytorch.nn.utils._weight_prepack import (
     may_import_deepspeed_modules,
     _IPEXLinear,
     _IPEXLinearAllreduce,
-    _IPEXLmHeadLinearAllreduce,
 )
 from intel_extension_for_pytorch.quantization import prepare, convert
 from intel_extension_for_pytorch.quantization._quantize import (
@@ -46,115 +43,32 @@ class MyBlock(nn.Module):
         return z
 
 
-class MyModel(nn.Module):
-    def __init__(self):
-        super().__init__()
-        # For deepspeed support, please do not change the ModuleList structure of the class.
-        self.linears = nn.ModuleList([MyBlock()])
-
-    def forward(self, x):
-        for l in self.linears:
-            x = l(x)
-        return x
-
-
 # For deepspeed support, please do not change the name of the class.
-class MyLmHeadModel(nn.Module):
+class MyModel(nn.Module):
     def __init__(self):
         super().__init__()
         # For deepspeed support, please do not change the ModuleList structure of the class.
         self.linears = nn.ModuleList([MyBlock()])
-        self.lm_head = nn.Linear(2, 2)
 
     def forward(self, x):
         for l in self.linears:
             x = l(x)
-        x = self.lm_head(x)
         return x
 
 
 # The class DeepSpeedTestM is written for deepspeed to recognize the modules and to be functional.
 # Please do not change it.
 class DeepSpeedTestM(nn.Module):
-    def __init__(self, module_type):
-        super().__init__()
-        self.linear = module_type()
-
-    def forward(self, x):
-        z = self.linear(x)
-        return z
-
-
-class GPTJAttention(nn.Module):
     def __init__(self):
         super().__init__()
-        self.q_proj = nn.Linear(4096, 4096, bias=False)
-        self.out_proj = nn.Linear(4096, 4096, bias=False)
-
-    def forward(self, x):
-        x = self.q_proj(x)
-        z = self.out_proj(x)
-        return z
-
-
-class GPTJMLP(nn.Module):
-    def __init__(self, krnl="tpp"):
-        super().__init__()
-        self.krnl = krnl
-        self.fc_in = nn.Linear(4096, 16384, bias=True)
-        self.fc_out = nn.Linear(16384, 4096, bias=True)
-        self.dropout = nn.Dropout()
-
-    def forward(self, x):
-        if self.krnl is "onednn":
-            x = self.fc_in(x)
-            x = nn.functional.gelu(x, approximate="tanh")
-        else:
-            x = torch.ops.torch_ipex.tpp_linear_gelu(
-                x, self.fc_in.weight, self.fc_in.bias
-            )
-        x = self.fc_out(x)
-        x = self.dropout(x)
-        return x
-
-
-class GPTJBlock(nn.Module):
-    def __init__(self, krnl):
-        super().__init__()
-        self.ln = nn.LayerNorm(4096, eps=1e-05)
-        self.attn = GPTJAttention()
-        self.mlp = GPTJMLP(krnl)
-
-    def forward(self, x):
-        x = self.ln(x)
-        y = self.attn(x)
-        z = self.mlp(x)
-        x = y + z + x
-        return x
-
-
-class GPTJModel(nn.Module):
-    def __init__(self, krnl):
-        super().__init__()
-        self.linears = nn.ModuleList([GPTJBlock(krnl)])
-
-    def forward(self, x):
-        for l in self.linears:
-            x = l(x)
-        return x
-
-
-class GPTJTestM(nn.Module):
-    def __init__(self, krnl):
-        super().__init__()
-        self.linear = GPTJModel(krnl)
+        self.linear = MyModel()
 
     def forward(self, x):
         z = self.linear(x)
         return z
 
 
-class DeepspeedTester(JitTestCase):
+class DeepspeedTester(TestCase):
     def _get_ds_model(self, m_linear):
         import deepspeed
 
@@ -175,55 +89,39 @@ class DeepspeedTester(JitTestCase):
     def test_ipex_optimize(self):
         deepspeed_modules = may_import_deepspeed_modules()
         if deepspeed_modules is not None:
-            LinearAllreduce, LinearLayer = deepspeed_modules[:2]
-            # TODO: remove check_lm_head logic once deepspeed LmHeadLinearAllreduce change has been upstream-ed.
-            check_lm_head = False
-            if len(deepspeed_modules) == 3:
-                check_lm_head = True
-                LmHeadLinearAllreduce = deepspeed_modules[2]
-
-            x = torch.randn(2, 3, 4)
-            m_linear = DeepSpeedTestM(MyLmHeadModel).eval()
+            LinearAllreduce, LinearLayer = deepspeed_modules
+            x = torch.randn(2, 4)
+            m_linear = DeepSpeedTestM().eval()
             y = m_linear(x)
 
             ds_model = self._get_ds_model(m_linear)
             self.assertTrue(module_found(ds_model, LinearLayer))
             self.assertTrue(module_found(ds_model, LinearAllreduce))
-            if check_lm_head:
-                self.assertTrue(module_found(ds_model, LmHeadLinearAllreduce))
 
             optimized = ipex.optimize(ds_model.eval(), inplace=True)
+            jit_optimized = torch.jit.trace(optimized, x)
+            jit_optimized = torch.jit.freeze(jit_optimized)
+            self.assertTrue(module_found(optimized, _IPEXLinear))
+            self.assertTrue(module_found(optimized, _IPEXLinearAllreduce))
 
-            with torch.no_grad():
-
-                y_optimized = optimized(x)
-                self.assertEqual(y, y_optimized)
-
-                jit_optimized = torch.jit.trace(optimized, x)
-                jit_optimized = torch.jit.freeze(jit_optimized)
-                self.assertTrue(module_found(optimized, _IPEXLinear))
-                self.assertTrue(module_found(optimized, _IPEXLinearAllreduce))
-
-                if check_lm_head:
-                    self.assertTrue(module_found(optimized, _IPEXLmHeadLinearAllreduce))
-
-                jit_optimized(x)
-                graph = jit_optimized.graph_for(x)
-                jit_res = jit_optimized(x)
-                self.assertEqual(y, jit_res)
+            optimized = optimized(x)
+            jit_res = jit_optimized(x)
+            self.assertEqual(y, jit_res)
+            self.assertEqual(y, optimized)
 
-    def _test_quantization(self, dynamic_qconfig, qmodules, graph_strings):
+    def test_dynamic_quantization(self):
         deepspeed_modules = may_import_deepspeed_modules()
         if deepspeed_modules is not None:
-            LinearAllreduce, LinearLayer = deepspeed_modules[:2]
+            LinearAllreduce, LinearLayer = deepspeed_modules
             x = torch.randn(2, 4)
-            m_linear = DeepSpeedTestM(MyModel).eval()
+            m_linear = DeepSpeedTestM().eval()
             y = m_linear(x)
 
             ds_model = self._get_ds_model(m_linear)
             self.assertTrue(module_found(ds_model, LinearLayer))
             self.assertTrue(module_found(ds_model, LinearAllreduce))
 
+            dynamic_qconfig = ipex.quantization.default_dynamic_qconfig
             prepared_model = prepare(
                 ds_model,
                 dynamic_qconfig,
@@ -232,86 +130,10 @@ class DeepspeedTester(JitTestCase):
                 bn_folding=False,
             )
             converted = convert(prepared_model, inplace=True)
-            self.assertTrue(
-                all(module_found(converted, qmodule) for qmodule in qmodules)
-            )
-
-            y_quantized = converted(x)
-            self.assertEqual(y, y_quantized, atol=0.005, rtol=1.3e-6)
-
-            with torch.no_grad():
-                converted = torch.jit.trace(converted, x)
-                traced = torch.jit.freeze(converted)
-
-                traced(x)  # profiling run
-                graph = traced.graph_for(x)
-                for graph_string in graph_strings:
-                    FileCheck().check(graph_string).run(graph)
-
-                y_traced = traced(x)
-                self.assertEqual(y, y_traced, atol=0.005, rtol=1.3e-6)
-
-                with tempfile.TemporaryDirectory() as tmp:
-                    path = os.path.join(tmp, "ds_model.pt")
-
-                    torch.jit.save(traced, path)
-                    loaded = torch.jit.load(path)
-
-                    loaded(x)  # profiling run
-                    graph_loaded = loaded.graph_for(x)
-                    for graph_string in graph_strings:
-                        FileCheck().check(graph_string).run(graph_loaded)
-
-                    y_loaded = loaded(x)
-                    self.assertEqual(y, y_loaded, atol=0.005, rtol=1.3e-6)
-
-    def test_dynamic_quantization(self):
-        self._test_quantization(
-            ipex.quantization.default_dynamic_qconfig,
-            [DynamicQuantizedLinearLayer, DynamicQuantizedLinearAllreduce],
-            ["quantized::linear_dynamic", "deepspeed_comm::all_reduce"],
-        )
-
-    def test_weight_only_quantization(self):
-        self._test_quantization(
-            ipex.quantization.get_weight_only_quant_qconfig_mapping(),
-            [
-                ipex.nn.modules.weight_only_quantization.IpexWoqLinear,
-                ipex.nn.modules.weight_only_quantization.IpexWoqLinearAllreduce,
-            ],
-            ["torch_ipex::ipex_woq_linear", "deepspeed_comm::all_reduce"],
-        )
-
-    def test_simplify_allreduce_for_gptj(self):
-        deepspeed_modules = may_import_deepspeed_modules()
-        if deepspeed_modules is not None:
-            ds_pattern = "deepspeed_comm::all_reduce"
-            x = torch.rand(4, 32, 4096)
-            for krnl in ["onednn", "tpp"]:
-                m = GPTJTestM(krnl).eval()
-                ds_model = self._get_ds_model(m)
-                if krnl is "tpp":
-                    ipex.tpp.Apply_TPP_optimization(
-                        ds_model, dtype=torch.bfloat16, distributed=True
-                    )
-                optimized = ipex.optimize(
-                    ds_model.eval(),
-                    inplace=True,
-                    auto_kernel_selection=True if krnl is "onednn" else False,
-                )
-                with torch.no_grad():
-                    y = optimized(x)
-                    jit_optimized = torch.jit.trace(
-                        optimized, x, strict=False, check_trace=False
-                    )
-                    jit_optimized = torch.jit.freeze(jit_optimized)
-                    graph = jit_optimized.graph_for(x)
-                    self.assertGraphContainsExactly(graph, ds_pattern, 2)
-                    jit_optimized(x)
-                    graph = jit_optimized.graph_for(x)
-                    self.assertGraphContainsExactly(graph, ds_pattern, 1)
-                    jit_res = jit_optimized(x)
-                    self.assertEqual(y, jit_res)
+            self.assertTrue(module_found(converted, DynamicQuantizedLinearLayer))
+            self.assertTrue(module_found(converted, DynamicQuantizedLinearAllreduce))
+            quantized = converted(x)
+            self.assertEqual(y, quantized, atol=0.005, rtol=1.3e-6)
 
 
 if __name__ == "__main__":
diff --git a/tests/cpu/test_emb.py b/tests/cpu/test_emb.py
index c32bc21d..e973222c 100644
--- a/tests/cpu/test_emb.py
+++ b/tests/cpu/test_emb.py
@@ -139,7 +139,7 @@ class TestEMB(TestCase):
             )
 
     def test_emb_fast_path(self):
-        for options in ([True, False], [True, False]):
+        for options in itertools.product([True, False], [True, False]):
             include_last_offset, sparse = options
             self._test_emb(
                 mode="sum", sparse=sparse, include_last_offset=include_last_offset
@@ -158,11 +158,14 @@ class TestEMB(TestCase):
         emb = Embeddingbag().eval()
         input = torch.LongTensor([1, 2, 4, 5, 4, 3, 2, 9])
         offsets = torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7])
-        for dtype, backend, dynamic in itertools.product(
-            [torch.float32, torch.bfloat16], ["ipex", "inductor"], [True, False]
+        for dtype, compiler_backend, dynamic in itertools.product(
+            [torch.float32, torch.bfloat16],
+            ["torchscript", "inductor"],
+            [True, False],
         ):
             torch._dynamo.reset()
-            emb_torchcompile = torch.compile(emb, backend=backend, dynamic=dynamic)
+            ipex._set_compiler_backend(compiler_backend)
+            emb_torchcompile = torch.compile(emb, dynamic=dynamic, backend="ipex")
             with torch.cpu.amp.autocast(
                 enabled=(dtype == torch.bfloat16)
             ), torch.no_grad():
diff --git a/tests/cpu/test_init_on_device.py b/tests/cpu/test_init_on_device.py
deleted file mode 100644
index 80d5248e..00000000
--- a/tests/cpu/test_init_on_device.py
+++ /dev/null
@@ -1,39 +0,0 @@
-# Copyright (c) Microsoft Corporation.
-# SPDX-License-Identifier: Apache-2.0
-# DeepSpeed Team
-# https://github.com/microsoft/DeepSpeed/blob/55243f3bc8d4e751734ee2000fe3979bd4b6228c/tests/unit/utils/test_init_on_device.py
-
-# Copyright (c) 2023 Intel Corporation
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-import torch
-import intel_extension_for_pytorch as ipex
-from common_utils import TestCase, TestModule
-import unittest
-
-
-class TestOnDevice(TestCase):
-    def test_on_device(self):
-        for device in ("cpu", "meta"):
-            with ipex._IPEXOnDevice(dtype=torch.half, device=device):
-                model = TestModule()
-
-            for p in model.parameters():
-                self.assertEqual(p.device, torch.device(device))
-                self.assertEqual(p.dtype, torch.half)
-                assert p.device == torch.device(device)
-                assert p.dtype == torch.half
-
-
-if __name__ == "__main__":
-    test = unittest.main()
diff --git a/tests/cpu/test_ipex_optimize.py b/tests/cpu/test_ipex_optimize.py
index 21600405..072b6890 100644
--- a/tests/cpu/test_ipex_optimize.py
+++ b/tests/cpu/test_ipex_optimize.py
@@ -21,7 +21,7 @@ from torch.optim import (
 import unittest
 import itertools
 import copy
-from common_utils import TestModule
+from common_utils import TestModule, _empty_weight_bias_parameter_names
 from intel_extension_for_pytorch.optim._lamb import Lamb
 import os
 
@@ -573,7 +573,16 @@ class TestOptimizeCases(TestCase):
             prefix, attr = p1[0].split(".")
             sub_m = getattr(ipex_inf_model, prefix)
             param = getattr(sub_m, attr)
-            self.assertNotEqual(p1[1], param)
+            # the empty weight and bias tensor will always be Tensor()
+            assert_fn = (
+                self.assertEqual
+                if p1[0]
+                in _empty_weight_bias_parameter_names(
+                    prefixes=["conv", "linear", "conv_transpose2d"]
+                )
+                else self.assertNotEqual
+            )
+            assert_fn(p1[1], param)
 
         # check parameters are same after load
         ipex_inf_model.load_state_dict(ipex_model_state)
@@ -644,7 +653,16 @@ class TestOptimizeCases(TestCase):
         for p1, p2 in zip(
             ipex_model.named_parameters(), ref_ipex_model.named_parameters()
         ):
-            self.assertNotEqual(p1[1], p2[1])
+            # the empty weight and bias tensor will always be Tensor()
+            assert_fn = (
+                self.assertEqual
+                if p1[0]
+                in _empty_weight_bias_parameter_names(
+                    prefixes=["conv", "linear", "conv_transpose2d"]
+                )
+                else self.assertNotEqual
+            )
+            assert_fn(p1[1], p2[1])
         for (_, v1), (_, v2) in zip(
             ipex_optimizer.state.items(), ref_ipex_optimizer.state.items()
         ):
diff --git a/tests/cpu/test_jit.py b/tests/cpu/test_jit.py
index 40be9814..ec9074a5 100644
--- a/tests/cpu/test_jit.py
+++ b/tests/cpu/test_jit.py
@@ -60,6 +60,7 @@ import contextlib
 import torch
 import torch.nn as nn
 import torch.fx.experimental.optimization as optimization
+from torch.optim import SGD
 from torch.testing import FileCheck
 import copy
 
@@ -5405,15 +5406,25 @@ class Tester(TestCase):
             torch.randn(2, 3),
             torch.randn(1, 3, 56, 56),
         ]
-        auto_kernel_selection_config = [True, False]
 
         for module, data in zip(modules, inputs):
-            for auto_kernel_selection in auto_kernel_selection_config:
+            for auto_kernel_selection, train_and_eval in itertools.product(
+                [True, False], [True, False]
+            ):
                 # Currently auto_kernel_selection only shows different behavior for nn.Linear
                 if auto_kernel_selection and not isinstance(module, nn.Linear):
                     continue
 
                 model = M(module)
+                if train_and_eval:
+                    model.train()
+                    origin_optimizer1 = SGD(model.parameters(), lr=0.01, momentum=0.9)
+                    model, _ = ipex.optimize(
+                        model,
+                        optimizer=origin_optimizer1,
+                        auto_kernel_selection=auto_kernel_selection,
+                    )
+
                 model.eval()
                 optimized = ipex.optimize(
                     model, auto_kernel_selection=auto_kernel_selection
diff --git a/tests/cpu/test_jit_llga_fuser.py b/tests/cpu/test_jit_llga_fuser.py
index abc59b43..a70fbd61 100644
--- a/tests/cpu/test_jit_llga_fuser.py
+++ b/tests/cpu/test_jit_llga_fuser.py
@@ -980,8 +980,8 @@ class TestDebugLog(JitLlgaTestCase):
             stderr=subprocess.STDOUT,
         ) as p:
             for line in p.stdout.readlines():
-                line = str(line, "utf-8").strip()
-                if line.__contains__("LLGA_bridge::prepareRunArgs"):
+                line = str(line, 'utf-8').strip()
+                if line.__contains__("LLGA_bridge::prepareKernel"):
                     num += 1
                 if line.__contains__("Executing partition"):
                     num_debug_str += 1
diff --git a/tests/cpu/test_masked_mha.py b/tests/cpu/test_masked_mha.py
deleted file mode 100644
index aa3ddc57..00000000
--- a/tests/cpu/test_masked_mha.py
+++ /dev/null
@@ -1,219 +0,0 @@
-import torch
-import torch.nn as nn
-import intel_extension_for_pytorch as ipex
-from common_utils import TestCase
-import unittest
-from typing import Optional, Tuple, Union
-
-class MaskedMHA(torch.nn.Module):
-    def __init__(self, hidden_size=4096, n_head=16, n_head_kv=16, head_dim = 256):
-        super().__init__()
-        self.num_heads = n_head
-        self.num_kv = n_head_kv
-        self.head_dim = head_dim
-        self.query_key_value = nn.Linear(hidden_size, (n_head_kv * 2 + n_head) * head_dim)
-        
-    def _split_heads(self, fused_qkv: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
-        """
-        Split the last dimension into (num_heads, head_dim), results share same memory
-        storage as `fused_qkv`
-
-        Args:
-            fused_qkv (`torch.tensor`, *required*): [batch_size, seq_length, (num_heads + kv_num * 2) * head_dim]
-
-        Returns:
-            query: [batch_size, seq_length, num_heads, head_dim]
-            key: [batch_size, seq_length, kv_num, head_dim]
-            value: [batch_size, seq_length, kv_num, head_dim]
-        """
-        bs = fused_qkv.shape[0]
-        query_layer = fused_qkv[:, :, : self.num_heads * self.head_dim]
-        query_layer = query_layer.view(bs, -1, self.num_heads, self.head_dim)
-        key_layer = fused_qkv[:, :, self.num_heads * self.head_dim : (self.num_heads + self.num_kv) * self.head_dim]
-        key_layer = key_layer.view(bs, -1, self.num_kv, self.head_dim)
-        value_layer = fused_qkv[:, :, (self.num_heads + self.num_kv) * self.head_dim :]
-        value_layer = value_layer.view(bs, -1, self.num_kv, self.head_dim)
-        return query_layer, key_layer, value_layer
-    
-    def _repeat_kv(self, x: torch.Tensor, n_rep: int) -> torch.Tensor:
-        "torch.repeat_interleave(x, dim=2, repeats=n_rep)"
-        bs, slen, n_kv_heads, head_dim = x.shape
-        if n_rep == 1:
-            return x
-        return(
-            x[:,:,:,None,:]
-            .expand(bs, slen, n_kv_heads, n_rep, head_dim)
-            .reshape(bs, slen, n_kv_heads * n_rep, head_dim)
-        )    
-        
-    def forward(self, input_t, key_cache, value_cache, max_position, attention_mask, beam_idx, indirect_access_kv_cache=False, offset=0):
-        head_size= self.head_dim
-        query, key, value = self._split_heads(self.query_key_value(input_t))        
-        if indirect_access_kv_cache:
-            query = query.contiguous()
-            key = key.contiguous()
-            value = value.contiguous()
-            return torch.ops.torch_ipex.masked_multihead_self_attention(query, key, value, key_cache, value_cache, beam_idx, offset, head_size**0.5, max_position, None, attention_mask)
-        else:
-            #Get the concatenated key and value
-            if key_cache is not None:
-                key = torch.cat([key_cache, key], dim=1)
-                value = torch.cat([value_cache, value], dim=1)
-            key_cache = key 
-            value_cache = value
-            n_rep = self.num_heads // self.num_kv
-            key = self._repeat_kv(key, n_rep)
-            value = self._repeat_kv(value, n_rep)          
-            
-            key = key.transpose(1, 2)
-            query = query.transpose(1, 2)
-            value = value.transpose(1, 2)            
-            #matmul new_key and new_value to get the attention score
-            attention_scores = torch.matmul(query, key.transpose(-1, -2))
-            #scale the attention score
-            attention_scores = attention_scores / (head_size ** 0.5)
-            #import pdb; pdb.set_trace()
-            if attention_mask is not None:
-                attention_scores = attention_scores + attention_mask
-            #softmax the attention score
-            attention_probs = attention_scores.softmax(dim=-1)
-            #matmul the attention score and value to get the context
-            attention_output = torch.matmul(attention_probs, value)
-            return attention_output, None, key_cache, value_cache, None
-            
-class MaskedMHATest(TestCase):
-    def test_mha(self):
-        beam_size_list = [1, 4]
-        batch_size_list = [1, 2, 4]
-        head_size = 256
-        head_num = 16
-        head_num_kv_list = [1, 4, 16]
-        max_seq_len = 64
-        first_seq_len = 32
-        for batch_size in batch_size_list:          
-            for beam_size in beam_size_list:
-                for head_num_kv in head_num_kv_list:
-                    key_cache = None
-                    value_cache = None
-                    offset = 0  
-                    mha = MaskedMHA(n_head=head_num, n_head_kv=head_num_kv, head_dim = head_size)
-                    #first token decode
-                    input_t = torch.randn(batch_size, first_seq_len, head_num * head_size, dtype=torch.float32)
-                    key_cache_iakv = torch.randn(max_seq_len, beam_size*batch_size, head_num, head_size, dtype=torch.float32) 
-                    value_cache_iakv = torch.randn(max_seq_len, beam_size*batch_size, head_num, head_size, dtype=torch.float32)
-                    beam_idx = torch.zeros(max_seq_len, beam_size*batch_size, dtype=torch.int64)        
-                    #create attention mask and causal mask
-                    attention_mask = torch.zeros(batch_size, 1, first_seq_len, first_seq_len, dtype=torch.float32)
-                    casual_mask = torch.full((first_seq_len, first_seq_len), -1e6, dtype=input_t.dtype)
-                    casual_mask = casual_mask.triu(1)    
-                    casual_mask = casual_mask.unsqueeze(0).unsqueeze(0)
-                    attention_mask = attention_mask + casual_mask #combine the attention mask and causal mask                    
-                    #UT for first token with fp32        
-                    naive_output, _, key_cache, value_cache, _ = mha(input_t, None, None, max_seq_len, attention_mask, None, None)
-                    indirect_access_kv_cache_output, _, key_cache_iakv, value_cache_iakv, beam_idx = mha(input_t, key_cache_iakv, value_cache_iakv, max_seq_len, attention_mask, beam_idx, True, torch.tensor(offset))      
-                    #import pdb ; pdb.set_trace()
-                    print("batch_size:", batch_size, "head_num:", head_num, "head_num_kv: ", head_num_kv)
-                    #self.assertEqual(naive_output, indirect_access_kv_cache_output)     
-                    key_cache = key_cache.repeat_interleave(beam_size, dim=0)
-                    value_cache = value_cache.repeat_interleave(beam_size, dim=0) 
-                    for i in range(batch_size):
-                        self.assertEqual(key_cache.transpose(0,1)[:,i*beam_size, :, :], key_cache_iakv[0:first_seq_len, i*beam_size,:,:])
-                        self.assertEqual(value_cache.transpose(0,1)[:,i*beam_size, :, :], value_cache_iakv[0:first_seq_len, i*beam_size,:,:])                             
-                    if beam_size == 4:    
-                        beam_idx_t = torch.zeros(beam_size*batch_size, dtype=torch.int64)
-                        for i in range(1, batch_size):
-                            beam_idx_t[i*beam_size:i*beam_size+beam_size] = beam_idx_t[i*beam_size:i*beam_size+beam_size] + i*beam_size                              
-                    elif beam_size == 1:
-                        beam_idx_t = torch.arange(batch_size)
-                    beam_idx[offset] = beam_idx_t
-                    #reorder cache for naive impelementation
-                    key_cache = torch.index_select(key_cache, 0, beam_idx_t)
-                    value_cache = torch.index_select(value_cache, 0, beam_idx_t)
-                        
-                    # # #UT for first token with bf16
-                    input_t_bf16 = input_t.bfloat16()
-                    key_cache_iakv_bf16 = key_cache_iakv.bfloat16()
-                    value_cache_iakv_bf16 = value_cache_iakv.bfloat16()
-                    attention_mask_bf16 = attention_mask.bfloat16()
-                    with torch.inference_mode(), torch.no_grad(), torch.autocast(
-                        device_type="cpu",
-                        enabled=True,
-                        dtype=torch.bfloat16,
-                    ):
-                        naive_output_bf16, _, key_cache_bf16, value_cache_bf16, _ = mha(input_t_bf16, None, None, max_seq_len, attention_mask_bf16, None, None)
-                        indirect_access_kv_cache_output_bf16, _, key_cache_iakv_bf16, value_cache_iakv_bf16, beam_idx = mha(input_t_bf16, key_cache_iakv_bf16, value_cache_iakv_bf16, max_seq_len, attention_mask_bf16, beam_idx, True, torch.tensor(offset))
-                        self.assertEqual(naive_output_bf16, indirect_access_kv_cache_output_bf16, prec=2e-2)
-                        key_cache_bf16 = key_cache_bf16.repeat_interleave(beam_size, dim=0)
-                        value_cache_bf16 = value_cache_bf16.repeat_interleave(beam_size, dim=0) 
-                        for i in range(batch_size):
-                            self.assertEqual(key_cache_bf16.transpose(0,1)[:,i*beam_size, :, :], key_cache_iakv_bf16[0:first_seq_len, i*beam_size,:,:])
-                            self.assertEqual(value_cache_bf16.transpose(0,1)[:,i*beam_size, :, :], value_cache_iakv_bf16[0:first_seq_len, i*beam_size,:,:])      
-                        key_cache_bf16 = torch.index_select(key_cache_bf16, 0, beam_idx_t)
-                        value_cache_bf16 = torch.index_select(value_cache_bf16, 0, beam_idx_t)
-                                
-                    offset = offset + first_seq_len
-                    #UT for next token with fp32
-                    input_t = torch.randn(beam_size * batch_size, 1, head_num * head_size, dtype=torch.float32)
-                    attention_mask = torch.zeros(beam_size*batch_size, 1, 1, offset+1, dtype=torch.float32)
-                    naive_output, _, key_cache, value_cache, _ = mha(input_t, key_cache, value_cache, max_seq_len, attention_mask, None, None)
-                    indirect_access_kv_cache_output, _, key_cache_iakv, value_cache_iakv, beam_idx = mha(input_t, key_cache_iakv, value_cache_iakv, max_seq_len, attention_mask, beam_idx, True, torch.tensor(offset))      
-                    self.assertEqual(naive_output, indirect_access_kv_cache_output)
-                    self.assertEqual(key_cache.transpose(0,1)[offset], key_cache_iakv[offset,:,:,:])
-                    self.assertEqual(value_cache.transpose(0,1)[offset], value_cache_iakv[offset,:,:,:])    
-                    # #UT for next token with bf16
-                    input_t_bf16 = input_t.bfloat16()
-                    attention_mask_bf16 = attention_mask.bfloat16()
-                    with torch.inference_mode(), torch.no_grad(), torch.autocast(
-                        device_type="cpu",
-                        enabled=True,
-                        dtype=torch.bfloat16,
-                    ):
-                        naive_output_bf16, _, key_cache_bf16, value_cache_bf16, _ = mha(input_t_bf16, key_cache_bf16, value_cache_bf16, max_seq_len, attention_mask_bf16, None, None)
-                        indirect_access_kv_cache_output_bf16, _, key_cache_iakv_bf16, value_cache_iakv_bf16, beam_idx = mha(input_t_bf16, key_cache_iakv_bf16, value_cache_iakv_bf16, max_seq_len, attention_mask_bf16, beam_idx, True, torch.tensor(offset))
-                        self.assertEqual(naive_output_bf16, indirect_access_kv_cache_output_bf16, prec=0.05)
-                        self.assertEqual(key_cache_bf16.transpose(0,1)[offset], key_cache_iakv_bf16[offset,:,:,:])
-                        self.assertEqual(value_cache_bf16.transpose(0,1)[offset], value_cache_iakv_bf16[offset,:,:,:])       
-                        if beam_size == 4:    
-                            beam_idx_t = torch.tensor([1,3,0,0]).repeat(batch_size)
-                            for i in range(1, batch_size):
-                                beam_idx_t[i*beam_size:i*beam_size+beam_size] = beam_idx_t[i*beam_size:i*beam_size+beam_size] + i*beam_size                           
-                        elif beam_size == 1:
-                            beam_idx_t = torch.arange(batch_size)
-                        beam_idx[offset] = beam_idx_t
-                        offset = offset + 1
-                        #reorder cache for naive impelementation
-                        key_cache = torch.index_select(key_cache, 0, beam_idx_t)
-                        value_cache = torch.index_select(value_cache, 0, beam_idx_t)     
-                        key_cache_bf16 = torch.index_select(key_cache_bf16, 0, beam_idx_t)
-                        value_cache_bf16 = torch.index_select(value_cache_bf16, 0, beam_idx_t)     
-                    #UT for next token with fp32
-                    input_t = torch.randn(beam_size * batch_size, 1, head_num * head_size, dtype=torch.float32)
-                    attention_mask = torch.zeros(beam_size*batch_size, 1, 1, offset+1, dtype=torch.float32)
-                    naive_output, _, key_cache, value_cache, _ = mha(input_t, key_cache, value_cache, max_seq_len, attention_mask, None, None)
-                    indirect_access_kv_cache_output, _, key_cache_iakv, value_cache_iakv, beam_idx = mha(input_t, key_cache_iakv, value_cache_iakv, max_seq_len, attention_mask, beam_idx, True, torch.tensor(offset))      
-                    self.assertEqual(naive_output, indirect_access_kv_cache_output)
-                    self.assertEqual(key_cache.transpose(0,1)[offset], key_cache_iakv[offset,:,:,:])
-                    self.assertEqual(value_cache.transpose(0,1)[offset], value_cache_iakv[offset,:,:,:])    
-                    # #UT for next token with bf16
-                    input_t_bf16 = input_t.bfloat16()
-                    attention_mask_bf16 = attention_mask.bfloat16()
-                    with torch.inference_mode(), torch.no_grad(), torch.autocast(
-                        device_type="cpu",
-                        enabled=True,
-                        dtype=torch.bfloat16,
-                    ):
-                        naive_output_bf16, _, key_cache_bf16, value_cache_bf16, _ = mha(input_t_bf16, key_cache_bf16, value_cache_bf16, max_seq_len, attention_mask_bf16, None, None)
-                        indirect_access_kv_cache_output_bf16, _, key_cache_iakv_bf16, value_cache_iakv_bf16, beam_idx = mha(input_t_bf16, key_cache_iakv_bf16, value_cache_iakv_bf16, max_seq_len, attention_mask_bf16, beam_idx, True, torch.tensor(offset))
-                        self.assertEqual(naive_output_bf16, indirect_access_kv_cache_output_bf16, prec=0.05)
-                        self.assertEqual(key_cache_bf16.transpose(0,1)[offset], key_cache_iakv_bf16[offset,:,:,:])
-                        self.assertEqual(value_cache_bf16.transpose(0,1)[offset], value_cache_iakv_bf16[offset,:,:,:])    
-                             
-if __name__ == "__main__":
-    test = unittest.main()
-    
-           
-      
-            
-            
-            
-        
\ No newline at end of file
diff --git a/tests/cpu/test_optimizer.py b/tests/cpu/test_optimizer.py
index 1c6ab5c1..3e7a2462 100644
--- a/tests/cpu/test_optimizer.py
+++ b/tests/cpu/test_optimizer.py
@@ -5,7 +5,7 @@ import intel_extension_for_pytorch as ipex  # flake8: noqa
 import itertools
 import unittest
 from torch.testing._internal.common_utils import TestCase
-from common_utils import TestModule
+from common_utils import TestModule, _empty_weight_bias_parameter_names
 import bench.custom_op_bench.optimizer
 from torch.optim import Adadelta, AdamW, Adamax, ASGD, RMSprop, Rprop
 import copy
@@ -996,7 +996,13 @@ class TestPatchedMethod(TestCase):
                 y1 = ipex_model(*ipex_model.input).sum()
             y1.backward()
             # check grad are correctly attached
-            for param in ipex_model.parameters():
+            for name, param in ipex_model.named_parameters():
+                # We won't use the grad of the empty weight and bias tensor.
+                # These tensors are only used during inference.
+                if name in _empty_weight_bias_parameter_names(
+                    prefixes=["conv", "linear"]
+                ):
+                    continue
                 self.assertTrue(param.grad is not None)
             uncast_weight = [
                 ipex_model.bn.weight.data_ptr(),
@@ -1014,7 +1020,13 @@ class TestPatchedMethod(TestCase):
             with torch.autograd.profiler.profile() as ipex_prof:
                 ipex_optimizer.zero_grad(set_to_none=set_to_none)
             # check grad are zeroed or are set to none
-            for param in ipex_model.parameters():
+            for name, param in ipex_model.named_parameters():
+                # We won't use the grad of the empty weight and bias tensor.
+                # These tensors are only used during inference.
+                if name in _empty_weight_bias_parameter_names(
+                    prefixes=["conv", "linear"]
+                ):
+                    continue
                 expected_grad = None if set_to_none else torch.zeros_like(param)
                 self.assertEqual(expected_grad, param.grad)
 
diff --git a/tests/cpu/test_quantization_default_recipe.py b/tests/cpu/test_quantization_default_recipe.py
index 81a34e33..0fbde8c5 100644
--- a/tests/cpu/test_quantization_default_recipe.py
+++ b/tests/cpu/test_quantization_default_recipe.py
@@ -443,7 +443,7 @@ class TestDefaultRecipe(JitLlgaTestCase):
         def test(feature, has_bias):
             model = M(feature[1], feature[2], has_bias)
             m = model.eval()
-            data = torch.rand(1, feature[0], feature[1])
+            data = torch.rand(feature[0], feature[1])
             weight = model.linear.weight
             weight_observer = (
                 ipex.quantization.get_weight_only_quant_qconfig_mapping().global_qconfig.weight()
@@ -467,110 +467,46 @@ class TestDefaultRecipe(JitLlgaTestCase):
                 assert isinstance(woq_model.linear, woq_linear_class)
 
                 output2 = woq_model(data)
-                torch.testing.assert_close(output1, output2)
+                torch.testing.assert_close(output1, output2, rtol=1e-04, atol=1e-05)
 
-        shape_list = [
+        case_list = [
             [3, 31, 31],
             [4, 4096, 4096],
             [9, 4095, 4095],
-            [9, 4096, 4096],
             [196, 4095, 16383],
-            [192, 4096, 16384],
         ]
-        use_bias_list = [True, False]
-        cases = itertools.product(shape_list, use_bias_list)
-        for shape, use_bias in cases:
-            test(shape, use_bias)
+        for case in case_list:
+            test(case, True)
+            test(case, False)
 
     def test_weight_only_quantization_autocast(self):
         class M(nn.Module):
-            def __init__(self, input_channel, output_channel, has_bias):
+            def __init__(self, use_bias):
                 super(M, self).__init__()
-                self.linear = torch.nn.Linear(input_channel, output_channel, has_bias)
+                self.linear = torch.nn.Linear(4, 4, use_bias)
 
             def forward(self, x):
                 return self.linear(x)
 
-        def tpp_is_used(N, K):
-            num_threads = torch.get_num_threads()
-            block_n = 32 if N // 64 // num_threads < 4 else 64
-            block_k = 64
-            while K % block_k != 0:
-                block_k //= 2
-                assert block_k > 0
-            return N % block_n == 0 and K % block_k == 0
-
-        def test(feature, has_bias, w_dtype):
-            model = M(feature[1], feature[2], has_bias)
-            m = model.eval()
-            data = torch.rand(feature[0], feature[1])
-                
-            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(weight_dtype=w_dtype)
-            prepared_model = prepare(m, qconfig, example_inputs=data, inplace=False)
-            
-            with torch.no_grad():
-                weight = m.linear.weight
-                weight_observer = qconfig.global_qconfig.weight()
-                weight_observer(weight)
-                weight_int8 = _quantize_weight(weight, weight_observer)
-                weight_fp32 = weight_int8.dequantize()
-                weight_bf16 = weight_fp32.bfloat16()
-                weight_fp16 = weight_fp32.half()
-                data_bf16 = data.bfloat16()
-                data_fp16 = data_bf16.half()
-                bias_fp32 = m.linear.bias
-                use_tpp = tpp_is_used(feature[2], feature[1])
-                if use_tpp:
-                    # if M >= 32, compute in bf16
-                    # if M < 32, compute in fp32 or fp16. Depends on fp16 support.
-                    if feature[0] >= 32:
-                        output1 = torch.matmul(data_bf16.float(), weight_bf16.float().T).bfloat16()
-                        if has_bias:
-                            output1 = output1 + bias_fp32.bfloat16()
-                    else:
-                        output1_fp32 = torch.matmul(data_bf16.float(), weight_bf16.float().T)
-                        if has_bias:
-                            output1_fp32 = output1_fp32 + bias_fp32
-                        output1_fp16 = torch.matmul(data_fp16.float(), weight_fp16.float().T).half()
-                        if has_bias:
-                            output1_fp16 = output1_fp16 + bias_fp32.half()
-                else:
-                    if feature[0] <= 4:
-                        output1 = torch.matmul(data_bf16.float(), weight_fp32.T)
-                    else:
-                        output1 = torch.matmul(data_bf16.float(), weight_bf16.float().T)
-                    if has_bias:
-                        output1 = output1 + bias_fp32
-                    output1 = output1.bfloat16()
-                with torch.autocast(device_type='cpu', enabled=True, dtype= torch.bfloat16):
-                    woq_model = convert(prepared_model)
-                    woq_linear_class = ipex.nn.modules.weight_only_quantization.IpexWoqLinear
-                    assert isinstance(woq_model.linear, woq_linear_class)   
-                                            
-                    woq_model = torch.jit.trace(woq_model, data)
-                    woq_model = torch.jit.freeze(woq_model)
-                    output2 = woq_model(data)
-                    output2 = output2.bfloat16()
-                if use_tpp and feature[0] < 32:
-                    try:
-                        torch.testing.assert_close(output1_fp32.bfloat16(), output2, atol=0.01, rtol=0.1)
-                    except Exception as e:
-                        torch.testing.assert_close(output1_fp16.bfloat16(), output2, atol=0.01, rtol=0.1)
-                else:
-                    torch.testing.assert_close(output1, output2)
-
-        shape_list = [
-            [3, 31, 31],
-            # [4, 4096, 4096], # not supported by TPP yet (block_n = 16 issue)
-            [9, 4095, 4095],
-            [196, 4095, 16383],
-        ]
-        use_bias_list = [True, False]
-        w_dtype_list = [torch.qint8, torch.quint4x2]
-        cases = itertools.product(shape_list, use_bias_list, w_dtype_list)
-        for shape, use_bias, w_dtype in cases:
-            test(shape, use_bias, w_dtype)
-
+        with torch.autocast(device_type="cpu", enabled=True, dtype=torch.bfloat16):
+            use_bias_list = [True, False]
+            w_dtype_list = [torch.qint8, torch.quint4x2]
+            cases = itertools.product(use_bias_list, w_dtype_list)
+            for use_bias, w_dtype in cases:
+                m = M(use_bias).eval()
+                x = torch.rand(4, 4)
+                qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
+                    weight_dtype=w_dtype
+                )
+                prepared_model = ipex.quantization.prepare(
+                    m, qconfig, example_inputs=x, inplace=False
+                )
+                woq_model = ipex.quantization.convert(prepared_model)
+                woq_model(x)
+                woq_linear_class = (
+                    ipex.nn.modules.weight_only_quantization.IpexWoqLinear
+                )
+                assert isinstance(woq_model.linear, woq_linear_class)
 
     def test_weight_only_quantization_jit_save_load(self):
         class M(nn.Module):
@@ -581,12 +517,12 @@ class TestDefaultRecipe(JitLlgaTestCase):
             def forward(self, x):
                 return self.linear(x)
 
-        def test(feature, has_bias, w_dtype):
+        def test(feature, has_bias):
             model = M(feature[1], feature[2], has_bias)
             m = model.eval()
             example_inputs = torch.rand(feature[0], feature[1])
 
-            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(weight_dtype=w_dtype)
+            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping()
             prepared_model = prepare(
                 m, qconfig, example_inputs=example_inputs, inplace=False
             )
@@ -608,329 +544,39 @@ class TestDefaultRecipe(JitLlgaTestCase):
                     output = loaded_model(example_inputs)
                     torch.testing.assert_close(output_ref, output)
 
-        shape_list = [
+        case_list = [
             [3, 31, 31],
             [4, 4096, 4096],
             [9, 4095, 4095],
             [196, 4095, 16383],
         ]
-        use_bias_list = [True, False]
-        w_dtype_list = [torch.qint8, torch.quint4x2]
-        cases = itertools.product(shape_list, use_bias_list, w_dtype_list)
-        for shape, use_bias, w_dtype in cases:
-            test(shape, use_bias, w_dtype)
+        for case in case_list:
+            test(case, True)
+            test(case, False)
 
     def test_weight_only_quantization_quint4x2_weight(self):
         class M(nn.Module):
-            def __init__(self, input_channel, output_channel, has_bias):
+            def __init__(self, use_bias):
                 super(M, self).__init__()
-                self.linear = torch.nn.Linear(input_channel, output_channel, has_bias)
+                self.linear = torch.nn.Linear(4, 4, use_bias)
 
             def forward(self, x):
                 return self.linear(x)
 
-        def test(feature, has_bias):
-            model = M(feature[1], feature[2], has_bias)
-            m = model.eval()
-            data = torch.rand(feature[0], feature[1])
-            weight = model.linear.weight
-            weight_observer = (
-                ipex.quantization.get_weight_only_quant_qconfig_mapping(weight_dtype=torch.quint4x2).global_qconfig.weight()
+        for use_bias in [True, False]:
+            m = M(use_bias).eval()
+            x = torch.rand(4, 4)
+            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
+                weight_dtype=torch.quint4x2
             )
-            weight_observer(weight)
-            weight_int4 = _quantize_weight(weight, weight_observer)
-            weight_fp32 = weight_int4.dequantize()
-            if has_bias:
-                bias = model.linear.bias
-                output1 = torch.matmul(data, weight_fp32.T) + bias
-            else:
-                output1 = torch.matmul(data, weight_fp32.T)
-
-            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(weight_dtype=torch.quint4x2)
-            prepared_model = prepare(m, qconfig, example_inputs=data, inplace=False)
-            with torch.no_grad():
-                woq_model = convert(prepared_model)
-                woq_linear_class = ipex.nn.modules.weight_only_quantization.IpexWoqLinear
-                assert isinstance(woq_model.linear, woq_linear_class)
-        
-                output2 = woq_model(data)
-                torch.testing.assert_close(output1, output2)
-                
-        shape_list = [
-            [3, 31, 31],
-            [4, 4096, 4096],
-            [4, 4096, 4095],
-            [9, 4095, 4095],
-            [196, 4095, 16383],
-        ]
-        use_bias_list = [True, False]
-        cases = itertools.product(shape_list, use_bias_list)
-        for shape, use_bias in cases:
-            test(shape, use_bias)
-
-    def test_weight_only_quantization_gelu_fused_op(self):
-        class Mod(nn.Module):
-            def __init__(self, bias):
-                super().__init__()
-                self.linear = nn.Linear(64, 64, bias=bias)
-                self.gelu = nn.GELU()
-
-            def forward(self, x):
-                return self.gelu(self.linear(x))
-
-        bias_list = [False, True]
-        bf16_list = [False, True]
-        cases = itertools.product(bias_list, bf16_list)
-        for bias, bf16 in cases:
-            with torch.cpu.amp.autocast(enabled=bf16, dtype=torch.bfloat16 if bf16 else None):
-                model = Mod(bias).eval()
-                data = torch.rand(4, 64)
-                qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(lowp_mode=2)
-                prepared_model = prepare(model, qconfig, example_inputs=data, inplace=False)
-                with torch.no_grad():
-                    woq_model = convert(prepared_model)
-                    output1 = woq_model(data)
-                    output2 = torch.ops.torch_ipex.woq_linear_gelu(data, woq_model.linear._op_context.get_data_handle())
-                    torch.testing.assert_close(output1, output2.to(output1.dtype), atol=1e-2, rtol=1e-4)
-
-    def test_weight_only_quantization_add_fused_op(self):
-        class Mod(nn.Module):
-            def __init__(self, bias):
-                super().__init__()
-                self.linear = nn.Linear(64, 64, bias=bias)
-
-            def forward(self, x, others):
-                y = self.linear(x)
-                for o in others:
-                    y = torch.add(y, o)
-                return y
-
-        bias_list = [False, True]
-        bf16_list = [False, True]
-        others_len_list = [1, 2]
-        cases = itertools.product(bias_list, bf16_list, others_len_list)
-        for bias, bf16, others_len in cases:
-            with torch.cpu.amp.autocast(enabled=bf16, dtype=torch.bfloat16 if bf16 else None):
-                model = Mod(bias).eval()
-                data = torch.rand(4, 64)
-                others = [torch.rand(4, 64)] * others_len
-                fused_op = torch.ops.torch_ipex.woq_linear_add if others_len == 1 \
-                    else torch.ops.torch_ipex.woq_linear_add_add
-                qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(lowp_mode=2)
-                prepared_model = prepare(model, qconfig, example_inputs=data, inplace=False)
-                with torch.no_grad():
-                    woq_model = convert(prepared_model)
-                    output1 = woq_model(data, others)
-                    output2 = fused_op(data, woq_model.linear._op_context.get_data_handle(), others)
-                    torch.testing.assert_close(output1, output2.to(output1.dtype), atol=1.5e-2, rtol=1e-3)
-
-    def test_weight_only_quantization_eltwise_fusion(self):
-        return # disabled for now
-        class M(nn.Module):
-            def __init__(self, input_channel, output_channel, has_bias, post_op):
-                super(M, self).__init__()
-                self.linear = torch.nn.Linear(input_channel, output_channel, has_bias)
-                self.post_op = post_op
-
-            def forward(self, x):
-                return self.post_op(self.linear(x))
-
-        def test(feature, has_bias, post_op):
-            post_op_name, post_op = post_op
-            model = M(feature[1], feature[2], has_bias, post_op)
-            m = model.eval()
-            data = torch.rand(feature[0], feature[1])
-            weight = model.linear.weight
-            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping()
-            weight_observer = qconfig.global_qconfig.weight()
-            weight_observer(weight)
-            weight_int8 = _quantize_weight(weight, weight_observer)
-            weight_fp32 = weight_int8.dequantize()
-            if has_bias:
-                bias = model.linear.bias
-                output1 = torch.matmul(data, weight_fp32.T) + bias
-            else:
-                output1 = torch.matmul(data, weight_fp32.T)
-            output1 = post_op(output1)
-
-            prepared_model = prepare(m, qconfig, example_inputs=data, inplace=False)
-            with torch.no_grad():
-                woq_model = convert(prepared_model)
-                woq_linear_class = (
-                    ipex.nn.modules.weight_only_quantization.IpexWoqLinear
-                )
-                assert isinstance(woq_model.linear, woq_linear_class)
-                woq_model = torch.jit.trace(woq_model, data)
-                woq_model = torch.jit.freeze(woq_model)
-                woq_model(data)
-                node_kind_list = [node.kind() for node in woq_model.graph_for(data).nodes()]
-                linear_node_kind = 'torch_ipex::ipex_woq_linear'
-                fused_node_kind = 'torch_ipex::woq_linear_' + post_op_name + '_run'
-                assert linear_node_kind not in node_kind_list and fused_node_kind in node_kind_list
-
-                output2 = woq_model(data)
-
-                torch.testing.assert_close(output1, output2)
-
-        shape_list = [
-            [4, 24, 24],
-            [4, 64, 64],
-            [196, 64, 64],
-        ]
-        use_bias_list = [True, False]
-        post_op_list = [
-            ('relu', nn.ReLU()),
-            ('relu', nn.functional.relu_),
-            ('gelu', nn.GELU(approximate='none')),
-            ('gelu', nn.GELU(approximate='tanh')),
-            ('gelu', torch._C._nn.gelu_),
-        ]
-        cases = itertools.product(shape_list, use_bias_list, post_op_list)
-        for shape, use_bias, post_op in cases:
-            test(shape, use_bias, post_op)
-
-    def test_weight_only_quantization_add_fusion(self):
-        return # disabled for now
-        class M(nn.Module):
-            # on_the_left = True: linear + Y -> Y
-            # on_the_left = False: Y + linear -> Y
-            def __init__(self, ic, oc, has_bias, alpha, on_the_left, use_relu):
-                super(M, self).__init__()
-                self.linear = torch.nn.Linear(ic, oc, has_bias)
-                self.relu = nn.ReLU()
-                self.alpha = alpha
-                self.on_the_left = on_the_left
-                self.use_relu = use_relu
-
-            def forward(self, x, y):
-                x = self.linear(x)
-                if self.on_the_left:
-                    y = torch.add(x, y, alpha=self.alpha)
-                else:
-                    y = torch.add(y, x, alpha=self.alpha)
-                if self.use_relu:
-                    return self.relu(y)
-                return y
-
-        def test(feature, has_bias, alpha, on_the_left, use_relu):
-            if not on_the_left and alpha != 1.0:
-                # alpha must be 1 in this case
-                return
-            model = M(feature[1], feature[2], has_bias, alpha, on_the_left, use_relu)
-            m = model.eval()
-            data = torch.rand(feature[0], feature[1])
-            accu = torch.rand(feature[0], feature[1])
-            weight = model.linear.weight
-            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping()
-            weight_observer = qconfig.global_qconfig.weight()
-            weight_observer(weight)
-            weight_int8 = _quantize_weight(weight, weight_observer)
-            weight_fp32 = weight_int8.dequantize()
-            if has_bias:
-                bias = model.linear.bias
-                output1 = torch.matmul(data, weight_fp32.T) + bias
-            else:
-                output1 = torch.matmul(data, weight_fp32.T)
-            if on_the_left:
-                output1 = output1 + accu * alpha
-            else:
-                output1 = accu + output1 * alpha
-            if use_relu:
-                output1 = nn.functional.relu(output1)
-
-            prepared_model = prepare(m, qconfig, example_inputs=data, inplace=False)
-            with torch.no_grad():
-                inputs = (data, copy.deepcopy(accu)) # accu is overwritten
-                woq_model = convert(prepared_model)
-                woq_model = torch.jit.trace(woq_model, inputs)
-                woq_model = torch.jit.freeze(woq_model)
-                woq_model(*inputs)
-                node_kind_list = [node.kind() for node in woq_model.graph_for(*inputs).nodes()]
-                linear_node_kind = 'torch_ipex::ipex_woq_linear'
-                fused_node_kind = 'torch_ipex::woq_linear_add_relu_run' if use_relu else 'torch_ipex::woq_linear_add_run'
-                assert linear_node_kind not in node_kind_list and fused_node_kind in node_kind_list
-
-                output2 = woq_model(data, accu)
-
-                torch.testing.assert_close(output1, output2)
-
-        shape_list = [
-            [4, 24, 24],
-            [4, 64, 64],
-            [196, 64, 64],
-        ]
-        use_bias_list = [True, False]
-        alpha_list = [1.0, 0.5, 1.5]
-        on_the_left_list = [True, False]
-        use_relu_list = [True, False]
-        cases = itertools.product(shape_list, use_bias_list, alpha_list, on_the_left_list, use_relu_list)
-        for shape, use_bias, alpha, on_the_left, use_relu in cases:
-            test(shape, use_bias, alpha, on_the_left, use_relu)
-
-    def test_weight_only_quantization_lowp_compute(self):
-        from intel_extension_for_pytorch.quantization import WoqLowpMode
-        class M(nn.Module):
-            def __init__(self):
-                super(M, self).__init__()
-                self.linear = torch.nn.Linear(64, 64)
-
-            def forward(self, x):
-                return self.linear(x)
-
-        data = torch.rand(4, 64)
-        m = M()
-        for mode in [WoqLowpMode.FP16, WoqLowpMode.BF16, WoqLowpMode.INT8]:
-            kwargs = {'lowp_mode': mode}
-            if mode == WoqLowpMode.INT8:
-                kwargs['weight_dtype'] = torch.quint4x2
-            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(**kwargs)
-            prepared_model = prepare(m, qconfig, example_inputs=data, inplace=False)
-            with torch.no_grad():
-                woq_model = convert(prepared_model)
-                woq_model(data)
-                assert hasattr(woq_model.linear, '_lowp_mode') and woq_model.linear._lowp_mode == mode, \
-                    'Weight-only quantization: low precision gemm flag is not correctly set'
-
-    def test_weight_only_quantization_num_concats(self):
-        class Mod(nn.Module):
-            def __init__(self):
-                super().__init__()
-                self.q = nn.Linear(64, 64, bias=False)
-                self.k = nn.Linear(64, 64, bias=False)
-                self.v = nn.Linear(64, 64, bias=False)
-
-            def forward(self, x):
-                q = self.q(x)
-                k = self.k(x)
-                v = self.v(x)
-                return q, k, v
-
-        class Mod2(nn.Module):
-            def __init__(self):
-                super().__init__()
-                self.qkv = nn.Linear(64, 64 * 3, bias=False)
-                self.qkv._num_concats = 3
-
-            def forward(self, x):
-                qkv = self.qkv(x).view(3, -1, 64)
-                q, k, v = qkv[0], qkv[1], qkv[2]
-                return q, k, v
+            prepared_model = ipex.quantization.prepare(
+                m, qconfig, example_inputs=x, inplace=False
+            )
+            woq_model = ipex.quantization.convert(prepared_model)
+            woq_model(x)
+            woq_linear_class = ipex.nn.modules.weight_only_quantization.IpexWoqLinear
+            assert isinstance(woq_model.linear, woq_linear_class)
 
-        m = Mod().eval()
-        m2 = Mod2().eval()
-        m2.qkv.weight = nn.Parameter(torch.cat([m.q.weight, m.k.weight, m.v.weight], dim=0))
-        data = torch.rand(4, 64)
-        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(lowp_mode=2)
-        prepared = prepare(m, qconfig, example_inputs=data, inplace=True)
-        prepared2 = prepare(m2, qconfig, example_inputs=data, inplace=True)
-        for bf16 in [False, True]:
-            with torch.no_grad(), \
-                torch.cpu.amp.autocast(enabled=bf16, dtype=torch.bfloat16 if bf16 else None):
-                qm = convert(prepared)
-                qm2 = convert(prepared2)
-                output1 = qm(data)
-                output2 = qm2(data)
-                torch.testing.assert_close(output1, output2, atol=1e-2, rtol=1e-4)
 
 if __name__ == "__main__":
     run_tests()
diff --git a/tests/cpu/test_roialign.py b/tests/cpu/test_roialign.py
index ceda82ae..01da6894 100644
--- a/tests/cpu/test_roialign.py
+++ b/tests/cpu/test_roialign.py
@@ -6,6 +6,7 @@ from common_utils import TestCase
 
 import numpy as np
 import math
+import copy
 
 try:
     import torchvision
@@ -311,10 +312,10 @@ class RoIAlignTester(TestCase):
             )
 
     @skipIfNoTorchVision
-    def test_torchvision_roialign_torchcompile(self):
+    def test_torchvision_roialign_inference_torchcompile(self):
         pool_size = 5
         n_channels = 2 * (pool_size**2)
-        x = torch.rand(2, n_channels, 10, 10)
+        x = torch.rand(2, n_channels, 10, 10).to(memory_format=torch.channels_last)
         rois = torch.tensor(
             [
                 [0, 0, 0, 9, 9],  # format is (xyxy)
@@ -325,13 +326,13 @@ class RoIAlignTester(TestCase):
         )
         pool_h, pool_w = pool_size, pool_size
 
-        # TODO: add dynamic tests when 'ipex' backend supports it.
-        for dtype, backend, dynamic in itertools.product(
-            [torch.float32, torch.bfloat16], ["ipex", "inductor"], [False]
+        for dtype, compiler_backend, dynamic in itertools.product(
+            [torch.float32, torch.bfloat16], ["torchscript", "inductor"], [True, False]
         ):
             torch._dynamo.reset()
+            ipex._set_compiler_backend(compiler_backend)
             torchcompile_torchvision_fn = torch.compile(
-                torchvision_fn, backend=backend, dynamic=dynamic
+                torchvision_fn, dynamic=dynamic, backend="ipex"
             )
             x = x.to(dtype=dtype)
             rois = rois.to(dtype=dtype)
@@ -349,10 +350,10 @@ class RoIAlignTester(TestCase):
                 self.assertTrue(y1.dtype == dtype)
 
     @skipIfNoTorchVision
-    def test_roialign_torchcompile(self):
+    def test_torchvision_roialign_train_torchcompile(self):
         pool_size = 5
         n_channels = 2 * (pool_size**2)
-        x = torch.rand(2, n_channels, 10, 10)
+        input = torch.rand(2, n_channels, 10, 10).to(memory_format=torch.channels_last)
         rois = torch.tensor(
             [
                 [0, 0, 0, 9, 9],  # format is (xyxy)
@@ -362,27 +363,34 @@ class RoIAlignTester(TestCase):
             ]
         )
         pool_h, pool_w = pool_size, pool_size
-        torch._dynamo.allow_in_graph(ipex.nn.modules._roi_align.RoIAlign)
 
-        for dtype, backend, dynamic in itertools.product(
-            [torch.float32, torch.bfloat16], ["ipex", "inductor"], [True, False]
+        for dtype, compiler_backend, dynamic in itertools.product(
+            [torch.float32, torch.bfloat16], ["inductor"], [True, False]
         ):
             torch._dynamo.reset()
-            torchcompile_fn = torch.compile(
-                fn, backend=backend, dynamic=dynamic
+            ipex._set_compiler_backend(compiler_backend)
+            torchcompile_torchvision_fn = torch.compile(
+                copy.deepcopy(torchvision_fn), dynamic=dynamic, backend="ipex"
             )
-            x = x.to(dtype=dtype)
+            input = input.to(dtype=dtype)
             rois = rois.to(dtype=dtype)
+            ori_x = input.clone().requires_grad_()
+            x = input.clone().requires_grad_()
+
             # forward
-            with torch.cpu.amp.autocast(
-                enabled=(dtype == torch.bfloat16)
-            ), torch.no_grad():
-                y0 = fn(x, rois, pool_h, pool_w, spatial_scale=1, sampling_ratio=-1)
-                y1 = torchcompile_fn(
+            with torch.cpu.amp.autocast(enabled=(dtype == torch.bfloat16)):
+                ori_y = torchvision_fn(
+                    ori_x, rois, pool_h, pool_w, spatial_scale=1, sampling_ratio=-1
+                )
+                y = torchcompile_torchvision_fn(
                     x, rois, pool_h, pool_w, spatial_scale=1, sampling_ratio=-1
                 )
-                self.assertEqual(y0, y1)
-                self.assertTrue(y1.dtype == dtype)
+                grad_y = torch.randn(ori_y.shape, dtype=torch.float32)
+                ori_y.backward(grad_y)
+                y.backward(grad_y)
+                self.assertEqual(y, ori_y)
+                self.assertTrue(y.dtype == dtype)
+                self.assertEqual(x.grad, ori_x.grad)
 
 
 if __name__ == "__main__":
diff --git a/tests/cpu/test_torch_compile.py b/tests/cpu/test_torch_compile.py
index 1b62eaa5..87ed24eb 100644
--- a/tests/cpu/test_torch_compile.py
+++ b/tests/cpu/test_torch_compile.py
@@ -1,71 +1,66 @@
 import unittest
 import itertools
+import copy
 import torch
-
+import torch.nn.functional as F
+from torch.optim import SGD
 import intel_extension_for_pytorch as ipex
 
 from common_utils import TestCase
 
 conv_module = {1: torch.nn.Conv1d, 2: torch.nn.Conv2d, 3: torch.nn.Conv3d}
-convtranspose_module = {2: torch.nn.ConvTranspose2d, 3: torch.nn.ConvTranspose3d}
+convtranspose_module = {
+    1: torch.nn.ConvTranspose1d,
+    2: torch.nn.ConvTranspose2d,
+    3: torch.nn.ConvTranspose3d,
+}
 
 
-class ConvNd(torch.nn.Module):
+class ConvNd_Relu(torch.nn.Module):
     def __init__(
         self,
         dim,
         in_channels,
         out_channels,
         kernel_size,
-        stride,
-        padding,
-        dilation,
-        bias,
-        groups,
     ):
-        super(ConvNd, self).__init__()
+        super(ConvNd_Relu, self).__init__()
         self.conv = conv_module[dim](
             in_channels,
             out_channels,
             kernel_size=kernel_size,
-            stride=stride,
-            padding=padding,
-            dilation=dilation,
-            bias=bias,
-            groups=groups,
         )
 
     def forward(self, x):
-        return self.conv(x)
+        return F.relu(self.conv(x))
 
 
-class Linear(torch.nn.Module):
-    def __init__(self, in_f, out_f, bias):
-        super(Linear, self).__init__()
-        self.linear = torch.nn.Linear(in_f, out_f, bias=bias)
+class Linear_Relu(torch.nn.Module):
+    def __init__(self, in_f, out_f):
+        super(Linear_Relu, self).__init__()
+        self.linear = torch.nn.Linear(in_f, out_f)
 
     def forward(self, x):
-        return self.linear(x)
+        return F.relu(self.linear(x))
 
 
-class DeconvNd(torch.nn.Module):
+class DeconvNd_Relu(torch.nn.Module):
     def __init__(
-        self, dim, ic, oc, kernel_size, stride, padding, groups, bias, dilation
+        self,
+        dim,
+        ic,
+        oc,
+        kernel_size,
     ):
-        super(DeconvNd, self).__init__()
+        super(DeconvNd_Relu, self).__init__()
         self.deconv = convtranspose_module[dim](
             ic,
             oc,
             kernel_size=kernel_size,
-            stride=stride,
-            padding=padding,
-            groups=groups,
-            bias=bias,
-            dilation=dilation,
         )
 
     def forward(self, x):
-        return self.deconv(x)
+        return F.relu(self.deconv(x))
 
 
 class Lstm(torch.nn.Module):
@@ -80,167 +75,345 @@ class Lstm(torch.nn.Module):
         return x, h
 
 
-from typing import List
-
-
-def compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):
-    print("in compiler backend")
-    print(gm)
-    return gm.forward
-
-
 class TestCompileCases(TestCase):
-    def test_conv_inference(self):
+    def test_conv_relu_inference(self):
         for dim in [1, 2, 3]:
-            input_shapes = {1: (224,), 2: (224, 224), 3: (55, 55, 55)}
-            # TODO: add bfloat16 data type tests when 'inductor' backend supports bfloat16.
+            input_shapes = {1: (4,), 2: (4, 4), 3: (4, 4, 4)}
             options = itertools.product(
+                [torch.float32, torch.bfloat16],
+                ["torchscript", "inductor"],
                 [True, False],
-                [1, 2],
-                [1, 4],
-                [torch.float32],
-                ["ipex", "inductor"],
                 [True, False],
                 [True, False],
             )
             for (
-                bias,
-                dilation,
-                groups,
                 dtype,
-                backend,
+                compiler_backend,
                 dynamic,
                 ipex_optimize,
+                weight_prepack,
             ) in options:
-                N = torch.randint(1, 10, (1,)).item()
-                M = torch.randint(1, 3, (1,)).item() * groups
-                C = torch.randint(1, 3, (1,)).item() * groups
+                if weight_prepack is True and ipex_optimize is False:
+                    continue
+                N = 2
+                M = 2
+                C = 3
                 x_shape = (N, C) + input_shapes[dim]
                 x = torch.randn(x_shape, dtype=torch.float32)
-                model = ConvNd(
+                model = ConvNd_Relu(
                     dim=dim,
                     in_channels=C,
                     out_channels=M,
                     kernel_size=3,
-                    stride=2,
-                    padding=1,
-                    dilation=dilation,
-                    bias=bias,
-                    groups=groups,
                 ).eval()
+                if ipex_optimize:
+                    # TODO: support channels_last_1d.
+                    if dim == 1:
+                        ipex.disable_auto_channels_last()
+                    else:
+                        ipex.enable_auto_channels_last()
+                    model = ipex.optimize(
+                        model, weights_prepack=weight_prepack, dtype=dtype
+                    )
+                torch._dynamo.reset()
+                ipex._set_compiler_backend(compiler_backend)
+                compile_model = torch.compile(model, dynamic=dynamic, backend="ipex")
                 with torch.cpu.amp.autocast(
                     enabled=(dtype == torch.bfloat16), dtype=torch.bfloat16
                 ), torch.no_grad():
                     ori_y = model(x)
+                    for _ in range(3):
+                        y = compile_model(x)
+                self.assertEqual(y, ori_y)
+                self.assertTrue(y.dtype == dtype)
+
+    def test_conv_relu_train(self):
+        for dim in [1, 2, 3]:
+            input_shapes = {1: (4,), 2: (4, 4), 3: (4, 4, 4)}
+            options = itertools.product(
+                [torch.float32, torch.bfloat16],
+                ["inductor"],
+                [True, False],
+                [True, False],
+                [True, False],
+            )
+            for (
+                dtype,
+                compiler_backend,
+                dynamic,
+                ipex_optimize,
+                weight_prepack,
+            ) in options:
+                if weight_prepack is True and ipex_optimize is False:
+                    continue
+                N = 2
+                M = 2
+                C = 3
+                x_shape = (N, C) + input_shapes[dim]
+                input = torch.randn(x_shape, dtype=torch.float32)
+                ori_x = input.clone().requires_grad_()
+                x = input.clone().requires_grad_()
+                conv = ConvNd_Relu(
+                    dim=dim,
+                    in_channels=C,
+                    out_channels=M,
+                    kernel_size=3,
+                )
+                ori_model = copy.deepcopy(conv).train()
+                model = copy.deepcopy(conv).train()
+                optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)
                 if ipex_optimize:
-                    ipex.enable_auto_channels_last()
-                    model = ipex.optimize(model, dtype=dtype)
+                    # TODO: support channels_last_1d.
+                    if dim == 1:
+                        ipex.disable_auto_channels_last()
+                    else:
+                        ipex.enable_auto_channels_last()
+                    ori_model, _ = ipex.optimize(
+                        ori_model,
+                        weights_prepack=weight_prepack,
+                        dtype=dtype,
+                        optimizer=optimizer,
+                    )
+                    model, _ = ipex.optimize(
+                        model,
+                        weights_prepack=weight_prepack,
+                        dtype=dtype,
+                        optimizer=optimizer,
+                    )
                 torch._dynamo.reset()
-                compile_model = torch.compile(model, dynamic=dynamic, backend=backend)
+                ipex._set_compiler_backend(compiler_backend)
+                compile_model = torch.compile(model, dynamic=dynamic, backend="ipex")
                 with torch.cpu.amp.autocast(
                     enabled=(dtype == torch.bfloat16), dtype=torch.bfloat16
-                ), torch.no_grad():
+                ):
+                    ori_y = ori_model(ori_x)
                     y = compile_model(x)
-                self.assertEqual(y, ori_y)
-                self.assertTrue(y.dtype == dtype)
+                    grad_x = torch.randn(y.shape, dtype=torch.float32)
+                    ori_y.backward(grad_x)
+                    y.backward(grad_x)
+                    self.assertEqual(y, ori_y)
+                    self.assertTrue(y.dtype == dtype)
+                    self.assertEqual(x.grad, ori_x.grad)
 
-    def test_deconv_inference(self):
-        for dim in [2, 3]:
-            input_shapes = {2: (12, 12), 3: (12, 12, 12)}
-            input_channel_per_group = 15
+    def test_deconv_relu_inference(self):
+        for dim in [1, 2, 3]:
+            input_shapes = {1: (4,), 2: (4, 4), 3: (4, 4, 4)}
+            input_channel_per_group = 6
             output_channel_per_group = 3
             kernel_size = 3
-            if dim == 2:
-                channels_last = torch.channels_last
-            else:
-                channels_last = torch.channels_last_3d
-            # TODO: add bfloat16 data type tests when 'inductor' backend supports bfloat16.
             options = itertools.product(
+                [torch.float32, torch.bfloat16],
+                ["torchscript", "inductor"],
                 [True, False],
-                [1, 2],
-                [1, 2],
-                [1, 2],
-                [1, 2],
-                [torch.contiguous_format, channels_last],
-                [torch.float32],
-                ["ipex", "inductor"],
                 [True, False],
                 [True, False],
             )
             for (
-                bias,
-                stride,
-                padding,
-                groups,
-                dilation,
-                memory_format,
                 dtype,
-                backend,
+                compiler_backend,
                 dynamic,
                 ipex_optimize,
+                weight_prepack,
             ) in options:
-                ic = input_channel_per_group * groups
-                oc = output_channel_per_group * groups
+                if weight_prepack is True and ipex_optimize is False:
+                    continue
+                ic = input_channel_per_group
+                oc = output_channel_per_group
                 x_shape = (2, ic) + input_shapes[dim]
                 x = torch.randn(x_shape, dtype=torch.float32)
-                model = DeconvNd(
-                    dim, ic, oc, kernel_size, stride, padding, groups, bias, dilation
-                ).eval()
-                model = model.to(memory_format=memory_format)
-                x = x.to(memory_format=memory_format)
+                model = DeconvNd_Relu(dim, ic, oc, kernel_size).eval()
+                if ipex_optimize:
+                    # TODO: support channels_last_1d.
+                    if dim == 1:
+                        ipex.disable_auto_channels_last()
+                    else:
+                        ipex.enable_auto_channels_last()
+                    model = ipex.optimize(
+                        model, weights_prepack=weight_prepack, dtype=dtype
+                    )
+                torch._dynamo.reset()
+                ipex._set_compiler_backend(compiler_backend)
+                compile_model = torch.compile(model, dynamic=dynamic, backend="ipex")
                 with torch.cpu.amp.autocast(
                     enabled=(dtype == torch.bfloat16), dtype=torch.bfloat16
                 ), torch.no_grad():
                     ori_y = model(x)
+                    for _ in range(3):
+                        y = compile_model(x)
+                self.assertEqual(y, ori_y)
+                self.assertTrue(y.dtype == dtype)
+
+    def test_deconv_relu_train(self):
+        for dim in [1, 2, 3]:
+            input_shapes = {1: (4,), 2: (4, 4), 3: (4, 4, 4)}
+            input_channel_per_group = 6
+            output_channel_per_group = 3
+            kernel_size = 3
+            options = itertools.product(
+                [torch.float32, torch.bfloat16],
+                ["inductor"],
+                [True, False],
+                [True, False],
+                [True, False],
+            )
+            for (
+                dtype,
+                compiler_backend,
+                dynamic,
+                ipex_optimize,
+                weight_prepack,
+            ) in options:
+                if weight_prepack is True and ipex_optimize is False:
+                    continue
+                ic = input_channel_per_group
+                oc = output_channel_per_group
+                x_shape = (2, ic) + input_shapes[dim]
+                input = torch.randn(x_shape, dtype=torch.float32)
+                ori_x = input.clone().requires_grad_()
+                x = input.clone().requires_grad_()
+                deconv = DeconvNd_Relu(dim, ic, oc, kernel_size)
+                ori_model = copy.deepcopy(deconv).train()
+                model = copy.deepcopy(deconv).train()
+                optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)
                 if ipex_optimize:
-                    model = ipex.optimize(model, dtype=dtype)
+                    # TODO: support channels_last_1d.
+                    if dim == 1:
+                        ipex.disable_auto_channels_last()
+                    else:
+                        ipex.enable_auto_channels_last()
+                    ori_model, _ = ipex.optimize(
+                        ori_model,
+                        weights_prepack=weight_prepack,
+                        dtype=dtype,
+                        optimizer=optimizer,
+                    )
+                    model, _ = ipex.optimize(
+                        model,
+                        weights_prepack=weight_prepack,
+                        dtype=dtype,
+                        optimizer=optimizer,
+                    )
                 torch._dynamo.reset()
-                compile_model = torch.compile(model, dynamic=dynamic, backend=backend)
+                ipex._set_compiler_backend(compiler_backend)
+                compile_model = torch.compile(model, dynamic=dynamic, backend="ipex")
                 with torch.cpu.amp.autocast(
                     enabled=(dtype == torch.bfloat16), dtype=torch.bfloat16
-                ), torch.no_grad():
+                ):
+                    ori_y = ori_model(ori_x)
                     y = compile_model(x)
-                self.assertEqual(y, ori_y)
-                self.assertTrue(y.dtype == dtype)
+                    grad_x = torch.randn(ori_y.shape, dtype=torch.float32)
+                    ori_y.backward(grad_x)
+                    y.backward(grad_x)
+                    self.assertEqual(y, ori_y)
+                    self.assertTrue(y.dtype == dtype)
+                    self.assertEqual(x.grad, ori_x.grad)
 
-    def test_linear_inference(self):
-        out_features = torch.randint(3, 10, (1,)).item()
-        in_features = torch.randint(3, 10, (1,)).item()
-
-        input_shapes = [(8, in_features), (2, 4, in_features), (2, 2, 2, in_features)]
-        # TODO: add bfloat16 data type tests when 'inductor' backend supports bfloat16.
+    def test_linear_relu_inference(self):
+        out_features = 4
+        in_features = 3
+        input_shapes = [(2, in_features), (2, 2, in_features), (2, 2, 2, in_features)]
         options = itertools.product(
-            [True, False],
             input_shapes,
-            [torch.float32],
-            ["ipex", "inductor"],
+            [torch.float32, torch.bfloat16],
+            ["torchscript", "inductor"],
+            [True, False],
             [True, False],
             [True, False],
         )
-        for bias, x_shape, dtype, backend, dynamic, ipex_optimize in options:
+        for (
+            x_shape,
+            dtype,
+            compiler_backend,
+            dynamic,
+            ipex_optimize,
+            weight_prepack,
+        ) in options:
+            if weight_prepack is True and ipex_optimize is False:
+                continue
             x = torch.randn(x_shape, dtype=torch.float32)
-            model = Linear(in_features, out_features, bias).eval()
+            model = Linear_Relu(in_features, out_features).eval()
+            if ipex_optimize:
+                model = ipex.optimize(
+                    model, weights_prepack=weight_prepack, dtype=dtype
+                )
+            torch._dynamo.reset()
+            ipex._set_compiler_backend(compiler_backend)
+            compile_model = torch.compile(model, dynamic=dynamic, backend="ipex")
             with torch.cpu.amp.autocast(
                 enabled=(dtype == torch.bfloat16), dtype=torch.bfloat16
             ), torch.no_grad():
                 ori_y = model(x)
+                for _ in range(3):
+                    y = compile_model(x)
+            self.assertEqual(y, ori_y, prec=0.01)
+            self.assertTrue(y.dtype == dtype)
+
+    def test_linear_relu_train(self):
+        out_features = 4
+        in_features = 3
+
+        input_shapes = [(2, in_features), (2, 2, in_features), (2, 2, 2, in_features)]
+        options = itertools.product(
+            input_shapes,
+            [torch.float32, torch.bfloat16],
+            ["inductor"],
+            [True, False],
+            [True, False],
+            [True, False],
+        )
+        for (
+            x_shape,
+            dtype,
+            compiler_backend,
+            dynamic,
+            ipex_optimize,
+            weight_prepack,
+        ) in options:
+            if weight_prepack is True and ipex_optimize is False:
+                continue
+            input = torch.randn(x_shape, dtype=torch.float32)
+            ori_x = input.clone().requires_grad_()
+            x = input.clone().requires_grad_()
+            linear = Linear_Relu(in_features, out_features)
+            ori_model = copy.deepcopy(linear).train()
+            model = copy.deepcopy(linear).train()
+            optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)
             if ipex_optimize:
-                model = ipex.optimize(model, dtype=dtype)
+                ori_model, _ = ipex.optimize(
+                    ori_model,
+                    weights_prepack=weight_prepack,
+                    dtype=dtype,
+                    optimizer=optimizer,
+                )
+                model, _ = ipex.optimize(
+                    model,
+                    weights_prepack=weight_prepack,
+                    dtype=dtype,
+                    optimizer=optimizer,
+                )
             torch._dynamo.reset()
-            compile_model = torch.compile(model, dynamic=dynamic, backend=backend)
+            ipex._set_compiler_backend(compiler_backend)
+            compile_model = torch.compile(model, dynamic=dynamic, backend="ipex")
             with torch.cpu.amp.autocast(
                 enabled=(dtype == torch.bfloat16), dtype=torch.bfloat16
-            ), torch.no_grad():
+            ):
+                ori_y = ori_model(ori_x)
                 y = compile_model(x)
-            self.assertEqual(y, ori_y)
-            self.assertTrue(y.dtype == dtype)
+                grad_x = torch.randn(ori_y.shape, dtype=torch.float32)
+                ori_y.backward(grad_x)
+                y.backward(grad_x)
+                self.assertEqual(y, ori_y, prec=0.01)
+                self.assertTrue(y.dtype == dtype)
+                self.assertEqual(x.grad, ori_x.grad, prec=0.01)
 
     def test_lstm_inference(self):
-        # TODO: add bfloat16 data type tests when 'inductor' backend supports bfloat16.
         options = itertools.product(
-            [torch.float32], ["ipex", "inductor"], [True, False], [True, False]
+            [torch.float32, torch.bfloat16],
+            ["torchscript", "inductor"],
+            [True, False],
+            [True, False],
         )
-        for dtype, backend, dynamic, ipex_optimize in options:
+        for dtype, compiler_backend, dynamic, ipex_optimize in options:
             input = torch.randn(5, 3, 10)
             h0 = torch.randn(2, 3, 20)
             c0 = torch.randn(2, 3, 20)
@@ -252,7 +425,8 @@ class TestCompileCases(TestCase):
             if ipex_optimize:
                 model = ipex.optimize(model, dtype=dtype)
             torch._dynamo.reset()
-            compile_model = torch.compile(model, dynamic=dynamic, backend=backend)
+            ipex._set_compiler_backend(compiler_backend)
+            compile_model = torch.compile(model, dynamic=dynamic, backend="ipex")
             with torch.cpu.amp.autocast(
                 enabled=(dtype == torch.bfloat16), dtype=torch.bfloat16
             ), torch.no_grad():
diff --git a/tests/cpu/test_tpp_ops.py b/tests/cpu/test_tpp_ops.py
index bebda4f5..c4e423a1 100644
--- a/tests/cpu/test_tpp_ops.py
+++ b/tests/cpu/test_tpp_ops.py
@@ -1,7 +1,6 @@
 import unittest
 import torch
 import random
-import copy
 import numpy
 import intel_extension_for_pytorch as ipex
 
@@ -178,75 +177,6 @@ class TPPOPsTester(TestCase):
         self._test_backward(
             hf_res, tpp_res, hf_intermediate, tpp_intermediate, prec=0.01
         )
-    def test_tpp_gptj_attention_rope(self):
-        def create_sinusoidal_positions(num_pos: int, dim: int) -> torch.Tensor:
-            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2) / dim))
-            sinusoid_inp = torch.einsum("i , j -> i j", torch.arange(num_pos, dtype=torch.float), inv_freq).float()
-            return torch.cat((torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)), dim=1)
-        def _get_embed_positions(embed_positions, position_ids):
-            if embed_positions.device != position_ids.device:
-                embed_positions = embed_positions.to(position_ids.device)
-                self.embed_positions = embed_positions
-            return embed_positions.repeat(position_ids.shape[0], 1, 1)
-        def rotate_every_two(x: torch.Tensor) -> torch.Tensor:
-            x1 = x[:, :, :, ::2]
-            x2 = x[:, :, :, 1::2]
-            x = torch.stack((-x2, x1), dim=-1)
-            return x.flatten(-2)  # in einsum notation: rearrange(x, '... d j -> ... (d j)')
-        def apply_rotary_pos_emb(tensor: torch.Tensor, sin: torch.Tensor, cos: torch.Tensor) -> torch.Tensor:
-            sin = torch.repeat_interleave(sin[:, :, None, :], 2, 3)
-            cos = torch.repeat_interleave(cos[:, :, None, :], 2, 3)
-            return (tensor * cos) + (rotate_every_two(tensor) * sin)
-        def hf_forward(query, key, position_ids, embed_positions, rotary_dim=None):
-            embed_positions = _get_embed_positions(embed_positions, position_ids)
-            repeated_position_ids = position_ids.unsqueeze(-1).repeat(1, 1, embed_positions.shape[-1])
-            sincos = torch.gather(embed_positions, 1, repeated_position_ids)
-            sin, cos = torch.split(sincos, sincos.shape[-1] // 2, dim=-1)
-
-            if rotary_dim is not None:
-                k_rot = key[:, :, :, : rotary_dim]
-                k_pass = key[:, :, :, rotary_dim :]
-
-                q_rot = query[:, :, :, : rotary_dim]
-                q_pass = query[:, :, :, rotary_dim :]
-
-                k_rot = apply_rotary_pos_emb(k_rot, sin, cos)
-                q_rot = apply_rotary_pos_emb(q_rot, sin, cos)
-
-                key = torch.cat([k_rot, k_pass], dim=-1)
-                query = torch.cat([q_rot, q_pass], dim=-1)
-            else:
-                key = apply_rotary_pos_emb(key, sin, cos)
-                query = apply_rotary_pos_emb(query, sin, cos)
-            return query, key
-
-        for rotary_dim in [64, None]:
-            query = torch.rand(1, 32, 16, 256) #(batch, head, seq_length, head_features)
-            key = torch.rand(1, 32, 16, 256)
-            query_tpp = copy.deepcopy(query)
-            key_tpp = copy.deepcopy(key)
-            position_ids = torch.arange(32).unsqueeze(0)
-
-            pos_embd_dim = rotary_dim or 256
-            embed_positions = create_sinusoidal_positions(2048, pos_embd_dim)
-            query_hf, key_hf = hf_forward(query, key, position_ids, embed_positions, rotary_dim)
-            torch.ops.torch_ipex.rotary_position_embedding(
-                key_tpp,
-                embed_positions,
-                position_ids,
-                16,
-                256,
-            )
-            torch.ops.torch_ipex.rotary_position_embedding(
-                query_tpp,
-                embed_positions,
-                position_ids,
-                16,
-                256
-            )
-
-            self.assertEqual(query_hf, query_tpp)
-            self.assertEqual(key_hf, key_tpp)
 
 
 if __name__ == "__main__":
diff --git a/tests/cpu/test_weight_prepack.py b/tests/cpu/test_weight_prepack.py
index 120a04bc..7dc39a87 100644
--- a/tests/cpu/test_weight_prepack.py
+++ b/tests/cpu/test_weight_prepack.py
@@ -1961,7 +1961,7 @@ class TestPrepackCases(TestCase):
         batch_size = 2
         seq_len = 3
 
-        num_directions = 2
+        num_directions = 2 if bidirectional else 1
 
         input = torch.randn(batch_size, seq_len, input_size)
         h = torch.randn(num_layers * num_directions, batch_size, hidden_size)
diff --git a/third_party/ideep b/third_party/ideep
index 9f20cf62..b5eadff6 160000
--- a/third_party/ideep
+++ b/third_party/ideep
@@ -1 +1 @@
-Subproject commit 9f20cf6299df99864de75a8298a5a7219c4f02f5
+Subproject commit b5eadff6966e484a8d19e9e7658146bc84c7edcd-dirty
diff --git a/third_party/libxsmm b/third_party/libxsmm
index 63cd57d3..c21bc5dd 160000
--- a/third_party/libxsmm
+++ b/third_party/libxsmm
@@ -1 +1 @@
-Subproject commit 63cd57d36807ce029e55d9b1c99b51c9feddfe1e
+Subproject commit c21bc5ddb47bbb1a17c2382c0450ebcd10824430
